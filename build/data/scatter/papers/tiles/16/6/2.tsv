id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
2236a9117fceeece8629144a782cb7805e32a670	a mixed approach for chinese word segmentation			microsoft word for mac;text segmentation	Zhen Wang	2014		10.3115/v1/W14-6829	segmentation-based object categorization	NLP	-29.593980961086732	-77.65207495573543	52481
16655c1cecc2de8d16944fe010ced99cd0f01a0b	review: towards a relational-perspective approach to syntactic semantics, by jun qian.				Jarmila Panevová	2002	Prague Bull. Math. Linguistics		natural language processing;speech recognition;computer science;linguistics	NLP	-30.5980100242846	-77.75616451449582	52673
713cbcc99fa1e1fd6955031b6b19717d7a89c8b2	using semantic clustering for detecting bengali multiword expressions		Multiword Expressions (MWEs), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. The semantic of a MWE cannot be expressed after combining the semantic of its constituents. In this study, we propose a novel approach called “semantic clustering” as an instrument for extracting the MWEs especially for resource constraint languages like Bengali. At the beginning, it tries to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potential candidate using a vector space model. Finally the judgment for the suitability of this phrase to be a MWE is carried out based on a predefined threshold. In this experiment, we apply the semantic clustering approach only for noun-noun bigram MWEs; however we believe that it can be extended to any types of MWEs. We compare our approach with the state-ofthe-art statistical approach. The evaluation results show that the semantic clustering outperforms all other competing methods. As a byproduct of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali linguistic thesaurus.	algorithm;baseline (configuration management);bigram;boolean expression;dictionary;finite-state machine;lexicon;minimal working example;natural language processing;semantic interpretation;semantic similarity;semiconductor industry;thesaurus;wordnet	Tanmoy Chakraborty	2014	Informatica (Slovenia)		natural language processing;speech recognition;computer science	NLP	-26.035805599983917	-72.69266163704351	52795
1bbb602509e81569ae4b39bd7926c316b1848049	semi-automatic semantic tagging of 3d images from pancreas cells	semantic annotation;3d imaging;artificial neural networks;multimedia semantic annotation;multimedia data;semantic gap;collaborative research;scientific knowledge;artificial neural network	Detailed, consistent semantic annotation of large collections of multimedia data is difficult and time-consuming. In domains such as eScience, digital curation and industrial monitoring, fine-grained high-quality labeling of regions enables advanced semantic querying, analysis and aggregation and supports collaborative research. Manual annotation is inefficient and too subjective to be a viable solution. Automatic solutions are often highly domain or application specific, require large volumes of annotated training corpi and, if using a 'black box' approach, add little to the overall scientific knowledge. This article evaluates the use of simple artificial neural networks to semantically annotate micrographs and discusses the generic process chain necessary for semi-automatic semantic annotation of images.	semiconductor industry	Suzanne Little;Ovidio Salvetti;Petra Perner	2007		10.1007/978-3-540-76300-0_7	semantic computing;image retrieval;computer science;data mining;semantic web stack;semantic technology;world wide web;information retrieval	Vision	-33.16466649574971	-67.81852599059138	52828
7f48d56c1193bba6cc2826a7b09e4f7cfff03e4d	linguistic features for named entity recognition using crfs	pragmatics;context cities and towns training machine learning syntactics pragmatics engines;morphologically rich;training;crf;tamil;linguistic feature;named entity recognition;engines;machine learning;named entitiy recognition;syntactics;tamil linguistic feature named entity recognition crf conditional random field machine learning;crfs;feature;conditional random field;cities and towns;morphologically rich named entitiy recognition crfs feature;computational linguistics;learning artificial intelligence;natural language processing computational linguistics learning artificial intelligence;natural language processing;context;named entity	We present a work on identification and classification of Named Entities in Tamil, a morphologically rich language. Here we have used a machine learning technique, Conditional Random Fields (CRFs) for this task. We discuss here about the linguistic features used in CRFs for this morphologically rich language.	conditional random field;entity;machine learning;named-entity recognition	R. Vijay Sundar Ram;A. Akilandeswari;Sobha Lalitha Devi	2010	2010 International Conference on Asian Language Processing	10.1109/IALP.2010.41	natural language processing;speech recognition;feature;computer science;computational linguistics;machine learning;pattern recognition;linguistics;tamil;conditional random field;pragmatics	NLP	-23.30031655195799	-68.3783204151786	52855
7a606b0484ab7d8e53e766a493dfb278081a2d57	detection of language (model) errors	good recall;bayesian classifier;language model;neural network;best recall performance;best precision;asian language;bigram language model;language model error;language processing application	The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages. However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation. We apply pattern recognition techniques (i.e. Bayesian, decision tree and neural network classifiers) to discover language model errors. We have examined 2 general types of features: modelbased and language-specific features. In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%). Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%). The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%). Introduction Language models are important post-processing modules to improve recognition accuracy of a wide variety of input, namely speech recognition (Balh et al., 1983), handwritten recognition (Elliman and Lancaster, 1990) and printed character recognition (Sun, 1991), for many human languages. They can also be used for text correction (Ron et al., 1994) and part-of-speech tagging. For Indo-European languages, the word-bigram language model is used in speech recognition (Jelinek, 1989) and handwriting recognition (Nathan et al., 1995). Various ways to improve language models were reported. First, the model has been extended with longer dependencies (e.g. trigram) (Jelinek, 1991) and using non-contiguous dependencies, like trigger pairs (Rosenfeid, 1994) or long distance n-gram language models (Huang et al., 1993). For better probability estimation, the model was extended to work with (hidden) word classes (Brown et al., 1992, Ward and Issar, 1996). A more error-driven approach is the use of hybrid language models, in which some detection mechanism (e.g. perplexity measures [Keene and O'Kane, 1996] or topic detection [Mahajan et al., 1999]) selects or combines with a more appropriate language model. For Asian languages (e.g. Chinese, Japanese and Korean) represented by ideographic characters, language models are widely used in computer entry because these Asian languages have a large set of characters (in thousands) that the conventional keyboard is not designed for. Apart from using speech and handwriting recognition for computer entry, language models for Asian languages can be used for sentence-based keyboard input (e.g. Lochovsky and Chung, 1997), as well as detecting improper writing (e.g. dialectspecific words or expressions). Unlike Indo-European languages, words in these Asian languages are not delimited by space and conventional approximate string matching techniques (Wagner and Fisher, 1974; Oommen and Zhang, 1974) in handwriting recognition are seldom used in Asian language models. Instead, a widely used and reported Asian language model is the character-bigram language model (Jin et al., 1995; Xia et al., 1996) because it (1) achieved high recognition accuracy (around 90-96%) (2) is easy to estimate model parameters (3) can be processed quickly and (4) is relatively easy to implement. Improvement of these language models for IndoEuropean languages can be applied for the Asian languages but words need to be identified. For Asian languages, the model was integrated with syntactic rules (Chien, Chen and Lee, 1993). Class based language model (Lee and Tung, 1995) was also examined but the classes are based on semantically related words. A-new approach (Yang et al., 1998) is reported using segments	approximate string matching;approximation algorithm;artificial neural network;bayesian network;bigram;decision tree;delimiter;entity–relationship model;feature vector;handwriting recognition;indo;language model;n-gram;naive bayes classifier;optical character recognition;part-of-speech tagging;pattern recognition;performance;perplexity;plover;precision and recall;printing;quad flat no-leads package;sensor;speech recognition;string searching algorithm;trigram;video post-processing;wagner–fischer algorithm;word lists by frequency;yang	Kei Yuen Hung;Robert Wing Pong Luk;Daniel S. Yeung;Korris Fu-Lai Chung;Wenhao Shu	2000			natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-20.898111848041648	-79.43159065502499	52956
1a021d80116433600718ec621a78ef8ce4197010	part-of-speech tagging with evolutionary algorithms	linguistique;genetic operator;population size;mutation rate;probabilistic approach;algoritmo genetico;mutacion;probabilistic model;analyse syntaxique;linguistica;marcacion;analisis sintaxico;syntactic analysis;part of speech tagging;marquage;disambiguation;part of speech;algorithme genetique;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;evolutionary algorithm;desambiguisation;mutation;fitness function;tagging;linguistics	This paper presents a part-of-speech tagger based on a genetic algorithm which, after the “evolution” of a population of sequences of tags for the words in the text, selects the best individual as solution. The paper describes the main issues arising in the algorithm, such as the chromosome representation and the evaluation and design of genetic operators for crossover and mutation. A probabilistic model, based on the context of each word (the tags of the surrounding words) has been devised in order to define the fitness function. The model has been implemented and different issues have been investigated: size of the training corpus, effect of the context size, and parameters of the evolutionary algorithm, such as population size and crossover and mutation rates. The accuracy obtained with this method is comparable to that of other probabilistic approaches, but evolutionary algorithms are more efficient in obtaining the results.	brill tagger;crossover (genetic algorithm);evolutionary algorithm;fitness function;genetic algorithm;genetic operator;mutation (genetic algorithm);part-of-speech tagging;statistical model;text corpus	Lourdes Araujo	2002		10.1007/3-540-45715-1_21	mutation;natural language processing;statistical model;mutation rate;evolutionary music;population size;genetic algorithm;part of speech;computer science;artificial intelligence;genetic operator;machine learning;evolutionary algorithm;parsing;linguistics;fitness function;algorithm	AI	-25.39673786592034	-78.98288262386903	52986
4545d192183f19ea774a7c52479ce27261d8e3dc	fss-timex for tempeval-3: extracting temporal information from text		We describe FSS-TimEx, a module for the recognition and normalization of temporal expressions we submitted to Task A and B of the TempEval-3 challenge. FSS-TimEx was developed as part of a multilingual event extraction system, Nexus, which runs on top of the EMM news processing engine. It consists of finite-state rule cascades, using minimalistic text processing stages and simple heuristics to model the relations between events and temporal expressions. Although FSS-TimEx is already deployed within an IE application in the medical domain, we found it useful to customize its output to the TimeML standard in order to have an independent performance measure and guide further developments.	flying-spot scanner;heuristic (computer science);temporal expressions;timeml	Vanni Zavarella;Hristo Tanev	2013			normalization (statistics);timeml;data mining;information retrieval;text processing;nexus (standard);expression (mathematics);heuristics;computer science	NLP	-23.751747768291786	-70.10224135357967	52992
7a9ef8c7124ab52bcf652b7441a3c10e375b2aab	multi-level analysis and annotation of arabic corpora for text-to-sign language mt		The Arabic language is morphologically rich and syntactically complex with many differences from European languages, and this creates a challenge when porting existing annotation tools to Arabic. In this paper, we present an ongoing effort in lexical semantic analysis and annotation of Modern Standard Arabic (MSA) text, a semi automatic annotation tool concerned with the morphologic, syntactic, and semantic levels of description. Besides the aim of providing a multi-level annotation tool for Arabic corpora, our goals are (1) to investigate the suitability of Frame Semantics (FS) approach (Fillmore 1985) for representing and analysing Arabic text (2) to provide corpus-attested linguistics materials for frame-based contrastive text analysis between Arabic and English in terms of lexicalization patterns; (3) to automatically derive mappings rules from annotated sentences. Such corpus-attested mapping rules between linguistic form and its meaning can support semantic analysis in knowledge-based NLP systems such as machine translation, information extraction etc. Following syntactically-based annotation projects for English, serious attempts have been made to annotate Arabic corpora, such as the Penn Arabic Treebank (PATB), (Maamouri et al. 2004) and the Quranic Arabic Dependency Treebank (QADT) (Dukes et al. 2010). However, semantically-based annotation for Arabic corpora has not yet been garnering the same attention. Our semantic representations are based upon use of frame-semantic paradigm; it is actually used in a MT system from Arabic to Algerian Sign Language aimed to assist deaf children and in order to bridge the gap between Arabic written texts and Algerian Sign Language (Lakhfif and Laskri 2010a,b, 2011). Annotation outputs are available in XML format compatible with the FrameNet project (Fillmore and Petruck 2003) design and can be portable to other NLP systems.	association for computers and the humanities;book;frame problem;framenet;information extraction;machine translation;multi-storey car park;natural language processing;programming paradigm;semiconductor industry;text corpus;treebank;xml	Abdelaziz Lakhfif;Mohamed Tayeb Laskri;Eric Atwell	2015	CoRR		natural language processing;speech recognition;computer science;linguistics;temporal annotation	NLP	-30.3399715618481	-76.49430916986994	53143
33884d4e8374087f8329440a6405a7e2244e40a1	leveraging collective knowledge	knowledge management;classification;faceted search;smallest of;indexation;taxonomy;subject areas;library;knowledge modeling;categorization;tagging	As more organizations begin to deploy taxonomies for categorization and faceted search, the cost of producing these knowledge models is becoming the largest expense on a project. At a cost of 200 - 300 dollars per topic, manually developing subject area taxonomies does not scale for any but the smallest of projects. This paper will discuss an approach called Orthogonal Corpus Indexing ( OCI ). OCI leverages existing published knowledge in the subject area of the taxonomy model. This knowledge is algorithmically mapped into multiple taxonomies via the OCI algorithm. The resulting taxonomy costs are 1/ 100th of the cost of manual methods and are created with embedded rule sets for categorization engines. This paper will discuss the theory of OCI, its practical use as well as examples of knowledge management techniques that are possible when taxonomies are large, detailed and inexpensive.	algorithm;categorization;cluster analysis;display resolution;embedded system;extensibility;faceted classification;interconnection;knowledge management;knowledge representation and reasoning;oracle call interface;scalability;taxonomy (general)	Henry Kon;Michael Hoey	2005		10.1145/1099554.1099702	library;biological classification;computer science;knowledge management;artificial intelligence;data science;data mining;database;world wide web;information retrieval;taxonomy;categorization	ML	-32.21698376579644	-68.05599084310909	53253
7b9237d71ec780257ca7585f7b346979fd3f5622	on word frequency information and negative evidence in naive bayes text classification	busqueda informacion;bayes estimation;modelizacion;lenguaje natural;analisis contenido;word frequency;linguistique;information retrieval;naive bayes;langage naturel;model performance;tratamiento lenguaje;probabilistic approach;text classification;modelisation;probabilistic model;naive bayes classifier;estimacion bayes;content analysis;linguistica;language processing;recherche information;enfoque probabilista;approche probabiliste;natural language;traitement langage;version management;analyse contenu;modeling;gestion version;estimation bayes;linguistics	The Naive Bayes classifier exists in different versions. One version, called multi-variate Bernoulli or binary independence model, uses binary word occurrence vectors, while the multinomial model uses word frequency counts. Many publications cite this difference as the main reason for the superior performance of the multinomial Naive Bayes classifier. We argue that this is not true. We show that when all word frequency information is eliminated from the document vectors, the multinomial Naive Bayes model performs even better. Moreover, we argue that the main reason for the difference in performance is the way that negative evidence, i.e. evidence from words that do not occur in a document, is incorporated in the model. Therefore, this paper aims at a better understanding and a clarification of the difference between the two probabilistic models of Naive Bayes.	bernoulli polynomials;document classification;multinomial logistic regression;naive bayes classifier;word lists by frequency	Karl-Michael Schneider	2004		10.1007/978-3-540-30228-5_42	natural language processing;bayes' rule;naive bayes classifier;speech recognition;content analysis;binary independence model;computer science;pattern recognition	Web+IR	-25.007106017088997	-79.06381899579945	53608
4b8e2782c8950b25b82b66950c881b9d3a10c866	a protein-protein interaction extraction approach based on deep neural network	unsupervised learning;ppi extraction;neural networks;text mining;protein protein interactions;unlabelled data;deep learning;biomedical literature;bioinformatics	Protein-Protein Interactions PPIs information extraction from biomedical literature helps unveil the molecular mechanisms of biological processes. Machine learning methods have been the most popular ones in PPI extraction area. However, these methods are still feature engineering-based, which means that their performances are also heavily dependent on the appropriate feature selection which is still a skill-dependent task. This paper presents a deep neural network-based approach which can learn complex and abstract features automatically from unlabelled data by unsupervised representation learning methods. This approach first employs the training algorithm of auto-encoders to initialise the parameters of a deep multilayer neural network. Then the gradient descent method using back propagation is applied to train this deep multilayer neural network model. Experimental results on five public PPI corpora show that our method can achieve better performance than can a multilayer neural network: on two 'toughest handling' corpora AImed and BioInfer, the former outperforms the latter with the improvements of 3.10 and 2.89 percentage units in F-score, respectively. In addition, the performance comparison with APG also verifies the effectiveness of our method.		Zhehuan Zhao;Zhihao Yang;Hongfei Lin;Jian Wang;Song Gao	2016	IJDMB	10.1504/IJDMB.2016.076534	protein–protein interaction;unsupervised learning;text mining;computer science;bioinformatics;machine learning;pattern recognition;data mining;time delay neural network;deep learning;deep belief network;artificial neural network	NLP	-19.241991921282718	-70.46656700894246	53643
0be76ad975940bc6fdf717b990f42c072df8f800	automatic detection of gender and number agreement errors in spanish texts written by japanese learners	conference paper	This paper describes the creation of a grammar to automatically detect agreement errors (gender and number) in Spanish texts written by Japanese learners. The grammar has been written using the Constraint Grammar formalism (Karlsson et al., 1995), and uses as input the morphosyntactic analysis provided by the Spanish parser HISPAL (Bick, 2006). For developing and testing the grammar, a learner corpus of 25,000 words has been manually annotated with agreement error tags. Both the grammar and the data from the corpus serve us to draw some conclusions about the characteristics of agreement errors in Japanese learners’ Spanish.	categorial grammar;constraint grammar;semantics (computer science)	Maria del Pilar Valverde Ibañez;Akira Ohtani	2012			natural language processing;speech recognition;computer science;linguistics;head-driven phrase structure grammar	NLP	-27.81208876005887	-75.97478990260944	53718
3b72d04b5a777385c5d59c3c93abb6cb7d4cd886	web-based relation extraction for the food domain	different part;food domain;relation type;different extraction method;web-based relation extraction;linguistic processing;different domain-specific relation;low level;particular method;large amount;single method	In this paper, we examine methods to extract different domainspecific relations from the food domain. We employ different extraction methods ranging from surface patterns to co-occurrence measures applied on different parts of a document. We show that the effectiveness of a particular method depends very much on the relation type considered and that there is no single method that works equally well for every relation type. As we need to process a large amount of unlabeled data our methods only require a low level of linguistic processing. This has also the advantage that these methods can provide responses in real time.	emergent;real-time computing;relationship extraction	Michael Wiegand;Benjamin Roth;Dietrich Klakow	2012		10.1007/978-3-642-31178-9_25	web application;data mining;deep linguistic processing;computer science;relationship extraction;mean reciprocal rank;ranging	NLP	-28.859236603192745	-66.57047495000916	53778
2d385f57aafaf3e8b43def7fc7ebbc24c447598a	muevo, a breast cancer consumer health vocabulary built out of web forums	biomedical ontologies;bioportal;consumer health vocabulary;semantic web;standard terminologies;web forums;breast cancer;skos	Semantically analyze patient-generated text from a biomedical perspective is challenging because of the vocabulary gap between patients and health professionals. The medical expertise and vocabulary is well formalized in standards terminologies and ontologies, which enable semantic analysis of expertgenerated text; however resources which formalize the vocabulary of health consumers (patients and their family, laypersons in general) remain scarce. The situation is even worse if one is interested in another language than English. In previous studies, we attempted to produce a French preliminary Consumer Health Vocabulary (CHV) by mining the language used within online public forums & Facebook groups about breast cancer. In this work, we show our effort to concretely align the vocabulary produced to standard terminologies and to represent its content (terms & mappings) using semantic web languages such as RDF, SKOS and PROV. We used a sample of 173 relations built around 64 expert concepts which have been automatically (89%) or manually (11%) aligned to standard biomedical terminologies, in our case: MeSH, MedDRA and SNOMEDint. The resulting vocabulary, called MuEVo (Multi-Expertise Vocabulary) and the mappings are publicly available in the SIFR BioPortal French biomedical ontology repository.	align (company);controlled vocabulary;meddra;multidimensional digital pre-distortion;ontology (information science);resource description framework;scalable inman flash replacement;semantic web;simple knowledge organization system	Solène Eholié;Mike Donald Tapi-Nzali;Sandra Bringay;Clement Jonquet	2016			open biomedical ontologies;computer science;artificial intelligence;simple knowledge organization system;breast cancer;semantic web;data mining;database;multimedia;world wide web;information retrieval	Web+IR	-33.60310464253714	-70.6916376386859	53815
a566a9b1dcb8df0e099ccdead70bccce8e15d73c	nist 2013 open handwriting recognition and translation (open hart'13) evaluation	nist;handwriting recognition;measurement;training;benchmark test;arabic text recognition;benchmark test arabic text recognition ocr machine translation of ocr text evaluation performance assessment;ocr;evaluation;training nist measurement handwriting recognition text recognition conferences educational institutions;text recognition;machine translation of ocr text;conferences;performance assessment	This paper describes the NIST 2013 Open Handwriting Recognition and Translation evaluation (OpenHaRT'13). A short background leading to the start of OpenHaRT is included. The test designs pertaining to the tasks, the data used, the performance measurements, and the protocols are presented. The participants and their submissions are mentioned followed by the evaluation results and some preliminary analyses. The paper concludes with some thoughts toward future evaluations.	handwriting recognition	Audrey Tong;Mark A. Przybocki;Volker Märgner;Haikal El Abed	2014	2014 11th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2014.43	natural language processing;speech recognition;nist;computer science;evaluation;handwriting recognition;optical character recognition;world wide web;measurement	Vision	-31.36449872895752	-72.7064135769863	53885
06e43829f9862f62ea38192a06e0f010737a07df	"""""""deep"""" learning for missing value imputationin tables with non-numerical data"""		The success of applications that process data critically depends on the quality of the ingested data. Completeness of a data source is essential in many cases. Yet, most missing value imputation approaches suffer from severe limitations. They are almost exclusively restricted to numerical data, and they either offer only simple imputation methods or are difficult to scale and maintain in production. Here we present a robust and scalable approach to imputation that extends to tables with non-numerical values, including unstructured text data in diverse languages. Experiments on public data sets as well as data sets sampled from a large product catalog in different languages (English and Japanese) demonstrate that the proposed approach is both scalable and yields more accurate imputations than previous approaches. Training on data sets with several million rows is a matter of minutes on a single machine. With a median imputation F1 score of 0.93 across a broad selection of data sets our approach achieves on average a 23-fold improvement compared to mode imputation. While our system allows users to apply state-of-the-art deep learning models if needed, we find that often simple linear n-gram models perform on par with deep learning methods at a much lower operational cost. The proposed method learns all parameters of the entire imputation pipeline automatically in an end-to-end fashion, rendering it attractive as a generic plugin both for engineers in charge of data pipelines where data completeness is relevant, as well as for practitioners without expertise in machine learning who need to impute missing values in tables with non-numerical data.	application programming interface;artificial neural network;column (database);computer multitasking;deep learning;end-to-end principle;experiment;f1 score;feature extraction;geo-imputation;graphics processing unit;heuristic (computer science);information privacy;information retrieval;language-independent specification;level of measurement;long short-term memory;loss function;machine learning;mathematical optimization;missing data;n-gram;nearest neighbor search;numerical analysis;open-source software;pipeline (computing);recommender system;scalability;sparse matrix;text corpus;wikipedia	Felix Bießmann;David Salinas;Sebastian Schelter;Philipp Schmidt;Dustin Lange	2018		10.1145/3269206.3272005	rendering (computer graphics);data mining;completeness (statistics);imputation (statistics);deep learning;row;scalability;missing data;computer science;artificial intelligence;data set	ML	-20.72515224920679	-75.19738888153171	53970
d4dda3153a59f9e54d11e8d8321402b0bbaecd55	new resources and perspectives for biomedical event extraction	recent bionlp shared task;new opportunity;new resource;new perspective;state-of-the-art event extraction system;reliable automatic extraction;recent work;domain event extraction system;biomedical information extraction;new biomedical event extraction;event extraction	Event extraction is a major focus of recent work in biomedical information extraction. Despite substantial advances, many challenges still remain for reliable automatic extraction of events from text. We introduce a new biomedical event extraction resource consisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the first time the outputs of a broad set of state-ofthe-art event extraction systems, this resource opens many new opportunities for studying aspects of event extraction, from the identification of common errors to the study of effective approaches to combining the strengths of systems. We demonstrate these opportunities through a multi-system analysis on three BioNLP ST 2011 main tasks, focusing on events that none of the systems can successfully extract. We further argue for new perspectives to the performance evaluation of domain event extraction systems, considering a document-level, “off-the-page” representation and evaluation to complement the mentionlevel evaluations pursued in most recent work.	biomedical text mining;information extraction;performance evaluation;system analysis	Sampo Pyysalo;Pontus Stenetorp;Tomoko Ohta;Jin-Dong Kim;Sophia Ananiadou	2012			computer science;data science;data mining;management science	NLP	-33.25440141745238	-66.86647552760837	53985
a6604edeb7e2ba1f430756649a57a8bee9290bf3	dmcb at semeval-2018 task 1: transfer learning of sentiment classification using group lstm for emotion intensity prediction.		This paper describes a system attended in the SemEval-2018 Task 1 “Affect in tweets” that predicts emotional intensities. We use Group LSTM with an attention model and transfer learning with sentiment classification data as a source data (SemEval 2017 Task 4a). A transfer model structure consists of a source domain and a target domain. Additionally, we try a new dropout that is applied to LSTMs in the Group LSTM. Our system ranked 8th at the subtask 1a (emotion intensity regression). We also show various results with different architectures in the source, target and transfer models.	dropout (neural networks);long short-term memory;parsing;power glove;sst (menter’s shear stress transport);semeval;source data	Youngmin Kim;Hyunju Lee	2018		10.18653/v1/s18-1044	transfer of learning;natural language processing;semeval;computer science;machine learning;artificial intelligence	NLP	-20.910647373926828	-70.14561145719581	54212
2d73adc3f58215be9c20dbc9806e74e3588e16cd	coordination boundary identification with similarity and replaceability		We propose a neural network model for coordination boundary detection. Our method relies on two common properties — similarity and replaceability in conjuncts — in order to detect both similar and dissimilar pairs of conjuncts. The model improves the identification of clause-level coordination using bidirectional recurrent neural networks incorporating two properties as features. We show that our model outperforms existing stateof-the-art methods for the coordination annotated Penn Treebank and Genia corpus without any syntactic information from parsers.	artificial neural network;bidirectional recurrent neural networks;computer performance;consistency model;feature vector;network model;parsing;recurrent neural network;treebank	Hiroki Teranishi;Hiroyuki Shindo;Yuji Matsumoto	2017			artificial intelligence;natural language processing;computer science	NLP	-23.340295193765932	-74.40387027016595	54306
46bc88ae82dd9e452016546ed3084b445b13b599	improvements in parsing the index thomisticus treebank. revision, combination and a feature model for medieval latin	feature modeling;latino;settore l lin 01 glottologia e linguistica;sintassi;parsing;indexation;linguistica computazionale;l lin 01 glottologia e linguistica	The creation of language resources for less-resourced languages like the historical ones benefits from the exploitation of language-independent tools and methods developed over the years by many projects for modern languages. Along these lines, a number of treebanks for historical languages started recently to arise, including treebanks for Latin. Among the Latin treebanks, the Index Thomisticus Treebank is a 68,000 token dependency treebank based on the Index Thomisticus by Roberto Busa SJ, which contains the opera omnia of Thomas Aquinas (118 texts) as well as 61 texts by other authors related to Thomas, for a total of approximately 11 million tokens. In this paper, we describe a number of modifications that we applied to the dependency parser DeSR, in order to improve the parsing accuracy rates on the Index Thomisticus Treebank. First, we adapted the parser to the specific processing of Medieval Latin, defining an ad-hoc configuration of its features. Then, in order to improve the accuracy rates provided by DeSR, we applied a revision parsing method and we combined the outputs produced by different algorithms. This allowed us to improve accuracy rates substantially, reaching results that are well beyond the state of the art of parsing for Latin.	algorithm;belief revision;feature model;hoc (programming language);language-independent specification;parsing;roberto busa;treebank;turris omnia	Marco Passarotti;Felice Dell'Orletta	2010			natural language processing;speech recognition;computer science;parsing;linguistics	NLP	-28.527518345654098	-75.74257392596795	54332
14da3e857afacc4e8ce701cc69cc754f65f68dfa	cross-lingual named entity recognition via wikification		Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages). We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in nonEnglish text into English Wikipedia entries. Thus, mentions in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong language-independent features. With this insight, we propose an NER model that can be applied to all languages in Wikipedia. When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on lowresource languages (e.g., Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. Moreover, our method allows us to train on multiple source languages, typically improving NER results on the target languages. Finally, we show that our languageindependent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages.	baseline (configuration management);category theory;document;freebase;language-independent specification;ner model;named entity;named-entity recognition;parallel projection;wikipedia	Chen-Tse Tsai;Stephen D. Mayhew;Dan Roth	2016			natural language processing;computer science;artificial intelligence;entity linking;named-entity recognition	NLP	-24.025295418810384	-74.6308984318947	54378
040ef663ad50b898d909aae884f9461e0caec179	a hybrid methods of aligning arabic qur’anic semantic resources		Ontology alignment is a necessary step for enabling interoperability between ontology entities and for avoiding redundancy and variation that may occur when integrating them. The automation of bilingual ontology alignment is challenging due to the variation an entity can be expressed in, in different ontologies and languages. The goal of this paper is to compare various ontology alignment methods for matching ontological bilingual Qur’anic resources and to go beyond them, which is achieved via a new hybrid alignment method. The new method consists of aggregating multiple similarity measures for a given pair of concepts into a single value, taking advantage of combining fuzzy bilingual lexical and structure-based methods for improving the performance of automatic ontology alignment.		Silvia M R Cadena;Mohammad M. Alqahtani;Eric Atwell	2018	2018 IEEE 2nd International Workshop on Arabic and Derived Script Analysis and Recognition (ASAR)	10.1109/ASAR.2018.8480309	redundancy (engineering);ontology alignment;fuzzy logic;arabic;automation;natural language processing;ontology;interoperability;ontology (information science);artificial intelligence;computer science	NLP	-31.176823745367287	-68.21650449204066	54379
8d466cb806c51f5cb133486eca237f7af9c53be6	a mixed model for cross lingual opinion analysis	会议论文;transfer self training;mixed model;cross lingual opinion analysis;co training	The performances of machine learning based opinion analysis systems are always puzzled by the insufficient training opinion corpus. Such problem becomes more serious for the resource-poor languages. Thus, the cross-lingual opinion analysis (CLOA) technique, which leverages opinion resources on one (source) language to another (target) language for improving the opinion analysis on target language, attracts more research interests. Currently, the transfer learning based CLOA approach sometimes falls to over fitting on single language resource, while the performance of the co-training based CLOA approach always achieves limited improvement during bi-lingual decision. Target to these problems, in this study, we propose a mixed CLOA model, which estimates the confidence of each monolingual opinion analysis system by using their training errors through bilingual transfer self-training and co-training, respectively. By using the weighted average distances between samples and classification hyper-planes as the confidence, the opinion polarity of testing samples are classified. The evaluations on NLP&CC 2013 CLOA bakeoff dataset show that this approach achieves the best performance, which outperforms transfer learning and co-training based approaches.	co-training;compiler;experiment;machine learning;mixed model;overfitting;performance	Lin Gui;Ruifeng Xu;Jun Xu;Li Yuan;Yuanlin Yao;Jiyun Zhou;Qiaoyun Qiu;Shuwei Wang;Kam-Fai Wong;Ricky Cheung	2013		10.1007/978-3-642-41644-6_10	natural language processing;speech recognition;computer science;data mining	NLP	-21.204859890292255	-70.33447806031388	54511
346549b3ea0dab07a26c9354e1855b74f2f18609	united we stand: improving sentiment analysis by joining machine learning and rule based methods	rule based;machine learning;sentiment analysis	In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations. We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system. These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches. Experimental results using the corpus of the Affective Text Task of SemEval ’07, provide evidence in favor of this direction.	experiment;logic programming;machine learning;rule-based system;semeval;sentiment analysis;text corpus	Vassiliki Rentoumi;Stefanos Petrakis;Manfred Klenner;George A. Vouros;Vangelis Karkaletsis	2010			sentiment analysis;rule-based system;natural language processing;semeval;affect (psychology);literal and figurative language;machine learning;computer science;artificial intelligence	AI	-21.676920113220273	-70.80977410832536	54628
6244bbedd8e1e49554044035d7c4d2b5e0c82410	evaluation of commonsense knowledge with mechanical turk	mechanical turk;world knowledge;usable judgement;commonsense knowledge;easy mean;large-scale evaluation;resulting knowledge;knowledge acquisition;initial experiment;associated quality judgement	Efforts to automatically acquire world knowledge from text suffer from the lack of an easy means of evaluating the resulting knowledge. We describe initial experiments using Mechanical Turk to crowdsource evaluation to nonexperts for little cost, resulting in a collection of factoids with associated quality judgements. We describe the method of acquiring usable judgements from the public and the impact of such large-scale evaluation on the task of knowledge acquisition.	amazon mechanical turk;commonsense knowledge (artificial intelligence);crowdsourcing;experiment;ibm notes;knowledge acquisition;knowledge base;natural language;open knowledge;the turk	Jonathan Gordon;Benjamin Van Durme;Lenhart K. Schubert	2010			computer science;knowledge management;data mining	NLP	-30.384191022959026	-72.77224453508823	54634
7efb9429743b67bca2a82955871e00720ea92782	on the application of lexical-syntactic knowledge to the answer validation exercise	answer validation exercise	This paper presents a system that applies Textual Entailment recognition techniques to the AVE task. This is performed comparing representations of text snippets by means of a variety of lexical measures and syntactic structures. The representations of the question and the answer are compared determining if there is an entailment relation between them. The performed experiments over the English test corpus obtained a maximum F-score of 0.39.	experiment;f1 score;textual entailment	Óscar Ferrández;Daniel Micol;Rafael Muñoz;Manuel Palomar	2007		10.1007/978-3-540-85760-0_48	natural language processing;computer science;data mining;algorithm	NLP	-27.02710859352261	-71.52303718572666	54665
322cd42d7d3e6660bb9e5bfdf88178954e0a1665	mt evaluation: human-like vs. human acceptable	comparative study;qarla framework;automatic evaluation;human acceptability;human likeness;additional syntax-based metrics;current metrics;mt evaluation;descriptive power;current evaluation metrics;machine translation evaluation;empirical evidence	We presenta comparati ve study on MachineTranslationEvaluationaccordingto two different criteria: Human Likeness and Human Acceptability. We provide empiricalevidencethatthereis a relationship betweenthesetwo kinds of evaluation: Human Likenessimplies Human Acceptability but the reverseis not true. Fromthepoint of view of automaticevaluation this implies that metricsbasedon HumanLikenessaremorereliablefor systemtuning. Our resultsalsoshow thatcurrentevaluation metricsarenot alwaysableto distinguishbetweenautomaticandhumantranslations. In order to improve the descriptive power of currentmetricswe propose the use of additional syntax-basedmetrics, and metric combinationsinside the QARLA Framework.		Enrique Amigó;Jesús Giménez;Julio Gonzalo;Lluís Màrquez i Villodre	2006			empirical evidence;artificial intelligence;comparative research	NLP	-27.453647523576443	-72.85881085190779	54771
4b6392dc8b095cab04938ccd3a3249200a7bd258	syntax-based statistical machine translation using tree automata and tree transducers	statistical machine translation;tree automata	In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. I propose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. I also present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.	automata theory;language model;lattice model (finance);parallel text;statistical machine translation;transducer;tree automaton	Daniel Emilio Beck	2011			natural language processing;abstract syntax;transfer-based machine translation;example-based machine translation;computer science;linguistics;rule-based machine translation;programming language;algorithm;abstract syntax tree	NLP	-22.1479883266266	-77.96791918647696	54881
263e8a5a05826efa5c3ac05675ae3dac4ecf1d4b	similarity measure for polish short texts based on wordnet-enhanced bag-of-words representation		We present a method for computing semantic similarity of Polish texts with main focus given to short texts. We have taken into account the limited set of language tools for Polish, and especially that syntactic and semantic parsers do not express accuracy and robustness high enough and to become a stable basis for similarity computation. A very large wordnet of Polish, namely plWordNet is used to construct meaning representations for words in such a way that different words of the similar meaning receive similar representations. The use of a Word Sense Disambiguation (WSD) tool for Polish brought positive results in one of the method variants, regardless of the limited accuracy of the WSD tool. The proposed measures have been compared with the manual evaluation of sentence pairs. The measures were also applied as a part of the Question Answering system. Improved performance of answer finding was achieved in several types of tests.	bag-of-words model;similarity measure;wordnet	Maciej Piasecki;Alexander Gut	2015		10.1007/978-3-319-93782-3_13	natural language processing;semantic similarity;similarity measure;bag-of-words model;question answering;syntax;parsing;wordnet;sentence;artificial intelligence;computer science	Vision	-27.472992174403362	-70.71601961635783	55165
1e52782b39fb97f5eaadfcb23bf03be9810c4115	dependency parsing by belief propagation	exact first-order method;global constraint;parsing algorithm;effective tool;loopy belief propagation;latent variable;exact parsing;graphical model;approximate learning;additional feature;second order;dependency parsing;first order;belief propagation	We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively.	approximation algorithm;belief propagation;casio loopy;first-order predicate;graphical model;iteration;language model;machine translation;matching (graph theory);n-gram;parse tree;parsing;priority queue;propagator;pstree;serialization;software propagation;terminal and nonterminal symbols	David A. Smith;Jason Eisner	2008			natural language processing;parser combinator;computer science;machine learning;pattern recognition;belief propagation	NLP	-20.29320085577298	-76.81944273514547	55257
ac97745ff3902cffc3ec3a7008395b0516d19ec3	consistent word segmentation, part-of-speech tagging and dependency labelling annotation for chinese language		In this paper, we propose a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments.	experiment;galaxy morphological classification;machine translation;part-of-speech tagging;sparse matrix;text segmentation;treebank	Mo Shen;Wingmui Li;HyunJeong Choe;Chenhui Chu;Daisuke Kawahara;Sadao Kurohashi	2016			artificial intelligence;labelling;natural language processing;pattern recognition;computer science;text segmentation;annotation;part-of-speech tagging	NLP	-21.672201728106412	-75.03595448740475	55444
efe6e910516a1a40472a8c696d2ef162176e3ca2	language identification and analysis of code-switched social media text		In this paper, we detail our work on comparing different word-level language identification systems for codeswitched Hindi-English data and a standard Spanish-English dataset. In this regard, we build a new code-switched dataset for Hindi-English. To understand the code-switching patterns in these language pairs, we investigate different codeswitching metrics. We find that the CRF model outperforms the neural network based models by a margin of 2-5 percentage points for Spanish-English and 3-5 percentage points for Hindi-English.	artificial neural network;assembly language;biological anthropology;conditional random field;deep learning;feature engineering;language identification;social media;word embedding	Deepthi Mave;Suraj Maharjan;Thamar Solorio	2018			artificial intelligence;computer science;language identification;natural language processing;social media	NLP	-20.13129998300032	-71.05487329237754	55546
85db2d575c43e154a4f29a3062ed410b202043b5	machine learning approaches for catchphrase extraction in legal documents		The purpose of this research was to automatically extract catchphrases given a set of Legal documents. For this task, our focus was mainly on the Machine learning approaches: a comparative approach was used between the unsupervised and supervised approaches. The idea was to compare the different approaches to see which one of the two was comparatively better for automatic catchphrase extraction given a dataset of Legal documents. To perform this, two open source text mining software were used; one for the unsupervised approach while another one was used for the supervised approach. We then fine tuned some parameters for each tool before extracting catchphrases. The training dataset was used when fine tuning parameters in order to find optimal parameters that were then used for generating the final catchphrases. Different metrics were used to evaluate the results. We used the most common measures in Information Extraction which include Precision and Recall and the results from the two Machine learning approaches were compared. In general our results showed that the supervised approach performed far much better than the unsupervised approach.	information extraction;machine learning;open-source software;precision and recall;rake;supervised learning;text mining;unsupervised learning	Tshepho Koboyatshwene;Moemedi Lefoane;Lakshmi Narasimhan	2017			natural language processing;artificial intelligence;computer science	NLP	-24.1841296042886	-69.0639958377446	55814
5052af1ae54ad1400e770eeecea7471be844d596	using word embedding and ensemble learning for highly imbalanced data sentiment analysis in short arabic text		Abstract: Sentiment analysis has gained increasing importance with the massive increase of online content. Although several studies have been conducted for western languages, not much has been done for the Arabic language. The purpose of this study is to compare the performance of different classifiers for polarity determination in highly imbalanced short text datasets using features learned by word embedding rather than hand-crafted features. Several base classifiers and ensembles have been investigated with and without SMOTE (Synthetic Minority Over-sampling Technique). Using a dataset of tweets in dialectical Arabic, the results show that applying word embedding with ensemble and SMOTE can achieve more than 15% improvement on average in F 1 score over the baseline, which is a weighted average of precision and recall and is considered a better performance measure than accuracy for imbalanced datasets.	ensemble learning;sentiment analysis;word embedding	Sadam Al-Azani;El-Sayed M. El-Alfy	2017		10.1016/j.procs.2017.05.365	sentiment analysis;word embedding;precision and recall;data mining;arabic;machine learning;computer science;ensemble learning;weighted arithmetic mean;artificial intelligence;pattern recognition	NLP	-21.07081119415397	-69.51873945356141	55954
fee6c3d72455584d4e50a55e7a6adb5e6b604667	improving multilingual semantic textual similarity with shared sentence encoder for low-resource languages		Measuring the semantic similarity between two sentences (or Semantic Textual Similarity STS) is fundamental in many NLP applications. Despite the remarkable results in supervised settings with adequate labeling, little attention has been paid to this task in low-resource languages with insufficient labeling. Existing approaches mostly leverage machine translation techniques to translate sentences into rich-resource language. These approaches either beget language biases, or be impractical in industrial applications where spoken language scenario is more often and rigorous efficiency is required. In this work, we propose a multilingual framework to tackle the STS task in a low-resource language e.g. Spanish, Arabic , Indonesian and Thai, by utilizing the rich annotation data in a rich resource language, e.g. English. Our approach is extended from a basic monolingual STS framework to a shared multilingual encoder pretrained with translation task to incorporate rich-resource language data. By exploiting the nature of a shared multilingual encoder, one sentence can have multiple representations for different target translation language, which are used in an ensemble model to improve similarity evaluation. We demonstrate the superiority of our method over other state of the art approaches on SemEval STS task by its significant improvement on non-MT method, as well as an online industrial product where MT method fails to beat baseline while our approach still has consistently improvements.	baseline (configuration management);encoder;machine translation;natural language processing;semeval;semantic similarity	Xin Tang;Riya Danait;Loc Do;Zhiyu Min;Feng Ji;Heng Yu;Ji Zhang;Haiqin Chen	2018	CoRR		encoder;semantic similarity;natural language processing;machine translation;semeval;artificial intelligence;indonesian;spoken language;computer science;annotation;sentence	NLP	-19.8185028890799	-74.63884104323802	55967
9c6e68a6d704d0d9518a807584a469f72a4c66c9	word formation in natural language processing systems	systematic information;word formation rule;idiosyncratic lexical information;active study;word structure;lexical subsystems;large number;possible word;natural language processing system;word formation;known word;natural language;natural language processing	"""Systems which process na tu ra l language requ i re a r e l i a b l e source of i n fo rmat ion about words. Not only must t h e i r l e x i c a l subsystems handle a large known words; they must a lso cope w i t h The morphological p r i n c i p l e s under l y ing """"poss ib le word"""" are under ac t i ve study by l i n g u i s t s , and are a r t i c u l a t e d in the theory of word fo rmat ion . This paper presents a technique fo r b u i l d i n g l e x i c a l subsystems which embody these p r i n ­ c ip les by emulat ing the behavior of word format ion r u l e s . These subsystems combine t o t a l l y i d i o s y n ­ c r a t i c l e x i c a l i n f o r m a t i o n , s tored in a d i c t i o n a r y , w i t h systematic in fo rmat ion der ived from word s t r u c ­ t u r e . App l i ca t i ons fo r l e x i c a l subsystems b u i l t along the l ines descr ibed here w i l l be d iscussed."""	fo (complexity);natural language processing;numerical aperture	Roy J. Byrd	1983			natural language processing;lexical item;computer science;lexical choice	AI	-30.294907659093177	-79.4451639651155	55974
b7b6e7b034a1ee0d03345e9b71f56fd476d064c6	ontology of human relation extraction based on dependency syntax rules		This paper proposed a novel scheme for extracting character relation from unstructured text based on dependency grammar rules. First of all, we took the Three Kingdoms characters as our research object, then selected articles containing target relationships and thus constructed a corpus consisting of 1000 sentences. Secondly, We analyzed the corpus and developed a set of dependent grammar rules for relation extraction. Finally, we proposed a system, which makes it possible for computers to automatically extract and identify character relationships.	computer;dependency grammar;heart rate variability;meaning–text theory;relationship extraction;text corpus;text-based (computing);web ontology language	Long He;Likun Qiu	2017		10.1145/3106426.3109050	lexical grammar;link grammar;syntax;relationship extraction;ontology;natural language processing;phrase structure rules;dependency grammar;grammar;artificial intelligence;computer science	NLP	-26.299329666601	-70.50490854200633	56110
c18839709e0ea512b189314a888922c41d9d4bce	chinese textual entailment recognition enhanced with word embedding	lexical entailment;chinese textual entailment;word embedding;期刊论文;rite	Textual entailment has been proposed as a unifying generic framework for modeling language variability and semantic inference in different Natural Language Processing (NLP) tasks. By evaluating on NTCIR-11 RITE3 Simplified Chinese subtask data set, this paper firstly demonstrates and compares the performance of Chinese textual entailment recognition models that combine different lexical, syntactic, and semantic features. Then a word embedding based lexical entailment module is added to enhance classification ability of our system further. The experimental results show that the word embedding for lexical semantic relation reasoning is effective and efficient in Chinese textual entailment.	modeling language;natural language processing;ontology components;spatial variability;textual entailment;word embedding	Zhichang Zhang;Dongren Yao;Yali Pang;Xiaoyong Lu	2015		10.1007/978-3-319-25816-4_8	natural language processing;textual entailment;linguistics	NLP	-23.856184054031637	-72.92970810878664	56131
a055360a31c3692ffcf05de2c0903b7439b3af1a	relation extraction for the food domain without labeled training data - is distant supervision the best solution?		We examine the task of relation extraction in the food domain by employing distant supervision. We focus on the extraction of two relations that are not only relevant to product recommendation in the food domain, but that also have significance in other domains, such as the fashion or electronics domain. In order to select suitable training data, we investigate various degrees of freedom. We consider three processing levels being argument level, sentence level and feature level. As external resources, we employ manually created surface patterns and semantic types on all these levels. We also explore in how far rule-based methods employing the same information are competitive.	argument map;association rule learning;logic programming;relationship extraction;stephanie forrest;supervised learning	Melanie Reiplinger;Michael Wiegand;Dietrich Klakow	2014		10.1007/978-3-319-10888-9_35	computer science;artificial intelligence;data mining;communication	NLP	-24.73112521784356	-71.44201453304521	56252
e03fcce70be260418f6b3d4ea2b7cd7bf79256ce	can projected chains in parallel corpora help coreference resolution	parallel corpus;machine learning;coreference resolution	The majority of current coreference resolution systems rely on annotated corpora to train classifiers for this task. However, this is possible only for languages for which annotated corpora are available. This paper presents a system that automatically extracts coreference chains from texts in Portuguese without the need for Portuguese corpora manually annotated with coreferential information. To achieve this, an English coreference resolver is run on the English part of an English-Portuguese parallel corpus. The coreference pairs identified by the resolver are projected to the Portuguese part of the corpus using automatic word alignment. These projected pairs are then used to train the coreference resolver for Portuguese. Evaluation of the system reveals that it does not outperform a head match baseline. This is due to the fact that most of the projected pairs have the same head, which is learnt by the Portuguese classifier. This suggests that a more accurate English coreference resolver is necessary. A better projection algorithm is also likely to improve the performance of the system.	algorithm;baseline (configuration management);bitext word alignment;parallel text;text corpus	José Guilherme Camargo de Souza;Constantin Orasan	2011		10.1007/978-3-642-25917-3_6	natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-23.645250696143275	-72.56627767192104	56287
8f617510cfe7f8ba24b238f43b135aca60c1b7b1	relevance weighting for combining multi-domain data for n-gram language modeling	broadcast news;word error rate;wall street journal;traitement automatique de la parole;language modelling;linguistique appliquee;n gram model;variation stylistique;probabilistic model;variation du contenu;automatic speech processing;multi domain;modelisation du langage;reconnaissance de la parole;part of speech;modele probabiliste;sparse data;computational linguistics;statistical language model;linguistique informatique;domain specificity;language model;applied linguistics	Standard statistical language modeling techniques suffer from sparse-data problems in tasks where large amounts of domain-specific text are not available. In this paper, we focus on improving the estimation of domain-dependent n-gram models by the selective use of out-of-domain text data. Previous approaches for estimating language models from multi-domain data have not accounted for the characteristic variations of style and content across domains. In contrast, this work aims at differentially weighting subsets of the out-of-domain data according to style and/or content similarity to the given task, where “style” is represented by part-of-speech statistics and “content” by the particular choice of vocabulary items. In addition to ngram estimation, the differential weights can be used for lexicon design. Recognition experiments are based on the Switchboard corpus of spontaneous conversations, with out-of-domain text drawn from the Wall Street Journal and Broadcast News corpora. The similarity weighting approach gives a 3–5% reduction in word error rate over a domain-specific n-gram language model, providing some of the largest language modeling gains reported for the Switchboard task in recent years. c © 1999 Academic Press	experiment;language model;lexicon;n-gram;relevance;sparse matrix;speech corpus;spontaneous order;telephone switchboard;text corpus;the wall street journal;vocabulary;word error rate	Rukmini Iyer;Mari Ostendorf	1999	Computer Speech & Language	10.1006/csla.1999.0124	natural language processing;statistical model;speech recognition;sparse matrix;part of speech;word error rate;computer science;computational linguistics;applied linguistics;linguistics;language model	NLP	-21.573268943327598	-79.84265974176807	56320
d4a4f966a249a4f838f8151bca69a272ba2e0134	a sentiment analysis system for indian language tweets	indian languages;multinomial naive bayes;bengali;tweets;hindi;sentiment	This paper reports about our work in the MIKE 2015, Shared Task on Sentiment Analysis in Indian Languages SAIL Tweets. We submitted runs for Hindi and Bengali. A multinomial Naive Bayes based model has been used to implement our system. The system has been trained and tested on the dataset released for SAIL TWEET CONTEST 2015. Our system obtains accuracy of 50.75i¾?%, 48.82i¾?%, 41.20i¾?%, and 40.20i¾?% for Hindi constrained, Hindi unconstrained, Bengali constrained and Bengali unconstrained run respectively.	sentiment analysis	Kamal Sarkar;Saikat Chakraborty	2015		10.1007/978-3-319-26832-3_66	geography;genealogy;communication;cartography	NLP	-22.296076319575402	-69.6600545042999	56385
1bc4ad28bf871190b6c38035fd1c4231cbfed6a6	usfd at semeval-2016 task 1: putting different state-of-the-arts into a box		In this paper we describe our participation in the STS Core subtask which is the determination of the monolingual semantic similarity between pair of sentences. In our participation we adapted state-ofthe-art approaches from related work applied on previous STS Core subtasks and run them on the 2016 data. We investigated the performance of single methods but also the combination of them. Our results show that Convolutional Neural Networks (CNN) are superior to both the Monolingual Word Alignment and the Word2Vec approaches. The combination of all the three methods performs slightly better than using CNN only. Our results also show that the performance of our systems varies between the datasets.	convolutional neural network;semeval;semantic similarity	Ahmet Aker;Frédéric Blain;Andrés Duque;Marina Fomicheva;Jurica Seva;Kashif Shah;Daniel Edward Robert Beck	2016			natural language processing;speech recognition;computer science;artificial intelligence;machine learning	NLP	-20.523675139734475	-72.4365491629289	56499
723b0d7a71fef329313549b84cdd0c5e4426e263	the uka/cmu statistical machine translation system for iwslt 2006		This paper describes the UKA/CMU statistical machine translation system used in the IWSLT 2006 evaluation campaign. The system is based on phrase-to-phrase translations extracted from a bilingual corpus. We compare two different phrase alignment techniques both based on word alignment probabilities. The system was used for all language pairs and data conditions in the evaluation campaign translating both the ASR output (as 1best) and the correct recognition results.	bitext word alignment;data structure alignment;statistical machine translation;text corpus	Matthias Eck;Ian R. Lane;Nguyen Bach;Sanjika Hewavitharana;Muntsin Kolss;Bing Zhao;Almut Silja Hildebrand;Stephan Vogel;Alexander H. Waibel	2006			machine translation;decoding methods;transcription (linguistics);natural language processing;speech recognition;language model;phrase;artificial intelligence;computer science	NLP	-22.66742182184981	-77.62059956790587	56518
ecb120b8c0e068f41f390779547ac16319596d05	punctuation normalisation for cleaner treebanks and parsers		Although punctuation is pervasive in written text, their treatment in parsers and corpora is often second-class. We examine the treatment of commas in CCGbank, a wide-coverage corpus for Combinatory Categorial Grammar (CCG), reanalysing its comma structures in order to eliminate a class of redundant rules, obtaining a more consistent treebank. We then eliminate these rules from C&C, a wide-coverage statistical CCG parser, obtaining a 37% increase in parsing speed on the standard CCGbank test set and a considerable reduction in memory consumed, without affecting parser accuracy.	attachments;baseline (configuration management);combinatory categorial grammar;lexical grammar;parsing;test set;text corpus;treebank	Daniel Tse;James R. Curran	2008			speech recognition;communication	NLP	-23.747146849508233	-75.83424308257403	56690
46309506f8d687e4963e3d0850a0506cfdc47563	hyter: meaning-equivalent semantics for translation evaluation	translation evaluation;common knowledge;translation accuracy;better estimate;meaning-equivalent semantics;meaning-equivalent translation;human translation accuracy;compactly encode;alternative evaluation metrics;1-to-n mapping process;annotation tool;correct translation	It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.	ambiguous grammar;encode;time complexity	Markus Dreyer;Daniel Marcu	2012			natural language processing;data mining;rule-based machine translation	NLP	-24.69517455568104	-74.96386088106996	56701
00dad5bb87bfd6404a5ed65538e58cc6d82c5536	text integrity assessment: sentiment profile vs rhetoric structure		We formulate the problem of text integrity assessment as learning the discourse structure of text given the dataset of texts with high integrity and low integrity. We use two approaches to formalizing the discourse structures, sentiment profile and rhetoric structures, relying on sentence-level sentiment classifier and rhetoric structure parsers respectively. To learn discourse structures, we use the graph-based nearest neighbor approach which allows for explicit feature engineering, and also SVM tree kernel–based learning. Both learning approaches operate on the graphs (parse thickets) which are sets of parse trees with nodes with either additional labels for sentiments, or additional arcs for rhetoric relations between different sentences. Evaluation in the domain of valid vs invalid customer complains (those with argumentation flow, non-cohesive, indicating a bad mood of a complainant) shows the stronger contribution of rhetoric structure information in comparison with the sentiment profile information. Both above learning approaches demonstrated that discourse structure as obtained by RST parser is sufficient to conduct the text integrity assessment. At the same time, sentiment profile-based approach shows much weaker results and also does not complement strongly the rhetoric structure ones.		Boris A. Galitsky;Dmitry I. Ilvovsky;Sergey O. Kuznetsov	2015		10.1007/978-3-319-18117-2_10	data mining;world wide web;information retrieval	NLP	-19.521413712120488	-68.10295683784571	56769
6aa2d812ce90ddabb3fcb16eaa2ce2354dad9166	opening schrödingers library: semi-automatic qa reduces uncertainty in object transformation	quality assurance	Object transformation for preservation purposes is currently a hit-ormiss affair, where errors in transformation may go unnoticed for years since manual quality assurance is too resource-intensive for large collections of digital objects. We propose an approach of semi-automatic quality assurance (QA), where numerous separate automatic checks of “aspects” of the objects, combined with manual inspection, provides greater assurance that objects are transformed with little or no loss of quality. We present an example of using this approach to appraise the quality of OpenOffice’s import of Word documents.	microsoft word for mac;openoffice basic;semiconductor industry;software quality assurance	Lars Ræder Clausen	2007		10.1007/978-3-540-74851-9_16	computer science;data mining;database;software quality analyst	PL	-30.22430454590937	-73.13532948718141	56855
235eb58c54b0d1122878f4c8bb46bfaac103d0d7	extracting social power relationships from natural language	natural language	Sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects 1 . This paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. Our results validate the treatment of lect modeling as a text classification problem – albeit a hard one – and constitute a case for future research in computational sociolinguistics.	document classification;email;machine learning;natural language;social network;support vector machine;vocabulary	Philip Bramsen;Martha Escobar-Molano;Ami Patel;Rafael Alonso	2011			natural language processing;computer science;artificial intelligence;machine learning;linguistics;natural language	AI	-20.280466069880628	-67.89891168947948	56887
37643aa6564a469e39e02da3f5528b0d7fdc694a	better word alignments with supervised itg models	many-to-one block alignment feature;supervised word alignment method;itg alignment model;inversion transduction grammar;cky parsing;supervised itg model;canonicalizing derivation;conditional likelihood objective;better word alignment;aer number;itg model;new normal form grammar	This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over	align (company);bleu;bitext word alignment;data structure alignment;downstream (software development);one-to-many (data model);parallel text;parsing;transduction (machine learning)	Aria Haghighi;John Blitzer;John DeNero;Dan Klein	2009			natural language processing;speech recognition;computer science;machine learning	NLP	-20.626405364117257	-76.7461874603035	56984
c465d325f93886f82c3fd6059d4c59928d31862d	a preliminary investigation of learner characteristics for unsupervised dialogue act classification		For tutorial dialogue systems, classifying the dialogue act (such as questions, requests for feedback, or statements) of student natural language utterances is a central challenge. Recently, momentum is building for the use of unsupervised machine learning approaches to address this problem because they reduce the manual tagging required to build dialogue act models from corpora. However, unsupervised models still do not perform as well as supervised models in terms of accuracy. This paper presents an unsupervised dialogue act modeling approach that leverages the influence of learner characteristics, particularly students’ perceptions of their own skill, on their language use. The experimental findings show that leveraging skill perception within dialogue act classification improves performance of the models, producing better accuracy. This line of investigation will inform the design of nextgeneration tutorial dialogue systems, which leverage machinelearned models to adapt to their users.	dialog system;machine learning;natural language;text corpus;unsupervised learning	Aysu Ezen-Can;Kristy Elizabeth Boyer	2014			artificial intelligence;machine learning;computer science;natural language processing;natural language;unsupervised learning	NLP	-26.694147665470812	-72.81684446966791	57033
47c262c9109addc78b6379c426e5e81dc6b452e5	detecting a change of style using text statistics: notebook for pan at clef 2018		In this paper we address style change detection problem at PAN’18 author identification task. For this task one should determine whether text is written by the same author or not. We consider supervised problem statement with the whole text as a training object. The roposed approach is based on three types of features: text statistics, hashing and high dimensional text vectors. The final algorithm is the ensemble of classifiers that were independently trained on each feature group.	algorithm;cryptographic hash function;sensor	Kamil Safin;Aleksandr Ogaltsov	2018			information retrieval;clef;computer science	NLP	-23.01092406804707	-70.68306048557353	57057
bf3fb95160c363b6d58c74d7e1712b36cfa09eb8	generation of thesaurus in different languages a computer based system	modulated subject-propositions;alphabetical thesaurus;library classification;required language;subject analysis technique;certain code;information retrieval thesaurus;subject indexing;deep structure;different language;indexation;information retrieval	The development of the theory of library classification and of subject indexing, for the organisation, storage and retrieval of subjects embodied in documents has a striking parallelism to the search for 'universal forms' and deep structure'ln language and linguistic studies. The significant contributions o f the theories o f classification and subject indexing are the subject analysis techniques of Ranganathan and Bhattacharyya's POPSI. A computer based system, for generating an information retrieval thesaurus, from modulated subject-proposltlons, formulated according to the subject analysis techniques, enriched with certain codes for relating the terms in the subject-proposltlons has been developed. The system generates hierarchlc, associative, coordinate and synonymous relationships between terms and presents them as an alphabetical thesaurus. Also, once a thesaurus is generated in one language it is possible to produce the same thesaurus in different languages by Just forming a table of equivalent terms in the required language.	code;information retrieval;library classification;modulation;parallel computing;subject indexing;theory;thesaurus	F. J. Devadason	1980			natural language processing;computer science;data mining;information retrieval	Web+IR	-32.56317755572617	-71.08744346393685	57180
12257f2fc2837b45b212e69e18c14e2f4c3f90ed	towards an integrated scheme for semantic annotation of multimodal dialogue data		This paper investigates the applicability of existing dialogue act annotation sch emes, designed for the analysis of spoken dialogue, to the semantic annotation of multimodal data, and the way a dialogue act annotation scheme can be extended to cover dialogue phenomena from multiple modalities.	multimodal interaction	Volha Petukhova;Harry Bunt	2010			modalities;natural language processing;artificial intelligence;semantic computing;semantic technology;image retrieval;information retrieval;semantic web stack;temporal annotation;annotation;computer science	NLP	-32.50344322747714	-76.80076109865608	57278
45410934f5eddd2bdecd9f8e49c16223ba5fda44	statistical machine translation of texts with misspelled words	statistical machine translation	This paper investigates the impact of misspelled words in statistical machine translation and proposes an extension of the translation engine for handling misspellings. The enhanced system decodes a word-based confusion network representing spelling variations of the input text. We present extensive experimental results on two translation tasks of increasing complexity which show how misspellings of different types do affect performance of a statistical machine translation decoder and to what extent our enhanced system is able to recover from such errors.	image noise;noise generator;social media;statistical machine translation;text-based (computing)	Nicola Bertoldi;Mauro Cettolo;Marcello Federico	2010			natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;pattern recognition;machine translation;rule-based machine translation	NLP	-21.838288538413877	-77.69617289370343	57423
0191383b0e63456e3dcf10e5fd844413fac15cff	part of speech tagging		Other problems can also be framed as tagging (sequence labelling): • Case restoration: If we just get lowercased text, we may want to restore proper casing, e.g. the river Thames • Named entity recognition: it may also be useful to find names of persons, organizations, etc. in the text, e.g. Barack Obama • Information field segmentation: Given specific type of text (classified advert, bibiography entry), identify which words belong to which “fields” (price/size/#bedrooms, author/title/year) • Prosodic marking: In speech synthesis, which words/syllables have stress/intonation changes, e.g. He’s going. vs He’s going?	categorization;circuit restoration;hidden markov model;item unique identification;markov chain;named entity;named-entity recognition;parsing;part-of-speech tagging;speech synthesis;statistical model;viterbi algorithm	Sharon Goldwater	2017		10.1007/978-1-4899-7687-1_100357	speech corpus	NLP	-25.681730399418612	-79.91321804980929	57469
4eb964fa45a05d2862a982d6397ab7f4b6b3406c	end-to-end learning of parsing models for information retrieval	information retrieval;traditional syntactic parse trees end to end learning parsing models information retrieval long span word dependencies parser parameters nonconvex ir measure nonsmooth ir measure large scale web search task real world query set document retrieval;grammars;learning artificial intelligence grammars information retrieval;training hidden markov models information retrieval syntactics cost function speech;learning artificial intelligence;tree edit distance information retrieval parsing model end to end optimization	Parsers have been shown to be helpful in information retrieval tasks because they are able to model long-span word dependencies efficiently. While previous work focused on using traditional syntactic parse trees, this paper proposes a new approach where, unlike previous work, the parser parameters are discriminatively trained to directly optimize a non-convex and non-smooth IR measure. The relevance between a document and a query is then modeled by the weighted tree edit distance between their parses. We evaluate our method on a large scale web search task consisting of a real world query set. Results show that the new parser is more effective for document retrieval than using traditional syntactic parse trees. It gives significant improvement, especially for long queries where proper modeling of long-span dependencies is crucial.	discriminative model;document retrieval;graph edit distance;information retrieval;parse tree;parsing;relevance;web search engine	Jennifer Gillenwater;Xiaodong He;Jianfeng Gao;Li Deng	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638271	natural language processing;document retrieval;parser combinator;query expansion;visual word;speech recognition;computer science;machine learning;pattern recognition;rule-based machine translation;top-down parsing;information retrieval;human–computer information retrieval;divergence-from-randomness model	NLP	-19.647415179210757	-75.17445869471274	57580
a460ed970c630af1e27bcce84e0a0f6b613ca9c5	an approach to couple two forms of feedback in conversational recommenders				Maria Salamó;Anna Puig;David Contreras	2013		10.3233/978-1-61499-320-9-273	natural language processing;artificial intelligence;computer science	HCI	-32.689526624081324	-77.61615619266352	57640
93d522a30132cdd83fa32255b38d4114c62354f3	acoustic unit discovery based on multilingual resource using variational bayesian method		Nonparametric Bayesian models are often used to automatically discovery acoustic units in unlabeled data. Most of them are trained by Gibbs sampling (GS) method. In this paper, Variational Bayesian (VB) inference is used to find the common acoustic units in multilingual speech data. Experimental results show that the acoustic units obtained from multilingual data can be transferred to learn another language. Moreover, VB is faster than GS, and the performance is better in accuracy.	acoustic cryptanalysis;bayesian network;gibbs sampling;roland gs;sampling (signal processing);variational principle	Rongrong Wang;Lianhai Zhang;Qi Chen	2018	2018 Tenth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2018.8377496	data modeling;nonparametric statistics;inference;hidden markov model;bayesian probability;pattern recognition;gibbs sampling;computer science;artificial intelligence	ML	-19.778425372868465	-76.72891011517851	57670
c8c0e6b3fe1b56e459c1719259baeb4d79ac97e5	a language model for improving the graph-based transcription approach for historical documents		Language Models (LMs) capture the contextual dependencies of a language and assign higher probabilities to well-formed sequences of words. For that reason, LMs have been commonly used in generic handwriting recognition, improving recognition results. In this paper, we present the integration of a Language Model along with a dictionary into a graph-based recognizer, which aims at transcribing handwritten historical documents. The results of such integration show a significant improvement on word accuracy when applied to our corpora.	language model;medical transcription	Graciela Lecireth Meza Lovón	2014		10.1007/978-3-319-12027-0_19	natural language processing;speech recognition;data mining	NLP	-23.764958630371005	-73.07087698556131	57689
939cba95b6d73a84e86b958111fd19caa642f594	towards compositional distributional discourse analysis		Categorical compositional distributional semantics provide a method to derive the meaning of a sentence from the meaning of its individual words: the grammatical reduction of a sentence automatically induces a linear map for composing the word vectors obtained from distributional semantics. In this paper, we extend this passage from word-to-sentence to sentence-to-discourse composition. To achieve this we introduce a notion of basic anaphoric discourses as a mid-level representation between natural language discourse formalised in terms of basic discourse representation structures (DRS); and knowledge base queries over the Semantic Web as described by basic graph patterns in the Resource Description Framework (RDF). This provides a high-level specification for compositional algorithms for question answering and anaphora resolution, and allows us to give a picture of natural language understanding as a process involving both statistical and logical resources.	algorithm;anaphora (linguistics);distributional semantics;high- and low-level;knowledge base;knowledge representation and reasoning;natural language understanding;question answering;resource description framework;semantic web;word embedding	Bob Coecke;Giovanni de Felice;Dan Marsden;Alexis Toumi	2018	CoRR	10.4204/EPTCS.283.1	rdf;discourse analysis;artificial intelligence;machine learning;computer science;distributional semantics;semantic web;natural language understanding;natural language;question answering;sentence	NLP	-31.354422966516978	-71.2908648001426	57714
64a1658494fd9b11aa565c8d17c71d9fb90554f5	generating tailored classification schemas for german patents		Patents and patent applications are important parts of a company’s intellectual property. Thus, companies put a lot of effort in designing and maintaining an internal structure for organizing their own patent portfolios, but also in keeping track of competitor’s patent portfolios. Yet, official classification schemas offered by patent offices (i) are often too coarse and (ii) are not mappable, for instance, to a company’s functions, applications, or divisions. In this work, we present a first step towards generating tailored classification. To automate the generation process, we apply key term extraction and topic modelling algorithms to 2.131 publications of German patent applications. To infer categories, we apply topic modelling to the patent collection. We evaluate the mapping of the topics found via the Latent Dirichlet Allocation method to the classes present in the patent collection as assigned by the domain expert.		Oliver Pimas;Stefan Klampfl;Thomas Kohl;Roman Kern;Mark Kröll	2016		10.1007/978-3-319-41754-7_20	computer science;latent dirichlet allocation;patent classification;data mining;subject-matter expert;topic model;keyword extraction;intellectual property;schema (psychology);german	NLP	-25.363771352940383	-68.75509669879047	57718
d0e4e526e3d7cd465ff0cdafe7b0b8ed8d3fb7c5	sentence filtering for bionlp: searching for renaming acts	supporting task;gene synonymy reminder;gene nomenclature;gene name;bacteria gene renaming;svm feature;bionlp shared task;effective post-competition improvement;renaming act;document segmentation	The Bacteria Gene Renaming (RENAME) task is a supporting task in the BioNLP Shared Task 2011 (BioNLP-ST’11). The task consists in extracting gene renaming acts and gene synonymy reminders in scientific texts about bacteria. In this paper, we present in details our method in three main steps: 1) the document segmentation into sentences, 2) the removal of the sentences exempt of renaming act (false positives) using both a gene nomenclature and supervised machine learning (feature selection and SVM), 3) the linking of gene names by the target renaming relation in each sentence. Our system ranked third at the official test with 64.4% of F-measure. We also present here an effective post-competition improvement: the representation as SVM features of regular expressions that detect combinations of trigger words. This increases the F-measure to 73.1%.	algorithm;amelioration pattern;bag-of-words model;biomedical text mining;document classification;document layout analysis;experiment;fagan inspection;feature selection;gene nomenclature;machine learning;parsing;regular expression;sentence boundary disambiguation;supervised learning	Pierre Warnier;Claire Nedellec	2011			speech recognition;computer science;data mining;communication	NLP	-23.997025995865222	-70.29722600942691	57749
b5550cd302d6e6a9cd391bb6781cf0fcde685fb4	utterance verification for the numeric language in a natural spoken dialogue				Mazin G. Rahim	1999			speech recognition;natural language processing;computer science;artificial intelligence;utterance	NLP	-29.60890486343565	-80.1735975457734	57953
7c6531a6bd972fcafe923d4a8bae29abbf93c26b	experiments in mutual exclusion bootstrapping	lexical semantics;mutual exclusion;named entity	Mutual Exclusion Bootstrapping (MEB) was designed to overcome the problem of semantic drift suffered by iterative bootstrapping, where the meaning of extracted terms quickly drifts from the original seed terms (Curran et al., 2007). MEB works by extracting mutually exclusive classes in parallel which constrain each other. In this paper we explore the strengths and limitations of MEB by applying it to two novel lexical-semantic extraction tasks: extracting bigram named entities and WordNet lexical file classes (Fellbaum, 1998) from the Google Web 1T 5-grams.	bigram;cognition;coherence (physics);experiment;feedback;iterative method;mutual exclusion;n-gram;named entity;named-entity recognition;traction teampage;vocabulary;wordnet;world wide web	Tara Murphy;James R. Curran	2007			natural language processing;pattern recognition;communication	NLP	-26.390997827355115	-70.90475021151693	58083
04aecf2431456540d515afeec9c655df344950e8	extracting and ranking question-focused terms using the titles of wikipedia articles		At the NTCIR-6 CLQA (Cross-Language Question Answering) task, we participated in the Chinese-Chinese (C-C) and English-Chinese (E-C) QA (Question Answering) subtasks. Without employing question type classification, we proposed a new resource, Wikipedia, to assist in extracting and ranking Question-Focused terms. We regarded the titles of Wikipedia articles as a multilingual noun-phrase corpus which is useful in QA systems. Experimental results showed that better performance was achieved for questions with type PERSON or LOCATION. Besides, we used an online MT (Machine Translation) system to deal with question translation in our CLQA task.	machine translation;question answering;wikipedia	Yi-Che Chan;Kuan-Hsi Chen;Wen-Hsiang Lu	2007			computer science;data mining;world wide web;information retrieval	NLP	-26.397773123476565	-67.40173035232115	58164
2cb0c1a614c32240de08132d95a363a3d8101317	on timeml-compliant temporal expression extraction in turkish		It is commonly acknowledged that temporal expression extractors are important components of larger natural language processing systems like information retrieval and question answering systems. Extraction and normalization of temporal expressions in Turkish has not been given attention so far except the extraction of some date and time expressions within the course of named entity recognition. As TimeML is the current standard of temporal expression and event annotation in natural language texts, in this paper, we present an analysis of temporal expressions in Turkish based on the related TimeML classification (i.e., date, time, duration, and set expressions). We have created a lexicon for Turkish temporal expressions and devised considerably wide-coverage patterns using the lexical classes as the building blocks. We believe that the proposed patterns, together with convenient normalization rules, can be readily used by prospective temporal expression extraction tools for Turkish.	timeml	Dilek Küçük;Dogan Küçük	2015	CoRR		natural language processing;computer science;data mining;temporal annotation	NLP	-29.018774209736645	-72.63506269954138	58208
d8a4f3a95fbeeda7b344d9c167d3ceb9fd5073b0	sews: an web-based server for evaluating syntactic annotation tools		Examples of Automated Evaluation platforms deployed as Web server are currently very rare and often underestimated. Time and, effort savings, faster system improvement, common paradigm of evaluation for a community, the benefits offered by such services are plentiful. In this paper, we present a platform for evaluating automatically parsers and we comment on its deployment during an evaluation campaign. First, we draw up a state-of-the-art for platforms used in evaluation of NLP systems, then we present the tools available for Web server deployment. Next, we describe our platform and its deployment in the PASSAGE project as a Web server. Finally we show the interest of generalizing such service to other NLP domains. MOTS-CLÉS : évaluation, plate-forme, serveur Web, annotation syntaxique.	natural language processing;parsing;programming paradigm;server (computing);software deployment;web application;web server	Olivier Hamon;Patrick Paroubek;Djamel Mostefa	2008	TAL		web application;world wide web;syntax;information retrieval;computer science;annotation	Web+IR	-32.82404656867144	-73.2909161260743	58211
32f490d675cb5d0e0b701e153411014ba3353e09	definiteness of polish noun phrases modified by relative clauses in the drt framework		In this paper, I investigate anaphoric coreference as one of the sources of definiteness of NPs. The main focus here is how  relative clauses influence the process of finding an antecedent for an NP in the standard DRT framework. Treelike indexes  are adapted for this goal. The analysis is performed for article-free language (Polish). Other sources of definiteness are  not considered.  		Elzbieta Hajnicz	2006		10.1007/3-540-33521-8_34	machine learning;definiteness;noun phrase;natural language processing;computer science;coreference;artificial intelligence	NLP	-26.797033528224613	-76.46452534099005	58523
df7b4f5cc0524608e7407d3f3728874ec0558f91	a computational study of the ba resultative construction : parsing mandarin ba sentences in hpsg	conference paper	Recognizing Mandarin ba sentences as the Ba Resultative Construction, this study implements a rigid test of Ding's (1993) treatment of ba as a matrix verb in the periphrastic resultative construction. The computational experiment is facilitated with a formal rendition of the analysis of ba in Head-driven Phrase Structure Grammar. Moreover, the small parsing experiment also highlights the important role of semantics in natural language processing, including common-sense knowledge.	artificial intelligence;bioinformatics resource centers;computation;goto;head-driven phrase structure grammar;holomatix rendition;lexical definition;modem;nancy leveson;natural language processing;parsing;super robot monkey team hyperforce go!;the superficial;theory	Picus Sizhi Ding	2000			natural language processing;speech recognition;computer science;linguistics	NLP	-30.30609118585462	-79.2299784014655	58534
0f9f06f8f500f27fbf3169bad73d58f1640d3791	bilingual document alignment with latent semantic indexing		We apply cross-lingual Latent Semantic Indexing to the Bilingual Document Alignment Task at WMT16. Reduced-rank singular value decomposition of a bilingual term-document matrix derived from known English/French page pairs in the training data allows us to map monolingual documents into a joint semantic space. Two variants of cosine similarity between the vectors that place each document into the joint semantic space are combined with a measure of string similarity between corresponding URLs to produce 1:1 alignments of English/French web pages in a variety of domains. The system achieves a recall of ca. 88% if no in-domain data is used for building the latent semantic model, and 93% if such data is included. Analysing the system’s errors on the training data, we argue that evaluating aligner performance based on exact URL matches under-estimates their true performance and propose an alternative that is able to account for duplicates and near-duplicates in the underlying data.	1:1 pixel mapping;circa;cosine similarity;document-term matrix;mpeg media transport;singular value decomposition;string metric;web crawler;web page	Ulrich Germann	2016		10.5281/zenodo.834343	natural language processing;latent semantic indexing;document clustering;pattern recognition;information retrieval	Web+IR	-22.8274070749178	-73.07768527352486	58640
e0930fcb02240fd81873f5a614bd79ead2e07582	a classification of semantic annotation systems	semantic annotation;vocabulary;tag;qa076 computer software;attributes;semantic;ontology;classification scheme	The Subject-Predicate-Object triple annotation system is now well adopted in the research community, however, it does not always speak to end-users. In fact, explaining all the complexity of semantic annotation systems to laymen can sometime be difficult. We believe that this communication can be simplified by providing a meaningful abstraction of the state of the art in semantic annotation models and thus, in this article, we describe the issue of semantic annotation and review a number of research and end-user tools in the field. Doing so, we provide a clear classification scheme of the features of annotation systems. We then show how this scheme can be used to clarify requirements of end-user use cases and thus simplify the communication between semantic annotation experts and the actual users of this technology.	cellular automaton;comparison and contrast of classification schemes in linguistics and metadata;feedback;requirement;structural complexity (applied mathematics);vocabulary	Pierre Andrews;Ilya Zaihrayeu;Juan Pane	2012	Semantic Web	10.3233/SW-2011-0056	natural language processing;minimum information required in the annotation of models;semantic similarity;semantic computing;image retrieval;computer science;data mining;semantic web stack;semantic technology;temporal annotation;information retrieval	Web+IR	-32.75339705199545	-70.5479985341777	58766
a0ab7fbac5de85ab8e603a0e601bcbafe1bea73a	global domains versus hidden indexicals	indexation			Christopher Gauker	2010	J. Semantics	10.1093/jos/ffq001	philosophy	AI	-31.676164905774648	-79.17378999580396	58920
90a4bee4421639d96d206b44926bf919b2ccbbf3	lilfes/genia project --- nlp tools and a biology domain corpus ---			natural language processing	Jun'ichi Tsujii	2001			natural language processing;computer science;artificial intelligence	NLP	-31.200594565795793	-77.48246907514759	58921
19829d9f61557a2f4bcf81886655ec7eb9b01732	talking wearables exploit context	focus of attention;natural language;natural language generation	This paper addresses the issue of how natural language generation technology can contribute to less intrusive wearable devices. Based on the investigation of how humans adapt the form of their utterances to the context of their hearer, we propose a strategy to relate (physical) context to the automated generation of natural language utterances. First we emphasise that different dimensions of context need to be taken into account and illustrate this with examples of lexical choice. Then we elaborate a strategy for determining sentence structure and prosody annotation based on the context relating to focus of attention. Our approach sets up an experimental basis in the context of an advice-giving wearable device (parrot).	human–computer interaction;lexical choice;natural language generation;semantic prosody;wearable computer;wearable technology	Sabine Geldof;Jacques M. B. Terken	2001	Personal and Ubiquitous Computing	10.1007/s007790170033	natural language processing;computer science;context model;natural language	NLP	-33.30239966234719	-79.71335604549168	58960
c08a0c1e5babc97449706d4ab9254a501d5359aa	the mitre logical form generation system		In this paper, we describe MITRE’s contribution to the logical form generation track of Senseval-3. We begin with a description of the context of MITRE’s work, followed by a description of the MITRE system and its results. We conclude with a commentary on the form and structure of this evaluation track.		Samuel Bayer;John D. Burger;Warren R. Greiff;Ben Wellner	2004			artificial intelligence;natural language processing;computer science;logical form	NLP	-32.053244786327134	-76.469715930718	59017
7fb80ce93d0d8a9dc786fa0366dba5e453c85bb2	exploiting catenae in a parallel treebank alignment		This paper aims to introduce the issues related to the syntactic alignment of a dependency-based multilingual parallel treebank, ParTUT. Our approach to the task starts from a lexical mapping and then attempts to expand it using dependency relations. In developing the system, however, we realized that the only dependency relations between the individual nodes were not sufficient to overcome some translation divergences, or shifts, especially in the absence of a direct lexical mapping and a different syntactic realization. For this purpose, we explored the use of a novel syntactic notion introduced in dependency theoretical framework, i.e. that of catena (Latin for ”chain”), which is intended as a group of words that are continuous with respect to dominance. In relation to the task of aligning parallel dependency structures, catenae can be used to explain and identify those cases of one-to-many or many-to-many correspondences, typical of several translation shifts, that cannot be detected by means of direct word-based mappings or bare syntactic relations. The paper presented here describes the overall structure of the alignment system as it has been currently designed, how catenae are extracted from the parallel resource, and their potential relevance to the completion of tree alignment in ParTUT sentences.	many-to-many;one-to-many (data model);relevance;treebank	Manuela Sanguinetti;Cristina Bosco;Loredana Cupi	2014			treebank;speech recognition;natural language processing;artificial intelligence;computer science	NLP	-26.43035703998528	-74.03093058458983	59076
7335c7db4c4ef7f6b8072e030d0e5d62162d323b	hierarchical text generation using an outline		Many challenges in natural language processing require generating text, including language translation, dialogue generation, and speech recognition. For all of these problems, text generation becomes more difficult as the text becomes longer. Current language models often struggle to keep track of coherence for long pieces of text. Here, we attempt to have the model construct and use an outline of the text it generates to keep it focused. We find that the usage of an outline improves perplexity. We do not find that using the outline improves human evaluation over a simpler baseline, revealing a discrepancy in perplexity and human perception. Similarly, hierarchical generation is not found to improve human evaluation scores.	baseline (configuration management);discrepancy function;http 404;language model;natural language generation;natural language processing;perplexity;speech recognition	Mehdi Drissi;Olivia Watkins;Jugal K. Kalita	2018	CoRR		machine learning;perplexity;natural language processing;artificial intelligence;language model;perception;computer science	NLP	-20.516572754196112	-79.02729148179834	59088
23e596dcefc317e5455c0834d4d32d42a4f16be4	automatic language identification using multivariate analysis	multivariate analysis;language identification;dimensional reduction	Identifying the language of an e-text is complicated by the existence of a number of character sets for a single language. We present a language identification system that uses the Multivariate Analysis (MVA) for dimensionality reduction and classification. We compare its performance with existing schemes viz., the N-grams and compression.	character encoding;dimensionality reduction;grams;language identification;model–view–adapter;n-gram;viz: the computer game	Vinosh Babu James;Baskaran Sankaran	2005		10.1007/978-3-540-30586-6_89	natural language processing;language identification;speech recognition;computer science;pattern recognition;multivariate analysis	NLP	-24.469702661397314	-79.30184406924178	59124
525307e398f2878771e059684e150800827fdece	effect of preprocessing on extractive summarization with maximal frequent sequences	text segmentation	The task of extractive summarization consists in producing a text summary by extracting a subset of text segments, such as sentences, and concatenating them to form a summary of the original text. The selection of sentences is based on terms they contain, which can be single words or multiword expressions. In a previous work, we have suggested so-called Maximal Frequent Sequences as such terms. In this paper, we investigate the effect of preprocessing on the process of selecting such sequences. Our results suggest that the accuracy of the method is, contrary to expectations, not seriously affected by preprocessing--which is both bad and good news, as we show.	automatic summarization;maximal set;preprocessor	Yulia Ledeneva	2008		10.1007/978-3-540-88636-5_11	text segmentation;speech recognition;computer science;automatic summarization;pattern recognition;data mining	NLP	-25.923338603344092	-68.01586586598076	59171
bf89d06a04b2656320acb49bf70f4aa6c8d847c1	ecnu at semeval-2016 task 7: an enhanced supervised learning method for lexicon sentiment intensity ranking		This paper describes our system submissions to task 7 in SemEval 2016, i.e., Determining Sentiment Intensity. We participated the first two subtasks in English, which are to predict the sentiment intensity of a word or a phrase in English Twitter and General English domains. To address this task, we present a supervised learning-to-rank system to predict the relevant scores, i.e., the strength associated with positive sentiment, for English words or phrases. Multiple linguistic and sentiment features are adopted, e.g., Sentiment Lexicons, Sentiment Word Vectors, Word Vectors, Linguistic Features, etc. Officially released results showed that our systems rank the 1st among all submissions in English, which proves the effectiveness of the proposed method.	internet of things;learning to rank;lexicon;microsoft word for mac;semeval;sentiment analysis;supervised learning;trustworthy computing;word embedding	Feixiang Wang;Zhihua Zhang;Man Lan	2016			supervised learning;artificial intelligence;computer science;natural language processing;machine learning;semeval;ranking;lexicon;pattern recognition	NLP	-22.154669780051204	-69.81291467992749	59315
51113304dcf67611a5227111ea35c26ef4862f15	rad: a scalable framework for annotator development	semantic search technology;document handling;optimized production technology;document collection;inverted index;rule based;indexes;large scale;gold;document at a time semantic search technology entity annotation rapid annotator development document collection inverted index annotation index lucene magnitude speedup;engines;indexing;document at a time;indexation;annotation index;dictionaries;magnitude speedup;performance evaluation large scale systems dictionaries speech gold measurement standards user interfaces visualization computational complexity costs;part of speech;semantic web;semantic web document handling indexing;ground truth;semantic search;entity annotation;user interfaces;regular expression;rapid annotator development;lucene;real time systems	Developments in semantic search technology have motivated the need for efficient and scalable entity annotation techniques. We demonstrate RAD: a tool for Rapid Annotator Development on a document collection. RAD builds on a recent approach (Ramakrishnan et al., 2006) that translates entity annotation rules into equivalent operations on the inverted index of the collection, to directly generate an annotation index (which can be used in search applications). To make the framework scalable, we use an industrial strength indexer, Lucene (http://lucene.apache.org) and introduce some modifications to its API. The index also serves as a suitable representation for making quick comparisons with an indexed ground truth of annotations on the same collection to evaluate precision and recall of the annotations. RAD achieves at least an order of magnitude speedup over the standard approach of annotating a document-at-a-time as adopted by GATE (Cunnignham et al., 2002). The speedup factor increases with increase in the size of the collection, making RAD scalable. We cache intermediate results from the index operations, enabling quick update of the annotation index as well as speedy evaluation when rules are modified. This makes RAD suitable for rapid and interactive development of annotators.	archive;gate;ground truth;inverted index;precision and recall;rapid application development;scalability;search engine indexing;semantic search;speedup	Sanjeet Khaitan;Ganesh Ramakrishnan;Sachindra Joshi;Anup Chalamalla	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497637	gold;database index;search engine indexing;inverted index;semantic search;ground truth;part of speech;computer science;semantic web;data mining;database;programming language;user interface;information retrieval;regular expression	DB	-32.80027493188959	-66.25826219313844	59459
86e62c5bd78ef2b4d475fd0f679e10f1a799a11e	overview of the 2016 alta shared task: cross-kb coreference		This paper presents an overview of the 7th ALTA shared task that ran in 2016. The task was to disambiguate endpoints by determining whether two URLs were referring to the same entity. We present the motivation for the task, the description of the data and the results of the participating teams.	end-of-file;ensemble kalman filter;f1 score;logistic regression;overfitting;test set	Andrew Chisholm;Ben Hachey;Diego Mollá Aliod	2016			natural language processing;coreference;computer science;artificial intelligence	NLP	-22.484781854505595	-70.1055398558413	59499
9901831dcede25ac02742fbcdc095bbcfe466f50	annotation schema for contemporary chinese based on jinxi li's grammar system	annotation schema;jinxi li's grammar;sentence pattern;sentencebased;treebank	The Sentence-based Grammar System which was created by linguist Jinxi Li, is one of the most representative Chinese grammar systems. After reviewing the outline of Li's Grammar Theory including some viewpoints on syntax, morphology and diagrammatic parsing method, the paper illustrated the formalization idea of the Sentence-based Grammar System from the perspective of Chinese Information Processing, and designed an annotation schema for contemporary Chinese sentence structure. Then a visual annotation tool was implemented, and a Treebank was built up by analyzing 11 thousand sentences from some Chinese text books. The fact that all these sentences can be analyzed and modeled within the annotation schema proves the correctness and completeness of Li's grammar system, and our work also provides the basic resources and theory for the study of Automatic Parsing and Machine Translation. © 2013 Springer-Verlag.		Jing He;Weiming Peng;Jihua Song;Hongzhang Liu	2013		10.1007/978-3-642-45185-0_69	natural language processing;computer science;linguistics;communication	NLP	-31.661770590427825	-76.20156452593248	59595
0e66ecba6b01ac2476eb9e362c2d489d460c0f45	comparing the performance of neural and statistical sentence embeddings on summarization and word sense disambiguation		We analyzed the performance of two sentence embeddings: SIF (Smoothed Inverse Frequency) created using weighted GloVe word embeddings, and sent2vec, trained using a neural network. Using these sentence embeddings without modification, we compared and contrasted their performance on extractive text summarization and word sense disambiguation using existing methods tailored for sentence embeddings. We find that our results are better than the simplest baselines and approach competitive baselines for both these tasks, proving that sentence embeddings are to some extent successful in capturing the structure of language.		Gaurav Juvekar;Abhishek Lolage;Dhruva Sahasrabudhe;Yashodhara Haribhakta	2018	2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2018.8554695	automatic summarization;control theory;task analysis;computer science;inverse;euclidean distance;artificial neural network;cluster analysis;information technology;pattern recognition;sentence;artificial intelligence	NLP	-19.774198496106017	-73.52822934012478	59724
8f8256c9bb9e8db33e60873560c27196f596bd87	columbia-jadavpur submission for emnlp 2016 code-switching workshop shared task: system description		We describe our present system for language identification as a part of the EMNLP 2016 Shared Task. We were provided with the Spanish-English corpus composed of tweets. We have employed a predictor-corrector algorithm to accomplish the goals of this shared task and analyzed the results obtained.	algorithm;columbia (supercomputer);empirical methods in natural language processing;kerrison predictor;language identification;predictor–corrector method;text corpus	Arunavha Chanda;Dipankar Das;Chandan Mazumdar	2016		10.18653/v1/W16-5814	distributed computing;world wide web;information retrieval	NLP	-22.282783395592453	-69.86198562625357	59797
0fee430d0af8f25a17f08366f1b5868bdf1d17a8	a quasi-synchronous dependence model for information retrieval	term dependency;stochastic process;quasi synchronous grammar;information retrieval;machine learning;retrieval model;dependency parsing;weighting retrieval model;machine translation	Incorporating syntactic features in a retrieval model has had very limited success in the past, with the exception of binary term dependencies. This paper presents a new term dependency modeling approach based on syntactic dependency parsing for both queries and documents. Our model is inspired by a quasi-synchronous stochastic process for machine translation[21]. We model four different types of relationships between syntactically dependent term pairs to perform inexact matching between documents and queries. We also propose a machine learning technique for predicting optimal parameter settings for a retrieval model incorporating syntactic relationships. The results on TREC collections show that the quasi-synchronous dependence model can improve retrieval performance and outperform a strong state-of-art sequential dependence baseline when we use predicted optimal parameters.	baseline (configuration management);information retrieval;machine learning;parsing;stochastic process	Jae Hyun Park;W. Bruce Croft;David A. Smith	2011		10.1145/2063576.2063585	natural language processing;stochastic process;computer science;machine learning;pattern recognition;machine translation;term discrimination;vector space model;information retrieval;statistics;dependency grammar;divergence-from-randomness model	NLP	-21.803025317596163	-73.78815121404593	59827
762b823a1266953baa4c23dbe6248861c0c7c2a2	toward a taxonomy of concepts using web documents structure	document structure;concept;web documents;taxonomic links;information retrieval;web	Due to the rise of the Web and the need to have structured knowledge, an interesting line for research is the formalization of ontologies and the creation of conceptual taxonomies from Web documents. The traditional methods for ontology learning and especially those extracting domain concepts from a textual corpus often privilege the analysis of the text itself, whether they are based on a statistical or linguistic approach. In this paper, we propose an approach which differs from the traditional ones since it uses information on the document structure to extract relevant information. Our approach studies each material form in the text in order to extract the most relevant concepts constituting the ontology related to a given field. The concepts are obtained by analyzing the occurrences of the candidate terms in the titles and in the links belonging to the documents and by considering the used styles.  Our approach has been experimented on a french corpus of Web documents related to the medical field. Primary results are encouraging and seem to validate our approach. We present also in this paper a new method for the extraction of the hierarchical links between the concepts of the ontology. The taxonomic links are established in three phases: a linguistic step is based on the canonical syntactic structure of the extracted concepts, the second step consists in applying lexico-syntactic patterns which convey the hyperonymy relation and the third step analyzes the hierarchy of the titles in each document to extract taxonomic relations.	lexico;ontology (information science);ontology learning;taxonomy (general);text corpus;web page;world wide web	Rim Zarrad;Narjes Doggaz;Ezzeddine Zagrouba	2012		10.1145/2428736.2428762	computer science;data mining;world wide web;information retrieval	Web+IR	-29.75147068711658	-66.29999945137068	59925
3c802de1c9e4ab4ffc4669eb7e39093c73b4d708	computing lexical contrast		Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not classify opposites into the different kinds, however. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and freezing. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly related. (For example, there exists the pair of opposites hot and cold such that tropical is related to hot, and freezing is related to cold.) We will call this the contrast hypothesis.We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we flesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a Roget-like thesaurus. We show how, using four different data sets, we evaluated our approach on two different tasks, solving “most contrasting word” questions and distinguishing synonyms from opposites. The results are analyzed across four parts of speech and across five different kinds of opposites. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods.	algorithm;amazon mechanical turk;automatic summarization;bilingual dictionary;compiler;computer performance;context-sensitive grammar;crowdsourcing;dialog system;download;experiment;information retrieval;jones calculus;lexical substitution;lexicon;machine translation;microsoft word for mac;natural language processing;pointwise mutual information;sudoku solving algorithms;synonym ring;the turk;thesaurus;word-sense disambiguation	Saif Mohammad;Bonnie J. Dorr;Graeme Hirst;Peter D. Turney	2013	Computational Linguistics	10.1162/COLI_a_00143	speech recognition;computer science;artificial intelligence	NLP	-26.559132131085644	-74.25283124897017	59965
b35a6a761d7b0d7a9dfd477f7ccb94fee1645f01	glàff, a large versatile french lexicon		This paper introduces GLÀFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLÀFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLÀFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLÀFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLÀFF to satisfy specific needs of various fields such as psycholinguistics.	dictionary;lexicon;natural language processing	Nabil Hathout;Franck Sajous;Basilio Calderone	2014				NLP	-29.629839486732756	-74.31800823121962	60026
2a79f870943ae32a01e7d2de46552b8696a0ea7c	a machine learning approach to identification and resolution of one-anaphora	british national corpus;written text;learning-based system;different use;informative domain;machine learning	We present a machine learning approach to identifying and resolving one-anaphora. In this approach, the system first learns to distinguish different uses of instances of the word one; in the second stage, the antecedents of those instances of one that are classified as anaphoric are then determined. We evaluated our approach on written texts drawn from the informative domains of the British National Corpus (BNC), and achieved encouraging results. To our knowledge, this is the first learningbased system for the identification and resolution of one-anaphora.	anaphora (linguistics);british national corpus;information;machine learning;refinement (computing)	Hwee Tou Ng;Yu Zhou;Robert Dale;Mary Gardiner	2005			natural language processing;digital library;computer science;artificial intelligence;data science;machine learning	AI	-25.53530576843271	-71.8578419520983	60040
e0e7ba7c05dac7a3c47cb6b35d5daca878506739	multichannel lstm-crf for named entity recognition in chinese social media		Named Entity Recognition (NER) is a tough task in Chinese social media due to a large portion of informal writings. Existing research uses only limited in-domain annotated data and achieves low performance. In this paper, we utilize both limited in-domain data and enough out-of-domain data using a domain adaptation method. We propose a multichannel LSTM-CRF model that employs different channels to capture general patterns, in-domain patterns and out-of-domain patterns in Chinese social media. The extensive experiments show that our model yields 9.8% improvement over previous state-of-the-art methods. We further find that a shared embedding layer is important and randomly initialized embeddings are better than the pretrained ones.	conditional random field;experiment;long short-term memory;named entity;randomness;sensitivity and specificity;social media	Chuanhai Dong;Huijia Wu;Jiajun Zhang;Chengqing Zong	2017		10.1007/978-3-319-69005-6_17	domain adaptation;social media;computer science;named-entity recognition;speech recognition;embedding;pattern recognition;artificial intelligence;communication channel	NLP	-19.21435712593263	-72.5229672172981	60049
36acbc5eda20dd7c20d3f20e312b9cc73d211ece	coreference-oriented interlingual slot structure & machine translation	interlingual mechanism;interlingual slot structure;correct pronominal anaphora generation;pronominal anaphora resolution;coreference-oriented interlingual slot structure;anaphora generation problem;mt system;machine translation;slot structure;ss store;english language;mechanism iss	One of the main problems of many commercial Machine Translation (MT) and experimental systems is that they do not carry out a correct pronominal anaphora generation. As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation. In this paper, we propose an Interlingual mechanism that we have called lnterlingual Slot Structure (ISS) based on Slot Structure (SS) presented in Ferrfindez et al. (1997). The SS stores the lexical, syntactic, morphologic and semantic information of every constituent of the grammar. The mechanism 1SS allows us to translate pronouns between different languages. In this paper, we have proposed and evaluated ISS for the translation between Spanish and English languages. We have compared pronominal anaphora resolution both in English and Spanish to accomplish a study of the existing discrepancies between two languages. This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem.	anaphora (linguistics);machine translation;statistical machine translation	Jesús Peral Cortés;Manuel Palomar;Antonio Ferrández Rodríguez	1999			natural language processing;speech recognition;transfer-based machine translation;computer science;linguistics;rule-based machine translation	NLP	-29.011440286111124	-72.24449972953752	60066
0b5e93bb7602929a7497488d5ed9107826f35793	a probabilistic rasch analysis of question answering evaluations.	probabilistic method;rasch model;rasch analysis;text retrieval;human language technology;question answer ing;model fitting;question answering	The field of Psychometrics routinely grapples with the question of what it means to measure the inherent ability of an organism to perform a given task, and for the last forty years, the field has increasingly relied on probabilistic methods such as the Rasch model for test construction and the analysis of test results. Because the underlying issues of measuring ability apply to human language technologies as well, such probabilistic methods can be advantageously applied to the evaluation of those technologies. To test this claim, Rasch measurement was applied to the results of 67 systems participating in the Question Answering track of the 2002 Text REtrieval Conference (TREC) competition. Satisfactory model fit was obtained, and the paper illustrates the theoretical and practical strengths of Rasch scaling for evaluating systems as well as questions. Most important, simulations indicate that a test invariant metric can be defined by carrying forward 20 to 50 equating questions, thus placing the yearly results on a common scale.	image scaling;language technology;question answering;rasch model;simulation;text retrieval conference	Rense Lange;Juan Moran;Warren R. Greiff;Lisa Ferro	2004			natural language processing;question answering;computer science;data mining;rasch model;information retrieval	NLP	-30.20533772227148	-71.79857574291938	60134
fb663a711e078ec0a04ddd055c03230f49bb5320	automatic linguistic annotation of historical language: totrtale and xix century slovene	tei encoded corpus;ocr transcription;digital library;historical text;xix century;highly-inflecting language;historical corpus;morphosyntactic tag;modern-day equivalent;full text search;historical language;automatic linguistic annotation	The paper describes a tool developed to process historical (Slovene) text, which annotates words in a TEI encoded corpus with their modern-day equivalents, morphosyntactic tags and lemmas. Such a tool is useful for developing historical corpora of highly-inflecting languages, enabling full text search in digital libraries of historical texts, for modernising such texts for today's readers and making it simpler to correct OCR transcriptions.	digital library;library (computing);optical character recognition;text encoding initiative;text corpus	Tomaz Erjavec	2011			natural language processing;computer science;linguistics	NLP	-31.565831705598868	-75.15917419373466	60251
13d66837dca7fb2d66434029704bad0d7c663f35	using semantic prototypes for discourse status classification	decision tree;machine learning	Discourse status is related to different aspects of entity mention in the discourse, such as whether they are first or subsequently mentioned and on what grounds. This paper presents the evaluation of semantic prototype as input feature for discourse status classification considering Decision Trees as machine learning algorithm. We show that the semantic prototypes improves classification of two specially difficult and scarce classes.		Sandra Collovini;Luiz Carlos Ribeiro;Patrícia Nunes Gonçalves;Vinicius M. Muller;Renata Vieira	2008		10.1007/978-3-540-85980-2_28	natural language processing;computer science;machine learning;pattern recognition	NLP	-25.41332961508485	-70.23335395790212	60305
80b95127684608356dc437ad7c3f18764b68e28b	term and collocation extraction by means of complex linguistic web services.	web service	We present a web service-based environment for the use of linguistic resources and tools to address issues of terminology and language varieties. We discuss the architecture, corpus representation formats, components and a chainer supporting the combination of tools into task-specific services. Integrated into this environment, single web services also become part of complex scenarios for web service use. Our web services take for example corpora of several million words as an input on which they perform preprocessing, such as tokenisation, tagging, lemmatisation and parsing, and corpus exploration, such as collocation extraction and corpus comparison. Here we present an example on extraction of single and multiword items typical of a specific domain or typical of a regional variety of German. We also give a critical review on needs and available functions from a user’s point of view. The work presented here is part of ongoing experimentation in the D-SPIN project, the German national counterpart of CLARIN.	collocation extraction;comparison of raster-to-vector conversion software;computation;lemmatisation;lexicography;parsing;preprocessor;spin;tag (metadata);text corpus;user interface;web service	Ulrich Heid;Fabienne Fritzinger;Erhard W. Hinrichs;Marie Hinrichs;Thomas Zastrow	2010			web service;web standards;computer science;social semantic web;data mining;semantic web stack;database;web intelligence;world wide web	NLP	-32.74946995553817	-72.22210953732352	60435
205a0613bdfad4b384c2a15138d921a275547f60	a spectral algorithm for learning class-based n-gram models of natural language		The Brown clustering algorithm (Brown et al., 1992) is widely used in natural language processing (NLP) to derive lexical representations that are then used to improve performance on various NLP problems. The algorithm assumes an underlying model that is essentially an HMM, with the restriction that each word in the vocabulary is emitted from a single state. A greedy, bottom-up method is then used to find the clustering; this method does not have a guarantee of finding the correct underlying clustering. In this paper we describe a new algorithm for clustering under the Brown et al. model. The method relies on two steps: first, the use of canonical correlation analysis to derive a low-dimensional representation of words; second, a bottom-up hierarchical clustering over these representations. We show that given a sufficient number of training examples sampled from the Brown et al. model, the method is guaranteed to recover the correct clustering. Experiments show that the method recovers clusters of comparable quality to the algorithm of Brown et al. (1992), but is an order of magnitude more efficient.	bottom-up parsing;bottom-up proteomics;brown clustering;cluster analysis;greedy algorithm;hidden markov model;hierarchical clustering;model checking;n-gram;natural language processing;vocabulary	Karl Stratos;Do-kyum Kim;Michael Collins;Daniel J. Hsu	2014				NLP	-20.328387566695067	-76.3733168831057	60444
4c991884c63469a71bedcb0b6391c0bef94e2aa0	unsupervised event coreference resolution with rich linguistic features	coreference resolution	This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both withinand cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.	baseline (configuration management);bayesian network;cluster analysis;experiment;transitive closure	Cosmin Adrian Bejan;Sanda M. Harabagiu	2010			natural language processing;computer science;pattern recognition;data mining	NLP	-20.028481534816706	-69.82283656102676	60565
7408b6807010c6df39b135d317e9978491c6f22a	lexicalizing linked data towards a human friendly web of data	grammar;compounds;metadata;natural languages;data mining;transforms;context	The consumption of Linked Data has dramatically increased with the increasing momentum towards semantic web. Linked data is essentially a very simplistic format for representation of knowledge in that all the knowledge is represented as triples which can be linked using one or more components from the triple. To date, most of the efforts has been towards either creating linked data by mining the web or making it available for users as a source of knowledgebase for knowledge engineering applications. In recent times there has been a growing need for these applications to interact with users in a natural language which required the transformation of the linked data knowledge into a natural language. The aim of the RealText project described in this paper, is to build a scalable framework to transform Linked Data into natural language by generating lexicalization patterns for triples. A lexicalization pattern is a syntactical pattern that will transform a given triple into a syntactically correct natural language sentence. Using DBpedia as the Linked Data resource, we have generated 283 accurate lexicalization patterns for a sample set of 25 ontology classes. We performed human evaluation on a test sub-sample with an inter-rater agreement of 0.86 and 0.80 for readability and accuracy respectively. This results showed that the lexicalization patterns generated language that are accurate, readable and emanates qualities of a human produced language. In a full automatic mode, it increased the accuracy by 91.2% compared to the Lemon lexicalization model.	dbpedia;expect;human-readable medium;inter-rater reliability;knowledge base;knowledge engineering;linked data;natural language;scalability;semantic web;synchronized multimedia integration language	Rivindu Perera;Parma Nand	2016	2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2016.0135	natural language processing;computer science;artificial intelligence;machine learning;data mining;grammar;database;natural language;programming language;metadata	AI	-30.87438131701221	-67.4577922364692	60621
f88d3657e4a492ea1179319c324295d8b6b38944	an upgrading sentivoice - a system for querying hotel service reviews via phone	perceptrons data mining hotel industry;speech;aspect extraction sentivoice hotel service reviews query opinion mining sentiment analysis computer sciences definite clause grammar dcg pattern rules gate jape perceptron algorithm with uneven margins technique paljm technique opinion summarization;intelligentsystems sentiment analysis opinion mining vietnamese hotel reviews sentivoice	Opinion mining or Sentiment analysis, although being a new research field, plays an important role in computer sciences and attracts interests of academia as well as industry. In this paper, an upgrading SentiVoice - system for querying hotel service reviews via phone [10] is described. Vietnamese questions understanding task was dealt by using Definite Clause Grammar (DCG). Aspects and opinion words, which are extracted, are based on some pattern rules thanks to GATE'S JAPE. Besides, Perceptron Algorithm with Uneven Margins (PAUM) technique was applied to classify an overall review comment as positive, negative or neutral. Experiments on aspect extraction, opinion summarization on aspects and the overall sentiment of the review are showed.	algorithm;application domain;computer science;definite clause grammar;experiment;gate;jape (software);knowledge base;logic programming;machine learning;perceptron;plausibility structure;sentiment analysis	Thien Khai Tran;Tuoi Thi Phan	2015	2015 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2015.7451545	natural language processing;speech recognition;speech;data science;machine learning;data mining;linguistics	DB	-22.730815461944108	-67.25609220857092	60678
d7c4d9917e74f71222b986019b6f2680d0e59b9c	semantic expectation-based causation knowledge extraction: a study on hong kong stock movement analysis	lenguaje natural;stock market;information extraction;attente;connaissance;langage naturel;knowledge extraction;semantics;tratamiento lenguaje;conocimiento;semantica;semantique;human behavior;relation causalite;extraccion;knowledge;causation relation;language processing;natural language;indexation;movement analysis;traitement langage;expectation;information financiere;natural language processing;south china;extraction;financial information	Human beings generally analyze information with some kinds of semantic expectations. This not only speeds up the processing time, it also helps to put the analysis in the correct context and perspective. To capitalize on this type of intelligent human behavior, this paper proposes a semantic expectation-based knowledge extraction methodology (SEKE) for extracting causation relations from text. In particular, we study the application of a causation semantic template on the Hong Kong Stock market movement (Hang Seng Index) with English financial news from Reuters, South China Morning Post and Hong Kong Standard. With one-month data input and over a two-month testing period, the system shows that it can correctly analyzes single reason sentences with about 76% precision and 74% recall rates. If partial reason extraction (two out of one reason) is included and weighted by a factor of 0.5, the performance is improved to about 83% and 81% respectively. As the proposed framework is language independent, we expect cross lingual knowledge extraction can work better with this semantic expectation-based framework.		Boon Toh Low;Ki Chan;Lei-Lei Choi;Man-Yee Chin;Sin-Ling Lay	2001		10.1007/3-540-45357-1_15	natural language processing;extraction;computer science;artificial intelligence;database;semantics;knowledge;knowledge extraction;natural language;human behavior;information extraction;algorithm	NLP	-29.305656749942116	-70.17300814072364	60896
1abfe950bb77ddb821cee88948bb8682630ac856	building a model of disease symptoms using text processing and learning from examples	user interfaces diseases learning by example medical computing pattern clustering semantic web statistical analysis text analysis;pragmatics;pattern clustering;semantics;text analysis;medical computing;learning by example;statistical analysis;statistical analysis disease symptoms text processing learning from examples semantic model text analysis text analyser user interface case clusterisation;semantic web;diseases;ontologies;humans;ontologies diseases buildings pragmatics semantics humans text analysis;user interfaces;buildings	The paper describes a methodology of building a semantic model of disease symptoms. The fundamental techniques used for creating the model are text analysis and learning from examples. The text analyser is used for extracting a set of symptom descriptions. The descriptions are a foundation for delivering a user interface, necessary for collecting patient cases. Given the cases a semantic model is built, which is achieved through clusterisation and statistical analysis of cases. The approach to creating the model eliminates the need of direct model manipulation, because the meaning is retrieved from association to diseases instead of purely linquistic interpretation of symptom descriptions. Detection of synonyms is also completely automatized.	user interface	Marek Jaszuk;Grazyna Szostek;Andrzej Walczak;Leszek Puzio	2011	2011 Federated Conference on Computer Science and Information Systems (FedCSIS)		natural language processing;computer science;ontology;machine learning;semantic web;data mining;database;semantics;linguistics;user interface;information retrieval;pragmatics	Vision	-32.83223230328667	-69.41586039944217	61015
37333b1ee942f5c47a815498ee6cdd28e649f848	asymmetric features of human generated translation		Distinct properties of translated text have been the subject of research in linguistics for many year (Baker, 1993). In recent years computational methods have been developed to empirically verify the linguistic theories about translated text (Baroni and Bernardini, 2006). While many characteristics of translated text are more apparent in comparison to the original text, most of the prior research has focused on monolingual features of translated and original text. The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level, rather than using monolingual statistics at the document level. We show that these bilingual features outperform the monolingual features used in prior work (Kurokawa et al., 2009) for the task of classifying translation direction.		Sauleh Eetemadi;Kristina Toutanova	2014			natural language processing;speech recognition;computer science;linguistics	NLP	-21.359155125542753	-72.87554532083793	61485
7713a5d2ef053e1627bf7b24630f7ba21042e491	interpolated dirichlet class language model for speech recognition incorporating long-distance n-grams		We propose a language modeling (LM) approach incorporating interpolated distanced n-grams in a Dirichlet class language model (DCLM) (Chien and Chueh, 2011) for speech recognition. The DCLM relaxes the bag-of-words assumption and documents topic extraction of latent Dirichlet allocation (LDA). The latent variable of DCLM reflects the class information of an n-gram event rather than the topic in LDA. The DCLM model uses default background n-grams where class information is extracted from the (n-1) history words through Dirichlet distribution in calculating n-gram probabilities. The model does not capture the long-range information from outside of the n-gram window that can improve the language modeling performance. In this paper, we present an interpolated DCLM (IDCLM) by using different distanced n-grams. Here, the class information is exploited from (n-1) history words through the Dirichlet distribution using interpolated distanced n-grams. A variational Bayesian procedure is introduced to estimate the IDCLM parameters. We carried out experiments on a continuous speech recognition (CSR) task using the Wall Street Journal (WSJ) corpus. The proposed approach shows significant perplexity and word error rate (WER) reductions over the other approach.	artificial neural network;bag-of-words model;calculus of variations;experiment;grams;interpolation;language model;latent dirichlet allocation;latent variable;n-gram;perplexity;speech recognition;the wall street journal;time complexity;variational principle;word error rate	Md. Akmal Haidar;Douglas D. O'Shaughnessy	2014			latent dirichlet allocation;natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.294226959077356	-75.5208188294561	61498
34208650510d97e6bfe0cc9ebe68576a8cbf3572	diversion of hierarchical phrases as reordering templates	information processing;machine translation	In the hierarchical phrase-based translation model (Chiang 2007), translation rules handle both contextsensitive translation and reordering of phrases at the same time. This simultaneity is strengths and weaknesses of the model. Although it enables the rules to be applied to the accurate and correct context, it deteriorates the applicability of the rules. In other words, the rules work very well in domains of training data, but they lost robustness in out of the domains. In this paper, we will try to improve the applicability of the original model by adding extra reordering templates which are separated out from hierarchical phrase translation rules. An original hierarchical phrase rule with two non-terminals is regarded as either monotone or swap reordering template according to if the two non-terminals in the source side have monotone or swap relation to the target side in the original rule. We will describe experiments in which the original model compares with our extensions in BLEU as a metric of translation quality using shared data at the NTCIR-7 patent translation task.	bleu;experiment;machine translation;paging;monotone	Mikio Yamamoto;Jyunya Norimatsu;Mitsuru Koshikawa;Takahiro Fukutomi;Taku Nishio;Kugatsu Sadamitsu;Takehito Utsuro;Masao Utiyama;Shunji Umetani;Tomomi Matsui	2008			natural language processing;speech recognition;transfer-based machine translation;computer science;rule-based machine translation;algorithm	NLP	-24.56433423366921	-76.35567346781562	61551
9332cec1c979cc6cd5e837891c5f0caa496e57df	english to hindi machine translation system in the context of homoeopathy literature		Over the years, researches in machine translation MT systems have gain momentum due to their widespread applicability. A number of systems have come up doing the task successfully for different language pairs. However, to the best of the authoru0027s knowledge, no significant work has been done in clinical and medical related domain especially in Homoeopathy. This paper describes a rule based English-Hindi MT system for Homoeopathic sentences. It has been designed to translate a variety of sentences from Homoeopathic literature. To achieve the task, the author developed English and Hindi Homoeopathic corpuses presently having the size 21096 and 23145 sentences respectively. For translation, the input sentences in English have been categorised in four different typeu0027s i.e. simple, complex, interrogative and ambiguous sentences. The authors tested the translation accuracy using BLEU score. At present, the overall Bleu score of the system is 0.7808 and the accuracy percentage is 82.25%.		Pramod P. Sukhadeve	2016	IJALR	10.4018/IJALR.2016010103	natural language processing;speech recognition	NLP	-28.206401524786617	-73.84056469548142	61590
de9238d71e24699aed3f78beaab720d0204cc3a2	lexical based two-way rte system at rte-5		The note describes the lexical based two-way Recognizing Textual Entailment (RTE) system developed at the Computer Science and Engineering Department, Jadavpur University, India. We participated in the two-way main task at RTE-5. The system is based on the composition of the following six lexical based RTE methods: WordNet based unigram match, bigram match, longest common sub-sequence, skip-gram, stemming and named entity matching. Each of these methods were applied on the development data to obtain two-way decisions. It was observed on the development data that final entailment decision on a text-hypothesis pair that is based on positive entailment decisions from at least two lexical based RTE methods was producing a better precision and recall figure. An accuracy figure of 58.17% was obtained on the test data. Ablation tests were performed for each of the six RTE methods and these are reported in the present note. The RTE task was based on three application settings: QA, IE and IR but this information was not taken into consideration during the system development. The relatively higher accuracy figures for the IR application setting obtained during the various tests suggest that identification of appropriate RTE methods based on the application settings might have improved the accuracy scores further.	bigram;computer science;iterative method;mathematical optimization;monte carlo method;n-gram;named entity;norm (social);precision and recall;significant figures;software quality assurance;stemming;test data;textual entailment;wordnet	Partha Pakray;Sivaji Bandyopadhyay;Alexander F. Gelbukh	2009			textual entailment;information retrieval;precision and recall;computer science and engineering;named entity;bigram;test data;computer science;wordnet	NLP	-22.99862108849234	-70.40971590071578	61723
3b265645e4656e529ad52a2233a71b37eb4e73f7	learning formulation and transformation rules for multilingual named entities	frequency-based approach;entity corpus;transformation rule;individual language;formulation rule;compound keyword;cross language information retrieval	This paper investigates three multilingual named entity corpora, including named people, named locations and named organizations. Frequency-based approaches with and without dictionary are proposed to extract formulation rules of named entities for individual languages, and transformation rules for mapping among languages. We consider the issues of abbreviation and compound keyword at a distance.	cross-language information retrieval;dictionary;image organizer;mined;named entity;phoneme;testbed;text corpus;tf–idf	Hsin-Hsi Chen;Changhua Yang;Ying Lin	2003			natural language processing;computer science;entity linking;linguistics;information retrieval	NLP	-28.00517983745972	-69.62265679525248	61918
2b03aaf95644057119cf18de494725f2d0165855	commoncow: massively huge web corpora from commoncrawl data and a method to distribute them freely under restrictive eu copyright laws		In this paper, I describe a method of creating massively huge web corpora from the CommonCrawl data sets and redistributing the resulting annotations in a stand-off format. Current EU (and especially German) copyright legislation categorically forbids the redistribution of downloaded material without express prior permission by the authors. Therefore, stand-off annotations or other derivates are the only format in which European researchers (like myself) are allowed to re-distribute the respective corpora. In order to make the full corpora available to the public despite such restrictions, the stand-off format presented here allows anybody to locally reconstruct the full corpora with the least possible computational effort. In Section 1., I briefly introduce the technology behind the COW project (Corpora from the Web), which is used to create the CommonCrawl-derived corpora. In Section 2., I provide some details about the resulting CommonCOW (COCO) web corpora. Finally, in Section 3., I introduce a method to circumvent restrictive EU copyright laws by distributing only the corpus annotations (under a CC-BY license) together with a tool that allows users to locally reconstruct the corpus from the annotations and the original CommonCrawl files.	text corpus;world wide web	Roland Schäfer	2016			world wide web;computer science	NLP	-33.486166495615635	-74.39538917267735	61963
e000e1ffd9aed78a825f56bcd76857ad64ff8bac	unsupervised classification of opinions		Opinion mining is gaining more interest thanks to the ever growing data available on the internet. This work proposes an unsupervised approach that clusters opinions in fine grain ranges. The approach is able to generate its own seed words for better applicability to the context and eliminating user input. Furthermore, we devise a computation strategy for the influence of valence shifters and negations on opinion words. The method is general enough to perform well while reducing subjectivity to a minimum.	barrel shifter;compiler;computation;iterative method;modern valence bond theory;natural language;performance;semi-supervised learning;unsupervised learning	Vlad Vasile Itu;Rodica Potolea;Mihaela Dînsoreanu	2016		10.5220/0006069903600366	the internet;sentiment analysis;data mining;computation;unsupervised learning;subjectivity;negation;machine learning;computer science;artificial intelligence	ML	-24.11559133931115	-68.37369369598777	61970
e34f328bc15160e258e89ad5de53a1346ed4774e	legal question answering using ranking svm and syntactic/semantic similarity		We describe a legal question answering system which combines legal information retrieval and textual entailment. We have evaluated our system using the data from the first competition on legal information extraction/entailment (COLIEE) 2014. The competition focuses on two aspects of legal information processing related to answering yes/no questions from Japanese legal bar exams. The shared task consists of two phases: legal ad hoc information retrieval and textual entailment. The first phase requires the identification of Japan civil law articles relevant to a legal bar exam query. We have implemented two unsupervised baseline models (tf-idf and Latent Dirichlet Allocation (LDA)-based Information Retrieval (IR)), and a supervised model, Ranking SVM, for the task. The features of the model are a set of words, and scores of an article based on the corresponding baseline models. The results show that the Ranking SVM model nearly doubles the Mean Average Precision compared with both baseline models. The second phase is to answer “Yes” or “No” to previously unseen queries, by comparing the meanings of queries with relevant articles. The features used for phase two are syntactic/semantic similarities and identification of negation/antonym relations. The results show that our method, combined with rule-based model and the unsupervised model, outperforms the SVM-based supervised model.	question answering;ranking svm;semantic similarity	Mi-Young Kim;Ying Xu;Randy Goebel	2014		10.1007/978-3-662-48119-6_18	semantic similarity	NLP	-25.87433295030703	-69.08762020009972	62112
7f81112a92f904072fa06843a4dc6cae347efec3	modeling syntactic and semantic structures in hierarchical phrase-based translation		Incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. In this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy.	hierarchical database model	Junhui Li;Philip Resnik;Hal Daumé	2013			natural language processing;noun phrase;syntactic predicate;determiner phrase;linguistics;rule-based machine translation	NLP	-23.83020184579274	-75.4833148893831	62137
80614745be9ba6800ac46f8b189bbd86f5a0ad10	doing more with named entities - turning text into a linked data hub			data hub;entity;linked data;named-entity recognition;usb hub	Theo van Veen;Michel Koppelaar	2013		10.1007/978-3-319-08425-1_16	world wide web;linked data;data mining;computer science	NLP	-31.533395749577906	-76.46504873432154	62154
b3adbc9251063532aea1abe051f6e26a3d3549a7	towards unl-based machine translation for moroccan amazigh language			machine translation;universal networking language	Imane Taghbalout;Fadoua Ataa Allah;Mohamed El Marraki	2018	IJCSE	10.1504/IJCSE.2016.10009693		NLP	-30.629764304601345	-78.03641293226602	62257
c66865ccf336037dfc10f6aebbbc0beb0df9dad2	reordering grammar induction		We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation. Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora. Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.	grammar induction;latent variable;machine translation;parallel text;rsa problem;stochastic context-free grammar;text corpus;transduction (machine learning);treebank	Milos Stanojevic;Khalil Sima'an	2015			permutation;bracketing;computer science;machine translation;artificial intelligence;machine learning;branching (version control);grammar induction;phrase;latent variable	NLP	-20.295187448762828	-75.88398595716878	62258
ffaea9027868147b06cfdfd4a79403e88bef49d9	collective semantic role labeling on open news corpus by leveraging redundancy	semantic role labeling	We propose a novel MLN-based method that collectively conducts SRL on groups of news sentences. Our method is built upon a baseline SRL, which uses no parsers and leverages redundancy. We evaluate our method on a manually labeled news corpus and demonstrate that news redundancy significantly improves the performance of the baseline, e.g., it improves the F-score from 64.13% to 67.66%. *	baseline (configuration management);f1 score;parsing;redundancy (engineering);semantic role labeling;text corpus	Xiaohua Liu;Kuanyu Li;Bo Han;Ming Zhou;Long Jiang;Daniel Tse;Zhongyang Xiong	2010			natural language processing;semantic role labeling;computer science;linguistics;world wide web;information retrieval	NLP	-22.173784360569258	-72.92319391913153	62520
b725f1de252b24aedc4806b06ea6b4b062f34c95	name origin recognition using maximum entropy model and diverse features		Name origin recognition is to identify the source language of a personal or location name. Some early work used either rulebased or statistical methods with single knowledge source. In this paper, we cast the name origin recognition as a multi-class classification problem and approach the problem using Maximum Entropy method. In doing so, we investigate the use of different features, including phonetic rules, ngram statistics and character position information for name origin recognition. Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44% for names written in English and 98.10% for names written in Chinese, which are significantly and consistently better than those in reported work.	database;multiclass classification;multinomial logistic regression;n-gram;principle of maximum entropy	Min Zhang;Chengjie Sun;Haizhou Li;AiTi Aw;Chew Lim Tan;Xiaolong Wang	2008			computer science;principle of maximum entropy;artificial intelligence;pattern recognition;personal name	NLP	-20.212824523254074	-69.2642486580692	62567
6a33dc51d1561ec9b78eaf92fc9baf643c46c26e	irony detection: from the twittersphere to the news space		English. Automatic detection of irony is one of the hot topics for sentiment analysis, as it changes the polarity of text. Most of the work has been focused on the detection of figurative language in Twitter data due to relative ease of obtaining annotated data, thanks to the use of hashtags to signal irony. However, irony is present generally in natural language conversations and in particular in online public fora. In this paper, we present a comparative evaluation of irony detection from Italian news fora and Twitter posts. Since irony is not a very frequent phenomenon, its automatic detection suffers from data imbalance and feature sparseness problems. We experiment with different representations of text – bag-of-words, writing style, and word embeddings to address the feature sparseness; and balancing techniques to address the data imbalance. Italiano. Il rilevamento automatico di ironia è uno degli argomenti più interessanti in sentiment analysis, poiché modifica la polarità del testo. La maggior parte degli studi si sono concentrati sulla rilevazione del linguaggio figurativo nei dati di Twitter per la relativa facilità nell’ottenere dati annotati con gli hashtags per segnalare l’ironia. Tuttavia, l’ironia è un fenomeno che si trova nelle conversazioni umane in generale e in particolare nei forum online. In questo lavoro presentiamo una valutazione comparativa sul rilevamento dell’ironia in blogs giornalistici e conversazioni su Twitter. Poiché l’ironia non è un fenomeno molto frequente, il suo rilevamento automatico risente di problemi di mancanza di bilanciamento nei dati e feature sparseness. Per ovviare alla feature sparseness proponiamo esperimenti con diverse rappresentazioni del testo – bag-of-words, stile di scrittura e word embeddings; per ovviare alla mancanza di bilanciamento nei dati utilizziamo invece tecniche di bilanciamento.	bag-of-words model;blog;downstream (software development);experiment;hashtag;information;irony;linear algebra;naruto shippuden: clash of ninja revolution 3;natural language;neural coding;sensor;sentiment analysis;stylometry;text corpus;unique name assumption;uno;word embedding	Alessandra Cervone;Evgeny A. Stepanov;Fabio Celli;Giuseppe Riccardi	2017			literature;irony;computer science	NLP	-21.808328760850916	-68.0384058117631	62665
5bf3bdb7f3e1eb20a520ba50f2d83caef535a0be	measuring readability of polish texts: baseline experiments		Measuring readability of a text is the first sensible step to its simplification. In this paper we present an overview of the most common approaches to automatic measuring of readability. Of the described ones, we implemented and evaluated: Gunning FOG index, Flesch-based Pisarek method. We also present two other approaches. The first one is based on measuring distributional lexical similarity of a target text and comparing it to reference texts. In the second one, we propose a novel method for automation of Taylor test – which, in its base form, requires performing a large amount of surveys. The automation of Taylor test is performed using a technique called statistical language modelling. We have developed a free on-line web-based system and constructed plugins for the most common text editors, namely Microsoft Word and OpenOffice.org. Inner workings of the system are described in detail. Finally, extensive evaluations are performed for Polish – a Slavic, highly inflected language. We show that Pisarek’s method is highly correlated to Gunning FOG Index, even if different in form, and that both the similarity-based approach and automated Taylor test achieve high accuracy. Merits of using either of them are discussed.	baseline (configuration management);experiment;gunning transceiver logic;language model;level of detail;microsoft word for mac;online and offline;plug-in (computing);reference work;text editor;web application	Bartosz Broda;Bartlomiej Niton;Wlodzimierz Gruszczynski;Maciej Ogrodniczuk	2014			computer science;world wide web;information retrieval	NLP	-32.11422910187199	-74.38128004088433	62730
faede83a6073a0fd3db9cfd9701b3f3b84e720c6	parallel corpora for wordnet construction: machine translation vs. automatic sense tagging	lexical resources;wordnet;automatic sense tagging;parallel corpora;machine translation	In this paper we present a methodology for WordNet construction based on the exploitation of parallel corpora with semantic annotation of the English source text. We are using this methodology for the enlargement of the Spanish and Catalan versions of WordNet 3.0, but the methodology can also be used for other languages. As big parallel corpora with semantic annotation are not usually available, we explore two strategies to overcome this problem: to use monolingual sense tagged corpora and machine translation, on the one hand; and to use parallel corpora and automatic sense tagging on the source text, on the other. With these resources, the problem of acquiring a WordNet from parallel corpora can be seen as a word alignment task. Fortunately, this task is well known, and some aligning algorithms are freely available.	algorithm;bitext word alignment;machine translation;parallel text;part-of-speech tagging;sequence alignment;text corpus;wordnet	Antoni Oliver;Salvador Climent	2012		10.1007/978-3-642-28601-8_10	natural language processing;wordnet;extended wordnet;computer science;linguistics;machine translation;information retrieval	NLP	-29.29730009961639	-73.23791964972276	62804
5119941e015164d83b34421e889087a13f026976	vers une modélisation statistique multi-niveau du langage, application aux langues peu dotées. (toward a multi-level statistical language modeling for under-resourced language)		This PhD thesis focuses on the problems encountered when developing automatic speech recognition for under-resourced languages with a writing system without explicit separation between words. The specificity of the languages covered in our work requires automatic segmentation of text corpus into words in order to make the n-gram language modeling applicable. While the lack of text data has an impact on the performance of language model, the errors introduced by automatic segmentation can make these data even less usable. To deal with these problems, our research focuses primarily on language modeling, and in particular the choice of lexical and sub-lexical units, used by the recognition systems. We investigate the use of multiple units in speech recognition system. At language models level, the models are trained with hybrid vocabularies created using both the lexical and the sub-lexical unit. At the system output level, we try to combine the outputs of several recognition systems. Each system is based on a different modeling unit : lexical or sub-lexical. To better exploit the textual data using different views on the same data, we propose a method that performs multiple segmentations on the training corpus instead of a conventional single segmentation. This method based on finite state machines allows generating all possible segmentations from a sequence of characters and then we can extract n-grams to train the language model. It allows finding the n-grams not found by unique segmentation method and adding new n-grams in the language model. We validate these modeling approaches based on multiple units in recognition systems for a group of languages : Khmer, Vietnamese, Thai and Laotian.	finite-state machine;grams;http 404;language model;lexicon;n-gram;sensitivity and specificity;speech recognition;text corpus;vocabulary	Sopheap Seng	2010				NLP	-25.69224270649661	-78.12940798039044	62805
7557af386dc7265c1fa7609e358bee6109d461ff	automatic extraction of briefing templates		An approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps, namely, extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates. In this paper, we describe two novel unsupervised approaches for extracting briefing templates from human written reports. Since the problem is non-standard, we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies.	rouge (metric)	Dipanjan Das;Mohit Kumar;Alexander I. Rudnicky	2008			artificial intelligence;pattern recognition;computer science;data mining;template	NLP	-24.938554280497453	-70.3914163433495	62881
501b729c2831d828f220eb9565561419977e70b8	a lexical analyzer for html and basic sgml			html;lexical analysis;standard generalized markup language	Dan Connolly	1996	World Wide Web Journal		computer science;lexical analysis;database;sgml	ML	-30.666590335860224	-78.29252794217994	62902
e7cc7a4d2cb1eda8a0a68466195629a78fb3c29b	manatee/bonito - a modular corpus manager	corpus manager;corpus	A corpus is a large collection of texts in electronic form. Corpus managers are tools or sets of tools for coping with corpora. They can encode, query, and visualise texts. This paper describes widely used corpus manager Manatee that has many unique features: modular design, dynamic attributes, multi-values, multi-language support, almost unlimited corpus size and others. The second part of the paper presents Bonito – the graphical user interface of theManatee system.Other extensions of the system are alsomentioned.	corpus manager;encode;graphical user interface;modular design;text corpus	Pavel Rychlý	2007			natural language processing;speech recognition;computer science;text corpus;communication	NLP	-28.77420801339193	-79.91342694577719	62951
466f1d7733f971eeca40f2b8fe387909a8da325e	unsupervised parts-of-speech induction for bengali.	interaction network;complex network;gold standard;part of speech	We present a study of the word interaction networks of Bengali in the framework of complex networks. The topological properties of these networks reveal interesting insights into the morpho-syntax of the language, whereas clustering helps in the induction of the natural word classes leading to a principled way of designing POS tagsets. We compare different network construction techniques and clustering algorithms based on the cohesiveness of the word clusters. Cohesiveness is measured against two gold-standard tagsets by means of the novel metric of tag-entropy. The approach presented here is a generic one that can be easily extended to any language.	algorithm;cluster analysis;cohesion (computer science);complex network;group cohesiveness;interaction network;unsupervised learning	Joy Deep Nath;Monojit Choudhury;Animesh Mukherjee;Christian Biemann;Niloy Ganguly	2008			speech recognition;part of speech;natural language processing;complex network;cluster analysis;bengali;machine learning;computer science;group cohesiveness;interaction network;artificial intelligence	ML	-25.08251490403077	-73.05215261208163	62970
e8b60ca7d20a4d04c06d444470a5ab9b195ef977	learning improved reordering models for urdu, farsi and italian using smt		This paper presents some experiments which have been carried out as part of a shared task for the workshop “Reordering for Statistical Machine Translation” (RSMT, collocated with COLING 2012). The shared task objective is to learn reordering models by making use of a manually word-aligned, bilingual parallel data. We view this task as that of a statistical machine translation (SMT) system which implicitly employ such models. These models are obtained using empirical methods and machine learning techniques. We have therefore used “Moses”; a state of the art SMT system to conduct experiments for the task at hand. The training and the development datasets used for the experiments have been provided by RSMT and we report our work on three pair of languages namely English-Urdu, EnglishFarsi and EnglishItalian.	data structure alignment;experiment;machine learning;moses;statistical machine translation	Rohit Gupta;Raj Nath Patel;Ritesh Shah	2012			natural language processing;speech recognition;computer science;machine learning	NLP	-22.3675698384665	-75.10858517993765	62990
24b79c8f36133a9c71d6c2eefab8f46535632157	language-independent probabilistic answer ranking for question answering	answer sets;perforation;question answering	This paper presents a language-independent probabilistic answer ranking framework for question answering. The framework estimates the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NTCIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection.	answer set programming;baseline (configuration management);language-independent specification;ontology (information science);question answering;relevance;software quality assurance;text retrieval conference;text corpus	Jeongwoo Ko;Teruko Mitamura;Eric Nyberg	2007			natural language processing;question answering;computer science;data mining;information retrieval	NLP	-28.081048160839067	-67.10849124775642	62995
73cb6c929026bf5475602bad3fdde033da3c86c9	partial parsing: a report on work in progress	work in progress;experience design	This paper reports a handful of experiments designed to test the feasibility of applying well-known partial parsing techniques to the problem of automatic data base update from an open-ended source of messages, and the feasiblity of automatically learning semantic knowledge from annotated examples. The challenges arise from the incompleteness of any lexicon, sentences that average over 20 words in length, and the lack of a complete semantics.	database;experiment;lexicon;nonlinear gameplay;parsing	Ralph M. Weischedel;Damaris M. Ayuso;Robert J. Bobrow;Sean Boisen;Robert Ingria;Jeff Palmucci	1991			experience design;computer science;knowledge management;work in process	NLP	-28.029171393884695	-72.71409376738353	63040
7ed8c7efe979fd523a9d6c7e8ac8adf03a7893d6	separating language dependent and independent tasks for the semantic transformation of numerical programs.				Jean Utke;Uwe Naumann	2004			natural language processing;computer science;linguistics;communication	PL	-30.333375310763305	-78.78565675356866	63073
3770cef5e321b31eaee71b82f2ad84ce260f3140	learning vocabularies over a fine quantization	vocabulary;feature track;image retrieval	A novel similarity measure for bag-of-words type large scale image retrieval is presented. The similarity function is learned in an unsupervised manner, requires no extra space over the standard bag-of-words method and is more discriminative than both L2-based soft assignment and Hamming embedding. The novel similarity function achieves mean average precision that is superior to any result published in the literature on the standard Oxford 5k, Oxford 105k and Paris datasets/protocols. We study the effect of a fine quantization and very large vocabularies (up to 64 million words) and show that the performance of specific object retrieval increases with the size of the vocabulary. This observation is in contradiction with previously published results. We further demonstrate that the large vocabularies increase the speed of the tf-idf scoring step.	bag-of-words model in computer vision;heaps' law;image retrieval;information retrieval;oxford spelling;quantization (signal processing);similarity measure;tf–idf;unsupervised learning;vocabulary;window function	Andrej Mikulík;Michal Perdoch;Ondrej Chum;Jiri Matas	2012	International Journal of Computer Vision	10.1007/s11263-012-0600-1	speech recognition;image retrieval;computer science;data mining;information retrieval	Vision	-22.616602597196945	-73.27667449535767	63080
3a0702967f9a28a54151bde328873260723a11b9	does it have to be trees?: data-driven dependency parsing with incomplete and noisy training data		We present a novel approach to training data-driven dependency parsers on incomplete annotations. Our parsers are simple modifications of two well-known dependency parsers, the transition-based Malt parser and the graph-based MST parser. While previous work on parsing with incomplete data has typically couched the task in frameworks of unsupervised or semi-supervised machine learning, we essentially treat it as a supervised problem. In particular, we propose what we call agnostic parsers which hide all fragmentation in the training data from their supervised components. We present experimental results with training data that was obtained by means of annotation projection. Annotation projection is a resource-lean technique which allows us to transfer annotations from one language to another within a parallel corpus. However, the output tends to be noisy and incomplete due to cross-lingual non-parallelism and error-prone word alignments. This makes the projected annotations a suitable test bed for our fragment parsers. Our results show that (i) dependency parsers trained on large amounts of projected annotations achieve higher accuracy than the direct projections, and that (ii) our agnostic fragment parsers perform roughly on a par with the original parsers which are trained only on strictly filtered, complete trees. Finally, (iii) when our fragment parsers are trained on artificially fragmented but otherwise gold standard dependencies, the performance loss is moderate even with up to 50% of all edges removed.	cognitive dimensions of notations;fragmentation (computing);machine learning;parallel computing;parallel text;parsing;semi-supervised learning;semiconductor industry;supervised learning;testbed;unsupervised learning	Kathrin Spreyer	2011				NLP	-20.782014841164862	-75.55173988991122	63115
bd90e2b1d0c56a1433df145f6f032062f04b28fe	a robust pos tagger using multiple taggers and lexical knowledge			brill tagger;part-of-speech tagging	R. Mahesh K. Sinha	2010			natural language processing;artificial intelligence;pattern recognition;computer science	NLP	-25.361063374535842	-76.94916866405023	63263
4856e4a2ff64ea8f7204b0359d00550d4a648c64	an author verification approach based on differential features: notebook for pan at clef 2015		We describe the approach that we submitted to the 2015 PAN competition [7] for the author identification task. The task consists in determining if an unknown document was authored by the same author of a set of documents with the same author. We propose a machine learning approach based on a number of different features that characterize documents from widely different points of view. We construct non-overlapping groups of homogeneous features, use a random forest regressor for each features group, and combine the output of all regressors by their arithmetic mean. We train a different regressor for each language. Our approach achieved the first position in the final rank for the Spanish language.	machine learning;random forest	Alberto Bartoli;Alex Dagri;Andrea De Lorenzo;Eric Medvet;Fabiano Tarlao	2015			random forest;homogeneous;speech recognition;clef;arithmetic mean;computer science	NLP	-22.458206822921877	-69.61262860980142	63329
def9fc382a1f45dcc525a3d04a9deea5752580bd	source-target mapping model of streaming data flow for machine translation		Streaming information flow allows identification of linguistic similarities between language pairs in real time as it relies on pattern recognition of grammar rules, semantics and pronunciation especially when analyzing so called international terms, syntax of the language family as well as tenses transitivity between the languages. Overall, it provides a backbone translation knowledge for building automatic translation system that facilitates processing any of various abstract entities which combine to specify underlying phonological, morphological, semantic and syntactic properties of linguistic forms and that act as the targets of linguistic rules and operations in a source language following professional human translator. Streaming data flow is a process of mining source data into target language transformation during which any inference impedes the system effectiveness by producing incorrect translation. We address a research problem of exploring streaming data from source-target parallels for detection of linguistic similarities between languages originated from different groups.	compiler;dataflow;entity;internet backbone;machine translation;noisy-channel coding theorem;parallels desktop for mac;pattern recognition;source data;stream (computing);streaming media;vertex-transitive graph	Jolanta Mizera-Pietraszko;Grzegorz Kolaczek;Ricardo Rodriguez Jorge	2017	2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)	10.1109/INISTA.2017.8001209	universal networking language;computer-assisted translation;machine translation;deep linguistic processing;direct method;natural language processing;semantics;transfer-based machine translation;rule-based machine translation;computer science;artificial intelligence	SE	-28.60269595679014	-76.91467796315382	63343
4d47cc69c22a703cc0c3660db3273b123cf6db84	japanese zero reference resolution considering exophora and author/reader mentions		In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.	baseline (configuration management);categorization;entity;experiment;feature vector;lexico;machine learning	Masatsugu Hangyo;Daisuke Kawahara;Sadao Kurohashi	2013			computer science;artificial intelligence;information retrieval;algorithm	NLP	-23.97313052261243	-67.28923043736143	63411
0f8b52e54b7aca44360abbf3d87f9f60d39a000c	unknown word extraction for chinese documents	low frequency;statistical significance;statistical method	There is no blank to mark word boundaries in Chinese text. As a result, identifying words is difficult, because of segmentation ambiguities and occurrences of unknown words. Most previous works focus their attention only on the resolution of ambiguous segmentation. The problem of unknown word identification is considered more difficult and needs further investigation. Conventionally unknown words were extracted by statistical methods for statistical methods are simple and efficient. However the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall. Because character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistic methods. In addition to statistical information, we try to use as much information as possible, such as morphology, syntax, semantics, and world knowledge. The identification system fully utilizes the context and content information of unknown words in the steps of detection process, extraction process, and verification process. A practical unknown word extraction system was implemented which online identifies new words, including low frequency new words, with high precision and high recall rates.	commonsense knowledge (artificial intelligence);mathematical morphology;microsoft word for mac;protologism	Keh-Jiann Chen;Wei-Yun Ma	2002		10.3115/1072228.1072277	natural language processing;statistical semantics;speech recognition;computer science;pattern recognition;statistical significance;low frequency;stop words	NLP	-25.38994058825468	-77.11785206361769	63417
42589c82f82ac9ab328a86ad4006391070118cfa	semantic role labeling of english tweets		Semantic role labeling (SRL) is a task of defining the conceptual role to the arguments of predicate in a sentence. This is an important task for a wide range of tweet related applications associated with semantic information extraction. SRL is a challenging task due to the difficulties regarding general semantic roles for all predicates. It is more challenging for Social Media Text (SMT) where the nature of text is more casual. This paper presents an automatic SRL system for English tweets based on Sequential Minimal Optimization (SMO) algorithm. Proposed system is evaluated through experiments and reports comparable performance with the prior state-of-the art SRL system.	semantic role labeling	Dwijen Rudrapal;Amitava Das	2018	Computación y Sistemas		information extraction;artificial intelligence;predicate (grammar);natural language processing;semantic role labeling;sequential minimal optimization;social media;computer science;casual;sentence	NLP	-21.774752913452836	-71.13367148642	63425
0c421abc0c1055886855cecf2a287dab70963d16	smoothing fine-grained pcfg lexicons	universiteitsbibliotheek	We present an approach for smoothing treebank-PCFG lexicons by interpolating treebank lexical parameter estimates with estimates obtained from unannotated data via the Inside-outside algorithm. The PCFG has complex lexical categories, making relative-frequency estimates from a treebank very sparse. This kind of smoothing for complex lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank.	inside–outside algorithm;interpolation;lexicon;parsing;smoothing;sparse matrix;stochastic context-free grammar;treebank	Tejaswini Deoskar;Mats Rooth;Khalil Sima'an	2009		10.3115/1697236.1697278	natural language processing;computer science;treebank;pattern recognition;linguistics	NLP	-21.249457238309393	-75.20123978971364	63431
584085cf7d9ffbad7b532bea6f30c3c48fd3b372	using multiple combined ranker for answering definitional questions	related terms extraction;term extraction;multiple combined ranker;definitional question answering;question answering	This paper presents a Multiple Combined Ranker (MCR) approach for answering definitional questions. Generally, our MCR approach first extracts question target-related knowledge as much as possible, then using this knowledge to pick up appropriate question answers. The knowledge includes both online definitions and related terms (RT). In our system, extraction of related terms is different from traditional methods which are largely based on calculating the co-occurred frequency of target words. We adopted the significance of sentences and documents, from which RT were extracted. The MCR approach shows state-in-art performance in handling with increasingly complex definitional questions.	definition;entity;memory card reader;question answering;software quality assurance	Junkuo Cao;Lide Wu;Xuanjing Huang;Yaqian Zhou;Fei Liu	2008		10.1007/978-3-540-68636-1_46	natural language processing;question answering;computer science;data mining;information retrieval	AI	-27.700076239993813	-68.59796208248038	63527
7b51c183da0e7de6757a269d83dd9afad695ac3d	heideltime: tuning english and developing spanish resources for tempeval-3		In this paper, we describe our participation in the TempEval-3 challenge. With our multilingual temporal tagger HeidelTime, we addressed task A, the extraction and normalization of temporal expressions for English and Spanish. Exploiting HeidelTime’s strict separation between source code and languagedependent parts, we tuned HeidelTime’s existing English resources and developed new Spanish resources. For both languages, we achieved the best results among all participants for task A, the combination of extraction and normalization. Both the improved English and the new Spanish resources are publicly available with HeidelTime.	brill tagger;database normalization;display resolution;temporal expressions	Jannik Strötgen;Julian Zell;Michael Gertz	2013			natural language processing;speech recognition;computer science;data mining	NLP	-26.365485644393363	-74.47512138072172	63679
6a29e049f298b223e8c982a8cbbde283142b932d	dependency-based answer validation for german		This article describes the Heidelberg contribution to the CLEF 2011 QA4MRE task for German. We focus on the objective of not using any external resources, building a system that represents questions, answers and texts as formulae in propositional logic derived from dependency structure. Background knowledge is extracted from the background corpora using several knowledge extraction strategies. We answer questions by attempting to infer answers from the test documents complemented by background knowledge, with a distance measure as fall-back. The main challenge is to specify the translation from dependency structure into a logical representation. For this step, we suggest different rule sets and evaluate various configuration parameters that tune accuracy and coverage. All of runs exceed a random baseline, but show different coverage/accuracy profiles (accuracy up to 44%, coverage up to 65%).	baseline (configuration management);causal filter;dependency grammar;knowledge acquisition;knowledge representation and reasoning;literal (mathematical logic);map;propositional calculus;text corpus;text normalization	Svitlana Babych;Alexander Henn;Jan Pawellek;Sebastian Padó	2011			artificial intelligence;data mining;mathematics;algorithm	NLP	-27.441598204011463	-71.4762758915945	63700
22fbe03429d8ef3bbab16c3e717e969dbeb7c048	conditional maximum likelihood estimation for improving annotation performance of n-gram models incorporating stochastic finite state grammars		Language models that combine stochastic grammars and N-grams are often used in speech recognition and language understanding systems. One useful aspect of these models is that they can be used to annotate phrases in the text with their constituent grammars; such annotation often plays an important role in subsequent processing of the text. In this paper we present an estimation procedure, under a conditional maximum likelihood objective, that aims at improving the annotation performance of these models over their maximum likelihood estimate. The estimation is carried out using the extended Baum-Welch procedure of Gopalakrishnan et.al. We find that with conditional maximum likelihood estimation the annotation accuracy of the language models can be improved by over 7% relative to their maximum likelihood estimation.	baum–welch algorithm;grams;language model;n-gram;natural language understanding;speech recognition;stochastic grammar;welch's method	Vaibhava Goel	2004			artificial intelligence;n-gram;computer science;likelihood function;language model;machine learning;pattern recognition;expectation–maximization algorithm;maximum likelihood sequence estimation;conditional variance;rule-based machine translation;annotation	NLP	-23.961681616240902	-78.78892875494819	63708
d693af8f74e8df591788fdd185591c759b1cf4cc	investigating the consistency of emoji sentiment lexicons constructed using diferent languages		Emojis have been widely used in recent text-based communications and can be important features for sentiment analysis of social media posts such as tweets. Our previous work presented a method for automatically constructing an emoji sentiment lexicon based on the co-occurrence frequency between sentiment words and emojis. This paper investigates whether the proposed framework can be valid over different languages. To conduct this study, we constructed two datasets comprising 150,000 tweets written in English and Japanese and applied a sentiment dictionary to each dataset separately. The results demonstrated that the two constructed lexicons were almost consistent, while the sentiments of some emojis were differently interpreted depending on the language. We also showed the reasonable performance of tweet sentiment analysis using each emoji sentiment lexicon.		Mayu Kimura;Marie Katsurai	2018		10.1145/3282373.3282417	natural language processing;data mining;sentiment analysis;emoji;computer science;artificial intelligence;lexicon;social media	NLP	-20.372573422676815	-68.72244740578114	63999
b09b2f74cb46096945d1306a192ceb1f25d86433	precise information extraction from text based on two-level concept lattice	content extraction;content concept lattice;content concept lattice information extraction structure concept lattice;text;complexity theory;web pages;lattices;information extraction;time complexity;information retrieval;vocabulary;text analysis;natural languages;data mining;two level concept lattice;text analysis information retrieval;concept lattice;information processing;performance analysis;logical storage;data mining lattices web pages costs performance analysis natural languages information analysis information processing computer science natural language processing;computer science;semantic structure;information analysis;natural language processing;structure concept lattice;content extraction information extraction text two level concept lattice logical storage semantic structure	Aiming at the problems of high cost of time consumption and low precision of IE (information extraction) in IE systems based on concept lattice, a novel mechanism of two-level concept lattice-based IE is put forward. The structure concept lattice is the logical storage of semantic structure of documents, and the content concept lattice is used to store content information of documents. The paper gives the formal descriptions of structure and content concept lattice and analyses the time complexity of them. The consequence shows our method has obvious advantages concerning the time consumption and the precision of content extraction compared with existing IE methods.	formal concept analysis;information extraction;time complexity	Zhaoman Zhong;Zong-tian Liu;Yan Guan	2008	2008 International Symposiums on Information Processing	10.1109/ISIP.2008.40	natural language processing;computer science;theoretical computer science;lattice miner;information retrieval	DB	-28.59280010742691	-66.28769099817076	64070
4aadf030f9fa316f041d00a90d74b2c553772d29	data selection and smoothing in an open-source system for the 2008 nist machine translation evaluation	machine translation	This paper gives a detailed description of a statistical machine translation system developed for the 2008 NIST open MT evaluation. The system is based on the open source toolkit Moses with extensions for language model rescoring in a second pass. Significant improvements were obtained with data selection methods for the language and translation model. An improvement of more than 1 point BLEU on the test set was achieved by a continuous space language model which performs the probability estimation with a neural network. The described system has achieved a very good ranking in the 2008 NIST open MT evaluation.	artificial neural network;bleu;language model;moses;open-source software;smoothing;statistical machine translation;test set	Holger Schwenk;Yannick Estève	2008			rouge;machine translation;bleu;speech recognition;artificial intelligence;pattern recognition;smoothing;language model;nist;artificial neural network;computer science;test set	NLP	-21.04267470718487	-77.6738840490466	64097
95ba5aa968cf1a0af5dd0f409906f9dfad68bb76	netease automatic chinese word segmentation		This document analyses the bakeoff results from NetEase Co. in the SIGHAN5 Word Segmentation Task and Named Entity Recognition Task. The NetEase WS system is designed to facilitate research in natural language processing and information retrieval. It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination. Evaluation result shows our WS system has a passable precision in word segmentation except for the unknown words recognition.	information retrieval;named-entity recognition;natural language processing;part-of-speech tagging;text segmentation	Xin Li;Shuaixiang Dai	2006			artificial intelligence;computer science;natural language processing;part-of-speech tagging;named-entity recognition;text segmentation;phrase	NLP	-26.265040561350514	-76.82782426256787	64342
8a6c096affd183c45fa8925fba900a7eb1d92c12	information flow analysis with chinese text	lexical semantics;high dimensionality;average precision;conceptual space;hyperspace analogue to language;information flow;chinese word segmentation;information processing;next generation;query expansion	This article investigates the effectiveness of an information inference mechanism on Chinese text. The information inference derives implicit associations via computation of information flow on a high dimensional conceptual space, which is approximated by a cognitively motivated lexical semantic space model, namely Hyperspace Analogue to Language (HAL). A dictionary-based Chinese word segmentation system was used to segment words. To evaluate the Chinese-based information flow model, it is applied to query expansion, in which a set of test queries are expanded automatically via information flow computations and documents are retrieved. Standard recall-precision measures are used to measure performance. Experimental results for TREC-5 Chinese queries and People Daily’s corpus suggest that the Chinese information flow model significantly increases average precision, though the increase is not as high as those achieved using English corpus. Nevertheless, there is justification to believe that the HALbased information flow model, and in turn our psychologistic stance on the next generation of information processing systems, have a promising degree of language independence.	approximation algorithm;computation;dictionary;hal;information flow (information theory);information processing;information retrieval;information theory;query expansion;text segmentation	Paulo Cheong;Dawei Song;Peter Bruza;Kam-Fai Wong	2004		10.1007/978-3-540-30211-7_11	natural language processing;lexical semantics;query expansion;speech recognition;information flow;information processing;computer science;machine learning;linguistics	NLP	-28.52894753983333	-69.98189572067416	64388
3818042cc6a3370d42544212a9cf3bee37d7d6a0	towards weakly supervised resolution of null instantiations	computerlinguistik;thematische rolle;semantic role labeling;automatische sprachanalyse;locally uninstantiated arguments;argument	This paper addresses the task of finding antecedents for locally uninstantiated arguments. To resolve such null instantiations, we develop a weakly supervised approach that investigates and combines a number of linguistically motivated strategies that are inspired by work on semantic role labeling and corefence resolution. The performance of the system is competitive with the current state-of-the-art supervised system.	increment and decrement operators;multimodal interaction;semantic role labeling;source;star filler;supervised learning;test data	Philip Gorinski;Josef Ruppenhofer;Caroline Sporleder	2013			artificial intelligence;machine learning;mathematics;algorithm	AI	-23.47628974940524	-74.6434458684692	64417
c3aec00a7fa9196739ce09e0994c852f0090c1ea	fbk: machine translation evaluation and word similarity metrics for semantic textual similarity	best model;semantic machine translation evaluation;semantic textual similarity;mean ranking;mean score;test set;average correlation;knowledge-based word similarity metrics;human judgement	This paper describes the participation of FBK in the Semantic Textual Similarity (STS) task organized within Semeval 2012. Our approach explores lexical, syntactic and semantic machine translation evaluation metrics combined with distributional and knowledgebased word similarity metrics. Our best model achieves 60.77% correlation with human judgements (Mean score) and ranked 20 out of 88 submitted runs in the Mean ranking, where the average correlation across all the sub-portions of the test set is considered.	lexicon;machine translation;semeval;test set	José Guilherme Camargo de Souza;Matteo Negri;Yashar Mehdad	2012			natural language processing;semantic similarity;semeval;computer science;pattern recognition;information retrieval	NLP	-26.650821988144266	-68.35386286349583	64434
5a99aa15c868f23bdbe95a877d8ff6b0fdfbcab0	on the complementary distribution of plurals and classifiers in east asian classifier languages				Kyumin Kim;Paul B. Melchin	2018	Language and Linguistics Compass	10.1111/lnc3.12271	natural language processing;artificial intelligence;computer science;complementary distribution;classifier (linguistics);east asia	NLP	-30.075828955422487	-78.1510986178506	64521
edeebc1d69810b38e6949fd85078741b8deb6bca	corpus linguistics for vocabulary: a guide for research. paweł szudarski			corpus linguistics;vocabulary	Huayong Li	2018	DSH	10.1093/llc/fqy045	corpus linguistics;linguistics;vocabulary;computer science	NLP	-31.020259466579233	-77.61014840470672	64555
73f9a012ecc5bb8bb3d9e3c802ad5260df33eb7b	rule-based chunking and reusability		In this paper we discuss a rule-based approach to chunking implemented using the LT-XML2 and LT-TTT2 tools. We describe the tools and the pipeline and grammars that have been developed for the task of chunking. We show that our rule-based approach is easy to adapt to different chunking styles and that the mark-up of further linguistic information such as nominal and verbal heads can be added to the rules at little extra cost. We evaluate our chunker against the CoNLL 2000 data and discuss discrepancies between our output and the CoNLL mark-up as well as discrepancies within the CoNLL data itself. We contrast our results with the higher scores obtained using machine learning and argue that the portability and flexibility of our approach still make it a more practical solution.	logic programming;machine learning;pipeline (computing);shallow parsing	Claire Grover;Richard Tobin	2006			rule-based system;natural language processing;artificial intelligence;computer science;chunking (psychology);software portability;reusability;rule-based machine translation	NLP	-25.436856655939305	-76.52326296514238	64562
47dd3be216a2298ef41eab6fadbed2bffd9316ca	adapting text to data in documents through a natural language processor	presentacion documento;generation;lenguaje natural;generacion;document layout;langage naturel;tratamiento lenguaje;correction;presentation document;corrections;language processing;natural language;burotica;traitement langage;correccion;natural language processor;office automation;bureautique	This paper presents an approach to the problem of generating documents from schemas and variable data so that the schematic contents are made linguistically consistent with the injected data. A key aspect of the proposed approach consists in performing most of the required linguistic analysis during the schema definition phase instead of during runtime document production. This is achieved by abstracting at schema definition time the main linguistic properties of the data which will be injected. Another important characteristic of the approach consists in the fact that not only variable data can be injected in the schematic contents but also a whole document; in this case the resulting document is linguistically adjusted, as a whole, resolving references between the outer and the inner subdocuments. Key wards: Document generation, document correction, linguistic processing, linguistic consistency	database schema;natural language processing;schematic	Mauro Negri;Giuseppe Pelagatti;Licia Sbattella	1991	Inf. Syst.	10.1016/0306-4379(91)90048-E	natural language processing;generation;speech recognition;computer science;artificial intelligence;database;natural language;programming language	Web+IR	-30.353479619744153	-71.25984300707096	64610
11ac25d01816503ad3626d5d2a00e31923e55114	korean-chinese machine translation using three-stage verb pattern matching	construction process;verb pattern;korean chinese machine translation;pattern matching;pattern construction;machine translation	In this paper, we describe three-stage pattern matching approach and an effective pattern construction process in the Korean-Chinese Machine Translation System for technical documents. We automatically extracted about 100,000 default verb patterns and about 10,000 default ordering patterns from the existing patterns. With the new three-stage approach, additionally using default verb and ordering patterns, the matching hit rate increases 3.8% , comparing with one-stage approach.	machine translation;pattern matching	Chang Hao Yin;Young Ae Seo;Young-Gil Kim	2009		10.1007/978-3-642-00831-3_30	natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;pattern matching;pattern recognition;machine translation;rule-based machine translation	NLP	-24.1006666947484	-77.60415641538715	64613
6599e9a28d86da5af82c7bb81934f3400690b114	detecting change and emergence for multiword expressions		This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and its status as a MWE change over time. We propose a model in which context words have particular probabilities given a usage choice for an n-gram, and those usage choices have time dependent probabilities, and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice. For a range of MWE usages of recent coinage, we evaluate whether the technique is able to detect the emerging usage.	emergence;expression (computer science);minimal working example;n-gram;sensor	Martin Emms;Arun Jayapal	2014			artificial intelligence;pattern recognition;expression (mathematics);annotation;computer science	SE	-25.989062334890455	-72.68789824101444	64661
2273ba45c0eb580de0df36559a9856748cb1ce38	fast and robust arabic error correction system		In this paper we describe the implementation of an Arabic error correction system developed for the EMNLP2014 shared task on automatic error correction for Arabic text. We proposed a novel algorithm, where we find some correction rules and calculate their probability based on the training data, they we rank the correction rules, then we apply them on the text to maximize the overall Fscore for the provided data. The system achieves and F-score of 0.6573 on the test data.	algorithm;error detection and correction;f1 score;forward error correction;qr code;test data	Michael N. Nawar;Moheb M. Ragheb	2014		10.3115/v1/W14-3619	speech recognition;computer science;pattern recognition;data mining	NLP	-23.58073939826667	-76.40883788286334	64671
55dcc650f1c0d48b8fae50ef984ae5164a00d676	scalable attribute-value extraction from semi-structured text	information resources;generators;web pages;information extraction;f measure scalable attribute value extraction semistructured text web pages candidate generation candidate filtering;training;information resources data mining;semistructured text;data mining;cloud computing data mining decision trees costs machine learning algorithms clustering algorithms computer networks data processing training data conferences;scalable attribute value extraction;feature extraction;world wide web;f measure;candidate generation;context;candidate filtering	This paper describes a general methodology for extracting attribute-value pairs from web pages. It consists of two phases: candidate generation, in which syntactically likely attribute-value pairs are annotated; and candidate filtering, in which semantically improbable annotations are removed. We describe three types of candidate generators and two types of candidate filters, all of which are designed to be massively parallelizable. Our methods can handle 1 billion web pages in less than 6 hours with 1,000 machines. The best generator and filter combination achieves 70% F-measure compared to a hand-annotated corpus.	attribute–value pair;central processing unit;f1 score;scalability;semiconductor industry;structured text;web page;whitelist	Yuk Wah Wong;Dominic Widdows;Tom Lokovic;Kamal Nigam	2009	2009 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2009.81	feature extraction;computer science;data science;machine learning;web page;data mining;f1 score;information extraction;information retrieval	Robotics	-31.156644334735155	-68.50855643823724	64864
8a54d8d44344e4db6a17bcbbb329e229c707f75b	anita: a powerful morphological analyser for italian		In this paper we present AnIta, a powerful morphological analyser for Italian implemented within the framework of finite-state-automata models. It is provided by a large lexicon containing more than 110,000 lemmas that enable it to cover relevant portions of Italian texts. We describe our design choices for the management of inflectional phenomena as well as some interesting new features to explicitly handle derivational and compositional processes in Italian, namely the wordform segmentation structure and Derivation Graph. Two different evaluation experiments, for testing coverage (Recall) and Precision, are described in detail, comparing the AnIta performances with some other freely available tools to handle Italian morphology. The experiments results show that the AnIta Morphological Analyser obtains the best performances among the tested systems, with Recall = 97.21% and Precision = 98.71%. This tool was a fundamental building block for designing a performant PoS-tagger and Lemmatiser for the Italian language that participated to two EVALITA evaluation campaigns ranking, in both cases, together with the best performing systems.	automata theory;brill tagger;experiment;finite-state machine;galaxy morphological classification;lemmatisation;lexicon;morphological parsing;part-of-speech tagging;performance;precision and recall	Fabio Tamburini;Matias Melandri	2012			morphology (linguistics);artificial intelligence;analyser;natural language processing;computer science;lexicon;recall;graph;ranking	NLP	-28.720107386688472	-74.90984333967862	64972
4fe4d441682263e7b1e717cf05037c427aec8c85	integration of complex language models in asr and lu systems	integration architectures;hierarchical models;finite state models;word graph	Throughout this work, we explore different methods to integrate a complex Language Model (a hierarchical Language Model based on classes of phrases) into an automatic speech recognition (ASR) system. First of all, an integrated architecture is considered, where the integration is carried out via the composition of the different Stochastic Finite-State Automata associated with the specific Language Model (LM). On the other hand, a decoupled architecture with a two-pass decoder is employed, where the complex LM is used to reorder the N-best list. The formal definition of both methods is provided in this work, thus enabling the theoretical comparison between them. Additionally, different experiments were carried out to compare empirically the proposed approaches. The results show that although the hierarchical LMs outperform a baseline word-based LM in both cases, the integrated architecture can provide better ASR system performance. However, the decoupled architecture could be more versatile due to the two-pass strategy, allowing the integration of different models using a standard decoder. Additionally, the use of this kind of complex LMs can also be extended to other NLP applications, such as language understanding, by employing the proposed architectures.	automata theory;automated system recovery;automatic system recovery;baseline (configuration management);experiment;finite-state machine;lu decomposition;language model;natural language processing;natural language understanding;out-of-order execution;speech recognition	Raquel Justo;M. Inés Torres	2014	Pattern Analysis and Applications	10.1007/s10044-014-0436-0	natural language processing;speech recognition;computer science;machine learning	NLP	-19.195663582874307	-75.51883088712862	65014
274bb5335d84fb2e6386acb3e55f73ca6274252b	discourse cues for broadcast news segmentation	automated segmentation;discourse cue;information extraction technique;story segmentor;broadcast news navigator;computational implementation;broadcast news corpus;broadcast news segmentation;video news browsing;finite state model;broadcast news;information extraction	This paper describes the design and application of time-enhanced, finite state models of discourse cues to the automated segmentation of broadcast news. We describe our analysis of a broadcast news corpus, the design of a discourse cue based story segmentor that builds upon information extraction techniques, and finally its computational implementation and evaluation in the Broadcast News Navigator (BNN) to support video news browsing, retrieval, and summarization.	information extraction	Mark T. Maybury	1998			natural language processing;computer science;multimedia;world wide web;information extraction;information retrieval	NLP	-31.430547979851802	-76.2232683715522	65108
615a25a9077537457f9e6344f869640fa96ecf06	estimating translation probabilities considering semantic recoverability of phrase retranslation			serializability	Hyoung-Gyu Lee;Min-Jeong Kim;YingXiu Quan;Hae-Chang Rim;So Young Park	2012	IEICE Transactions		noun phrase	NLP	-28.76729384364534	-78.27678987996353	65120
79b9e2681a990e9fb22cb265a5d6237c4d3164bd	a compositional structured query approach to automated inference.	composite structure			Yousri El Fattah;Mark A. Peot	2000			computer science;machine learning;pattern recognition	NLP	-32.25772915749862	-78.38019359376715	65184
2f7f87d6994e92bb20e2012cbb18e000fbc4665e	simple tools for exploring variation in code-switching for linguists		One of the benefits of language identification that is particularly relevant for code-switching (CS) research is that it permits insight into how the languages are mixed (i.e., the level of integration of the languages). The aim of this paper is to quantify and visualize the nature of the integration of languages in CS documents using simple language-independent metrics that can be adopted by linguists. In our contribution, we (a) make a linguistic case for classifying CS types according to how the languages are integrated; (b) describe our language identification system; (c) introduce an Integration-index (I-index) derived from HMM transition probabilities; (d) employ methods for visualizing integration via a language signature (or switching profile); and (e) illustrate the utility of our simple metrics for linguists as applied to Spanish-English texts of different switching profiles.	emoticon;hidden markov model;ietf language tag;language identification;language-independent specification;markov chain;n-gram;named entity;text corpus	Gualberto A. Guzmán;Jacqueline Serigos;Barbara E. Bullock;Almeida Jacqueline Toribio	2016		10.18653/v1/W16-5802	humanities;natural language processing;computer science;linguistics	NLP	-29.335324359659595	-75.05506397883173	65209
25422c4018de9fcf4cffffcef3329724a67a2421	deep pivot-based modeling for cross-language cross-domain transfer with minimal guidance		While cross-domain and cross-language transfer have long been prominent topics in NLP research, their combination has hardly been explored. In this work we consider this problem, and propose a framework that builds on pivotbased learning, structure-aware Deep Neural Networks (particularly LSTMs and CNNs) and bilingual word embeddings, with the goal of training a model on labeled data from one (language, domain) pair so that it can be effectively applied to another (language, domain) pair. We consider two setups, differing with respect to the unlabeled data available for model training. In the full setup the model has access to unlabeled data from both pairs, while in the lazy setup, which is more realistic for truly resource-poor languages, unlabeled data is available for both domains but only for the source language. We design our model for the lazy setup so that for a given target domain, it can train once on the source language and then be applied to any target language without re-training. In experiments with nine EnglishGerman and nine English-French domain pairs our best model substantially outperforms previous models even when it is trained in the lazy setup and previous models are trained in the full setup.1		Yftah Ziser;Roi Reichart	2018			natural language processing;machine learning;artificial intelligence;computer science	NLP	-19.506692368528853	-74.8544401691833	65350
8d26d6d6871e724ad2f8c0bb227e9aae1ec3b76e	a feature-rich crf segmenter for chinese micro-blog		This paper describes our system for Chinese word segmentation of micro-blog text, one of the NLPCC-ICCPOL 2016 Shared Tasks [1]. The CRF (Conditional Random Field) model is employed to model word segmentation as a sequence labeling problem, 7 sets of features are selected to train the CRF model. The system achieves fb 0.798144 on closed track, 0.81968 on semi-open track, and 0.82217 on open track with weighted measures [2].	blog;conditional random field;semiconductor industry;sequence labeling;text segmentation	Yabin Leng;Weiwei Liu;Sheng Wang;Xiaojie Wang	2016		10.1007/978-3-319-50496-4_78	microblogging;sequence labeling;conditional random field;artificial intelligence;social media;text segmentation;computer science;pattern recognition	NLP	-21.927935479884443	-68.81298601087231	65474
beef7f7687721d4d963d4e03bcaf5976b11a6ba9	caesar: context awareness enabled summary-attentive reader		Comprehending meaning from natural language is a primary objective of Natural Language Processing (NLP), and text comprehension is the cornerstone for achieving this objective upon which all other problems like chat bots, language translation and others can be achieved. We report a Summary-Attentive Reader we designed to better emulate the human reading process, along with a dictiontarybased solution regarding out-of-vocabulary (OOV) words in the data, to generate answer based on machine comprehension of reading passages and question from the SQuAD benchmark. Our implementation of these features with two popular models (Match LSTM and Dynamic Coattention) was able to reach close to matching the results obtained from humans.	benchmark (computing);context awareness;long short-term memory;natural language processing;vocabulary	Long-Huei Chen;Kshitiz Tripathi	2018	CoRR		artificial intelligence;natural language processing;natural language;comprehension;context awareness;computer science	NLP	-24.98994020754473	-71.11283627279349	65493
cc055171d67af1c8f837c8c948a1d67be5e5b35e	linguistically regularized lstm for sentiment classification		This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.	lexicon;long short-term memory;parse tree;parsing;sentience;tree structure	Qiao Qian;Minlie Huang;Jinhao Lei;Xiaoyan Zhu	2017		10.18653/v1/P17-1154	natural language processing;artificial intelligence;machine learning;computer science	NLP	-19.27152914973979	-73.64911857605229	65596
8810c9dd04ae6852234cd3403bb2c5b5a29e6b9e	poly-co: a multilayer perceptron approach for coreference detection	identity measure;syntactic information;multilayer perceptron approach;feature selection method;coreference candidate;closed track;pipeline approach;multilayer perceptron classifier;coreference detection;conll-2011 shared task;coreference resolution system	This paper presents the coreference resolution system Poly-co submitted to the closed track of the CoNLL-2011 Shared Task. Our system integrates a multilayer perceptron classifier in a pipeline approach. We describe the heuristic used to select the pairs of coreference candidates that are feeded to the network for training, and our feature selection method. The features used in our approach are based on similarity and identity measures, filtering informations, like gender and number, and other syntactic information.	algorithm;collaborative filtering;feature selection;heuristic;multilayer perceptron;named entity	Eric Charton;Michel Gagnon	2011			natural language processing;computer science;machine learning;pattern recognition	NLP	-22.5878604863531	-70.30566440046107	65897
5985e41f8b1bdca631ff7bc95dde59b25e03f5c3	a linear-time bottom-up discourse parser with constraints and post-editing		Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be applied in practice. In this work, we develop a much faster model whose time complexity is linear in the number of sentences. Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers. To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF. In addition to efficiency, our parser also significantly outperforms the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.	bottom-up parsing;conditional random field;experiment;greedy algorithm;human reliability;parser;period-doubling bifurcation;postediting;post–turing machine;time complexity;top-down and bottom-up design;tree (data structure);viterbi decoder	Vanessa Wei Feng;Graeme Hirst	2014		10.3115/v1/P14-1048	natural language processing;speech recognition;computer science;machine learning;algorithm	NLP	-21.5389296312351	-76.4412775914521	66042
00b5ea1847866ce197466642d61cf130e62b6efd	writing systems, transliteration and decipherment	available data;encode language;core data;computational linguists deal;language data;systems work;machine translation;ancient script;standard writing system;brief review	Description Nearly all of the core data that computational linguists deal with is in the form of text, which is to say that it consists of language data written (usually) in the standard writing system for the language in question. Yet surprisingly little is generally understood about how writing systems work. This tutorial will be divided into three parts. In the first part we discuss the history of writing and introduce a wide variety of writing systems, explaining their structure and how they encode language. We end this section with a brief review of how some of the properties of writing systems are handled in modern encoding systems, such as Unicode, and some of the continued pitfalls that can occur despite the best intentions of standardization. The second section of the tutorial will focus on the problem of transcription between scripts (often termed “transliteration”), and how this problem—which is important both for machine translation and named entity recognition—has been addressed. The third section is more theoretical and, at the same time we hope, more fun. We will discuss the problem of decipherment and how computational methods might be brought to bear on the problem of unlocking the mysteries of as yet undeciphered ancient scripts. We start with a brief review of three famous cases of decipherment. We then discuss how techniques that have been used in speech recognition and machine translation might be applied to the problem of decipherment. We end with a survey of the as-yet undeciphered ancient scripts and give some sense of the prospects of deciphering them given currently available data.	computation;computational linguistics;core data;decipherment;encode;machine translation;named-entity recognition;speech recognition;transcription (software);unicode	Kevin Knight;Richard Sproat	2009			natural language processing;speech recognition;computer science;machine learning;linguistics;undeciphered writing systems	NLP	-31.475000167275372	-75.054453392129	66128
04965011d0523b829ebba3677e8c066b67fdb1b7	unsupervised discovery of scenario-level patterns for information extraction	unsupervised discovery;new pattern;expensive process;ie system;seed pattern;new scenario;information extraction;incremental discovery procedure;new pattern base;high precision;scenario-level pattern;pattern matching	Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base—a timeconsuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall.	algorithm;dictionary;document;domain-specific language;entity;experiment;general-purpose modeling;information extraction;knowledge base;lexicon;message understanding conference;natural language;pattern matching;precision and recall;regular expression;relational database;scenario planning;sensitivity and specificity;succession	Roman Yangarber;Ralph Grishman;Pasi Tapanainen	2000			computer science;machine learning;pattern recognition;data mining	AI	-30.445053559606553	-69.13886358338515	66225
990fad923fd213c412d737c9d1f5f0fca442c776	a person-name filter for automatic compilation of bilingual person-name lexicons		This paper proposes a simple and fast person-name filter, which plays an important role in automatic compilation of a large bilingual person-name lexicon. This filter is based on pn score, which is the sum of two component scores, the score of the first name and that of the last name. Each score is calculated from two term sets: one is a dense set in which most of the members are person names; another is a baseline set that contains less person names. The pn score takes one of five values, {+2, +1, 0, −1, −2 }, which correspond to strong positive, positive, undecidable, negative, and strong negative, respectively. This pn score can be easily extended to bilingual pn score that takes one of nine values, by summing scores of two languages. Experimental results show that our method works well for monolingual person names in English and Japanese; the F-score of each language is 0.929 and 0.939, respectively. The performance of the bilingual person-name filter is better; the F-score is 0.955.	baseline (configuration management);compiler;lexicon;like button;undecidable problem	Satoshi Sato;Sayoko Kaide	2010			undecidable problem;natural language processing;monolingual person;speech recognition;artificial intelligence;dense set;computer science;bilingual person;lexicon	NLP	-23.829166838197718	-72.19545516805857	66341
c2ad4adc39a68a26bebf0fa5f608a053bea510d2	polish coreference corpus		The Polish Coreference Corpus (PCC) is a large corpus of Polish general nominal coreference built upon the National Corpus of Polish. With its 1900 documents from 14 text genres, containing about 540,000 tokens, 180,000 mentions and 128,000 coreference clusters, the PCC is among the largest coreference corpora in the international community. It has some novel features, such as the annotation of the quasiidentity relation, inspired by Recasens’ near-identity, as well as the markup of semantic heads and dominant expressions. It shows a good interannotator agreement and is distributed in three formats under an open license. Its by-products include freely available annotation tools with custom features such as file distribution management and annotation adjudication.	markup language;national corpus of polish;open content;portable c compiler;proof-carrying code;text corpus	Maciej Ogrodniczuk;Katarzyna Glowinska;Mateusz Kopec;Agata Savary;Magdalena Zawislawska	2013		10.1007/978-3-319-43808-5_17	license;natural language processing;adjudication;artificial intelligence;coreference;expression (mathematics);computer science;annotation	NLP	-32.02210872331981	-75.30477408781292	66363
d08dfcf2114bf7317a5b4b25e95b62c538550509	a deep learning approach to deal with data uncertainty in sentiment analysis		Sentiment Analysis refers to the process of computationally identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer’s attitude towards a particular topic or product is positive, negative, or even neutral. Recently, deep learning approaches emerge as powerful computational models that discover intricate semantic representations of texts automatically from data without hand-made feature engineering. These approaches have improved the state-of-the-art in many Sentiment Analysis tasks including sentiment classification of sentences or documents. In this paper we propose a semi-supervised neural network model, based on Deep Belief Networks, able to deal with data uncertainty for text sentences and adopting the Italian language as a reference language. We test this model against some datasets from literature related to movie reviews, adopting a vectorized representation of text and exploiting methods from Natural Language Processing (NLP) pre-processing.	deep learning;sentiment analysis	Michele Di Capua;Alfredo Petrosino	2016		10.1007/978-3-319-52962-2_15	machine learning;pattern recognition;data mining;sentiment analysis	AI	-19.518223431838294	-69.18045661518744	66396
59337684fe109cc8cd1c285d8383cac967c4837c	the university of washington's uwclmaqa system	open architecture;named entity recognition;question answering system;system design;passage retrieval;document retrieval;document classification;open source	The University of Washington’s UWCLMAQA is an open-architecture Question Answering system, built around open source tools unified into one system design using customized enhancements. The goal was to develop an end-to-end QA system that could be easily modified by switching out tools as needed. Central to the system is Lucene, which we use for document retrieval. Various other tools are used, such the GoogleAPI for web boosting, fnTBL Chunker for text chunking, Lingua::Stem for stemming, Lingpipe for Named Entity Recognition, etc. We also developed several in-house evaluation tools for gauging our progress at each major milestone (e.g., document classification, document retrieval, passage retrieval, etc.) and statistical classifiers were developed that we use for various classification tasks.	document classification;document retrieval;end-to-end principle;named-entity recognition;open-source software;question answering;shallow parsing;software quality assurance;statistical classification;stemming;systems design	Dan Jinguji;William D. Lewis;Efthimis N. Efthimiadis;Joshua Minor;Albert Bertram;Shauna Eggers;Joshua Johanson;Brian Nisonger;Ping Yu;Zhengbo Zhou	2006			natural language processing;document retrieval;question answering;open architecture;computer science;data mining;world wide web;information retrieval;systems design	Web+IR	-32.67500508883187	-73.2738636596004	66438
5111ecb77fbe6b8d9b372b2f60ec7ffc652a83c9	referential translation machines for predicting translation quality	artificial intelligence;computational linguistics;machine learning	We use referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations and achieve the top performance in WMT13 quality estimation task (QET13). We improve our RTM models with the Parallel FDA5 instance selection model, with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT14 QET (QET14) subtask, obtain improvements over QET13 results, and rank 1st in all of the tasks and subtasks of QET14.	computational model;statistical machine translation	Ergun Biçici;Andy Way	2014			natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;computational linguistics;machine learning;rule-based machine translation	NLP	-20.76025509167277	-74.99994358906105	66550
b440a2c00f6aef80020d4247acea0b3fb319bf71	localization in modern standard arabic	analyse de corpus;variation lexicale;traitement automatique des langues naturelles;language use;quantitative linguistics;localization;linguistique quantitative;modern standard arabic;langue standard;arabe moderne;press;localisation;corpus analysis;presse;usage linguistique;natural language processing;lexical analysis;lexical variation;standard language	Modern Standard Arabic (MSA) is the official language used in all Arabic countries. In this paper we describe an investigation of the uniformity of MSA across different countries. Many studies have been carried out locally or regionally on Arabic and its dialects. Here we look on a more global scale by studying language variations between countries. The source material used in this investigation was derived from national newspapers available on the Web, which provided samples of common media usage in each country. This corpus has been used to investigate the lexical characteristics of Modern Standard Arabic as found in 10 different Arabic speaking countries. We describe our collection methods, the types of lexical analysis performed, and the results of our investigations. With respect to newspaper articles, MSA seems to be very uniform across all the countries included in the study, but we have detected various types of differences, with implications for computational processing of MSA.	circuit complexity;computation;lexical analysis;world wide web	Ahmed Abdelali	2004	JASIST	10.1002/asi.10340	natural language processing;speech recognition;internationalization and localization;lexical analysis;quantitative linguistics;computer science	NLP	-28.455253161198307	-76.47352187296694	66672
9851e89fec74ab8cd3c5120ad9465c87cf93202d	legal document retrieval using document vector embeddings and deep learning		Domain specific information retrieval process has been a prominent and ongoing research in the field of natural language processing. Many researchers have incorporated different techniques to overcome the technical and domain specificity and provide a mature model for various domains of interest. The main bottleneck in these studies is the heavy coupling of domain experts, that makes the entire process to be time consuming and cumbersome. In this study, we have developed three novel models which are compared against a golden standard generated via the on line repositories provided, specifically for the legal domain. The three different models incorporated vector space representations of the legal domain, where document vector generation was done in two different mechanisms and as an ensemble of the above two. This study contains the research being carried out in the process of representing legal case documents into different vector spaces, whilst incorporating semantic word measures and natural language processing techniques. The ensemble model built in this study, shows a significantly higher accuracy level, which indeed proves the need for incorporation of domain specific semantic similarity measures into the information retrieval process. This study also shows, the impact of varying distribution of the word similarity measures, against varying document vector dimensions, which can lead to improvements in the process of legal information retrieval. keywords: Document Embedding, Deep Learning, Information Retrieval		Keet Sugathadasa;Buddhi Ayesha;Nisansa de Silva;Amal Perera;Vindula Jayawardana;Dimuthu Lakmal;Madhavi Perera	2018	CoRR	10.1007/978-3-030-01177-2_12	information retrieval;legal case;semantic similarity;computer science;specific-information;deep learning;document retrieval;domain specificity;artificial intelligence;ensemble forecasting;legal information retrieval	Web+IR	-30.326738175598507	-67.98280502155323	66674
fef7cfacecd821d61b49c981da3bf0ac897bbf05	chinese zero pronoun resolution with deep neural networks		While unsupervised anaphoric zero pronoun (AZP) resolvers have recently been shown to rival their supervised counterparts in performance, it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings. Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus.	anaphora (linguistics);artificial neural network;deep learning;feature engineering;supervised learning;word embedding	Chen Chen;Vincent Ng	2016			natural language processing	NLP	-19.423960172352118	-72.93398346497898	66702
fe906b487e28c37d764fd69e18795ac5ee151fd9	thread-level analysis over technical user forum data		This research focuses on improving information access over troubleshootingoriented technical user forums via threadlevel analysis. We describe a modular task formulation and novel dataset, and go on to describe a series of preliminary classification experiments over the data. We find that a class composition strategy achieves the best results, surpassing multiclass classification approaches.	experiment;information access;multiclass classification	Li Wang;Su Nam Kim;Timothy Baldwin	2010			technical communication	ML	-21.47902664552144	-69.6321147347365	66708
bf6be56eb089ce71cfda05d8f04bdf655390595b	comparison of s-gram proximity measures in out-of-vocabulary word translation	approximate string matching;out of vocabulary;edit distance;cross language information retrieval	Classified s-grams have been successfully used in cross-language information retrieval (CLIR) as an approximate string matching technique for translating out-of-vocabulary (OOV) words. For example, s-grams have consistently outperformed other approximate string matching techniques, like edit distance or n-grams. The Jaccard coefficient has traditionally been used as an s-gram based string proximity measure. However, other proximity measures for s-gram matching have not been tested. In the current study the performance of seven proximity measures for classified s-grams in CLIR context was evaluated using eleven language pairs. The binary proximity measures performed generally better than their non-binary counterparts, but the difference depended mainly on the padding used with s-grams. When no padding was used, the binary and non-binary proximity measures were nearly equal, though the performance at large deteriorated.	approximate string matching;approximation algorithm;bitwise operation;coefficient;cross-language information retrieval;edit distance;grams;jaccard index;n-gram;string searching algorithm;vocabulary	Anni Järvelin;Antti Järvelin	2008		10.1007/978-3-540-89097-3_9	natural language processing;speech recognition;edit distance;approximate string matching;computer science;pattern recognition;mathematics;programming language	SE	-26.862591308571	-68.37843880452982	66756
6495c662fdff28bf7cc0951c6adbd38850e62b48	simple yet effective methods for cross-lingual link discovery (clld) - kmi @ ntcir-10 crosslink-2		Cross-Lingual Link Discovery (CLLD) aims to automatically find links between documents written in different languages. In this paper, we first present a relatively simple yet effective methods for CLLD in Wiki collections, explaining the findings that motivated their design. Our methods (team KMI) achieved in the NTCIR-10 CrossLink-2 evaluation the best overall results in the English to Chinese, Japanese and Korean (E2CJK) task and were the top performers in the Chinese, Japanese, Korean to English task (CJK2E) [Tang et al.,2013]. Though tested on these language combinations, the methods are language agnostic and can be easily applied to any other language combination with sufficient corpora and available pre-processing tools. In the second part of the paper, we provide an in-depth analysis of the nature of the task, the evaluation metrics and the impact of the system components on the overall CLLD performance. We believe a good understanding of these aspects is the key to improving CLLD systems in the future.	experiment;language-independent specification;preprocessor;text corpus;wiki;word-sense disambiguation	Petr Knoth;Drahomira Herrmannova	2013			computer science;artificial intelligence;data mining;communication	NLP	-26.990771118759817	-73.72537660465339	66931
a844a25e2ae1be8856a9878a3c1129de69551fb4	real-valued syntactic word vectors (rsv) for greedy neural dependency parsing		We show that a set of real-valued word vectors formed by right singular vectors of a transformed co-occurrence matrix are meaningful for determining different types of dependency relations between words. Our experimental results on the task of dependency parsing confirm the superiority of the word vectors to the other sets of word vectors generated by popular methods of word embedding. We also study the effect of using these vectors on the accuracy of dependency parsing in different languages versus using more complex parsing architectures.	co-occurrence matrix;document-term matrix;experiment;greedy algorithm;parsing expression grammar;word embedding	Ali Basirat;Joakim Nivre	2017			syntax;computational linguistics;mathematics;matrix (mathematics);dependency grammar;artificial intelligence;pattern recognition	NLP	-20.013407935350273	-72.66033560747184	67024
cafb474172fa7280693d2ab042b24206e53eddd1	legal nerc with ontologies, wikipedia and curriculum learning		In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.	argument map;coherence (physics);error analysis (mathematics);finite-state machine;machine learning;named entity;ontology (information science);overfitting;population;preprocessor;text corpus;wikipedia;word lists by frequency;yago	Cristian Cardellino;Milagro Teruel;Laura Alonso Alemany;Serena Villata	2017				NLP	-24.26217119727674	-72.55369600428907	67027
ee4aa470a303e53596c7d6343a0a4e559ecdd97b	the computer as a research tool in the construction of models of linguistic performance				Victoria A. Fromkin	1968			natural language processing;linguistic performance;artificial intelligence;computer science	Logic	-31.096557533506672	-78.04115007473553	67040
c96956c75104d290736a7242b9a7714eba0205dd	a pre-identification method for chinese named entity recognition	conjunction degree;potential entity name;named entity recognition;context keywords;segmentation degree	In this paper, a pre-identification method for Chinese named entity recognition is proposed. Internal information of entity name like family name, first name in person name, feature word in place name and organization name do not needed. Through entity name guessing based on context keywords, pre-identification is realized. Definition of bidirectional potential entity name recognition, rough confirmation of potential entity name, segmentation word is proposed. To solve the possible ambiguity in entity name identification, the degree of segmentation and conjunction is presented as well as cascade recognition and final confirmation. Combining with this pre-processing method, performance will be improved by using internal information of entity name. Experiment proves that the method have a special advantage in recognition special entity name, ambiguity name and irregular name. In this paper, Chinese person name is taken as an example for entity name recognition. Nevertheless, the method is not limit to person name recognition but also a preidentification method for other entity name.	algorithm;experiment;microsoft word for mac;named-entity recognition;preprocessor	Hongjian Liu;Defeng Guo;Quan Zhou;Kenji Nagamatsu;Qinghua Sun	2010	JSW	10.4304/jsw.5.1.73-80	natural language processing;fully qualified domain name;computer science;pattern recognition;data mining;entity linking;weak entity	ML	-25.66735687247964	-70.97288219209591	67051
c67978528e1f36e1ae9926e369f55f2362eea659	cascade: contextual sarcasm detection in online discussion forums		The literature in automated sarcasm detection has mainly focused on lexical, syntactic and semantic-level analysis of text. However, a sarcastic sentence can be expressed with contextual presumptions, background and commonsense knowledge. In this paper, we propose CASCADE (a ContextuAl SarCasm DEtector) that adopts a hybrid approach of both content and context-driven modeling for sarcasm detection in online social media discussions. For the latter, CASCADE aims at extracting contextual information from the discourse of a discussion thread. Also, since the sarcastic nature and form of expression can vary from person to person, CASCADE utilizes user embeddings that encode stylometric and personality features of the users. When used along with content-based feature extractors such as Convolutional Neural Networks (CNNs), we see a significant boost in the classification performance on a large Reddit corpus.	artificial neural network;commonsense knowledge (artificial intelligence);contextual inquiry;conversation threading;convolutional neural network;data point;e-governance;encode;eigen (c++ library);feature extraction;fractional cascading;generalized canonical correlation;loss function;optimization problem;social media;software testing controversies;stylometry;text corpus;the legend of zelda: breath of the wild;x/open transport interface	Devamanyu Hazarika;Soujanya Poria;Sruthi Gorantla;Erik Cambria;Roger Zimmermann;Rada Mihalcea	2018			artificial intelligence;machine learning;natural language processing;online discussion;convolutional neural network;cascade;computer science;sarcasm;commonsense knowledge;social media;syntax;sentence	NLP	-21.073025357843974	-68.5375158523908	67122
6912acd5cad929212a8c4d035e04f275427caee4	unsupervised learning of disambiguation rules for part of speech tagging	part of speech;rule based;baum welch;unsupervised learning	In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text. I n t r o d u c t i o n There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past [Klein and Simmons, 1963; Harris, 1962]. Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging [Jelinek, 1985; Church, 1988; Derose, 1988; DeMarcken, 1990; Cutting et al., 1992; Kupiec, 1992; Charniak et al., 1993; Weischedel et al., 1993; Schutze and Singer, 1994; Lin et al., 1994; Elworthy, 1994; Merialdo, 1995]. 2 For a Markov-model based tagger, training consists of learning both lexical probabilities ( P ( w o r d l t a g ) ) and contextual probabilities (P( tag i l tag i_ l . . . tag i -n) ) . Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities. The most accurate stochastic taggers use estimates of lexical and contextual probabilities extracted from large manually annotated corpora (eg. [Weischedel et al., 1993; Charniak et al., 1993]). It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm [Baum, 1972; Jelinek, 1985; Cutting et al., 1992; Kupiec, 1992; Elworthy, 1994; Merialdo, 1995]. This algorithm works by iteratively adjusting the lexical and contextual probabilities to increase the overall probability of the training corpus. If no prior knowledge is available, probabilities are initially either assigned randomly or evenly distributed. Although less accurate than the taggers built using manually annotated corpora, the fact that they can be trained using only a dictionary listing the allowable parts of speech for each word and not needing a manually tagged corpus is a huge advantage in many situations. Although a number of manually tagged corpora are available (eg. [Francis and Kucera, 1982; Marcus et al., 1993]), training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy [Weischedel et al., 1993]. Therefore, if tagged text is needed in training, this would require manually tagging 1This work was funded in part by NSF grant IRI-9502312. 2Some other approaches to tagging are described in [Hindle, 1989; Black et al., 1992]. text each time the tagger is to be apphed to a new language, and even when being applied to a new type of text. In [Brill, 1992; Brill, 1994], a rule-based part of speech tagger is described which achieves highly competitive performance compared to stochastic taggers, and captures the learned knowledge in a set of simple deterministic rules instead of a large table of statistics. In addition, the learned rules can be converted into a deterministic finite state transducer. Tagging with this finite state transducer requires n steps to tag a sequence of length n, independent of the number of rules, and results in a part of speech tagger ten times faster than the fastest stochastic tagger [Roche and Schabes, 1995]. One weakness of this rulebased tagger is that no unsupervised training algorithm has been presented for learning rules automatically without a manually annotated corpus. In this paper we present such an algorithm. We describe an algorithm for both unsupervised and weakly supervised training of a rule-based part of speech tagger, and compare the performance of this algorithm to that of the Baum-Welch algorithm. Transformation-Based Error-Driven Learning The rule-based tagger is based on a learning algorithm called transformation-based errordriven learning. Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, speech generation and syntactic parsing [Brill, 1992; Brill, 1994; Ramshaw and Marcus, 1994; Roche and Schabes, 1995; Brill and Resnik, 1994; Huang et al., 1994; Brill, 1993a; Brill, 1993b]. Figure 1 illustrates the learning process. First, unannotated text is passed through an initial-state annotator. The initial-state annotator can range in complexity from assigning random structure to assigning the output of a sophisticated manually created annotator. Once text has been passed through the initial-state annotator, it is then compared to the truth as specified in a manually annotated corpus, and transformations are learned that can be applied to the output of the initial state annotator to make it better resemble the truth. In all of the applications explored to date, the following greedy search is applied: at each iteration of learning, the transformation is found whose application results in the highest score; that transformation is then added to the ordered transformation list and the training corpus is updated by applying the learned transformation. To define a specific application of transformation-based learning, one must specify the following: 1. The initial state annotator. 2. The space of transformations the learner is allowed to examine. 3. The scoring function for comparing the corpus to the truth and choosing a transformation. Once an ordered list of transformations is learned, new text can be annotated by first applying the initial state annotator to it and then applying each of the learned transformations, in order.	attachments;baum–welch algorithm;brill tagger;cutting-plane method;dictionary;error-driven learning;francis;fastest;finite-state transducer;greedy algorithm;harris affine region detector;ibm notes;iteration;local interconnect network;logic programming;markov chain;markov model;natural language;p (complexity);parsing;part-of-speech tagging;randomness;speech synthesis;tagged architecture;text corpus;unsupervised learning;welch's method;word lists by frequency;word-sense disambiguation	Eric Brill	1995				NLP	-23.850401245959556	-78.08877293445356	67169
ff39fb9eedda71a7f038de599b0962d2f518f727	constraint-based sentence compression: an integer programming approach		The ability to compress sentences while preserving their grammaticality and most of their meaning has recently received much attention. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art.	algorithm;display resolution;f1 score;integer programming;jean;loss function;mathematical optimization;maxima and minima;optimization problem;parallel text;parsing;software portability;text corpus	James Clarke;Mirella Lapata	2006			natural language processing;machine learning;algorithm;global optimization	NLP	-19.90458489488549	-76.03847228261478	67202
72fcf7858f3b2cb96bfd06871c49e62039498f4b	a graph-based cross-lingual projection approach for weakly supervised relation extraction	significant annotation effort;english-korean parallel corpus;external resource;extensive study;novel graph-based projection approach;graph-based cross-lingual projection approach;relation extraction;parallel corpus;korean relation extraction system;cross-lingual annotation projection;weakly supervised relation extraction;relation extractor	Although researchers have conducted extensive studies on relation extraction in the last decade, supervised approaches are still limited because they require large amounts of training data to achieve high performances. To build a relation extractor without significant annotation effort, we can exploit cross-lingual annotation projection, which leverages parallel corpora as external resources for supervision. This paper proposes a novel graph-based projection approach and demonstrates the merits of it by using a Korean relation extraction system based on projected dataset from an English-Korean parallel corpus.	database dump;label propagation algorithm;parallel text;performance;randomness extractor;relationship extraction;software propagation;text corpus;wikipedia	Seokhwan Kim;Gary Geunbae Lee	2012			computer science;machine learning;pattern recognition;data mining	NLP	-21.04905422422661	-72.09003813726245	67304
e1b0ab3e72c0678b96819d2f7a18fae0cdca5806	unsupervised generation of context-relevant training-sets for visual object recognition employing multilinguality	google;clutter;search engines;training;unsupervised learning image classification indexing iterative methods natural language processing object recognition;fasteners;context google nails clutter training search engines fasteners;multipath hierarchical matching pursuit unsupervised context relevant training set generation visual object recognition multilinguality image based object classification search engines noun focused indexing automatic clean data set generation transclean algorithm polysemes hit the nail fingernail metal nail human robot collaboration clean image data sets verb noun tuple;context;nails	"""Image based object classification requires clean training data sets. Gathering such sets is usually done manually by humans, which is time-consuming and laborious. On the other hand, directly using images from search engines creates very noisy data due to ambiguous noun-focused indexing. However, in daily speech nouns and verbs are always coupled. We use this for the automatic generation of clean data sets by the here-presented TRANSCLEAN algorithm, which through the use of multiple languages also solves the problem of polyesters (a single spelling with multiple meanings). Thus, we use the implicit knowledge contained in verbs, e.g. in an imperative such as """"hit the nail"""", implicating a metal nail and not the fingernail. One type of reference application where this method can automatically operate is human-robot collaboration based on discourse. A second is the generation of clean image data sets, where tedious manual cleaning can be replaced by the much simpler manual generation of a single relevant verb-noun tuple. Here we show the impact of our improved training sets for several widely used and state-of-the-art classifiers including Multipath Hierarchical Matching Pursuit. All tested classifiers show a substantial boost of about +20% in recognition performance."""	algorithm;imperative programming;matching pursuit;multipath propagation;outline of object recognition;plasma cleaning;signal-to-noise ratio;web search engine	Markus Schoeler;Florentin Wörgötter;Tomas Kulvicius;Jeremie Papon	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.112	computer vision;speech recognition;computer science;machine learning;data mining;clutter	Vision	-22.374928178130865	-73.97528380290296	67363
65dd05e45b90e692d8a6c62e399f82246ba70126	age and gender classification of tweets using convolutional neural networks		Determining age and gender from a series of texts is useful for areas such as business intelligence and digital forensics. We explore the use of convolutional neural networks together with word2vec word embeddings for this task in comparison to handcrafted features. The network constructed consists of five layers and is trained using adadelta. It starts with an embedding layer where a word is represented by a vector, followed by a convolutional layer composed of three filters, each with 100 feature maps. It is followed by a max-over-time pooling layer which is done on each map and the resulting features are concatenated before a dropout layer and a softmax layer. The network was trained to classify age and gender for English and Spanish tweets. The predictions per tweet were aggregated using the majority prediction as the final prediction for the user who gave the tweets. The results outperform previous experiments. The highest English age and gender classification accuracy obtained are 49.6% and 72.1% respectively. The highest Spanish age and gender classification accuracy obtained on the other hand are 56.0% and 69.3% respectively.	convolutional neural network;neural networks	Roy Khristopher Bayot;Teresa Gonçalves	2017		10.1007/978-3-319-72926-8_28	convolutional neural network;word2vec;business intelligence;pooling;digital forensics;softmax function;pattern recognition;computer science;artificial intelligence	NLP	-20.043723933982918	-70.24458768500091	67366
bd6acb4e23e83156aafe2756f5f266fa04e7111f	geolocation with subsampled microblog social media	text subsampling;twitter;joint sparse coding;geolocation	We propose a data-driven geolocation method on microblog text. Key idea underlying our approach is sparse coding, an unsupervised learning algorithm. Unlike conventional positioning algorithms, we geolocate a user by identifying features extracted from her social media text. We also present an enhancement robust to a random erasure of words in the text and report our experimental results with uniformly or randomly subsampled microblog text. Our solution features a novel two-step procedure consisting of upconversion and iterative refinement by joint sparse coding. As a result, we can reduce the computational cost of geolocation while preserving accuracy. In the light of information preservation and privacy, we remark potential applications of this paper.	algorithm;algorithmic efficiency;case preservation;chroma subsampling;geolocation;iterative method;iterative refinement;neural coding;privacy;randomness;refinement (computing);social media;sparse matrix;unsupervised learning	Miriam Cha;Youngjune Gwon;H. T. Kung	2015		10.1145/2733373.2806357	computer science;data mining;geolocation;internet privacy;world wide web	NLP	-20.665476086526958	-66.61356846205622	67648
da90bd7ae4137ffe72522ca15dfaaaa3291a088e	hfst - framework for compiling and applying morphologies	finite state methods;a3 contribution to book other compilations refereed;software architecture;6121 languages;natural language;morphological analysis	HFST–Helsinki Finite-State Technology (hfst.sf.net) is a framework for compiling and applying linguistic descriptions with finite-state methods. HFST currently connects some of the most important finite-state tools for creating morphologies and spellers into one open-source platform and supports extending and improving the descriptions with weights to accommodate the modeling of statistical information. HFST offers a path from language descriptions to efficient language applications in key environments and operating systems. HFST also provides an opportunity to exchange transducers between different software providers in order to get the best out of each finite-state library.	algorithm;compiler;galaxy morphological classification;helsinki finite-state technology;library (computing);open-source software;operating system;programming language;transducer	Krister Lindén;Erik Axelson;Sam Hardwick;Tommi A. Pirinen;Miikka Silfverberg	2011		10.1007/978-3-642-23138-4_5	natural language processing;computer science;theoretical computer science	NLP	-32.59517628622005	-75.65918957354073	67722
eb3fea28ddad9105046a72c378502100db363b3d	a crowdsourcing approach for annotating causal relation instances in wikipedia		This paper presents a crowdsourcing approach for annotating causal relation instances to Wikipedia. Because an annotation task cannot be decomposed into multiple-choice problems, we integrate a crowdsourcing service and brat, a popular on-line annotation tool, to provide an easy-to-use interface and quality control for annotation work. We design simple micro-tasks that involve annotating textual spans with causal relations. We issued the micro-tasks to crowd workers, and collected 95,008 annotations of causal relation instances among 8,745 summary sentences in 1,494 Wikipedia articles. The annotated corpus not only provides supervision data for automatic recognition of causal relation instances, but also reveals valuable facts for improving the annotation process of this task.	brat;causal filter;causality;crowdsourcing;downstream (software development);natural language processing;online and offline;question answering;text corpus;wikipedia	Kazuaki Hanawa;Akira Sasaki;Naoaki Okazaki;Kentaro Inui	2017			data mining;crowdsourcing;computer science	NLP	-27.324915241082515	-69.76568490213606	67873
6721a7f801e1695255ebcd4c5b313c597dd17a5a	chanot: an intelligent annotation tool for indigenous and highly agglutinative languages in peru		Linguistic corpus annotation is one of the most important phases for addressing Natural Language Processing (NLP) tasks, as these methods are deeply involved with corpus-based techniques. However, meta-data annotation is a highly laborious manual task. A supportive alternative requires the use of computational tools. They are likely to simplify some of these operations, while can be adjusted appropriately to the needs of particular language features at the same time. Therefore, this paper presents ChAnot, a web-based annotation tool developed for Peruvian indigenous and highly agglutinative languages, where Shipibo-Konibo was the case study. This new tool is able to support a diverse set of linguistic annotation tasks, such as morphological segmentation markup, POS-tag markup, among others. Also, it includes a suggestion engine based on historic and machine learning models, and a set of statistics about previous	machine learning;markup language;natural language processing;norm (social);programming language;text corpus;web application	Rodolfo Mercado-Gonzales;José Pereira-Noriega;Marco Antonio Sobrevilla Cabezudo;Arturo Oncevay-Marcos	2018			natural language processing;indigenous;artificial intelligence;computer science;agglutinative language;annotation	NLP	-30.622523884472766	-73.54021820476136	67929
e79a3b55e55215b25d6a72bf39f627084ed451c9	a neural network based translation constrained reranking model for chinese dependency parsing	会议论文	Bilingual dependency parsing aims to improve parsing performance with the help of bilingual information. While previous work have shown improvements on either or both sides, most of them mainly focus on designing complicated features and rely on golden translations during training and testing. In this paper, we propose a simple yet effective translation constrained reranking model to improve Chinese dependency parsing. The reranking model is trained using a max-margin neural network without any manually designed features. Instead of using golden translations for training and testing, we relax the restrictions and use sentences generated by a machine translation system, which dramatically extends the scope of our model. Experiments on the translated portion of the Chinese Treebank show that our method outperforms the state-of-the-art monolingual Graph/Transition-based parsers by a large margin (UAS).	artificial neural network;parsing	Miaohong Chen;Baobao Chang;Yang Liu	2015		10.1007/978-3-319-25816-4_20	natural language processing;speech recognition;pattern recognition	NLP	-21.693072646638228	-76.49574529139304	67937
d1eb3bd7fb012505a4e8ddd920d6d70fddb069b5	bengali named entity recognition using support vector machine	information extraction;information retrieval;named entity recognition;machine learning;cross validation;question answer ing;support vector machine;natural language processing;named entity	Named Entity Recognition (NER) aims to classify each word of a document into predefined target named entity classes and is nowadays considered to be fundamental for many Natural Language Processing (NLP) tasks such as information retrieval, machine translation, information extraction, question answering systems and others. This paper reports about the development of a NER system for Bengali using Support Vector Machine (SVM). Though this state of the art machine learning method has been widely applied to NER in several well-studied languages, this is our first attempt to use this method to Indian languages (ILs) and particularly for Bengali. The system makes use of the different contextual information of the words along with the variety of features that are helpful in predicting the various named entity (NE) classes. A portion of a partially NE tagged Bengali news corpus, developed from the archive of a leading Bengali newspaper available in the web, has been used to develop the SVM-based NER system. The training set consists of approximately 150K words and has been manually annotated with the sixteen NE tags. Experimental results of the 10-fold cross validation test show the effectiveness of the proposed SVM based NER system with the overall average Recall, Precision and F-Score of 94.3%, 89.4% and 91.8%, respectively. It has been shown that this system outperforms other existing Bengali NER systems.	acceptance testing;archive;experiment;f1 score;information extraction;information retrieval;machine learning;machine translation;named entity;named-entity recognition;natural language processing;principle of maximum entropy;question answering;support vector machine;test set	Asif Ekbal;Sivaji Bandyopadhyay	2008			natural language processing;support vector machine;speech recognition;computer science;machine learning;pattern recognition;entity linking;information extraction;cross-validation	AI	-23.37924125203405	-71.08694144176559	67986
d6315056d1ee5be30ae9fd424a28c28be2d1fd34	chinese textual entailment recognition based on syntactic tree clipping	minimum information tree;syntactic tree clipping;machine learning;期刊论文;textual entailment	Textual entailment has been proposed as a unifying generic framework for modeling language variability and semantic inference in different Natural Language Processing (NLP) tasks. This paper presents a novel statistical method for recognizing Chinese textual entailment in which lexical, syntactic with semantic matching features are combined together. In order to solve the problems of syntactic tree matching difficulty and tree structure errors caused by Chinese word segmentation, the method firstly clips the syntactic trees into minimum information trees and then computes syntactic matching similarity on them. All features will be used in a voting style under different machine learning methods to predict whether the text sentence can entail the hypothesis sentence in a text-hypothesis pair. The experimental results show that the feature on changing structure of syntactic tree is effective and efficient in Chinese textual entailment.	machine learning;modeling language;natural language processing;semantic matching;spatial variability;text segmentation;textual entailment;tree structure	Zhichang Zhang;Dongren Yao;Songyi Chen;Huifang Ma	2014		10.1007/978-3-319-12277-9_8	natural language processing;computer science;machine learning;pattern recognition	NLP	-23.897273580022432	-72.87875780703438	67988
81a5f9110f72133b2bb58f55d966e2727aa94ffc	on deriving rules for nativised pronunciation in navigation queries.	grapheme- to-phone;lexicon;pronunciation;navigation;non-native	Navigation queries are typical examples of contexts in which a recognizer may have to deal with non-native names. In order to build a pronunciation lexicon with these names, special GtoP rules may be derived. The paper addresses this problem in the context of navigation queries in French including German names and viceversa. The special GtoP rules were mostly based on statistics derived from cross-lingual spoken corpora.	finite-state machine;lexicon;text corpus	Isabel Trancoso;Céu Viana;Isabel Mascarenhas;Carlos Teixeira	1999			linguistics	Web+IR	-27.715853076322734	-79.19502884325739	68050
a6d06317aa69a7ea5079b48b2ef43c5adf634e0d	building a corpus-based historical portuguese dictionary: challenges and opportunities	nineteenth century;brazilian portuguese;profitability;human language technology	Historical corpora are important resources for different areas. Philology, Human Language Technology, Literary Studies, History, and Lexicography are some that benefit from them. However, compiling historical corpora is different from compiling contemporary corpora. Corpus designers have to deal with several characteristics inherent in historical texts, such as: absence of a spelling standard, pervasive use of abbreviations plus their spelling variations, lack of space between words, irregular use of hyphenation, nonstandard typographical symbols. This paper addresses the challenges posed in processing the corpus designed for the Historical Dictionary of Brazilian Portuguese (HDBP) project, which is composed of texts from the sixteenth through the beginning of the nineteenth century, and the solutions found to support the compilation of a Historical Portuguese dictionary based on	central processing unit;compiler;computer scientist;corpus linguistics;data dictionary;dictionary;emoticon;glossary;heuristic (computer science);human–computer interaction;hyphenation algorithm;information retrieval;language technology;lexicography;lexicon;machine learning;open-source software;pervasive informatics;sputter cleaning;text encoding initiative;text corpus;tycho-2 catalogue;universal storage platform;vocabulary	Arnaldo Candido;Sandra M. Aluísio	2009	TAL		humanities;art;linguistics;literature	AI	-31.643739758082706	-75.20581735827011	68062
1af0530a8e6ce171cdca512ce85a69e8e835bb1c	biomedical named entity recognition based on extended recurrent neural networks	biocreative ii gm data set biomedical named entity recognition extended recurrent neural networks bio ner entity extraction genes proteins biomedical knowledge acquisition linguistic analysis extended rnn context information topical information clustering information word embeddings;text analysis bioinformatics genetics knowledge acquisition medical computing pattern clustering proteins recurrent neural nets;context information;word embedding;bio ner;hand designed features;context information bio ner recurrent neural network hand designed features word embedding;recurrent neural network;biological system modeling artificial neural networks	Biomedical named entity recognition (bio-NER), which extracts important entities such as genes and proteins, has become one of the most fundamental tasks in biomedical knowledge acquisition. However, the performance of traditional NER systems is always limited to the construction of complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area. In this paper we mainly focus on building a simple and efficient system for bio-NER with the extended Recurrent Neural Network (RNN) which considers the predicted information from the prior node and external context information (topical information & clustering information). Extracting complex hand-designed features is skipped and replaced with word embeddings. The experiments conducted on the BioCreative II GM data set demonstrate RNN models outperform CRF model and deep neural networks (DNN); furthermore, the extended RNN model performs better than the original RNN model.	artificial neural network;biocreative;british informatics olympiad;cluster analysis;conditional random field;deep learning;experiment;knowledge acquisition;named entity;named-entity recognition;neural networks;random neural network;recurrent neural network;word embedding	Lishuang Li;Liuke Jin;Zhenchao Jiang;Dingxin Song;Degen Huang	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359761	natural language processing;computer science;recurrent neural network;machine learning;pattern recognition;data mining	ML	-19.17085187792354	-70.53174030977823	68163
bb84e1fd470c3ac257e51e63381ed9ac6506c9cb	la reconnaissance automatique de la fonction des pronoms démonstratifs en langue arabe (automatic recognition of demonstrative pronouns function in arabic) [in french]		________________________________________________________________________________________________________ Automatic recognition of demonstrative pronouns function in Arabic Anaphora resolution is one of the most difficult tasks in NLP. Classifying pronouns before attempting a task of anaphora resolution is important because to handle the cataphoric pronoun, the system should determine the antecedent into the segment following the pronoun. Although, for the anaphoric pronoun, the system should look for the antecedent into the segment before the pronoun. In addition, the number of demonstrative pronouns is very important in Arabic. In this paper, we describe a machine learning method for classifying demonstrative pronouns in Arabic. We have evaluated our approach on a corpus of 365585 words which contain 14318 demonstrative pronouns and we have obtained encouraging results: 99.3% as F-Measure. MOTS-CLES : Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe.	anaphora (linguistics);linear algebra;machine learning;natural language processing;text corpus	Yacine Ben Yahia;Souha Hammami;Lamia Hadrich Belguith	2012				NLP	-27.07573487966868	-77.60833504506292	68252
e4315527f967e88fb1bebe836b3e8ece1b5ab3e9	automatic tm cleaning through mt and pos tagging: autodesk's submission to the nlp4tm 2016 shared task		We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English–Italian and English–Spanish.	binary classification;machine learning;machine translation;part-of-speech tagging;plasma cleaning;translation memory	Alena Zwahlen;Olivier Carnal;Samuel Läubli	2016	CoRR		computer science;data mining;database;information retrieval	NLP	-23.299661387344592	-75.19581105336644	68287
1525f96aac727000272ca034947aeb83af40d7c3	introduction to the special issue on word sense disambiguation: the state of the art	traitement automatique des langues naturelles;methode;word sense disambiguation;recent studies;meaning;meaning sense;etat de la recherche;polysemia;sens;polysemie;word;computational linguistics;semantique lexicale;desambiguisation;linguistique informatique;method;natural language processing;mot	ANIMATE, HUMAN, etc. and encode type restrictions on nouns and adjectives and on the arguments of verbs. Subject codes use another set of primitives to classify senses of words by subject (ECONOMICS, ENGINEERING, etc.). Guthrie et al. (1991) demonstrate a typical use of this information: in addition to using the Lesk-based method of counting overlaps between definitions and contexts, they impose a correspondence of subject codes in an iterative process. No quantitative evaluation of this method is available, but Cowie et al. (1992) improve the method using simulated annealing and report results of 47% for sense distinctions and 72% for homographs. The use of LDOCE box codes, however, is problematic: the codes are not systematic (see, for example, Fontenelle, 1990); in later work, Braden-Harder (1993) showed that simply matching box or subject codes is not sufficient for disambiguation. For example, in I tipped the driver, the codes for several senses of the words in the sentence satisfy the necessary constraints (e.g. tip-money + human object or tip-tilt + movable solid object). In many ways, the supple7 Note that the assumptions underlying this method are very similar to Quillian’s: Thus one may think of a full concept analogically as consisting of all the information one would have if he looked up what will be called the “patriarch” word in a dictionary, then looked up every word in each of its definitions, then looked up every word found in each of these, and so on, continually branching outward[...] (Quillian, 1968, p. 238). However, Quillian’s network also keeps track of semantic relationships among the words encountered along the path between two words, which are encoded in his semantic network; the neural network avoids the overhead of creating the semantic network but loses this relational information. 13 mentary information in the LDOCE, and in particular the subject codes, are similar to those in a thesaurus, which, however, are more systematically structured. Inconsistencies in dictionaries, noted earlier, are not the only and perhaps not the major source of their limitations for WSD. While dictionaries provide detailed information at the lexical level, they lack pragmatic information that enters into sense determination (see, e.g., Hobbs, 1987). For example, the link between ash and tobacco, cigarette or tray in a network such as Quillian’s is very indirect, whereas in the Brown Corpus, the word ash co-occurs frequently with one of these words. It is therefore not surprising that corpora have become a primary source of information for WSD; this development is outlined below in section 2.3. 1.3.2 Thesauri. Thesauri provide information about relationships among words, most notably synonymy. Roget's International Thesaurus, which was put into machine-tractable form in the 1950's8 and has been used in a variety of applications including machine translation (Masterman, 1957), information retrieval (Sparck Jones, 1964, 1986), and content analysis (Sedelow and Sedelow, 1969; see also Sedelow and Sedelow, 1986, 1992), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels. Typically, each occurrence of the same word under different categories of the thesaurus represent different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky, 1992). A set of words in the same category are semantically related. The earliest known use of Roget’s for WSD is the work of Masterman (1957), described above in section 2.1. Several years later, Patrick (1985) used Roget’s to discriminate among verb senses, by examining semantic clusters formed by “e-chains” derived from the thesaurus (Bryan, 1973, 1974; see also Sedelow and Sedelow, 1986). He uses “word-strong neighborhoods,” comprising word groups in low-level semicolon groups, which are the most closely related semantically in the thesaurus, and words connected to the group via chains. He is able to discriminate the correct sense of verbs such as inspire (to raise the spirits vs. to inhale, breathe in, sniff, etc.), question (to doubt vs. to ask a question) with “high reliability.” Bryan's earlier work had already demonstrated that homographs can be distinguished by applying a metric based on relationships defined by his chains (Bryan, 1973, 1974). Similar work is described in Sedelow and Mooney (1988). Yarowsky (1992) derives classes of words by starting with words in common categories in Roget's (4th ed.). A 100-word context of each word in the category is extracted from a corpus (the 1991 electronic text of Grolier's Ency8 The work of Masterman (1957) and Sparck Jones (1964) relied on a version of Roget’s that was hand-punched onto cards in the 1950’s; the Sedelow’s (1969) work relied on a machine readable version of the 3rd Edition. Roget’s is now widely available via anonymous ftp from various sites.	artificial neural network;book;brown corpus;cobham's thesis;code;computation;computational linguistics;definition;dictionary;document retrieval;encode;generative model;high- and low-level;hobbs meter;human-readable medium;information access;information retrieval;information source;iterative method;jones calculus;lesk algorithm;lexicon;location-based service;machine translation;overhead (computing);packet analyzer;pragmatic theory of information;primary source;roget's thesaurus;semantic network;simulated annealing;stumbleupon;text corpus;web services for devices;word sense;word-sense disambiguation	Nancy Ide;Jean Véronis	1998	Computational Linguistics		natural language processing;method;computer science;computational linguistics;word;linguistics;meaning	NLP	-29.386359814347838	-77.23080906984406	68359
a72b8d8dd2f799fb0a8b5045a3e0d01ed9659186	genetic algorithm (ga) in feature selection for crf based manipuri multiword expression (mwe) identification	information retrieval;part of speech tagging;conditional random field;genetic algorithm;feature selection;cross validation;natural language processing;machine translation;question answering;fitness function;evolutionary computing	This paper deals with the identification of Multiword Expressions (MWEs) in Manipuri, a highly agglutinative Indian Language. Manipuri is listed in the Eight Schedule of Indian Constitution. MWE plays an important role in the applications of Natural Language Processing(NLP) like Machine Translation, Part of Speech tagging, Information Retrieval, Question Answering etc. Feature selection is an important factor in the recognition of Manipuri MWEs using Conditional Random Field (CRF). The disadvantage of manual selection and choosing of the appropriate features for running CRF motivates us to think of Genetic Algorithm (GA). Using GA we are able to find the optimal features to run the CRF. We have tried with fifty generations in feature selection along with three fold cross validation as fitness function. This model demonstrated the Recall (R) of 64.08%, Precision (P) of 86.84% and F-measure (F) of 73.74%, showing an improvement over the CRF based Manipuri MWE identification without GA application.	conditional random field;cross-validation (statistics);f1 score;feature selection;fitness function;genetic algorithm;information retrieval;machine translation;minimal working example;natural language;parsing expression grammar;part-of-speech tagging;question answering;regular expression	Kishorjit Nongmeikapam;Sivaji Bandyopadhyay	2011	CoRR	10.5121/ijcsit.2011.3505	natural language processing;speech recognition;genetic algorithm;question answering;computer science;machine learning;pattern recognition;machine translation;feature selection;fitness function;conditional random field;cross-validation;evolutionary computation	NLP	-23.330096667896264	-71.02904431247045	68402
cbc1bbc6f9152f922d869cdb33139b92f9184499	the role of discourse units in near-extractive summarization.		Although human-written summaries of documents tend to involve significant edits to the source text, most automated summarizers are extractive and select sentences verbatim. In this work we examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers to produce a wider range of human-like summaries. Our analysis demonstrates that EDU segmentation is effective in preserving human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger summarization performance in near-extractive scenarios, especially under tight budgets.	automatic summarization;the new york times	Junyi Jessy Li;Kapil Thadani;Amanda Stent	2016		10.18653/v1/W16-3617	linguistics;communication	NLP	-29.5455452260167	-72.56610268023766	68407
835a43146c5f61ef77f4ed5f4c52e5beb66043c0	multilingual extended wordnet knowledge base: semantic parsing and translation of glosses		This paper presents a method to create WordNet-like lexical resources for different languages. Instead of directly translating glosses from one language to another, we perform first semantic parsing of WordNet glosses and then translate the resulting semantic representation. The proposed approach simplifies the machine translation of the glosses. The approach provides ready to use semantic representation of glosses in target languages instead of just plain text.	extended wordnet;gloss (annotation);knowledge base;lexical definition;machine translation;parsing	Tatiana N. Erekhinskaya;Meghana Satpute;Dan I. Moldovan	2014			natural language processing;extended wordnet;programming language;information retrieval	NLP	-29.761496580760596	-71.46724439607223	68486
881e0f03ffd7889e1321ec3483834526f5470af7	identifying computer-translated paragraphs using coherence features		We have developed a method for extracting the coherence features from a paragraph by matching similar words in its sentences. We conducted an experiment with a parallel German corpus containing 2000 human-created and 2000 machine-translated paragraphs. The result showed that our method achieved the best performance (accuracy = 72.3%, equal error rate = 29.8%) when it is compared with previous methods on various computer-generated text including translation and paper generation (best accuracy = 67.9%, equal error rate = 32.0%). Experiments on Dutch, another rich resource language, and a low resource one (Japanese) attained similar performances. It demonstrated the efficiency of the coherence features at distinguishing computer-translated from human-created paragraphs on diverse languages.		Hoang-Quoc Nguyen-Son;Ngoc-Dung T. Tieu;Huy H. Nguyen;Junichi Yamagishi;Isao Echizen	2018	CoRR			NLP	-26.262782292325756	-74.097889477798	68628
7188d328e037f112c105cccaf50436c5643e5022	improving word sense induction by exploiting semantic relevance		Word Sense Induction (WSI) is the task of automatically inducing the different senses of a target word from unannotated text. Traditional approaches based on the vector space model (VSM) represent each context of a target word as a vector of selected features (e.g. the words occurring in the context). These approaches assume that the words occurring in the context are independent and do not exploit semantic relevance between words. In this paper we propose a WSI method which can exploit semantic relevance between words by incorporating a word graph into the framework of clustering of context vectors. The method is evaluated on the testing data of the Chinese Word Sense Induction task of the first CIPSSIGHAN Joint Conference on Chinese Language Processing (CLP2010). Experimental results show that our method significantly outperforms the baseline methods.	baseline (configuration management);cluster analysis;relevance;viable system model;wafer-scale integration;word sense;word-sense induction	Zhenzhong Zhang;Le Sun	2011			natural language processing;speech recognition;semeval;computer science;machine learning;pattern recognition	NLP	-24.733821740011958	-68.80245261740005	68889
88a5ef6e4d31e102edd972e09349f99a2572725f	linking imagact ontology to babelnet through action videos		English. Herein we present a study dealing with the linking of two multilingual and multimedia resources, BabelNet and IMAGACT, which seeks to connect videos contained in the IMAGACT Ontology of Actions with related action concepts in BabelNet. The linking is based on a machine learning algorithm that exploits the lexical information of the two resources. The algorithm has been firstly trained and tested on a manually annotated dataset and then it was run on all the data, allowing to connect 773 IMAGACT action videos with 517 BabelNet synsets. This linkage aims to enrich BabelNet verbal entries with a visual representations and to connect the IMAGACT ontology to the huge BabelNet semantic network. Italiano. In questo articolo si presenta uno studio sul linking tra due risorse linguistiche multilingui e multimediali, BabelNet e IMAGACT. L’esperimento ha l’obiettivo di collegare i video dell’ontologia dell’azione IMAGACT con i concetti azionali contenuti in BabelNet. Il collegamento è realizzato attraverso un algoritmo di Machine Learning che sfrutta l’informazione lessicale delle due risorse. L’algoritmo è stato addestrato e valutato su un dataset annotato manualmente e poi eseguito sull’insieme totale dei dati, permettendo di collegare 773 video di IMAGACT con 517 synset di BabelNet. Questo linking ha lo scopo di arricchire le entrate verbali di BabelNet con una rappresentazione visuale e di collegare IMAGACT alla rete semantica di BabelNet.	apache poi;babelnet;computer vision;entity;linkage (software);machine learning;naruto shippuden: clash of ninja revolution 3;natural language processing;rete algorithm;scalability;semantic network;statistical model;synonym ring;unique name assumption;uno	Lorenzo Gregori;Alessandro Panunzi;Andrea Amelio Ravelli	2016			natural language processing;ontology;artificial intelligence;computer science	AI	-30.19437992613795	-76.31955564872354	68926
496f395d4d4038e85ba666691382f717b83c564b	unitn: training deep convolutional neural network for twitter sentiment classification		This paper describes our deep learning system for sentiment analysis of tweets. The main contribution of this work is a process to initialize the parameter weights of the convolutional neural network, which is crucial to train an accurate model while avoiding the need to inject any additional features. Briefly, we use an unsupervised neural language model to initialize word embeddings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model which is then trained on the supervised training data from Semeval-2015. According to results on the official test sets, our model ranks 1st in the phrase-level subtask A (among 11 teams) and 2nd on the messagelevel subtask B (among 40 teams). Interestingly, computing an average rank over all six test sets (official and five progress test sets) puts our system 1st in both subtasks A and B.	artificial neural network;convolutional neural network;deep learning;language model;sentiment analysis;statistical classification;unsupervised learning	Aliaksei Severyn;Alessandro Moschitti	2015			natural language processing;speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-21.277295935950498	-70.179297026593	69054
a14758da64e24d4524cacd70567a18e47d8311a5	mte-nn at semeval-2016 task 3: can machine translation evaluation help community question answering?		We present a system for answer ranking (SemEval-2016 Task 3, subtask A) that is a direct adaptation of a pairwise neural network model for machine translation evaluation (MTE). In particular, the network incorporates MTE features, as well as rich syntactic and semantic embeddings, and it efficiently models complex non-linear interactions between them. With the addition of lightweight task-specific features, we obtained very encouraging experimental results, with sizeable contributions from both the MTE features and from the pairwise network architecture. We also achieved good results on subtask C.	artificial neural network;interaction;lexicon;machine translation;network architecture;network model;nonlinear system;question answering;semeval;semantic similarity;textual entailment;veracity;word lists by frequency	Francisco Guzmán;Preslav Nakov;Lluís Màrquez i Villodre	2016			natural language processing;database;information retrieval	NLP	-19.338535588676045	-71.82657141235318	69076
4ea9028e3e784f923102c057412efe75e1177e3a	discourse-aware statistical machine translation as a context-sensitive spell checker		Real-word errors or context sensitive spelling errors, are misspelled words that have been wrongly converted into another word of vocabulary. One way to detect and correct real-word errors is using Statistical Machine Translation (SMT), which translates a text containing some real-word errors into a correct text of the same language. In this paper, we improve the results of mentioned SMT system by employing some discourseaware features into a log-linear reranking method. Our experiments on a real-world test data in Persian show an improvement of about 9.5% and 8.5% in the recall of detection and correction respectively. Other experiments on standard English test sets also show considerable improvement of real-word checking results.	algorithm;baseline (configuration management);context-sensitive grammar;experiment;learning to rank;linear model;log-linear model;spell checker;statistical machine translation;test data;test set;vocabulary	Behzad Mirzababaei;Heshaam Faili;Nava Ehsan	2013			natural language processing;speech recognition;computer science;programming language	NLP	-23.286002716359416	-76.47543423161315	69091
5e7891ce1ee02d514d3adac378e95e681007e380	model-portability experiments for textual temporal analysis	information extraction;semi supervised learning	We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.	language model;semi-supervised learning;semiconductor industry;speech corpus;unsupervised learning;wikipedia;wordnet	Oleksandr Kolomiyets;Steven Bethard;Marie-Francine Moens	2011			natural language processing;computer science;machine learning;data mining;information extraction;information retrieval	NLP	-22.572091464708564	-72.88509026855718	69203
87050ec98529edb0deb320423ba3fd30ec47d1cf	standard for morphosyntactic and syntactic corpus annotation: the morphosyntactic and the syntactic annotation framework, maf and synaf		This talk is about the standards for morpho-syntactic and syntactic corpus annotation: MAF (Morpho-syntactic Annotation Framework, ISO/FDIS 24611) and SynAF (Syntactic Annotation Framework, ISO 24615:2010). Both standards complement each other and are closely related. In contrast to MAF, which describes features such as part of speech, morphological and grammatical features, SynAf describes relations between single words, and how words are arranged with each other and connected to build phrases and sentences. The talk presents an overview of the current state of both standards.		Laurent Romary	2012			natural language processing;speech recognition;computer science;linguistics	NLP	-30.82187789697577	-76.23175974920876	69307
270bce46fe01558f5ee6fd2c0c5915297f901a13	applying feature coupling generalization for protein-protein interaction extraction	unlabeled data;kernel;class distinguishing features;semisupervised learning strategy;syntactic information;support vector machines;boolean functions;example distinguishing features;data mining kernel protein engineering semisupervised learning machine learning national electric code bioinformatics computer science biomedical engineering application software;data mining;semi supervised learning;proteins;boolean lexical features;feature extraction;proteins boolean functions molecular biophysics;molecular biophysics;protein protein interaction;syntactic information feature coupling generalization protein protein interaction extraction semisupervised learning strategy example distinguishing features class distinguishing features boolean lexical features;protein protein interaction extraction;feature coupling generalization;protein engineering;bioinformatics	We present the application of a recently proposed semi-supervised learning strategy – feature coupling generalization (FCG) – in the task of protein-protein interaction extraction from biomedical literatures. FCG is a framework that generates new features from relatedness of two special types of old features: example-distinguishing features (EDFs) and class-distinguishing features (CDFs). Their relatedness estimated from unlabeled data tends to capture indicative information not available in labeled data. For this task, we designed several EDFs and CDFs derived from the text patterns surrounding the co-occurrence proteins, and combined the new features generated by FCG with Boolean lexical features. The experimental results on AIMED corpus show that the new features yield significant improvement over a strong baseline, and the combined method achieves state-of-the-art performance without using any syntactic information.	baseline (configuration management);experiment;family computer disk system;fluid construction grammar;image processing;machine learning;natural language processing;nonlinear system;parsing;pixel density;relationship extraction;semi-supervised learning;semiconductor industry;supervised learning	Yanpeng Li;Hongfei Lin;Zhihao Yang	2009	2009 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2009.65	protein–protein interaction;support vector machine;kernel;feature extraction;computer science;bioinformatics;machine learning;pattern recognition;data mining;protein engineering;boolean function;molecular biophysics	NLP	-24.529672461654673	-72.77567673859481	69371
4be847a2ec7d360a8e66951dc8cf7fc1b877e46b	bootstrapping a multilingual part-of-speech tagger in one person-day	libraries;high rate;automatic;language use;transfer;english language;generic model;learning;agreements;training;resources;speech;windows;accuracy;grammars;translations;interactions;part of speech;foreign languages;speech recognition;algorithms;data acquisition;supervision;global	This paper presents a method for bootstrapping a fine-grained, broad-coverage part-of-speech (POS) tagger in a new language using only one personday of data acquisition effort. It requires only three resources, which are currently readily available in 60-100 world languages: (1) an online or hard-copy pocket-sized bilingual dictionary, (2) a basic library reference grammar, and (3) access to an existing monolingual text corpus in the language. The algorithm begins by inducing initial lexical POS distributions from English translations in a bilingual dictionary without POS tags. It handles irregular, regular and semi-regular morphology through a robust generative model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort.	algorithm;bilingual dictionary;brill tagger;brown corpus;data acquisition;experiment;galaxy morphological classification;generative model;ibm notes;library (computing);semiconductor industry;tag cloud;text corpus	Silviu Cucerzan;David Yarowsky	2002		10.3115/1118853.1118859	foreign language;natural language processing;interaction;speech recognition;part of speech;computer science;speech;english;accuracy and precision;linguistics;data acquisition;automatic transmission;resource	NLP	-26.12075960845298	-75.40727667203545	69447
13e051c825439ddbb476dbf8e812622ff4d87a47	punctuating speech for information extraction	automatic sentence boundary detection;information extraction;noun phrase;informa tion extraction;natural language processing speech punctuation information extraction automatic sentence boundary detection comma prediction speech entity extraction speech relation extraction error analysis noun phrase splitting syntactic parser machine generated transcript maximum f measure;information retrieval;speech;syntactic parser;noun phrase splitting;indexing terms;data mining error analysis speech recognition automatic speech recognition broadcasting predictive models natural language processing speech processing natural languages tagging;maximum f measure;error analysis;grammars;relative error;speech recognition grammars information retrieval natural language processing;speech punctuation;speech entity extraction;speech recognition;extraction speech punctuation prediction information;punctuation prediction;speech relation extraction;machine generated transcript;natural language processing;information;extraction;comma prediction	This paper studies the effect of automatic sentence boundary detection and comma prediction on entity and relation extraction in speech. We show that punctuating the machine generated transcript according to maximum F-measure of period and comma annotation results in suboptimal information extraction. Precisely, period and comma decision thresholds can be chosen in order to improve the entity value score and the relation value score by 4% relative. Error analysis shows that preventing noun-phrase splitting by generating longer sentences and fewer commas can be harmful for IE performance. Indeed, it seems that missed punctuation allows syntactic parsers to merge noun-phrases and prevent the extraction of correct information.	information extraction;parsing;relationship extraction	Benoît Favre;Ralph Grishman;Dustin Hillard;Heng Ji;Dilek Z. Hakkani-Tür;Mari Ostendorf	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518784	natural language processing;approximation error;extraction;noun phrase;speech recognition;index term;information;computer science;speech;information extraction	NLP	-24.167186517001117	-76.29396110693254	69450
05cbbdd9f496d4e311774a55f6f03ba6c9ca1323	towards semi-supervised classification of discourse relations using feature correlations	supervised classification	Two of the main corpora available for training discourse relation classifiers are the RST Discourse Treebank (RST-DT) and the Penn Discourse Treebank (PDTB), which are both based on the Wall Street Journal corpus. Most recent work using discourse relation classifiers have employed fully-supervised methods on these corpora. However, certain discourse relations have little labeled data, causing low classification performance for their associated classes. In this paper, we attempt to tackle this problem by employing a semi-supervised method for discourse relation classification. The proposed method is based on the analysis of feature cooccurrences in unlabeled data. This information is then used as a basis to extend the feature vectors during training. The proposed method is evaluated on both RST-DT and PDTB, where it significantly outperformed baseline classifiers. We believe that the proposed method is a first step towards improving classification performance, particularly for discourse relations lacking annotated data.	baseline (configuration management);daisy digital talking book;discourse relation;feature vector;intel matrix raid;machine learning;semi-supervised learning;semiconductor industry;statistical classification;supervised learning;text corpus;the wall street journal;treebank	Hugo Hernault;Danushka Bollegala;Mitsuru Ishizuka	2010			natural language processing;computer science;pattern recognition;data mining	NLP	-21.28322853773487	-70.94142009206848	69581
c8665ff9b320bccc6b41a3bde6f61f55a346c12d	abstractive text classification using sequence-to-convolution neural networks		ive Text Classification Using Sequence-to-convolution Neural Networks Taehoon Kim Jihoon Yang Data Mining Research Laboratory Department of Computer Science and Engineering Sogang University {taehoonkim, yangjh}@sogang.ac.kr	artificial neural network;computer multitasking;computer science;convolution;data mining;document classification;encoder;loss function;multilayer perceptron;performance;preprocessor;variational principle;vocabulary;word embedding;yang	Taehoon Kim;Jihoon Yang	2018	CoRR		machine learning;artificial intelligence;artificial neural network;data pre-processing;preprocessor;convolution;computer science	ML	-20.089976915860497	-70.67089746195592	69590
1dd30cbeab4f26e6fe66b9084e48e4325d48ee40	sequence models for automatic highlighting and surface information extraction	information extraction;information retrieval;machine learning;sequence analysis;knowledge representation;probability model	With the increase of textual information available electronicall y, we assist to a great diversification of the demands on Information Retrieval (IR) and Information Extraction (IE) systems. In this paper we apply Machine Learning techniques of sequence analysis to the tasks of highlighting and labeling text with respect to an information extraction task. Specificall y, dynamic probabilit y models are used. Like IR systems, they use littl e semantics, are full y trainable and do not require any knowledge representation of the domain. Unli ke IR approaches, documents are considered as a dynamic sequence of words. Furthermore, additional word information is naturall y included in the representation. Models are evaluated on a sub-task of the MUC6 Scenario Template corpus. When morpho-syntactic word information is introduced into the representation, an increase in performances is observed.	diversification (finance);information extraction;information retrieval;knowledge representation and reasoning;machine learning;performance;sequence analysis	Massih-Reza Amini;Hugo Zaragoza;Patrick Gallinari	1999			natural language processing;cognitive models of information retrieval;computer science;machine learning;sequence analysis;pattern recognition;information extraction;information retrieval	Web+IR	-25.485511285987446	-70.49450137086033	69642
09bf596be6acc3e28c974bff771ebe887734d57c	learning non-taxonomic relations on demand for ontology extension	learning on demand;information retrieval;non taxonomic relations;ontology extension;dependency parsing	Learning non-taxonomic relations becomes an important research topic in ontology extension. Most of the existing learning approaches are mainly based on expert crafted corpora. These approaches are normally domain-speci ̄c and the corpora acquisition is laborious and costly. On the other hand, based on the static corpora, it is not able to meet personalized needs of semantic relations discovery for various taxonomies. In this paper, we propose a novel approach for learning non-taxonomic relations on demand. For any supplied taxonomy, it can focus on the segment of the taxonomy and collect information dynamically about the taxonomic concepts by using Wikipedia as a learning source. Based on the newly generated corpus, non-taxonomic relations are acquired through three steps: a) semantic relatedness detection; b) relations extraction between concepts; and c) relations generalization within a hierarchy. The proposed approach is evaluated on three di®erent prede ̄ned taxonomies and the experimental results show that it is e®ective in capturing non-taxonomic relations as needed and has good potential for the ontology extension on demand.	archive;emoticon;nl (complexity);numerical aperture;personalization;robustness (computer science);scalability;semantic similarity;taxonomy (general);text corpus;web ontology language;wikipedia	Yan Xu;Ge Li;Lili Mou;Yangyang Lu	2014	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194014400099	natural language processing;computer science;data mining;information retrieval;dependency grammar	AI	-30.231771681563906	-67.18014529278514	69648
c21a76d1f48fc7163a8f532a78fab183ca168fe6	enriching complex networks with word embeddings for detecting mild cognitive impairment from speech transcripts		Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce nongrammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments.	bag-of-words model;binary classification;complex network;microsoft word for mac;netware;parsing;sensor;support vector machine;word embedding	Leandro Borges dos Santos;Edilson Anselmo Corrêa Júnior;Osvaldo N. Oliveira;Diego R. Amancio;Leticia Lessa Mansur;Sandra M. Aluísio	2017		10.18653/v1/P17-1118	natural language processing;speech recognition;computer science;artificial intelligence;machine learning	NLP	-20.18333042357321	-71.35887551192356	69720
4dc9acb30a3569b2c2ddd850a38fe2fbbefd48a7	aspectual type and temporal relation classification	recent tempeval challenge;specific aspectual type;improved performance;unsupervised method;temporal relation classification;lexical aspect;large list;temporal information processing;aspectual type	In this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. the problems of the recent TempEval challenges. For a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types. We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. The improved performance of the resulting models shows that (i) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that (ii) temporal information processing can profit from information about aspectual type.	information processing;mined;relevance	Francisco Costa;António Branco	2012			natural language processing;computer science;linguistics	NLP	-25.957810158006797	-70.33644080601867	69790
2eb2f01dad53f1f675631841daa84ec743a58c01	a reason to optimize information processing with a core property of natural language	pragmatics natural languages search engines support vector machines semantics software;question answering information retrieval;question answering information retrieval natural language processing;relevant document retrieval information processing optimization linguistic expressions structural asymmetry argument structure property information retrieval system question answering systems natural language argument structure asymmetrical relations large databases;natural language processing	We focus on a property of natural language enabling the processing of information conveyed by linguistic expressions: structural asymmetry. We provide evidence that structural asymmetry is a property of argument structure. We focus on Information Retrieval and Question Answering systems and we provide evidence that these systems fail to recover natural language argument structure asymmetrical relations and thus they may fail to retrieve relevant documents from large databases and to provide relevant answers to questions. The processing of the underlying asymmetric relations will contribute to the optimization of Information Retrieval and Question Answering systems.	database;information processing;information retrieval;mathematical optimization;natural language;question answering	Anna Maria Di Sciullo	2013	2013 IEEE 12th International Conference on Intelligent Software Methodologies, Tools and Techniques (SoMeT)	10.1109/SoMeT.2013.6645660	natural language processing;question answering;cognitive models of information retrieval;computer science;data mining;information extraction;information retrieval	DB	-28.59403353363733	-67.07390002966444	69819
2f02990e869aaf2256fb675927b34429494f122a	a systematic review of automated grammar checking in english language		Grammar checking is the task of detection and correction of grammatical errors in the text. English is the dominating language in the field of science and technology. Therefore, the non-native English speakers must be able to use correct English grammar while reading, writing or speaking. This generates the need of automatic grammar checking tools. So far many approaches have been proposed and implemented. But less efforts have been made in surveying the literature in the past decade. The objective of this systematic review is to examine the existing literature, highlighting the current issues and suggesting the potential directions of future research. This systematic review is a result of analysis of 12 primary studies obtained after designing a search strategy for selecting papers found on the web. We also present a possible scheme for the classification of grammar errors. Among the main observations, we found that there is a lack of efficient and robust grammar checking tools for real time applications. We present several useful illustrationsmost prominent are the schematic diagrams that we provide for each approach and a table that summarizes these approaches along different dimensions such as target error types, linguistic dataset used, strengths and limitations of the approach. This facilitates better understandability, comparison and evaluation of previous research.	diagram;grammar checker;schematic;systematic review	Madhvi Soni;Jitendra Singh Thakur	2018	CoRR		english grammar;artificial intelligence;natural language processing;machine learning;schematic;computer science;grammar	NLP	-31.793571958045295	-73.98132972001824	69898
4d337fdffca3daf6ab8eb8b1b469e48730c6c6f5	semeval-2015 task 13: multilingual all-words sense disambiguation and entity linking		In this paper we present the Multilingual AllWords Sense Disambiguation and Entity Linking task. Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language. Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information. Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods. Moreover, we investigate this task in a multilingual setting and for some specific domains.	babelnet;baseline (configuration management);electronic signature;entity linking;heuristic;inventory;lesk algorithm;lexicography;memory disambiguation;named entity;natural language processing;performance;portland pattern repository;semeval;semantic similarity;supervised learning;web services for devices;word sense;word-sense disambiguation;wordnet	Andrea Moro;Roberto Navigli	2015			natural language processing;computer science;entity linking;linguistics	NLP	-27.462608519152305	-70.72888141949412	70340
18be53ba55282ce5cbd0ddebc99f137d07fae3b8	continuous multilinguality with language vectors		Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.	experiment;language model;natural language processing;text-based (computing)	Robert Östling;Jörg Tiedemann	2017			natural language processing;language identification;cache language model;natural language programming;speech recognition;universal networking language;language primitive;object language;computer science;language transfer;context-free language;linguistics;modeling language;natural language;programming language	NLP	-21.508752735145272	-79.41600211150873	70347
f2ddfefd3d0d1b166e485b712dc8ecc7107925b2	ushef and usaar-ushef participation in the wmt15 qe shared task		We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation. The USHEF submissions explored several document and discourse-aware features. The USAARUSHEF submissions used an exhaustive search approach to select the best features from the official baseline. Results show slight improvements over the baseline with the use of discourse features. More interestingly, we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure.	baseline (configuration management);brute-force search;feature selection;quadratic equation;query expansion;random forest	Carolina Scarton;Liling Tan;Lucia Specia	2015			computer science	NLP	-22.03733856418957	-72.59166925448486	70392
c63c41c1b240e7217b705a3afcdd68b4f64061bc	using on-line available sources of bilingual information for word-level machine translation quality estimation		This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c © 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND.	apertium;binary classification;black box;concordancer;experiment;google translate;norsk data;on the fly;online and offline;sl (complexity);statistical machine translation;transform, clipping, and lighting	Miquel Esplà-Gomis;Felipe Sánchez-Martínez;Mikel L. Forcada	2015			natural language processing;speech recognition;pattern recognition	NLP	-22.91291327899805	-73.14094184541007	70402
afa5bf88d849fe32502956b7622f28e44d60354e	parameter optimization for machine-learning of word sense disambiguation	filologias;linguistica;grupo a	Various Machine Learning (ML) approaches have been demonstrated to produce relatively successful Word Sense Disambiguation (WSD) systems. There are still unexplained differences among the performance measurements of different algorithms, hence it is warranted to deepen the investigation into which algorithm has the right ‘bias’ for this task. In this paper, we show that this is not easy to accomplish, due to intricate interactions between information sources, parameter settings, and properties of the training data. We investigate the impact of parameter optimization on generalization accuracy in a memory-based learning approach to English and Dutch WSD. A ‘word-expert’ architecture was adopted, yielding a set of classifiers, each specialized in one single wordform. The experts consist of multiple memory-based learning classifiers, each taking different information sources as input, combined in a voting scheme. We optimized the architectural and parametric settings for each individual word-expert by performing cross-validation experiments on the learning material. The results of these experiments show that the variation of both the algorithmic parameters and the information sources available to the classifiers leads to large fluctuations in accuracy. We demonstrate that optimization per word-expert leads to an overall significant improvement in the generalization accuracies of the produced WSD systems.	machine learning;mathematical optimization;word sense;word-sense disambiguation	Véronique Hoste;Iris Hendrickx;Walter Daelemans;Antal van den Bosch	2002	Natural Language Engineering	10.1017/S1351324902003005	natural language processing;computer science;artificial intelligence;machine learning	NLP	-20.951870803456014	-72.04790998679734	70513
f6151de15e7637ba85c1cc883a3c79ca04519df2	using semantic features to improve task identification in email messages	construction grammar;document representation;classification;feature construction;semantic features;ontology;domain specificity	Automated identification of tasks in email messages can be very useful to busy email users. What constitutes a task varies across individuals and must be learned for each user. However, training data for this purpose tends to be scarce. This paper addresses the lack of training data using domain-specific semantic features in document representation for reducing vocabulary mismatches and enhancing the discriminative power of trained classifiers when the number of training examples is relatively small.	domain-specific language;email;vocabulary	Shinjae Yoo;Donna Gates;Lori S. Levin;Simon Fung;Sachin Agarwal;Michael Freed	2008		10.1007/978-3-540-69858-6_43	natural language processing;biological classification;computer science;ontology;pattern recognition;data mining	NLP	-23.912216189088696	-72.46192556743536	70514
a33fb98367737513821dcc7807caefe52470a450	computer-based processes for tablature transcription: input language applications and development; analytical aspects			formal aspects of computing;medical transcription	Warren E. Hultberg	1978			natural language processing;theoretical computer science;tablature;artificial intelligence;computer science	Logic	-32.740125513726866	-78.03022582749772	70664
aa1fbc0fd55a07f5636b94920b64b45f7de58fd1	neural architectures for open-type relation argument extraction		In this work, we introduce the task of Open-Type Relation Argument Extraction (ORAE): Given a corpus, a query entity Q and a knowledge base relation (e.g., “Q authored notable work with title X”), the model has to extract an argument of non-standard entity type (entities that cannot be extracted by a standard named entity tagger, e.g., X: the title of a book or a work of art) from the corpus. We develop and compare a wide range of neural models for this task yielding large improvements over a strong baseline obtained with a neural question answering system. The impact of different sentence encoding architectures and answer extraction methods is systematically compared. An encoder based on gated recurrent units combined with a conditional random fields tagger yields the best results. We release a data set to train and evaluate ORAE, based on WikiData and obtained by distant supervision.		Benjamin Roth;Costanza Conforti;Nina Pörner;Sanjeev Karn;Hinrich Schütze	2018	CoRR		encoder;artificial intelligence;natural language processing;type relation;knowledge base;named entity;computer science;encoding (memory);conditional random field;question answering;sentence	NLP	-19.582691335589303	-72.85889579172998	70665
464b1964eb318e68afdd8527a731ce43c12d58f1	illc-uva adaptation system (scorpio) at wmt'16 it-domain task		This paper describes Scorpio, the ILLCUvA Adaptation System submitted to the IT-DOMAIN translation task at WMT 2016, which participated with the language pair of English-Dutch. This system consolidates the ideas in our previous work on latent variable models for adaptation, and demonstrates their effectiveness in a competitive setting.	digimon;latent variable	Cuong Hoang;Stella Frank;Khalil Sima'an	2016			machine learning;artificial intelligence;computer science;latent variable	NLP	-20.713093832103755	-73.82661436773213	70699
3091464eb1cfa10677f38249147d57089994a76b	an improved automatic term recognition method for spanish	automatic term recognition;hybrid approach	The C-value/NC-value algorithm, a hybrid approach to automatic term recognition, has been originally developed to extract multiword term candidates from specialised documents written in English. Here, we present three main modifications to this algorithm that affect how the obtained output is refined. The first modification aims to maximise the number of real terms in the list of candidates with a new approach for the stop-list application process. The second modification adapts the C-value calculation formula in order to consider single word terms. The third modification changes how the term candidates are grouped, exploiting a lemmatised version of the input corpus. Additionally, size of candidate’s context window is variable. We also show the necessary linguistic modifications to apply this algorithm to the recognition of term candidates in Spanish.	algorithm;linear algebra;microsoft windows;terminology extraction;value (computer science)	Alberto Barrón-Cedeño;Gerardo Sierra;Patrick Drouin;Sophia Ananiadou	2009		10.1007/978-3-642-00382-0_10	speech recognition;computer science;machine learning;algorithm	NLP	-26.592819930150007	-75.70780701687593	70709
7156ffd38e589392ef5bf67c0679dfe1af2f17ea	large-scale online semantic indexing of biomedical articles via an ensemble of multi-label classification models	bioasq;machine learning;multi-label ensemble;multi-label learning;semantic indexing;supervised learning	BACKGROUND In this paper we present the approach that we employed to deal with large scale multi-label semantic indexing of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge (2013-2017), a challenge concerned with biomedical semantic indexing and question answering.   METHODS Our main contribution is a MUlti-Label Ensemble method (MULE) that incorporates a McNemar statistical significance test in order to validate the combination of the constituent machine learning algorithms. Some secondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to the BioASQ's super-set, the PubMed articles collection) and the proper parametrization of the algorithms used to deal with this challenging classification task.   RESULTS The ensemble method that we developed is compared to other approaches in experimental scenarios with subsets of the BioASQ corpus giving positive results. In our participation in the BioASQ challenge we obtained the first place in 2013 and the second place in the four following years, steadily outperforming MTI, the indexing system of the National Library of Medicine (NLM).   CONCLUSIONS The results of our experimental comparisons, suggest that employing a statistical significance test to validate the ensemble method's choices, is the optimal approach for ensembling multi-label classifiers, especially in contexts with many rare labels.	algorithm;body of uterus;indexes;machine learning;moving target indication;multi-label classification;national library of medicine (u.s.);netware loadable module;numerous;p-value;paper;pubmed;question answering	Yannis Papanikolaou;Grigorios Tsoumakas;Manos Laliotis;Nikos Markantonatos;Ioannis P. Vlahavas	2017		10.1186/s13326-017-0150-0	semantic computing;data mining;computer science;data science;supervised learning;multi-label classification;search engine indexing;information retrieval;question answering;information and computer science	AI	-27.444966453113633	-69.24967157429232	70729
50c58dff5cefc83e649daffd08f9e3bf359848e8	semantic-based multilingual document clustering via tensor modeling	clustering	A major challenge in document clustering research arises from the growing amount of text data written in different languages. Previous approaches depend on language-specific solutions (e.g., bilingual dictionaries, sequential machine translation) to evaluate document similarities, and the required transformations may alter the original document semantics. To cope with this issue we propose a new document clustering approach for multilingual corpora that (i) exploits a large-scale multilingual knowledge base, (ii) takes advantage of the multi-topic nature of the text documents, and (iii) employs a tensor-based model to deal with high dimensionality and sparseness. Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches, in both a balanced and an unbalanced corpus evaluation.	bilingual dictionary;categorization;cluster analysis;cross-language information retrieval;document classification;knowledge base;machine translation;neural coding;semantic similarity;synonym ring;text corpus;unbalanced circuit	Salvatore Romeo;Andrea Tagarelli;Dino Ienco	2014			natural language processing;speech recognition;document clustering;computer science;machine learning;data mining;linguistics;cluster analysis	NLP	-25.60618932578091	-67.64515790032132	70805
6b599144223ef978c0a804c3a11b19ddf2cc223e	memory-based learning for article generation	penn treebank;important role;article use;corpus data;memory-based learning;difficult problem;article choice;english base noun phrase;competitive result;automated article generation;automated summarization;machine translation;noun phrase	Article choice can pose difficult problems in applications such as machine translation and automated summarization. In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase. We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation.	lexicon;machine translation;treebank	Guido Minnen;Francis Bond;Ann A. Copestake	2000			natural language processing;speech recognition;computer science;treebank;linguistics	NLP	-27.850176534074677	-73.54419544536817	70898
b8434eecf8cdcedc9ae5f33af382f175b3d5d815	modeling word perception using the elman network	authorship;word perception;stylistic similarity;linguistic analysis;compositional representation;content addressable memory;semantic search;polysemous word;elman network;personalized code;categorization	This paper presents an automatic acquisition process to acquire the semantic meaning for the words. This process obtains the representation vectors for stemmed words by iteratively improving the vectors, using a trained Elman network. Experiments performed on a corpus composed of Shakespeare’s writings show its linguistic analysis and categorization abilities. & 2008 Elsevier B.V. All rights reserved.	artificial neural network;categorization;code;experiment;fletcher's checksum;personalization;recurrent neural network;response surface methodology;text corpus	Cheng-Yuan Liou;Jau-Chi Huang;Wen-Chie Yang	2008	Neurocomputing	10.1016/j.neucom.2008.04.030	natural language processing;speech recognition;semantic search;computer science;machine learning;content-addressable memory;categorization	AI	-26.654867060120605	-79.19001753412387	70961
eed5b65df8ac04400961d029eb169df14db0644f	sentiment analysis for media reputation research		As a subtask of qualitative media reputation research, human annotators manually encode the polarity of actors in media products. Seeking to automate this process, we have implemented two baseline classifiers that categorize actors in newspaper articles under six and four polarity classes. Experiments have shown that our approach is not suitable for distinguishing between six finegrained classes, which has turned out to be difficult for humans also. In contrast, we have obtained promising results for the four class model, through which we argue that automated sentiment analysis has a considerable potential in qualitative reputation research.	algorithm;baseline (configuration management);categorization;encode;experiment;high- and low-level;machine learning;prototype;sentiment analysis	Samuel Läubli;Mario Schranz;Urs Christen;Manfred Klenner	2012			computer science;data mining;communication;world wide web	NLP	-21.542782597070342	-68.07781054596639	70989
aadabea7b251c6323d8f4f85e82961625d3435f9	a knowledge-based approach to ontology learning and semantic annotation	semantic web;query expansion;knowledge base	The so-called Semantic Web vision will certainly benefit from automatic semantic annotation of words in documents. We present a method, called structural semantic interconnections (SSI), that creates structural specifications of the possible senses for each word in a context, and selects the best hypothesis according to a grammar G, describing relations between sense specifications. The method has been applied to different semantic disambiguation problems, like automatic ontology construction, sensebased query expansion, disambiguation of words in glossary definitions. Evaluation experiments have been performed on each disambiguation task, as well	algorithm;computational linguistics;experiment;gene ontology term enrichment;glossary;hoc (programming language);knowledge-based systems;machine learning;ontology learning;open research;query expansion;refinement (computing);rewriting;semantic web;text corpus;type system;word sense;word-sense disambiguation	Roberto Navigli;Paola Velardi	2004			ontology-based data integration;semantic similarity;data mining;social semantic web;owl-s;semantic computing;ontology inference layer;natural language processing;computer science;information retrieval;semantic web stack;upper ontology;artificial intelligence	NLP	-31.943493215594238	-70.67396425727365	70996
403c1578fbab8bf17fcb69409e6dff3f9ecdc23e	parsing the voyager domain using pearl	time-asynchronous bottom-up chart parser;part-of-speech assignment;parsing tool;earley-type top-down prediction;previous work;previous attempt;unknown word model;voyager domain;unknown word;part-of-speech ambiguity;highest-scoring theory	"""This paper* describes a .al, ural language p~rsi.g algorithm &)r unresu'icl,ed I,ext which uses a probabilhq-hased scoring funct,iow I,o sele(:l, I,he """"besC: parse of ~/ sent,enos acc:ording t,c~ a given gra0nunar. The parser~ """"Pearl~ ix a i,hne-asynciironous t)ol,l,orn-u 1) chart, parser with Earley-l,ype I,Ol)-dowil t)redic:l,ion which pursues I,he }6gtmsl,-s(:ori.g t,heory in I,he (:|larl,, where I,he set)re of a IJmory represe.l,s I,|le exl,e.l, I,o W|lic:|l L}le ~:on|,exl, ()~ I,|le seutl,euic:e predic:l;s I,}lat, int,erprel,at,ion. This parser {lifters front previous au,eUnl)l,s el, sto(:}la.st,i(: parsers in I,|lal, it, .ses a richer h)rm of condii,ional probabilil, ies |)a.~ed on conl,exl, l,o predict, likelihood. Pearl Mso provides a framework for in(:orporat,ing the resuh,s of previo.s work in part,-of-st)eeeh assig.tnenl,) InlkutowIi word u||odels, and olJher l)rol)abilist,ic models (ff linguistfic feauJres im,o o.e parsi .g U)t)], inl,erleaving I,}lese I,e(:imiques | .stead of usi.g I, he I,ra~lil,iona] pipeline archil,eci, ure. In I, esl~ perh~ruvled on I,|ie Voyager (lirecl,io.finding domain, """"Pearl has been s.ccessful el, resolvi.g parl,-of-speech aunifiguhq, del,ermiufing cal,egories for umknow, words, and selecl,ing correcl, parses firsl, using a very loosely fil, l,lng covering grammar. ~"""	algorithm;artificial intelligence;bottom-up parsing;chart parser;experiment;ext js javascript framework;ncr voyager;pokémon: giratina and the sky warrior;statistical model;text corpus;ural (computer)	David M. Magerman;Mitchell P. Marcus	1991			speech recognition;linguistics	NLP	-24.01798251462241	-78.21110180570673	71145
46c0a7fb189b9d9abd04a8af617faf0109900c5e	stochastic inversion transduction grammars for obtaining word phrases for phrase-based statistical machine translation	stochastic inversion transduction grammar;bilingual training corpus;shared task;important problem;stochastic inversion translation;europarl corpus;word phrase;phrase-based statistical translation model;phrase-based statistical machine translation;good result	An important problem that is related to phrase-based statistical translation models is the obtaining of word phrases from an aligned bilingual training corpus. In this work, we propose obtaining word phrases by means of a Stochastic Inversion Translation Grammar. Experiments on the shared task proposed in this workshop with the Europarl corpus have been carried out and good results have been obtained.	europarl corpus;statistical machine translation;transduction (machine learning)	Joan-Andreu Sánchez;José-Miguel Benedí	2006			natural language processing;synchronous context-free grammar;noun phrase;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-22.09975599120739	-77.54400229046932	71216
c4f17e6c4d3dfa6540f7fb4febdc76f123e621ef	a method for extraction of future reference sentences based on semantic role labeling			semantic role labeling	Yoko Nakajima;Michal Ptaszynski;Hirotoshi Honma;Fumito Masui	2016	IEICE Transactions		natural language processing;computer science;information extraction;information retrieval	NLP	-30.45288502269422	-77.02091750836702	71317
9f3a2d5bec434d793ca5ccb6010f397e0e848dc6	optimizing question answering accuracy by maximizing log-likelihood	question answering	In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.	algorithm;cluster analysis;experiment;optimizing compiler;question answering;statistical model;strongly correlated material	Matthias H. Heie;Edward W. D. Whittaker;Sadaoki Furui	2010			natural language processing;question answering;computer science;data mining;database;information retrieval	NLP	-23.296155322487948	-72.00665892806059	71487
9da253d1d85844dfd0958af8a3f8ea369314c722	the exploration of deterministic and efficient dependency parsing	part-of-speech feature;efficient parsing algorithm;efficient parser;efficient model;efficient dependency parsing;language independent parsing;three-step multilingual dependency parser;main focus;root parser	In this paper, we propose a three-step multilingual dependency parser, which generalizes an efficient parsing algorithm at first phase, a root parser and postprocessor at the second and third stages. The main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing. The experimental results show that our method outperforms Maltparser in 13 languages. We expect that such an efficient model is applicable for most languages.	algorithm;deterministic parsing;language-independent specification;natural language processing	Yu-Chieh Wu;Yue-Shi Lee;Jie-Chi Yang	2006			natural language processing;parser combinator;memoization;speech recognition;canonical lr parser;parsing expression grammar;top-down parsing language;computer science;bottom-up parsing;parsing;glr parser;s-attributed grammar;programming language;ll parser;top-down parsing;simple lr parser	NLP	-22.018283977110364	-77.22795524951262	71488
ac21adfb98bbc626e731ed1bf1a8a905b12401b7	corpus linguistics 25 years on.facchinetti, roberta (ed.)	corpus linguistic		corpus linguistics	Guy De Pauw	2008	LLC	10.1093/llc/fqm033	natural language processing;computer science;corpus linguistics;linguistics	NLP	-30.646102836377167	-77.46662915458349	71576
9c30fd2f96f39a9398f9d57b7962c8d71ac1e38e	grammatical relations of myanmar sentences augmented by transformation-based learning of function tagging	context free grammar	In this paper we describe function tagging using Transformation Based Learning (TBL) for Myanmar that is a method of extensions to the previous statistics-based function tagger. Contextual and lexical rules (developed using TBL) were critical in achieving good results. First, we describe a method for expressing lexical relations in function tagging that statistical function tagging are currently unable to express. Function tagging is the preprocessing step to show grammatical relations of the sentences. Then we use the context free grammar technique to clarify the grammatical relations in Myanmar sentences or to output the parse trees. The grammatical relations are the functional structure of a language. They rely very much on the function tag of the tokens. We augment the grammatical relations of Myanmar sentences with transformation-based learning of function tagging.	brill tagger;context-free grammar;natural language processing;parse tree;parsing;preprocessor;tag (metadata);well-formed element	Win Win Thant;Tin Myat Htwe;Ni Lar Thein	2011	CoRR		natural language processing;speech recognition;computer science;linguistics	NLP	-20.37035181106676	-79.92382407166329	71652
6f8d131cb74f5fae322d607a86167da31a31f383	chinese grammatical error correction using statistical and neural models		This paper introduces the Alibaba NLP team’s system for NLPCC 2018 shared task of Chinese Grammatical Error Correction (GEC). Chinese as a Second Language (CSL) learners can use this system to correct grammatical errors in texts they wrote. We proposed a method to combine statistical and neural models for the GEC task. This method consists of two modules: the correction module and the combination module. In the correction module, two statistical models and one neural model generate correction candidates for each input sentence. Those two statistical models are a rule-based model and a statistical machine translation (SMT)-based model. The neural model is a neural machine translation (NMT)-based model. In the combination module, we implemented it in a hierarchical manner. We first combined models at a lower level, which means we trained several models with different configurations and combined them. Then we combined those two statistical models and a neural model at the higher level. Our system reached the second place on the leaderboard released by the official.		Junpei Zhou;Chen Li;Hengyou Liu;Zuyi Bao;Guangwei Xu;Linlin Li	2018		10.1007/978-3-319-99501-4_10	machine translation;error detection and correction;machine learning;artificial intelligence;sentence;statistical model;computer science	NLP	-22.743194171981155	-76.31708043371134	71847
cbda924e9648566300d65b69fa6c25840fc3466b	extraction of translation unit from chinese-english parallel corpora	chinese-english translation equivalent pairs;chinese-english parallel corpus;machine translation;chinese english translation unit;potential value;parallel corpus;iterative algorithm;multiword unit;different statistical association measurement;word association;translation unit	More and more researchers have recognized the potential value of the parallel corpus in the research on Machine Translation and Machine Aided Translation. This paper examines how Chinese English translation units could be extracted from parallel corpus. An iterative algorithm based on degree of word association is proposed to identify the multiword units for Chinese and English. Then the Chinese-English Translation Equivalent Pairs.are extracted from the parallel corpus. We also made comparison between different statistical association measurement in this paper.	algorithm;iterative method;machine translation;parallel text;text corpus;translation unit (programming)	Baobao Chang;Pernilla Danielsson;Wolfgang Teubert	2002			natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;evaluation of machine translation;text corpus;machine translation;rule-based machine translation;programming language	NLP	-23.272907760928028	-77.61473422565736	71915
28ca381ac5ca20c94485b543e7220a959b112d9f	automatic question categorization: a new approach for text elaboration	corpus annotation;apprentissage automatique;traitement automatique des langues naturelles;text elaboration;question wh;categorisation;computacion informatica;comprehension ecrite;verbal argument;filologias;reading comprehension;wh question;etiquetage;wh question labels;info eu repo semantics article;annotation de corpus;informacion documentacion;algorithme;algorithm;linguistica;machine learning;ciencias basicas y experimentales;semantic role;argument verbal;semantic role labeling;rotulado de preguntas;elaboracion de texto;labelling;computational linguistics;role semantique;grupo a;ciencias sociales;linguistique informatique;portuguese;etiquetado de roles semanticos;grupo b;natural language processing;portugais;categorization;algoritmo	Text adaptation is a normal activity of teachers to facilitate reading comprehension of specific contents; the general approaches for it are Text Simplification and Text Elaboration (TE). TE aims at clarifying, explaining information and making connections explicit in texts. In this paper, we present a new approach for TE: an automatic question categorization system which assigns wh-question labels to verbal arguments in a sentence. For example, in ―Mary danced yesterday.‖ ―Who?‖ is the label linking the verb ―danced‖ to the argument ―Mary‖ and ―When?‖ links ―danced‖ to the argument ―yesterday‖. This annotation is similar to semantic role labeling, approached successfully via statistical language processing techniques. Specifically, we present experiments to build the system using a fine-grained question set in Portuguese language and address two key research questions: (1) Which machine-learning algorithm presents the best results? (2) Which problems this task presents and how to overcome them?	algorithm;categorization;experiment;f1 score;feature selection;machine learning;selection algorithm;semantic role labeling;test engineer;text simplification	Marcelo Adriano Amâncio;Magali Sanches Duran;Sandra M. Aluísio	2011	Procesamiento del Lenguaje Natural		natural language processing;semantic role labeling;computer science;computational linguistics;linguistics;portuguese;categorization	NLP	-26.922016983322035	-77.56525361833734	71982
b647aec0b1af79ff4b4a8d352a528ea37f6bed6b	simple linguistic processing effect on multi-label emotion classification	unigram approach linguistic processing multilabel emotion classification human communication human machine interface text based emotion reaction recognition world wide web support vector machine emotion keywords negative words blog post;human computer interaction;unigram approach;human communication;support vector machines;blog post;svm emotion classification multi label;training;web sites information services internet support vector machines support vector machine classification emotion recognition telecommunication computing humans speech synthesis man machine systems;text based emotion reaction recognition;text analysis;emotion recognition;information services;classification;multi label;multilabel emotion classification;linguistic processing;accuracy;internet;emotion classification;web sites classification emotion recognition human computer interaction linguistics support vector machines text analysis;negative words;emotion keywords;human machine interface;web sites;world wide web;svm;support vector machine;linguistics	Emotion plays a significant role in human communications in our daily life. With progress in human-machine interface technology, recent research has placed more emphasis on the recognition of emotion reaction. Comparing to some other ideal experimental settings, blog posts online would be respond more to real-world events. And a huge resource of text-based emotion can be found from the World Wide Web nowadays. This paper reports a study to investigate the effectiveness of using SVM (Support Vector Machine) on linguistic features considering emotion keywords and negative words, and classify a collection of blog posts sentences tagged by one or more labels finally. Our results show that individual emotions can be clearly separated by the proposed approach. To the multi-label classification of emotion, it also obtained a higher accuracy rate than the baseline unigram approach using SVM.	baseline (configuration management);blog;co-occurrence matrix;dictionary;experiment;exploratory testing;feature vector;human–computer interaction;machine learning;multi-label classification;n-gram;support vector machine;test set;text-based (computing);user interface;world wide web	Ye Wu;Fuji Ren	2009	2009 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2009.5313832	natural language processing;speech recognition;computer science;pattern recognition	AI	-21.916459192518328	-66.94422518218066	72020
ec5e67e8deb1e367b40bfc3c2d39d3327127b1e1	extracting named entity translingual equivalence with limited resources	transliteration;information extraction;named entity translation;bilingual corpus;parallel corpora;named entity;machine translation	In this article we present an automatic approach to extracting Hindi-English (H-E) Named Entity (NE) translingual equivalences from bilingual parallel corpora. In the absence of a Hindi NE tagger or H-E translation dictionary, this approach adapts a Chinese-English (C-E) surface string transliteration model for H-E NE extraction. The model is initially trained using automatically extracted C-E NE pairs, then iteratively updated based on newly extracted H-E NE pairs. For each English person and location NE in each sentence pair, this approach searches for its Hindi correspondence with minimum transliteration cost and constructs an H-E NE list from the bilingual corpus. Experiments show that this approach extracted 1000 H-E NE pairs with a precision of 91.8%.	bilingual dictionary;brill tagger;named entity;parallel text;text corpus;turing completeness	Fei Huang;Stephan Vogel;Alexander H. Waibel	2003	ACM Trans. Asian Lang. Inf. Process.	10.1145/974740.974745	natural language processing;speech recognition;transliteration;computer science;linguistics;machine translation;information extraction	NLP	-25.017617473285945	-75.67087594352965	72033
eca1f7388ca481533859248456052fe700097135	semantic role labeling using maximum entropy model	semantic role labeling;maximum entropy model	In this paper, we propose a semantic role labeling method using a maximum entropy model, which enables not only to exploit rich features but also to alleviate the data sparseness problem in a well-founded model. For applying the maximum entropy model to semantic role labeling, we take a incremental approach as follows: firstly, the semantic roles are assigned to the arguments in the immediate clause including a predicate, and then, the semantic roles are assigned to the arguments in the upper clauses by using previously assigned labels. The experimental result shows that the proposed method has about 64.76% (F1-measure) on the test set.	cluster analysis;f1 score;neural coding;principle of maximum entropy;semantic role labeling;test set;well-founded semantics	Joon-Ho Lim;Young-Sook Hwang;So Young Park;Hae-Chang Rim	2004			machine learning;principle of maximum entropy;artificial intelligence;computer science;predicate (grammar);semantic role labeling;exploit;pattern recognition;test set	NLP	-23.06480843468241	-73.61297117170555	72136
79d6fff23283158802744695612b2978477afb06	statistical machine transliteration with multi-to-multi joint source channel model		This paper describes DFKI’s participation in the NEWS2011 shared task on machine transliteration. Our primary system participated in the evaluation for English-Chinese and Chinese-English language pairs. We extended the joint sourcechannel model on the transliteration task into a multi-to-multi joint source-channel model, which allows alignments between substrings of arbitrary lengths in both source and target strings. When the model is integrated into a modified phrasebased statistical machine translation system, around 20% of improvement is observed. The primary system achieved 0.320 on English-Chinese and 0.133 on Chinese-English in terms of top-1 accuracy.	channel (communications);german research centre for artificial intelligence;simultaneous multithreading;statistical machine translation	Yu Chen;Rui Wang;Yi Zhang	2011			natural language processing;speech recognition;computer science;pattern recognition	NLP	-22.606026822428234	-77.73346494556887	72204
db089c3117474f690279f299662b4f722bb43d15	cmuml system for kbp 2015 cold start slot filling.		In this paper, we present an overview of the CMUML system for KBP 2015 English Cold Start Slot Filling (SF) task.The CMUML 2015 SF system aggregates the output of several semantic analysis sub-components that read natural language at the sentence level. These sub-components, which we refer to as microreaders, have different reading capabilities. In addition, we ran a fraction of the queries on our 2014 CRF-based system. We also used our 2014 rule-based system	cold start;conditional random field;natural language;rule-based system	Bryan Kisiel;Bill McDowell;Matt Gardner;Ndapandula Nakashole;Emmanouil A. Platanios;Abulhair Saparov;Shashank Srivastava;Derry Wijaya;Tom M. Mitchell	2015			information retrieval;computer science;cold start (automotive)	NLP	-28.260564036862117	-75.61765412122448	72229
9c9fb705af572ea57bd7aeca9e17dc84fefda1a7	review of korean speech act classification: machine learning methods	korean speech act classification;machine learning method	To resolve ambiguities in speech act classification, various machine learning models have been proposed over the past 10 years. In this paper, we review these machine learning models and present the results of experimental comparison of three representative models, namely the decision tree, the support vector machine (SVM), and the maximum entropy model (MEM). In experiments with a goaloriented dialogue corpus in the schedule management domain, we found that the MEM has lighter hardware requirements, whereas the SVM has better performance characteristics. Category: Human computing	c4.5 algorithm;decision tree;experiment;logic programming;machine learning;meltwater entrepreneurial school of technology;principle of maximum entropy;requirement;statistical model;support vector machine	Harksoo Kim;Choong-Nyoung Seon;Jungyun Seo	2011	JCSE	10.5626/JCSE.2011.5.4.288	speech recognition;computer science;online machine learning;machine learning;pattern recognition;relevance vector machine;computational learning theory;active learning;structured support vector machine	ML	-21.65571280536921	-70.85836085360668	72265
425e6976d542ca01338fafa421b9e0f56dc2a434	predicting discourse connectives for implicit discourse relation recognition	language model	Existing works indicate that the absence of explicit discourse connectives makes it difficult to recognize implicit discourse relations. In this paper we attempt to overcome this difficulty for implicit relation recognition by automatically inserting discourse connectives between arguments with the use of a language model. Then we propose two algorithms to leverage the information of these predicted connectives. One is to use these predicted implicit connectives as additional features in a supervised model. The other is to perform implicit relation recognition based only on these predicted connectives. Results on Penn Discourse Treebank 2.0 show that predicted discourse connectives help implicit relation recognition and the first algorithm can achieve an absolute average f-score improvement of 3% over a state of the art baseline system.	algorithm;baseline (configuration management);discourse relation;f1 score;implicit solvation;language model;logical connective;treebank	Zhi-Min Zhou;Yu Xu;Zheng-Yu Niu;Man Lan;Jian Su;Chew Lim Tan	2010			natural language processing;computer science;linguistics;language model	NLP	-24.494828591369856	-74.24005942917078	72311
01d99bfa62e7cc66ee70115ea5d6e4fc471d5c96	assamese wordnet based quality enhancement of bilingual machine translation system		Machine Translation is a task to translate the text from a source language to a target language in an automatic manner. Here, we describe a system that translate the English language to Assamese language text which is based on Phrase based statistical translation technique. To overcome the translation problem related with highly open word class like Proper Noun or the Out Of Vocabulary words we develop a transliteration system which is also embedded with our translation system. We enhance the translation output by replacing words with their most appropriate synonymous word for that particular context with the help of Assamese WordNet Synset. This Machine Translation system outcomes with a reasonable translation output when analyzed by linguist for Assamese language which is a less computationally aware language among the Indian languages.	compiler;embedded system;natural language processing;parallel text;statistical machine translation;synonym ring;vocabulary;wordnet	Anup Kumar Barman;Jumi Sarmah;Shikhar Kr. Sarma	2014			natural language processing;speech recognition;linguistics	NLP	-22.681918530985357	-79.37338940638755	72316
98992331bbf9c57dc92ecf6727edb04da4e12922	large, huge or gigantic? identifying and encoding intensity relations among adjectives in wordnet	scales;gradable adjectives;wordnet;intensity relation	We propose a new semantic relation for gradable adjectives in WordNet, which enriches the present, vague, similar relation with information on the degree or intensity with which different adjectives express a shared attribute. Using lexicalsemantic patterns, we mine the Web for evidence of the relative strength of adjectives like ‘‘large’’, ‘‘huge’’ and ‘‘gigantic’’ with respect to their attribute (‘‘size’’). The pairwise orderings we derive allow us to construct scales on which the adjectives are located. To represent the intensity relation among gradable adjectives in WordNet, we combine ordered scales with the current WordNet dumbbells based on the relation between a pair of central adjectives and a group of undifferentiated semantically similar adjectives. A new intensity relation links the adjectives in the dumbbells and their concurrent representation on scales. Besides capturing the semantics of gradable adjectives in a way that is both intuitively clear as well as consistent with corpus data, the introduction of an intensity relation would potentially result in several specific benefits for NLP. V. Sheinman is currently with Google Inc. V. Sheinman (&) T. Tokunaga Computer Science Department, Tokyo Institute of Technology, Ookayama 2-12-1, Meguro-ku, Tokyo 152-8552, Japan e-mail: vera@sheinman.org T. Tokunaga e-mail: take@cl.cs.titech.ac.jp C. Fellbaum I. Julien P. Schulam Computer Science Department, Princeton University, 35 Olden Street, Princeton, NJ 08540, USA C. Fellbaum e-mail: fellbaum@princeton.edu I. Julien e-mail: ijulien@princeton.edu P. Schulam e-mail: pschulam@princeton.edu 123 Lang Resources & Evaluation (2013) 47:797–816 DOI 10.1007/s10579-012-9212-1		Vera Sheinman;Christiane Fellbaum;Isaac Julien;Peter F. Schulam;Takenobu Tokunaga	2013	Language Resources and Evaluation	10.1007/s10579-012-9212-1	natural language processing;wordnet;computer science;data mining;linguistics	NLP	-27.904890123050972	-69.40889885228383	72405
591cdbe60fffc7de1caa74466280fc3c1a40c0e6	acquisition of common sense knowledge for basic level concepts		Feature norms can be regarded as repositories of common sense knowledge for basic level concepts. We acquire from very large corpora feature-norm-like concept descriptions using a combination of a weakly supervised method and an unsupervised method. The success in identifying the specific properties listed in the feature norms as well as the success in acquiring the classes of properties present in the norms are	commonsense knowledge (artificial intelligence);focal (programming language);software repository;supervised learning;text corpus;unsupervised learning	Eduard Barbu	2009			computer science;knowledge management;data mining	NLP	-30.071879348605425	-69.24468675484515	72442
08322d5c93efef2863925c842b19b25b4d831e36	statistical analysis of the interaction between word order and definiteness in polish		Although (in-)de niteness is semantically relevant in Polish, the language lacks explicit linguistic features for marking it. The paper presents the rst quantitative, statistical evaluation of the correlation between word order and de niteness. Our results support previous qualitative theories about the in uence of the verb-relative position on de niteness in Polish.	item unique identification;theory	Adrian Czardybon;Oliver Hellwig;Wiebke Petersen	2014		10.1007/978-3-319-10888-9_15	natural language processing;speech recognition;linguistics	NLP	-27.42685794996682	-76.2302269588685	72563
2d197e5b045f3fd1226a4c34144b7e8ce554cbf6	mining for unambiguous instances to adapt part-of-speech taggers to new domains		We present a simple, yet effective approach to adapt part-of-speech (POS) taggers to new domains. Our approach only requires a dictionary and large amounts of unlabeled target data. The idea is to use the dictionary to mine the unlabeled target data for unambiguous word sequences, thus effectively collecting labeled target data. We add the mined instances to available labeled newswire data to train a POS tagger for the target domain. The induced models significantly improve tagging accuracy on held-out test sets across three domains (Twitter, spoken language, and search queries). We also present results for Dutch, Spanish and Portuguese Twitter data, and provide two novel manually-annotated test sets.	brill tagger;dictionary;domain adaptation;hector;mined;part-of-speech tagging;test set;web search query	Dirk Hovy;Barbara Plank;Héctor Martínez Alonso;Anders Søgaard	2015			part of speech;natural language processing;artificial intelligence;spoken language;computer science	NLP	-23.17138726606619	-72.0199963757016	72674
b31e4035c8d30892e69c9f0f645b07e182209a6d	alignment-based reordering for smt	sprakteknologi sprakvetenskaplig databehandling;language technology computational linguistics	We present a method for improving word alignment quality for phrase-based SMT by reordering the source text according to the target word order suggested by an initial word alignment. The reordered text is used to create a second word alignment which can be an improvement of the first alignment, since the word order is more similar. The method requires no other pre-processing such as part-of-speech tagging or parsing. We report improved Bleu scores for English–German and English–Swedish translation. We also examined the effect on word alignment quality and found that the reordering method increased recall while lowering precision, which partly can explain the improved Bleu scores. A manual evaluation of the translation output was also performed to understand what effect our reordering method has on the translation system. We found that where the system employing reordering differed from the baseline in terms of having more words, or a different word order, this generally led to an improvement in translation quality.	bleu;baseline (configuration management);bitext word alignment;data structure alignment;machine translation;parsing;part-of-speech tagging;preprocessor;simultaneous multithreading	Maria Holmqvist;Sara Stymne;Lars Ahrenberg;Magnus Merkel	2012			natural language processing;speech recognition;computer science;linguistics	NLP	-21.990749715323556	-77.30787569885365	72712
00b8876554111716e88589b21a420eb072f02bc4	sentence extraction using asymmetric word similarity and topic similarity	sentence extraction;text summarization;term frequency;test collection	We propose a text summarization system known as MySum in finding the significance of sentences in order to produce a summary based on asymmetric word similarity and topic similarity.  We use mass assignment theory to compute similarity between words based on the basis of their contexts. The algorithm is incremental  so that words or documents can be added or subtracted without massive re-computation. Words are considered similar if they  appear in similar contexts, however, these words do not have to be synonyms. We also compute the similarity of a sentence  to the topic using frequency of overlapping words. We compare the summaries produced with the ones by humans and other system  known as TF.ISF (term frequency-inverse sentence frequency). Our method generates summaries that are up to 60% similar to the manually created summaries taken from DUC 2002 test collection.  	sentence extraction	Masrah Azmi-Murad;Trevor P. Martin	2004		10.1007/3-540-31662-0_39	natural language processing;computer science;automatic summarization;pattern recognition;tf–idf;information retrieval	NLP	-26.611892278552407	-67.13910511978709	72719
96eddab3a63f131b81a8d41fd5d67b0239d3aaee	english to hindi machine transliteration system at news 2009	hindi machine transliteration system;hindi transliteration;non-standard run;test set;mean f-score;modified joint source-channel model;ranking algorithm;paper report;standard run;machine transliteration shared task;tokens present	This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard run and two nonstandard runs for English to Hindi transliteration. The modified joint source-channel model has been used along with a number of alternatives. The system has been trained on the NEWS 2009 Machine Transliteration Shared Task datasets. For standard run, the system demonstrated an accuracy of 0.471 and the mean F-Score of 0.861. The non-standard runs yielded the accuracy and mean F-scores of 0.389 and 0.831 respectively in the first one and 0.384 and 0.828 respectively in the second one. The non-standard runs resulted in substantially worse performance than the standard run. The reasons for this are the ranking algorithm used for the output and the types of tokens present in the test set.	algorithm;channel (communications);f1 score;test set	Amitava Das;Asif Ekbal;Tapabrata Mondal;Sivaji Bandyopadhyay	2009			natural language processing;speech recognition;engineering;engineering drawing	NLP	-23.11937323896709	-76.38041407118713	72819
8db775ced3166ff9696f612ec5e97541804a933d	machine translation system development based on human likeness	decoding stage parameters;system configuration;statistical machine translation system human likeness empirical machine translation systems metric combinations decoding stage parameters;statistical machine translation;language translation;humans error analysis surface mount technology robustness power system reliability iterative decoding optimization methods testing;empirical machine translation systems;evaluation metric;language translation computational linguistics;statistical machine translation system;human likeness;system development;computational linguistics;machine translation;metric combinations	We present a novel approach for parameter adjustment in empirical machine translation systems. Instead of relying on a single evaluation metric, or in an ad-hoc linear combination of metrics, our method works over metric combinations with maximum descriptive power, aiming to maximise the Human Likeness of the automatic translations. We apply it to the problem of optimising decoding stage parameters of a state- of-the-art Statistical machine translation system. By means of a rigorous manual evaluation, we show how our methodology provides more reliable and robust system configurations than a tuning strategy based on the BLEU metric alone.	bleu;hoc (programming language);statistical machine translation	Patrik Lambert;Jesús Giménez;Marta R. Costa-Jussà;Enrique Amigó;Rafael E. Banchs;Lluís Màrquez i Villodre;José A. R. Fonollosa	2006	2006 IEEE Spoken Language Technology Workshop	10.1109/SLT.2006.326801	rouge;natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;computational linguistics;machine learning;linguistics;machine translation;rule-based machine translation;translation	NLP	-21.989901604851767	-78.14013215839887	72835
73bb901e63c6a5ad082e3d4c636c080b945e00cf	fbk's machine translation systems for iwslt 2012's ted lectures		This paper reports on FBK’s Machine Translation (MT) submissions at the IWSLT 2012 Evaluation on the TED talk translation tasks. We participated in the English-French and the Arabic-, Dutch-, German-, and Turkish-English translation tasks. Several improvements are reported over our last year baselines. In addition to using fill-up combinations of phrase-tables for domain adaptation, we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. We describe challenges encountered in under-resourced languages (Turkish) and language-specific preprocessing needs.	baseline (configuration management);cross entropy;domain adaptation;language model;machine translation;preprocessor;text corpus	Nicholas Ruiz;Arianna Bisazza;Roldano Cattoni;Marcello Federico	2012			natural language processing;speech recognition;computer science	NLP	-23.0609212220976	-75.63008736337277	72936
938bc149c5277c83923aa46a6860c48e1728273d	arabic sentiment analysis: an empirical study of machine translation's impact		The largest amount of Sentiment Analysis has been carried out for English language. To deal with Arabic sentiment analysis, machine translation of English resources or Arabic texts may be applied to built Arabic sentiment analysis systems. In this paper, we translate Arabic dataset into English and study the impact of machine translation while considering a standard Arabic system as a baseline. Experiments show that sentiment analysis of Arabic content translated into English reach a competitive performance with respect to standard sentiment analysis of Arabic texts. This suggests that machine translation can successfully transfer the expression of sentiment or polarity. Moreover , we explored the multi-domain extending of training data in order to enhance performance and we show that we should have, in the training set, data whose domain is the same as the domain of evaluation dataset.		Amira Barhoumi;Chafik Aloulou;Nathalie Camelin;Yannick Estève;Lamia Hadrich Belguith	2018			machine translation;natural language processing;empirical research;sentiment analysis;arabic;training set;artificial intelligence;computer science	NLP	-20.6973500109537	-73.30545149808802	72948
4c774a8edf4f3371d2093f2fb7c8d30a0571df8d	word sense disambiguation based on word sense clustering	modelizacion;analyse amas;noun;generic model;intelligence artificielle;classification;word sense disambiguation;desambiguisacion;modelisation;cluster analysis;polisemia;disambiguation;polysemy;polysemie;artificial intelligence;analisis cluster;inteligencia artificial;desambiguisation;discriminacion;modeling;clasificacion;star clusters;discrimination	In this paper we address the problem of Word Sense Disambiguation by introducing a knowledge-driven framework for the disambiguation of nouns. The proposal is based on the clustering of noun sense representations and it serves as a general model that includes some existing disambiguation methods. A first prototype algorithm for the framework, relying on both topic signatures built from WordNet and the Extended Star clustering algorithm, is also presented. This algorithm yields encouraging experimental results for the SemCor corpus, showing improvements in recall over other knowledge-driven methods.		Henry Anaya-Sánchez;Aurora Pons-Porrata;Rafael Berlanga Llavori	2006		10.1007/11874850_51	natural language processing;noun;discrimination;speech recognition;systems modeling;star cluster;semeval;biological classification;computer science;artificial intelligence;machine learning;cluster analysis	NLP	-26.684518412050735	-77.28139662468735	72995
c04f771ff263890b2fb51def64f662bbf2a7513a	graph-based concept weighting for medical information retrieval	graph theory;medical information retrieval	This paper presents a graph-based method to weight medical concepts in documents for the purposes of information retrieval. Medical concepts are extracted from free-text documents using a state-of-the-art technique that maps n-grams to concepts from the SNOMED CT medical ontology. In our graph-based concept representation, concepts are vertices in a graph built from a document, edges represent associations between concepts. This representation naturally captures dependencies between concepts, an important requirement for interpreting medical text, and a feature lacking in bag-of-words representations.  We apply existing graph-based term weighting methods to weight medical concepts. Using concepts rather than terms addresses vocabulary mismatch as well as encapsulates terms belonging to a single medical entity into a single concept. In addition, we further extend previous graph-based approaches by injecting domain knowledge that estimates the importance of a concept within the global medical domain.  Retrieval experiments on the TREC Medical Records collection show our method outperforms both term and concept baselines. More generally, this work provides a means of integrating background knowledge contained in medical ontologies into data-driven information retrieval approaches.	bag-of-words model;experiment;grams;information retrieval;map;n-gram;ontology (information science);systematized nomenclature of medicine;vertex (graph theory);vocabulary mismatch	Bevan Koopman;Guido Zuccon;Peter Bruza;Laurianne Sitbon;Michael Lawley	2012		10.1145/2407085.2407096	computer science;graph theory;theoretical computer science;data mining;world wide web;information retrieval	Web+IR	-33.15484153027758	-68.45641927207555	73054
1f1c533fc7dc1e67e55262a9185f20266bc4ed53	the discourse of print advertising in the philippines: generic structures and linguistic features	conference paper	This paper aims to examine the generic structures and linguistic properties of ads in Philippine magazines. Taken from the Corpus of Asian Magazine Advertising: The Philippine Database, the corpus consists of seventy-four ads for consumer nondurables such as medicines, vitamins and food supplements, and cosmetic/beauty/personal hygiene products. The study found that the ads demonstrated preference for certain generic structures and linguistic features, making them ‘reason’ (rather than ‘tickle’) ads which may be described as direct. The paper argues that the directness of these ads contributes to making them covert communication.	text corpus	Danilo Dayag	2008			development economics;geography;advertising;cartography	NLP	-31.858571966836756	-76.35774620057335	73310
1df20717952b0842511224a19f6f6314c9421fbf	intersection of multitape transducers vs. cascade of binary transducers: the example of {e}gyptian hieroglyphs transliteration		This paper uses the task of transliterating an Egyptian Hieroglyphic text into the latin alphabet as a model problem to compare two finite-state formalisms : the first one is a cascade of binary transducers; the second one is a class of multitape transducers expressing simultaneous constraints. The two systems are compared regarding their expressivity and readability. The first system tends to produce smaller machines, but is more tricky, whereas the second one leads to more abstract and structured rules.	google transliteration;high- and low-level;low-level programming language;rewrite (programming);rewriting;semantics (computer science);transducer;tree structure	François Barthélemy;Serge Rosmorduc	2011				NLP	-28.99476428189234	-78.97719560681877	73414
1837e5433e7632e6c1bbd1f5876839782e9cf396	semantic extraction with wide-coverage lexical resources	information extraction;graphical model;knowledge base	We report on results of combining graphical modeling techniques with Information Extraction resources (Pattern Dictionary and Lexicon) for both frame and semantic role assignment. Our approach demonstrates the use of two human built knowledge bases (WordNet and FrameNet) for the task of semantic extraction.	dictionary;framenet;information extraction;knowledge base;lexicon;wordnet	Behrang Mohit;Srini Narayanan	2003		10.3115/1073483.1073505	natural language processing;relationship extraction;knowledge base;semantic computing;computer science;machine learning;pattern recognition;graphical model;knowledge extraction;information extraction;information retrieval	NLP	-25.73620356054593	-69.82242967461193	73566
23e6540590912e4244ee7de30d0460c47f620ef6	arabic word analogies and semantics of simple phrases		Vector semantic spaces, in which a multi-dimenstional numeric vector is used to represent the meaning of a word, are making new natural language applications possible. Word analogies have become a standard tool to evaluate semantic spaces. They also teach us something about what kinds of information the vectors in the semantic space can embody. Arabic orthography has morphological constructs which are realized in syntax in some other languages: the presence or absence of the article ??; bi-??, ka-?? prepositional prefixes; verbs with object suffixes can constitute an entire sentence. The structured word-forms offer the opportunity to study how vector representations of meaning interact in the semantic space to form verb phrases, noun phrases, and prepositional phrases. We provide a corpus of Arabic analogies focused on the morphological constructs which can participate in some of these phrases. We conducted an examination of ten different semantic spaces to see which of them is most appropriate for this set of analogies, and we illustrate the use of the corpus to examine phrase-building.	bigram;encode;natural language;programming idiom;sparse matrix;text corpus;trigram;vocabulary	Stephen Eugene Taylor;Tomas Brychcin	2018	2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)	10.1109/ICNLSP.2018.8374386	orthography;noun phrase;natural language processing;semantics;syntax;natural language;verb;sentence;mathematics;prefix;artificial intelligence	NLP	-31.859657417069347	-80.00174345198327	73626
232986f1f67d179dcd7e12a4f54ed83e6a14d513	semantic publishing challenge - assessing the quality of scientific output	technology and engineering	The Semantic Publishing Challenge series aims at investigating novel approaches for improving scholarly publishing using Linked Data technology. In 2014 we had bootstrapped this effort with a focus on extracting information from non-semantic publications – computer science workshop proceedings volumes and their papers – to assess their quality. The objective of this second edition was to improve information extraction but also to interlink the 2014 dataset with related ones in the LOD Cloud, thus paving the way for sophisticated end-user services.	computer science;information extraction;linked data;semantic publishing	Christoph Lange;Angelo Di Iorio	2015		10.1007/978-3-319-25518-7_6	computer science;data science;data mining;world wide web;information retrieval	HPC	-32.653409204994695	-66.9026103687821	73716
df6b894e0e4a93ca27e3cef757ddda02d6c5fa13	a scaleable automated quality assurance technique for semantic representations and proposition banks	bad representation;analytical technique;bad semantic representation;small set;semantic representation;scaleable automated quality assurance;proposition bank;previous work;classic structural linguistic methodology;important class;predicate argument structure;automated quality assurance technique	This paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure. These representations are crucial to the development of an important class of corpus known as a proposition bank. Previous work (Cohen and Hunter, 2006) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology. Cohen and Hunter applied the technique manually to a small set of representations. Here we test the feasibility of automating the technique, as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times larger than that used by Cohen and Hunter. We conclude that the technique is completely automatable, uncovers missing sense distinctions and other bad semantic representations, and does scale well, performing at an accuracy of 69% for identifying bad representations. We also report on the implications of our findings for the correctness of the semantic representations in PropBank.	correctness (computer science);logico-linguistic modeling;propbank;test automation;text corpus	K. Bretonnel Cohen;Lawrence Hunter;Martha Palmer	2011			computer science;artificial intelligence;data mining;algorithm	SE	-27.419159079543007	-72.19896339717111	73812
aae01fd0efab4ec60d92eb7233d10bd573ef67e8	a machine learning approach to automatic term extraction using a rich feature set		In this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms. In our preliminary experiments, we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction. We achieved state of the art results for unigram extraction in Brazilian Portuguese.	experiment;keyword extraction;machine learning;n-gram;terminology extraction	Merley da Silva Conrado;Thiago Alexandre Salgueiro Pardo;Solange Oliveira Rezende	2013			feature learning;feature extraction;machine learning;pattern recognition;information retrieval	NLP	-24.425618744375694	-69.8003881382474	73834
5443f7dc737ab19c6154e97005c853d2510b2b13	overview of the chinese word sense induction task at clp2010.		In this paper, we describe the Chinese word sense induction task at CLP2010. Seventeen teams participated in this task and nineteen system results were submitted. All participant systems are evaluated on a dataset containing 100 target words and 5000 instances using the standard cluster evaluation. We will describe the participating systems and the evaluation results, and then find the most suitable method by comparing the different Chinese word sense induction systems.	baseline (configuration management);performance;text corpus;word sense;word-sense induction	Le Sun;Zhenzhong Zhang;Qiang Dong	2010			communication;word-sense induction;computer science	NLP	-24.513175308910707	-69.57055372827284	73859
bc1ecf5af87d3de31f96bd6d09439f7051900305	combining semantic and prior polarity for boosting twitter sentiment analysis	support vector machines;training;semantics;semantic feature;feature extraction;sentiment analysis;twitter;distributed representation of sentence;context	Twitter sentiment analysis offers organizations an ability to monitor public feeling towards the products and events related to them in real time. Most existing researches for Twitter sentiment analysis are focused on the extraction of sentiment feature of lexical and syntactic feature that are expressed explicitly through words, emoticons, exclamation marks etc, although sentiment implicitly expressed via latent contextual semantic relations, dependencies among words in tweets are ignored. In this paper, we introduce distributed representation of sentence that can capture co-occurrence statistics and contextual semantic relations of words in tweets, and represent a tweet via a fixed size feature vector. We used the feature vector as sentence semantic feature for the tweet. We combined semantic feature, prior polarity score feature and n-grams feature as sentiment feature set of tweets, and incorporated the feature set into Support Vector Machines(SVM) model training and predicting sentiment classification label. We used six Twitter datasets in our evaluation and compared the performance against n-grams model baseline. Results show the superior performance of our method in accuracy sentiment classification.	sentiment analysis	Xueliang Cao	2015		10.1109/SmartCity.2015.171	natural language processing;computer science;pattern recognition;data mining;sentiment analysis	NLP	-20.11798229930577	-68.85386099509853	73876
b03a50c8acd64a27d99200d0c4fdc6c35172c6bd	text cluster trimming for better descriptions and improved quality	text clustering	Text clustering is potentially very useful for explorationf text sets that are too large to study manually. The success of uch a tool depends on whether the results can be explained to the user. A n automatically extracted cluster description usually con sists of a few words that are deemed representative for the cluster. It is p referably short in order to be easily grasped. However, text cluster content is often diverse. We introduce a trimming method that remove s texts that do not contain any, or a few of the words in the clus ter description. The result is clusters that match their descri ptions better. In experiments on two quite different text se ts we obtain significant improvements in both internal and external clustering qual ity for the trimmed clustering compared to the original. The trimming thus has two positive effects: it forces the clusters to agree wit h their descriptions (resulting in better descriptions) an d improves the quality of the trimmed clusters.	cluster analysis;experiment;naruto shippuden: clash of ninja revolution 3	Magnus Rosell	2010			trimming;cluster (physics);cluster analysis;computer science;artificial intelligence;document clustering;pattern recognition	NLP	-28.041952525905433	-70.52720943776633	73930
2e45f7f663c94139b8d6bb64fc0828ac23616bda	joint wmt 2013 submission of the quaero project		This paper describes the joint submission of the QUAERO project for the German→English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTH’s system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation.	bleu;baseline (configuration management);heart rate variability;systran;statistical machine translation;word lists by frequency	Stephan Peitz;Saab Mansour;Matthias Huck;Markus Freitag;Hermann Ney;Eunah Cho;Teresa Herrmann;Mohammed Mediani;Jan Niehues;Alexander H. Waibel;Alexander Allauzen;Quoc-Khanh Do;Bianka Buschbeck-Wolf;Tonio Wandmacher	2013				NLP	-22.53384341270208	-75.62101136424585	73936
33be1def7b3b4905e7220eee64e3dbf156579360	integer linear programming inference for conditional random fields	dynamic pro gramming;hidden markov model;viterbi algorithm;semantic role labeling;conditional random field;linear program;integer linear program	Inference in Conditional Random Fields and Hidden Markov Models is done using the Viterbi algorithm, an efficient dynamic programming algorithm. In many cases, general (non-local and non-sequential) constraints may exist over the output sequence, but cannot be incorporated and exploited in a natural way by this inference procedure. This paper proposes a novel inference procedure based on integer linear programming (ILP) and extends CRF models to naturally and efficiently support general constraint structures. For sequential constraints, this procedure reduces to simple linear programming as the inference process. Experimental evidence is supplied in the context of an important NLP problem, semantic role labeling.	conditional random field;discriminative model;dynamic programming;hidden markov model;integer programming;linear programming relaxation;markov chain;natural language processing;semantic role labeling;semantics (computer science);sequence labeling;shortest path problem;viterbi algorithm	Dan Roth;Wen-tau Yih	2005		10.1145/1102351.1102444	forward algorithm;semantic role labeling;mathematical optimization;integer programming;viterbi algorithm;computer science;machine learning;forward–backward algorithm;pattern recognition;mathematics;conditional random field;hidden markov model	ML	-20.485042828668057	-76.45566553954683	74024
5be83523ba7440615c336fadb28477702dd05957	a common xml-based framework for syntactic annotations	language resources;syntactic annotation;annotation syntaxique	It is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources, and that standards for linguistic annotation are becoming increasingly mandatory. To answer this need, we have developed a framework comprised of an abstract model for a variety of different annotation types (e.g., morpho-syntactic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotator’s approach and goals. In this paper we provide an overview of the framework, demonstrate its applicability to syntactic annotation, and show how it can contribute to comparative evaluation of parser output and diverse syntactic annotation schemes.	markup language;overhead (computing);text corpus;web application;xces;xml;xslt	Nancy Ide;Laurent Romary;Tomaz Erjavec	2009	CoRR		natural language processing;minimum information required in the annotation of models;computer science;temporal annotation;world wide web;information retrieval	NLP	-32.27716555544031	-72.74262796368457	74282
c51d3a50d35830d0c388edfcd94a17c77b2ca0a7	an adaptive information extraction system based on wrapper induction with pos tagging	information extraction;supervised classification;boosting;machine learning;natural language;part of speech tagging;wrapper induction;pos tagging	Information Extraction (IE) performs two important tasks: identifying certain pieces of information from documents and storing them for future use. This work proposes an adaptive IE system based on Boosted Wrapper Induction (BWI), a supervised wrapper induction algorithm. However, some authors have shown that boosting techniques face difficulties during the processing of natural language texts. This fact became the rationale for coupling Parts-of-Speech tagging with the BWI algorithm in our proposed system. In order to evaluate its performance, several experiments were carried out on three standard corpora. The results obtained suggest that the union of POS tagging and BWI offers a small gain of 3--5% of performance over the original BWI algorithm for unstructured texts. These results position our system among the very best similar IE systems endowed with POS tagging, according to a comparison presented and discussed in the article.	algorithm;design rationale;experiment;information extraction;natural language processing;part-of-speech tagging;point of sale;technical standard;text corpus;wrapper (data mining)	Rinaldo Lima;Bernard Espinasse;Frederico Luiz Gonçalves de Freitas	2010		10.1145/1774088.1774471	natural language processing;computer science;machine learning;pattern recognition;data mining;natural language;information extraction;boosting	NLP	-24.864995927085523	-68.84939031558226	74287
2bda95730665c4b69301b9634fed0d36119d755c	foreground and background lexicons and word sense disambiguation for information extraction	information extraction;word sense disambiguation	In recent years, lexicon acquisition from machine-readable dictionaries and corpora has been a dynamic field of research. However it has not always been evident how lexical information so acquired can be used, or how it relates to more structured meaning representations. In this paper I look at this issue in relation to one particular NLP task, Information Extraction (hereafter IE), and one subtask for which both lexical and general knowledge are required, Word Sense Disambiguation (WSD). The argument is as follows. For an IE task, the output formalism, that is, the database fields or templates which the system is to fill, specifies the objecttypes and relations that the system is to find out about; the ‘ontology’. An IE task operates in a specific domain. The task requires the key terms of that domain, the ‘foreground lexicon’, to be tightly bound to the ontology. This is a task that calls for human input. For all other vocabulary, the ‘background lexicon’, a far shallower semantics will be sufficient. This shallow semantics can be obtained automatically from sources such as machine-readable dictionaries and domain corpora. The foreground and background lexicons are suited to different kinds of WSD strategies. For the background lexicon, statistical methods for coarsegrained disambiguation are appropriate. For the foreground lexicon, WSD will occur as a by-product of finding a coherent semantic interpretation of an input sentence, in which all arguments are of the appropriate type. Once the foreground/background distinction is developed, there is a good match between what is possible, given the state of the art in WSD and acceptable levels of human input, and what is required, for high-quality IE. The two-tier approach has been adopted by a number of IE systems. The POETIC (Evans et al., 1996) and Sussex MUC-5 (Gaizauskas, Cahill, and Evans, 1994) systems used a hand-crafted foreground lexicon and the Alvey Tools lexicon (Carroll and Grover, 1989) as a background lexicon for syntactic	algorithm;alvey;carroll morgan (computer scientist);coherence (physics);dictionary;forward error correction;human-readable medium;information extraction;information retrieval;lexicography;lexicon;lynne jolitz;multitier architecture;natural language processing;regular expression;semantic interpretation;semantics (computer science);text corpus;vocabulary;web services for devices;word sense;word-sense disambiguation	Adam Kilgarriff	1997	CoRR		natural language processing;speech recognition;computer science;linguistics;information extraction	NLP	-29.11963430199858	-73.39634611295297	74412
7a5b3d20b437c73fca5d6a2cd3c29f0aa7cec710	joint parsing and disfluency detection in linear time		We introduce a novel method to jointly parse and detect disfluencies in spoken utterances. Our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data. We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time.	fastest;high-level programming language;parsing;time complexity	Mohammad Sadegh Rasooli;Joel R. Tetreault	2013			natural language processing;parser combinator;speech recognition;computer science;bottom-up parsing;top-down parsing	NLP	-23.042276627623785	-78.0855984753013	74437
2f647491032d0f79e20a66dc19ed001024a9b6ea	cross-lingual predicate mapping between linked data ontologies		Ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links. This difference is markedly visible when comparing a rich English language ontology with a non-English language counterpart. Discovering alignment between them is a useful endeavor as it serves as a starting point in bridging the disparity. In particular, our work is motivated by the absence of inter-language links for predicates in the localised versions of DBpedia. In this paper, we propose and demonstrate an ad-hoc system to find possible owl:equivalentProperty links between predicates in ontologies of different natural languages. We seek to achieve this mapping by using pre-existing inter-language links of the resources connected by the given predicate. Thus, our methodology stresses on semantic similarity rather than lexical. Moreover, through an evaluation, we show that our system is capable of outperforming a baseline system that is similar to the one used in recent OAEI campaigns.	algorithm;baseline (configuration management);binocular disparity;bridging (networking);dbpedia;heuristic;hoc (programming language);internal link;linked data;minimal mappings;natural language;ontology (information science);semantic similarity;wikipedia	Gautam Singh;Saemi Jang;Mun Yong Yi	2016	CoRR		computer science;data mining;semantic similarity;linked data;natural language;ontology;predicate (grammar);natural language processing;bridging (networking);ontology (information science);schema (psychology);artificial intelligence	AI	-26.3427238832577	-73.27093522584092	74540
a293e4bb51ac358233bfb27421b8f5773049026f	invited talk: evaluating automatic approaches for word meaning discovery and disambiguation using lexical substitution		There has been a surge of interest in Computational Linguistics in word sense disambiguation (WSD). A major catalyst has been the SENSEVAL evaluation exercises which have provided standard datasets for the field over the past decade. Whilst researchers believe that WSD will ultimately prove useful for applications which need some degree of semantic interpretation, the jury is still out on this point. One significant problem is that there is no clear choice of inventory for any given task, other than the use of a parallel corpus for a specific language pair for a machine translation application. Most of the datasets produced, certainly in English, have used WordNet. Whilst WordNet is a wonderful resource it would be beneficial if systems using other inventories could enter the WSD arena without the need for mappings between the inventories which may mask results. As well as the work in disambiguation, there is a growing interest in automatic acquisition of inventories of word meaning. It would be useful to investigate the merits of predefined inventories themselves, aside from their use for disambiguation, and compare automatic methods of acquring inventories. In this talk I will discuss these issues and some results in the context of the English Lexical Substitution Task, organised by myself and Roberto Navigli (University of Rome, “La Sapienza”) earlier this year under the auspices of SEMEVAL.	computation;computational linguistics;inventory;lexical substitution;linear algebra;machine translation;parallel text;semeval;semantic interpretation;web services for devices;while;word sense;word-sense disambiguation;wordnet	Diana McCarthy	2007				NLP	-30.77131917779287	-74.43897383461386	74707
3f1654d3b1aa97e72393e240aa9341aad1cb8da0	dialogue manager for a nlidb for solving the semantic ellipsis problem in query formulation	query formulation;natural language interfaces to databases;elliptic problem;natural language;typification;natural language interface;dialogue manager	A query written in natural language (NL) may involve several linguistic problems that cause a query not being interpreted or translated correctly into SQL. One of these problems is implicit information or semantic ellipsis, which can be understood as the omission of important words in the wording of a query written in NL. An exhaustive survey on NLIDB works has revealed that most of these works has not systematically dealt with semantic ellipsis. In experiments conducted on commercial NLIDBs, very poor results have been obtained (7% to 16.9%) when dealing with query corpora that involve semantic ellipsis. In this paper we propose a dialogue manager (DM) for a NLIDB for solving semantic ellipsis problems. The operation of this DM is based on a typification of elliptical problems found in queries, which permits to systematically deal with this problem. Additionally, the typification has two important characteristics: domain independence, which permits the typification to be applied to queries of different databases, and generality, which means that it holds for different languages such as English, French, Italian, Spanish, etc. These characteristics are inherited to the dialogue processes implemented in the DM, since they are based on this typification. In experiments conducted with this DM and a NLIDB on a corpus of elliptical queries, an increase of correctly answered queries of 30-35% was attained.	dialog system	Rodolfo A. Pazos Rangel;P. C. Rojas JuanC.Rojas;René Santaolaya Salgado;José Antonio Martínez Flores;Juan Javier González Barbosa	2010		10.1007/978-3-642-15390-7_21	natural language processing;computer science;linguistics;algorithm	Robotics	-27.06237138729176	-78.18757465543824	74720
922c9d30e5ef6cfbae403ac379a09eacd0b1e958	joint morphological and syntactic disambiguation		In morphologically rich languages, should morphological and syntactic disambiguation be treated sequentially or as a single problem? We describe several efficient, probabilisticallyinterpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing. Joint inference is shown to compare favorably to pipeline parsing methods across a variety of component models. State-of-the-art performance on Hebrew Treebank parsing is demonstrated using the new method. The benefits of joint inference are modest with the current component models, but appear to increase as components themselves improve.	mathematical morphology;parsing;treebank;word-sense disambiguation	Shay B. Cohen;Noah A. Smith	2007			artificial intelligence;natural language processing;syntax;computer science;treebank;parsing;inference	NLP	-22.256272499941044	-75.62937530322448	74745
ab6ebad94be2d146ebd00d8d6ec18f01150a593c	a machine-learning approach to estimating the referential properties of japanese noun phrases	apprentissage automatique;traitement automatique des langues naturelles;resolution de l anaphore;noun phrase;japonais;anaphora resolution;linguistique appliquee;methode;anaphora;traduction automatique;estimation;machine learning;syntagme nominal;reference;computational linguistics;anaphore;linguistique informatique;method;natural language processing;machine translation;applied linguistics	The referential properties of noun phrases in the Japanese language, which has no articles, are useful for article generation in JapaneseEnglish machine translation and for anaphora resolution in Japanese noun phrases. They are generally classi ed as generic noun phrases, definite noun phrases, and inde nite noun phrases. In the previous work, referential properties were estimated by developing rules that used clue words. If two or more rules were in con ict with each other, the category having the maximum total score given by the rules was selected as the desired category. The score given by each rule was established by hand, so the manpower cost was high. In this work, we automatically adjusted these scores by using a machine-learning method and succeeded in reducing the amount of manpower needed to adjust these scores.	anaphora (linguistics);machine learning;machine translation;naruto shippuden: clash of ninja revolution 3;the machine	Masaki Murata;Kiyotaka Uchimoto;Qing Ma;Hitoshi Isahara	2001		10.1007/3-540-44686-9_14	natural language processing;noun;estimation;noun phrase;method;speech recognition;computer science;computational linguistics;specifier;applied linguistics;proper noun;linguistics;machine translation	NLP	-27.024274416250705	-77.66342927426135	74769
d5979a71479e5b47b4772de700250ad6a2c76f99	graph ranking on maximal frequent sequences for single extractive text summarization		We suggest a new method for the task of extractive text summarization using graph-based ranking algorithms. The main idea of this paper is to rank Maximal Frequent Sequences (MFS) in order to identify the most important information in a text. MFS are considered as nodes of a graph in term selection step, and then are ranked in term weighting step using a graphbased algorithm. We show that the proposed method produces results superior to the-state-of-the-art methods; in addition, the best sentences were found with this method. We prove that MFS are better than other terms. Moreover, we show that the longer is MFS, the better are the results. If the stop-words are excluded, we lose the sense of MFS, and the results are worse. Other important aspect of this method is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, and languages.	algorithm;automatic summarization;cluster analysis;collocation extraction;document classification;maximal set;moose file system;multi-document summarization;n-gram;natural language processing;position-independent code;preprocessor;relevance;social network;text corpus;word sense;word-sense disambiguation	Yulia Ledeneva;René Arnulfo García-Hernández;Alexander F. Gelbukh	2014		10.1007/978-3-642-54903-8_39	pattern recognition;data mining;algorithm	NLP	-25.90358477549716	-67.49264465071816	74952
babf924210fa202d1f8c4ac6b48a901ddab5c775	turning distributional thesauri into word vectors for synonym extraction and expansion		In this article, we propose to investigate a new problem consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating graph embedding and distributed representation adaptation. We have applied and evaluated it for English nouns at a large scale about its ability to retrieve synonyms. In this context, we have also illustrated the interest of the developed method for three different tasks: the improvement of already existing word embeddings, the fusion of heterogeneous representations and the expansion of synsets.	artificial neural network;graph embedding;microsoft word for mac;semantic similarity;synonym ring;thesaurus;word embedding;wordnet	Olivier Ferret	2017			computer science;pattern recognition;artificial intelligence;natural language processing;synonym	NLP	-25.739924173489342	-69.20386388293889	75164
0a076b7670a4ec1dc014d978ca15edb3da122f1a	natural language processing for arabic metaphors: a conceptual approach		Metaphor is a literary device that allows us to express a concept in terms of another. In other words, it is based on similarity between concepts. Metaphorical expressions represent a great variety and they are used in conventional metaphors, which we reproduce and comprehend every day, poetic, novel, and Holy Qur’an. The use of metaphor is ubiquitous in natural language text and it is a serious bottleneck in automatic text understanding, and developing methods to identify and deal with metaphors is an open problem in Arabic natural language processing, especially Machine Translation. Due to the complexities involved in metaphor, it semantically influenced the meaning of machine-translated text. This makes metaphor an important research area for computational and cognitive linguistics, and its automatic identification and interpretation is indispensable for any semantics-oriented Arabic natural language processing. In this paper, we present the challenges of Arabic NLP of metaphors, which is very important in developing a computational NLP-based system for in Classical Arabic, Modern Standard Arabic and Dialect Arabic. We also highlight main problems that arises when translating an Arabic metaphor to another language.	natural language processing	Manar Alkhatib;Khaled Shaalan	2016		10.1007/978-3-319-48308-5_17	natural language processing;language identification;linguistics	NLP	-31.20494138080733	-75.15208391460536	75176
8ba860d032a76ffd7946e4dd7572b6b51622d730	unsupervised word segmentation in context		This paper extends existing word segmentation models to take non-linguistic context into account. It improves the token F-score of a top performing segmentation models by 2.5% on a 27k utterances dataset. We posit that word segmentation is easier in-context because the learner is not trying to access irrelevant lexical items. We use topics from a Latent Dirichlet Allocation model as a proxy for “activities” contexts, to label the Providence corpus. We present Adaptor Grammar models that use these context labels, and we study their performance with and without context annotations at test time.	latent dirichlet allocation;relevance;text segmentation	Gabriel Synnaeve;Isabelle Dautriche;Benjamin Börschinger;Mark Johnson;Emmanuel Dupoux	2014			natural language processing;speech recognition;computer science;linguistics	NLP	-21.705732566619396	-73.65367893249785	75177
222cf1c2c0d838aa8e5099d434cb24ea021329df	query translation in chinese-english cross-language information retrieval		This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora. Instead of looking up a • bilingual phrase dictionary, the compositional phrase (the translation of phrase can be derived from the translation of its components) in the query can be indirectly translated via a general-purpose Chinese-English dictionary look-up procedure. A novel selection method for translations of query terms is also presented in detail. Our query translation method ultimately constructs an English query in which each query term has a weight. The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple wordby-word translation way.	algorithm;coefficient;cross-language information retrieval;dictionary;general-purpose markup language;lookup table;mutual information;query expansion;selection (genetic algorithm);text corpus	Yibo Zhang;Le Sun;Lin Du;Yufang Sun	2000			natural language processing;sargable;query optimization;query expansion;web query classification;ranking;speech recognition;example-based machine translation;boolean conjunctive query;computer science;query by example;rdf query language;machine translation;web search query;information retrieval;query language	Web+IR	-29.22601935551344	-68.8721815612795	75315
fb33f3ce1a43f59ef383f2cd2953683ee0161acc	rule based hindi part of speech tagger		Part of Speech Tagger is an important tool that is used to develop language translator and information extraction. The problem of tagging in natural language processing is to find a way to tag every word in a sentence. In this paper, we present a Rule Based Part of Speech Tagger for Hindi. Our System is evaluated over a corpus of 26,149 words with 30 different standard part of speech tags for Hindi. The evaluation of the system is done on the different domains of Hindi Corpus. These domains include news, essay, and short stories. Our system achieved the accuracy of 87.55%.	brill tagger;information extraction;natural language processing;part-of-speech tagging;text corpus	Navneet Garg;Vishal Goyal;Suman Preet	2012			natural language processing;speech recognition;computer science;linguistics;trigram tagger	NLP	-28.394085904184514	-75.7489039769623	75326
37a4a1272e7f43e07fc6b67763fe2eed1168ddf6	political footprints: political discourse analysis using pre-trained word vectors		In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.	heuristic (computer science);lexicon;machine learning;text corpus;word embedding	Christophe Bruchansky	2017	CoRR		natural language processing;linguistics	NLP	-28.134228279618142	-74.08594372221165	75352
f117e450e4e48250873349b0f0880eeb552b6675	recognition and normalization of some classes of named entities in serbian	named entity normalization;named entity recognition;amount expressions;temporal expressions;finite state transducers	In this paper we present a system for recognition and normalization of measurement and money expressions and temporal expressions for dates and time in Serbian newspaper texts. The normalization of amount expressions involves a transformation of used numerals to a fixed-point notation as well as a transformation of currencies and measurement units into their standard or common abbreviations, while temporal expressions are transformed into the TimeML format. For this purpose, we use our general lexical resources and develop some new ones. The system itself consists of a large collection of finite-state transducers. Finally, we give some evaluation data that show that our system performs well, with well-balanced precision and recall.	database normalization;named entity;named-entity recognition;precision and recall;temporal expressions;timeml;transducer	Cvetana Krstev;Jelena Jacimovic;Dusko Vitas	2012		10.1145/2371316.2371327	natural language processing;speech recognition;computer science	NLP	-28.888455312838914	-76.6209147456192	75478
5083af78bed915849b726b89329e60aa2fb34273	annotators' agreement: the case of topic-focus articulation		The annotation of the Prague Dependency Treebank (PDT) is conceived of as a multilayered scenario that comprises also dependency representations (tectogrammatical tree structures, TGTS’s) of the underlying structure of the sentences. TGTS’s capture three basic aspects of the underlying structure of sentences: (a) the dependency tree structure, (b) the kinds of dependency syntactic relations, and (c) the basic characteristics of the topic-focus articulation (TFA). Since the PDT is a large collection and the annotations on the deepest layer are to a large extent performed by several human annotators (based on an automatic preprocessing module), it is more than necessary to observe the consistence of annotators and the agreement among them. In the present paper, we summarize the results of the evaluation of parallel annotations of several samples taken from PDT and the measures accepted to improve the consistency of annotations.	biconnected component;emoticon;php development tools;preprocessor;subject matter expert turing test;tree structure;treebank	Katerina Vesela;Jirí Havelka;Eva Hajicová	2004			natural language processing;treebank;syntax;artificial intelligence;tree structure;computer science;preprocessor;annotation	NLP	-27.39740388555498	-75.12544554730212	75483
16258ac2186f02397bd4888c99b60a2d1dc5d30e	open resources and tools for the shallow processing of portuguese: the tagshare project		This paper presents the linguistic resources and tools for the shallow processing of Portuguese developed in the scope of a research intitiative at the University of Lisbon. These resources include a 1 million token corpus that has been accurately hand annotated with a variety of linguistic information, as well as several state-of-the-art shallow processing tools capable of automatically producing that type of annotation. At present, the linguistic annotations in the corpus are sentence and paragraph boundaries, token boundaries, morphosyntactic POS categories, values of inflection features, lemmas and named-entities. Hence, the set of tools comprise a sentence chunker, a tokenizer, a POS tagger, nominal and verbal analyzers and lemmatizers, a verbal conjugator, a nominal “inflector”, and a named-entity recognizer, some of which underline several on-line services.	brill tagger;entity;finite-state machine;lemmatisation;lexical analysis;online and offline;part-of-speech tagging	Florbela Barreto;António Branco;Eduardo Ferreira;Amália Mendes;Maria Fernanda Bacelar do Nascimento;Filipe Nunes;João Ricardo Silva	2006			natural language processing;artificial intelligence;computer science;portuguese	NLP	-29.242230975424178	-76.23111419608122	75521
846019c424fbe58d03d56ccfd31ab5f2c5e1fa0a	a language modeling approach for extracting translation knowledge from comparable corpora	comparable corpora;translation language models;cross language information retrieval	A main challenge in Cross-Language information retrieval is to estimate a translation language model, as its quality directly affects the retrieval performance. The translation language model is built using translation resources such as bilingual dictionaries, parallel corpora, or comparable corpora. In general, high quality resources may not be available for scarce-resource languages. For these languages, efficient exploitation of commonly available resources such as comparable corpora is considered more crucial. In this paper, we focus on using only comparable corpora to extract translation information more efficiently. We propose a language modeling approach for estimating the translation language model. The proposed method is based on probability distribution estimation, and can be tuned easier in comparison with heuristically adjusted previous work. Experiment results show a significant improvement in the translation quality and CLIR performance compared to the previous approaches.	language model;text corpus	Razieh Rahimi;Azadeh Shakery	2013		10.1007/978-3-642-36973-5_51	natural language processing;speech recognition;transfer-based machine translation;universal networking language;example-based machine translation;computer science;machine translation;rule-based machine translation;machine translation software usability;information retrieval;language model	NLP	-21.217382465172378	-77.32823920195551	75560
2d4c302f8c5f1370e082f4c02ea54364af02bd20	addressing troublesome words in neural machine translation			neural machine translation	Yang Zhao;Jiajun Zhang;Zhongjun He;Chengqing Zong;Hua Wu	2018			machine translation;natural language processing;artificial intelligence;machine learning;computer science	NLP	-29.752136422531215	-79.15778168520099	75578
4651870a52b913c10d88829a880cdd648b0a7c0e	target-centric features for translation quality estimation		We describe the DCU-MIXED and DCUSVR submissions to the WMT-14 Quality Estimation task 1.1, predicting sentencelevel perceived post-editing effort. Feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations, which are included in task 1.1 of this year’s WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters.	experiment;postediting	Chris Hokamp;Iacer Calixto;Joachim Wagner;Jian Zhang	2014			simulation;engineering;operations management;data mining	NLP	-21.74635707376798	-74.0421956037696	75702
43cef5fa315cdcaa6abd682402d024628781f08b	an automatic contextual analysis and clustering classifiers ensemble approach to sentiment analysis.		Products reviews are one of the major resources to determine the public sentiment. The existing literature on reviews sentiment analysis mainly utilizes supervised paradigm, which needs labeled data to be trained on and suffers from domain-dependency. This article addresses these issues by describes a completely automatic approach for sentiment analysis based on unsupervised ensemble learning. The method consists of two phases. The first phase is contextual analysis, which has five processes, namely (1) data preparation; (2) spelling correction; (3) intensifier handling; (4) negation handling and (5) contrast handling. The second phase comprises the unsupervised learning approach, which is an ensemble of clustering classifiers using a majority voting mechanism with different weight schemes. The base classifier of the ensemble method is a modified k-means algorithm. The base classifier is modified by extracting initial centroids from the feature set via using SentWordNet (SWN). We also introduce new sentiment analysis problems of Australian airlines and home builders which offer potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalization ability.	algorithm;benchmark (computing);cluster analysis;ensemble learning;experiment;k-means clustering;programming paradigm;semantic analysis (compilers);sentiment analysis;unsupervised learning	Murtadha Talib AL-Sharuee;Fei Liu;Mahardhika Pratama	2017	CoRR		sentiment analysis;artificial intelligence;random subspace method;machine learning;cluster analysis;unsupervised learning;computer science;spelling;context analysis;ensemble learning;classifier (linguistics)	NLP	-23.324947045654838	-69.93460974628188	75801
34016f7e97478b4486abbb5666be7d809f70f572	graph algebraic combinatory categorial grammar		This paper describes CCG/AMR, a novel grammar for semantic parsing of Abstract Meaning Representations. CCG/AMR equips Combinatory Categorial Grammar derivations with graph semantics by assigning each CCG combinator an interpretation in terms of a graph algebra. We provide an algorithm that induces a CCG/AMR from a corpus and show that it creates a compact lexicon with low ambiguity and achieves a robust coverage of 78% of the examined sentences under ideal conditions. We also identify several phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues.	adaptive multi-rate audio codec;algorithm;arne sølvberg;combinatory categorial grammar;experiment;graph algebra;heuristic (computer science);hoc (programming language);lambda calculus;lexicon;mathematical induction;mind;parsing;the wall street journal	Sebastian Beschke;Wolfgang Menzel	2018			discrete mathematics;combinatory categorial grammar;algebraic number;mathematics;graph	NLP	-28.603074009614673	-79.00829585920151	75828
3d00911ad367ae6be6f803afca11cfc93af4a7e2	modeling and analysis of indian carnatic music using category theory	ontology categorical framework for carnatic music categorical structure for r aga category theory ct;hidden markov models metadata music computational modeling analytical models collaboration ontologies	This paper presents a category theoretic ontology of Carnatic music. Our goals here are twofold. First, we will demonstrate the power and flexibility of conceptual modeling techniques based on a branch of mathematics called category theory (CT), using the structure of Carnatic music as an example. Second, we describe a platform for collaboration and research sharing in this area. The construction of this platform uses formal methods of CT (colimits) to merge our Carnatic ontology with a generic model of music information retrieval tasks. The latter model allows us to integrate multiple analytical methods, such as hidden Markov models, machine learning algorithms, and other data mining techniques like clustering, bagging, etc., in the analysis of a variety of different musical features. Furthermore, the framework facilitates the storage of musical performances based on the proposed ontology, making them available for additional analysis and integration. The proposed framework is extensible, allowing future work in the area of rāga recognition to build on our results, thereby facilitating collaborative research. Generally speaking, the methods presented here are intended as an exemplar for designing collaborative frameworks supporting reproducibility of computational analysis and simulation.	algorithm;bootstrap aggregating;category theory;cluster analysis;common platform;data mining;database schema;formal methods;hidden markov model;information retrieval;machine learning;markov chain;ontology (information science);performance;simulation;text corpus	Sarala Padi;Spencer Breiner;Eswaran Subrahmanian;Ram D. Sriram	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2631130	music information retrieval;machine learning;conceptual model;data mining;artificial intelligence;ontology;hidden markov model;category theory;cluster analysis;formal methods;computer science;ontology (information science)	Web+IR	-19.58713119447404	-67.28075031301081	75866
7ccc22f16a027659a46b0f7469394d903e5a4b41	extraction of lexical translations from non-aligned corpora	stochastic matrix;target language;ambiguity resolution;source language	"""A method for extracting lexical translations from non-aligned corpora is proposed to cope with the unavailability of large aligned corpus. The assumption that \translations of two co-occurring words in a source language also co-occur in the target language"""" is adopted and represented in the stochastic matrix formulation. The translation matrix provides the co-occurring information translated from the source into the target. This translated co-occurring information should resemble that of the original in the target when the ambiguity of the translational relation is resolved. An algorithm to obtain the best translation matrix is introduced. Some experiments were performed to evaluate the e ectiveness of the ambiguity resolution and the re nement of the dictionary."""	algorithm;compiler;computational complexity theory;dictionary;experiment;gradient descent;linear programming;matrix mechanics;nonlinear programming;nonlinear system;relevance;stochastic matrix;text corpus;unavailability	Kumiko Tanaka-Ishii;Hideya Iwasaki	1996		10.3115/993268.993270	natural language processing;speech recognition;computer science;stochastic matrix;linguistics	NLP	-20.125735412862834	-76.74718237198918	75934
0dede3def38891ec0eca99d360736c97c5317d7f	knowledge acquisition with om - a heuristic solution	knowledge acquisition	Knowledge scattered through the Web inside unstructured documents (text documents) can not be easily interpreted by computers. To do so, knowledge contained from them must be extracted by a parser or a person and poured into a suitable data structure, the best form to do this, are with ontologies. For an appropriate merging of these “individual” ontologies, we consider repetitions, redundancies, synonyms, meronyms, different level of details, different viewpoints of the concepts involved, and contradictions, a large and useful ontology could be constructed. This paper presents OM algorithm, an automatic ontology merger that achieves the fusion of two ontologies without human intervention. Through repeated application of OM, we can get a growing ontology of a knowledge topic given. Using OM we hope to achieve automatic knowledge acquisition. There are two missing tasks: the conversion of a given text to its corresponding ontology (by a combination of syntactic and semantic analysis) is not yet automatically done; and the exploitation of the large resulting ontology is still under development.	algorithm;computer;data structure;heuristic;knowledge acquisition;ontology (information science);sensor;world wide web	Adolfo Guzmán-Arenas;Alma-Delia Cuevas-Rasgado	2008			computer science;knowledge management;data science;machine learning	AI	-32.40928080115893	-69.29981911000947	76013
be34bf5203e206e8a8e4a872ea75f21278b5e875	the ipr-cleared corpus of contemporary written and spoken romanian language		The article describes the current status of a large national project, CoRoLa, aiming at building a reference corpus for the contemporary Romanian language. Unlike many other national corpora, CoRoLa contains only IPR cleared texts and speech data, obtained from some of the country’s most representative publishing houses, broadcasting agencies, editorial offices, newspapers and popular bloggers. For the written component 500 million tokens are targeted and for the oral one 300 hours of recordings. The choice of texts is done according to their functional style, domain and subdomain, also with an eye to the international practice. A metadata file (following the CMDI model) is associated to each text file. Collected texts are cleaned and transformed in a format compatible with the tools for automatic processing (segmentation, tokenization, lemmatization, part-of-speech tagging). The paper also presents up-to-date statistics about the structure of the corpus almost two years before its official launching. The corpus will be freely available for searching. Users will be able to download the results of their searches and those original files when not against stipulations in the protocols we have with text providers.	blog;download;interpro;lemmatisation;part-of-speech tagging;text corpus;tokenization (data security)	Dan Tufis;Verginica Barbu Mititelu;Elena Irimia;Stefan Daniel Dumitrescu;Tiberiu Boros	2016			artificial intelligence;clearance;natural language processing;corpus linguistics;romanian;computer science	NLP	-33.04158230286075	-74.94473652711426	76038
aaed12c89f4df55cbc3b45c10e0b36b4be454455	relation extraction in vietnamese text using conditional random fields	information extraction;contextual information;system performance;research method;relation extraction;part of speech tagging;conditional random field;semantic relations	Relation extraction is the task of finding semantic relations between entities from text. This paper presents our approach to relation extraction for Vietnamese text using Conditional Random Field. The features used in the sys- tem are words, part-of-speech tag, entity type, type of other entities in the sen- tence, entity's index and contextual information. In order to evaluate the effect of the contextual information to the system performance, different window sizes have been tested in our experiments. It shown that the system performance is affected by the window size, but it is not directly proportional to the F-score of the system. Our future work includes: (i) testing the system with a larger corpus in order to get a more accurate evaluation of the system; (ii) investigating other features used in the CRF algorithm to increase the system performance; and (iii) researching methods to extract relations outside the sentence's scope.	conditional random field;relationship extraction	Rathany Chan Sam;Huong Thanh Le;Thuy Thanh Nguyen;The Minh Trinh	2010		10.1007/978-3-642-17187-1_32	natural language processing;relationship extraction;computer science;machine learning;pattern recognition;data mining;conditional random field;information extraction;information retrieval	NLP	-24.605168812187053	-69.47687133714837	76056
eeb3aac0bf4bd7bd9eed82de7dcc2adf95bd8ab3	semi-supervised learning for automatic conceptual property extraction	gold standard;automatic conceptual property extraction;direct human evaluation;semi-supervised learning;experimental psychology;concept-relation-feature triple;better performance;concrete noun concept;future focus;abstract mental representation;cognitive psychologist;enormous benefit	For a given concrete noun concept, humans are usually able to cite properties (e.g., elephant is animal , car has wheels ) of that concept; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain. Consequently, the ability to automatically extract such properties would be of enormous benefit to the field of experimental psychology. This paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora (Wikipedia and UKWAC) for concrete noun concepts. Previous approaches have relied on manually-generated rules and hand-crafted resources such as WordNet; our method requires neither yet achieves better performance than these prior approaches, measured both by comparison with a property norm-derived gold standard as well as direct human evaluation. Our technique performs particularly well on extracting features relevant to a given concept, and suggests a number of promising areas for future focus.	mental representation;semi-supervised learning;semiconductor industry;supervised learning;support vector machine;text corpus;wheels;wikipedia;wordnet	Colin Kelly;Barry Devereux;Anna Korhonen	2012			computer science;artificial intelligence;data mining;communication	NLP	-27.216073284263068	-69.37735828079694	76112
7c4126519e3ea89a9a310dad38ac73f1dc08fb95	exploring difficulties in parsing imperatives and questions		This paper analyzes the effect of the structural variation of sentences on parsing performance. We examine the performance of both shallow and deep parsers for two sentence constructions: imperatives and questions. We first prepare an annotated corpus for each of these sentence constructions by extracting sentences from a fiction domain that cover various types of imperatives and questions. The target parsers are then adapted to each of the obtained corpora as well as the existing query-focused corpus. Analysis of the experimental results reveals that the current mainstream parsing technologies and adaptation techniques cannot cope with different sentence constructions even with much in-domain data.	experiment;parsing;part-of-speech tagging;text corpus	Tadayoshi Hara;Takuya Matsuzaki;Yusuke Miyao;Jun'ichi Tsujii	2011			natural language processing;computer science;linguistics	NLP	-27.629448262509317	-74.64710539824225	76288
c955e2431bdcf9cc6f9e6eeeac2d4d306b3a0da3	mining language variation using word using and collocation characteristics	text mining;overall intimacy;language variation;frequency rank ratio	Two textual metrics ''Frequency Rank'' (FR) and ''Intimacy'' are proposed in this paper to measure the word using and collocation characteristics which are two important aspects of text style. The FR, derived from the local index numbers of terms in a sentences ordered by the global frequency of terms, provides single-term-level information. The Intimacy models relationship between a word and others, i.e. the closeness a term is to other terms in the same sentence. Two textual features ''Frequency Rank Ratio (FRR)'' and ''Overall Intimacy (OI)'' for capturing language variation are derived by employing the two proposed textual metrics. Using the derived features, language variation among documents can be visualized in a text space. Three corpora consisting of documents of diverse topics, genres, regions, and dates of writing are designed and collected to evaluate the proposed algorithms. Extensive simulations are conducted to verify the feasibility and performance of our implementation. Both theoretical analyses based on entropy and the simulations demonstrate the feasibility of our method. We also show the proposed algorithm can be used for visualizing the closeness of several western languages. Variation of modern English over time is also recognizable when using our analysis method. Finally, our method is compared to conventional text classification implementations. The comparative results indicate our method outperforms the others.	collocation	Peng Tang;Tommy W. S. Chow	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.05.018	natural language processing;text mining;speech recognition;computer science;artificial intelligence;machine learning;mathematics	NLP	-23.619589868037036	-66.92699129022981	76401
b318aab20499dc333bdaf8ca0bd5ce07310f9bda	summary of product characteristics content extraction for a safe drugs usage	support vector machines;information extraction;summary of product characteristics;conditional random fields;medication errors;adverse drug events	The use of medications has a central role in health care provision, yet on occasion, it may injure the person taking them as result of adverse drug events. A correct drug choice must be modulated to acknowledge both patients' status and drug-specific information. However, this information is locked in free-text and, as such, cannot be actively accessed and elaborated by computerized applications. The goal of this work lies in extracting content (active ingredient, interaction effects, etc.) from the Summary of Product Characteristics, focusing mainly on drug-related interactions, following a machine learning based approach. We compare two state of the art classifiers: conditional random fields with support vector machines. To this end, we introduce a corpus of 100 interaction sections, hand annotated with 13 labels that have been derived from a previously developed conceptual model. The results of our empirical analysis demonstrate that the two models perform well. They exhibit similar overall performance, with an overall accuracy of about 91%.	adverse reaction to drug;body of uterus;conditional random field;health care;interaction;machine learning;modulation;patients;support vector machine;text corpus	Stefania Rubrichi;Silvana Quaglini	2012	Journal of biomedical informatics	10.1016/j.jbi.2011.10.012	support vector machine;computer science;data science;machine learning;data mining;database;conditional random field;information extraction	ML	-23.44866556642658	-68.67626725805638	76422
97aef787d63aef75e6f8055cdac3771f8649f21a	a syllable-based technique for word embeddings of korean words		Word embedding has become a fundamental component to many NLP tasks such as named entity recognition and machine translation. However, popular models that learn such embeddings are unaware of the morphology of words, so it is not directly applicable to highly agglutinative languages such as Korean. We propose a syllable-based learning model for Korean using a convolutional neural network, in which word representation is composed of trained syllable vectors. Our model successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.	artificial neural network;convolutional neural network;machine translation;mathematical morphology;n-gram;named-entity recognition;natural language processing;syllable (computing);vocabulary;word embedding	Sanghyuk Choi;Taeuk Kim;Jinseok Seol;Sang-goo Lee	2017			artificial intelligence;morphology (linguistics);computer science;convolutional neural network;machine translation;natural language processing;word embedding;syllable;named-entity recognition;agglutinative language	NLP	-19.223652175130574	-73.95822986325803	76474
50a5d47ed4d771950551e053800a8bee0454940e	pjait systems for the iwslt 2015 evaluation campaign enhanced by comparable corpora		In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech English, Vietnamese English, French English and German English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality.	bleu;bitext word alignment;domain adaptation;experiment;language model;parallel text;software quality;statistical machine translation;ted;text corpus;wikipedia	Krzysztof Wolk;Krzysztof Marasek	2015	CoRR		natural language processing;speech recognition;computer science;artificial intelligence;machine learning;linguistics;programming language	NLP	-22.56288452384277	-77.18299234103353	76491
fc33f03159ae7d67939278f788e88d2983d6aa89	a computational framework to integrate different semantic resources	hierarchical clustering;lexical semantics;hierarchical networks;scaling up;semantic relatedness;large scale;computer application;knowledge base	In recent years, many large-scale semantic resources have been built in the NLP community, but how to apply them in real text semantic parsing is still a big problem. In this paper, we propose a new computational framework to deal with this problem. Its key parts are a lexical semantic ontology (LSO) representation to integrate abundant information contained in current semantic resources, and a LSO schema to automatically reorganize all this semantic knowledge in a hierarchical network. We introduce an algorithm to build the LSO schema by a three-step procedure: to build a knowledge base of lexical relationship, to accumulate all information in it to generate basic LSO nodes, and to build a LSO schema through hierarchical clustering based on different semantic relatedness measures among them. The preliminary experiments have shown promising results to indicate its computability and scaling-up characteristics. We hope it can play an important role in real world semantic computation applications.	computation	Qiang Zhou	2008		10.1007/978-3-540-87391-4_32	natural language processing;knowledge base;semantic similarity;semantic computing;lexical semantics;semantic grid;computer science;artificial intelligence;machine learning;data mining;semantic compression;database;hierarchical clustering;linguistics;semantic technology	NLP	-31.906531130186174	-69.83873322531882	76502
40eeb60b0922321b63d122044390ae9ce8e62d50	the users who say 'ni': audience identification in chinese-language restaurant reviews		We give an algorithm for disambiguating generic versus referential uses of secondperson pronouns in restaurant reviews in Chinese. Reviews in this domain use the ‘you’ pronoun 你 either generically or to refer to shopkeepers, readers, or for selfreference in reported conversation. We first show that linguistic features of the local context (drawn from prior literature) help in disambigation. We then show that document-level features (n-grams and document-level embeddings)— not previously used in the referentiality literature— actually give the largest gain in performance, and suggest this is because pronouns in this domain exhibit ‘one-senseper-discourse’. Our work highlights an important case of discourse effects on pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews.	algorithm;grams;n-gram;self-reference	Rob Voigt;Daniel Jurafsky	2015			multimedia	NLP	-27.607763699803353	-73.845374792055	76512
1c6feb76316d3ff5ffbd66be281cfdcdfeb5dfbd	a corpus-based investigation of definite description use	corpus annotation;analyse de corpus;article de presse;etude experimentale;description definie;semantics;semantique;classification;annotation de corpus;definite description;corpus analysis;anglais;interpretation semantique;computational linguistics;article defini;point of view;antecedent;linguistique informatique;newspaper report	We present the results of a study of definite descriptions use in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. Themost interesting result of this study from a corpus annotation perspective was the rather low agreement (K=0.63) that we obtained using versions of Hawkins’ and Prince’s classification schemes; better results (K=0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, first-mention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation. From a linguistic point of view, the most interesting observations were the great number of discourse-newdefinites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites which did not seem to require a complete disambiguation. This paper will appear in Computational Linguistics.	anaphora (linguistics);bridging (networking);computation;computational linguistics;experiment;prince;text corpus;word-sense disambiguation	Massimo Poesio;Renata Vieira	1998	Computational Linguistics		natural language processing;biological classification;computer science;computational linguistics;semantics;linguistics;algorithm	NLP	-27.69618152253222	-76.52593493387941	76519
33184925373bdd8d0b5606ad2f62cc914dad5ab6	good tools and models are not enough: on the necessity for bi-directional communication			bi-directional text	Ellen E. Robertson;Dottie Cunningham	1987				HPC	-31.502803039849468	-77.91079188846588	76555
92df37220c037cd9f38d6212ed409d8e8054d8c6	improving mood classification in music digital libraries by combining lyrics and audio	audio systems;music mood classification;fusion methods;access point;online music;measurement;conference_paper;metadata;supervised learning;learning curve;training sample;text feature;text processing;digital libraries;performance;large scale;lyric sentiment analysis;feature extraction;sentiment analysis;experiments;access points;music digital libraries;feature fusion;social tagging;data sets;classification accuracy;experimentation;learning curves;audio features;audio acoustics;music digital library	Mood is an emerging metadata type and access point in music digital libraries (MDL) and online music repositories. In this study, we present a comprehensive investigation of the usefulness of lyrics in music mood classification by evaluating and comparing a wide range of lyric text features including linguistic and text stylistic features. We then combine the best lyric features with features extracted from music audio using two fusion methods. The results show that combining lyrics and audio significantly outperformed systems using audio-only features. In addition, the examination of learning curves shows that the hybrid lyric + audio system needed fewer training samples to achieve the same or better classification accuracies than systems using lyrics or audio singularly. These experiments were conducted on a unique large-scale dataset of 5,296 songs (with both audio and lyrics for each) representing 18 mood categories derived from social tags. The findings push forward the state-of-the-art on lyric sentiment analysis and automatic music mood classification and will help make mood a practical access point in music digital libraries.	digital library;experiment;library (computing);online music store;sentiment analysis;wireless access point	Xiao Hu;J. Stephen Downie	2010		10.1145/1816123.1816146	digital library;speech recognition;computer science;multimedia;supervised learning;learning curve;world wide web;information retrieval;sentiment analysis	Web+IR	-21.559696925107648	-66.81090812449781	76792
90cc68591c8040604f8c813779b1f9bf6c411c70	special issue of corpora, language use, and grammar - introduction.	language use			Hongyin Tao	2004	Journal of Chinese Language and Computing		direct method;natural language processing;language identification;generative grammar;traditional grammar;speech recognition;universal networking language;object language;second-language acquisition;regular grammar;computer science;affix grammar;linguistics;natural language;comprehension approach	NLP	-30.40092353215188	-78.2397752306183	76916
8f1cdcaa6a1d564d0aaad8d8829ade0fefc89842	two-fold filtering for chinese subcategorization acquisition with diathesis alternations used as heuristic information	scf;chinese;filter;diathesis alternation	Automatically acquired lexicons with subcategorization information have been shown to be accurate and useful for some purposes, but their accuracy still shows room for improvement and their usefulness in many applications remains to be investigated. This paper proposes a two-fold filtering method, which in experiments improved the performance of a Chinese acquisition system remarkably, with an increased precision rate of 76.94% and a recall rate of 83.83%, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. And as far as we know, at the present time, these figures represent the best overall performance achieved in Chinese subcategorization acquisition and in similar researches focusing on other languages.	experiment;heuristic;lexicon;natural language processing;sensitivity and specificity	Xiwu Han;Tiejun Zhao	2006	IJCLCLP		proofreading;filter (signal processing);natural language processing;subcategorization;heuristic;recall;artificial intelligence;computer science;lexicon	NLP	-25.231003304618522	-75.52307123275315	77113
10e9d0933496a3c52198da63b7c8675d4b26dde6	madad: a readability annotation tool for arabic text		This paper introduces MADAD, a general-purpose annotation tool for Arabic text with focus on readability annotation. This tool will help in overcoming the problem of lack of Arabic readability training data by providing an online environment to collect readability assessments on various kinds of corpora. Also the tool supports a broad range of annotation tasks for various linguistic and semantic phenomena by allowing users to create their customized annotation schemes. MADAD is a web-based tool, accessible through any web browser; the main features that distinguish MADAD are its flexibility, portability, customizability and its bilingual interface (Arabic/English).	bespoke;general-purpose modeling;software portability;text corpus;web application	Nora Al-Twairesh;Abeer Al-Dayel;Hend Suliman Al-Khalifa;Maha M. Al-Yahya;Sinaa Alageel;Nora Abanmy;Nouf Al-Shenaifi	2016			arabic;readability;artificial intelligence;natural language processing;information retrieval;computer science;annotation	NLP	-32.08200303949102	-72.94826056831431	77139
26da11f7ae902b7845d6de08fa2a5c40e8bf0180	a syllable-based name transliteration system	final chinese character sequence;pinyin sequence;english syllable;chinese character sequence;name entity transliteration system;ep model;mapping model;syllable-based name transliteration system;pc model;english name;probable pinyin sequence	This paper describes the name entity transliteration system which we conducted for the “NEWS2009 Machine Transliteration Shared Task” (Li et al 2009). We get the transliteration in Chinese from an English name with three steps. We syllabify the English name into a sequence of syllables by some rules, and generate the most probable Pinyin sequence with the mapping model of English syllables to Pinyin (EP model), then we convert the Pinyin sequence into a Chinese character sequence with the mapping model of Pinyin to characters (PC model). And we get the final Chinese character sequence. Our system achieves an ACC of 0.498 and a Mean F-score of 0.786 in the official evaluation result.	expectation propagation;syllable	Xue Jiang;Le Sun;Dakun Zhang	2009			natural language processing;speech recognition;engineering;linguistics	NLP	-23.306357226824296	-78.05865118996735	77157
ac73de9349618b229cd376e51d004e2f236b9a6a	coupling maximum entropy and probabilistic context-free grammar models for xml annotation of documents	xml schema;probability distribution;maximum entropy	We consider the problem of semantic annotation of semi-str uctu ed documents according to a target XML schema. The task is to ann t te a document in a tree-like manner where the annotation tree is an ins tance of a tree class defined by DTD or W3C XML Schema descriptions. In the probabil istic setting, we cope with the tree annotation problem as a generalized probabilistic contextfree parsingof an observation sequence where each observation comes wit h a probability distribution over terminals supplied by a prob abilistic classifier associated with the content of documents. We determine the mos t pr bable tree annotation by maximizing the joint probability of selectin g a terminal sequence for the observation sequence and the most probable parse for the selected terminal sequence. We extend the inside-outside algorithm for proba bilistic context-free grammars and establish a Naive Bayes-like requirement that he content classifier should satisfy when estimating the terminal probabilities . Nous considérons le problème de l’annotation sémantiqu e de documents semistructurés guidée par un schéma xml cible. Le but est d’an noter un document de fa con arborescente où l’arbre d’annotation est l’insta nce d’une DTD ou d’un schéma W3C XML. Avec notre approche probabiliste, nous tra itons le problème de l’annotation comme une généralisation de la dérivation de grammaires horscontextes probabilistes pour des séquences d’observations. Chaque observation possède une distribution de probabilités sur les classes qui est fournie par un classificateur probabiliste associé au contenu du documen t. L’arbre d’annotation le plus probable est choisi en maximisant la probabilité jo inte de la séquence d’observations et de l’arbre de dérivation associé à cet te s ́ quence. Nous améliorons l’algorithme inside-outside pour les grammaires hors-con textes probabilistes et établissons des contraintes d’indépendances que le clas sific teur doit satisfaire pour estimer les probabilités des classes. Mots-clés: Apprentissage artificiel, Web sémantique, Extraction d’ i formations	approximation;cesg listed adviser scheme;context-free language;estdomains;html;inside–outside algorithm;linear algebra;naive bayes classifier;naruto shippuden: clash of ninja revolution 3;parsing;semiconductor industry;stochastic context-free grammar;world wide web;xml schema	Boris Chidlovskii;Jérôme Fuselier	2005			stochastic grammar;xml;parsing;document structure description;natural language processing;xml schema;probabilistic classification;artificial intelligence;xml validation;annotation;computer science;pattern recognition	ML	-25.420304206062568	-78.9996417808704	77181
1729b5a509b6a0915ad3bca7bc18e0dbda971f46	action languages and question answering		This paper describes a methodology for designing Question Answering systems that utilize an action language ALM to allow inferences based on complex interactions of events described in texts. This methodology assumes the extension of the VERBNET lexicon with interpretable semantic annotations in ALM and specifies the use of several other NLP resources to produce ALM system descriptions for input discourses.	action language;application lifecycle management;interaction;java annotation;lexicon;natural language processing;population;question answering;verbnet	Yuliya Lierler;Daniela Inclezan;Michael Gelfond	2017			natural language processing;question answering;computer science;artificial intelligence	NLP	-30.225842271429112	-76.46929432255801	77321
bb8b4e7c43459bf69ce8518797be216c7f9248c6	tools and methods for computational lexicology	machine readable dictionary;linguistique appliquee;methodologie;dictionnaires;lexicologie;lexicographie;traitement automatique;natural language processing;longman dictionary of contemporary english	This paper presents a set of tools and methods for acquiring, manipulating, and analyzing machine-readable dictionaries. We give several detailed examples of the use of these tools and methods for particular analyses. A novel aspect of our work is that it allows the combined processing of multiple machine-readable dictionaries. Our examples describe analyses of data from Webster's Seventh Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual dictionaries, the Collins Thesaurus, and the Zingarelli Italian dictionary. We describe existing facilities and results they have produced as well as planned enhancements to those facilities, particularly in the area of managing associations involving the senses of polysemous words. We show how these enhancements expand the ways in which we can exploit machine-readable dictionaries in the construction of large lexicons for natural language processing systems.	bilingual dictionary;computation;computational lexicology;human-readable medium;lexicon;machine-readable dictionary;natural language processing;thesaurus	Roy J. Byrd;Nicoletta Calzolari;Martin Chodorow;Judith L. Klavans;Mary S. Neff;Omneya A. Rizk	1987	Computational Linguistics		natural language processing;speech recognition;machine-readable dictionary;computer science;linguistics	NLP	-31.236564482473966	-75.1584474597697	77380
2fbe42997734010b1dbc9c44a62af29d60aecd16	knowledge based question answering	conceptual dependency conceptualization;conceptualization pattern;natural language database query;q-a system;production system rule base;question answering;knobs interactive planning system;conceptual dependency primitive;responsive database search;production system;dictionary entry;computational linguistics;data bases;knowledge base;information systems;artificial intelligence;surface structure;information retrieval;semantics;bottom up;database search;interpreters;natural language;real time systems;pattern matching;parsers;dictionaries;planning;rule based	The n a t u r a l language d a t a b a s e query system i n c o r p o r a t e d in the KNOBS i n t e r a c t i v e p l a n n i n g sys t em compr i ses a d i c t i o n a r y d r i v e n p a r s e r , APE-II , and s c r i p t i n t e r p r e t e r which y i e l d a c o n c e p t u a l dependency c o n c e p t u a l i z a t i o n as a r e p r e s e n t a t i o n of the manning of u s e r i n p u t . A c o n c e p t u a l i z a t i o n p a t t e r n ma tch ing p r o d u c t i o n sys tem then d e t e r m i n e s and e x e c u t e s a p rocedure fo r e x t r a c t i n g the d e s i r e d i n f o r m a t i o n from the d a t a b a s e . In c o n t r a s t to s y n t a x d r i v e n Q-A s y s t e m s , e . $ . , t ho se based on ATH p a r s e r s , AFE-II ia d r i v e n bot tom-up by e x p e c t a t i o n s a s s o c i a t e d w i th word ~ e a n i n g s . The p r o c e s a i n K of a query i s based on the c o n t e n t s of s e v e r a l knowledge sou rc e s i n c l u d i n g the d i c t i o n a r y e n t r i e s ( p a r t i a l c o n c e p t u a l i z a t i o n s and t h e i r e x p e c t a t i o n s ) , f rames r e p r e s e n t i n g concep tua l dependency p r i m i t i v e s , s c r i p t s which c o n t a i n s t e r e o t y p i c a l knowledge about p l ann ing t a s k s used to i n f e r s t a t e s e n a b l i n g or r e s u l t i n g from a c t i o n s , and two p roduc t i on sys tem r u l e bases for the i n f e r e n c e of i m p l i c i t case f i l l e r s , and fo r d e t e r m i n i n g the r e s p o n s i v e d a t ab ase s e a r c h . The g o a l s of t h i s approach , a l l of which are c u r r e n t l y a t l e a s t p a r t i a l l y a c h i e v e d , i n c lu d e u t i l i z i n g s i m i l a r r e p r e s e n t a t i o n s fo r q u e s t i o n s wi th s i m i l a r meanings but wide ly v a r y i n g s u r f a c e s t r u c t u r e s , deve lop ing a powerful mechanism for the d i s a m b i g u a t i o u of words wi th m u l t i p l e meanings and the d e t e r m i n a t i o n of pronoun r e f e r e n t s , answer ing q u e s t i o n s which r e q u i r e i n f e r e n c e s to be u n d e r s t o o d , and i n t e r p r e t i n g e l l i p s e s and unBra -na t i ca l u t t e r a n c e s .	analog front-end;artificial intelligence;database;fo (complexity);knowledge-based systems;lu decomposition;natural language user interface;question answering;radio frequency;tom;word sense	Michael J. Pazzani;Carl Engelman	1983			natural language processing;computer science;data mining;database	AI	-30.80559574281801	-79.0246672746995	77427
ab3adb64298e4d1a9fe761d6d0228ad9365b1c4e	sentence simplification as tree transduction		In this paper, we introduce a syntax-based sentence simplifier that models simplification using a probabilistic synchronous tree substitution grammar (STSG). To improve the STSG model specificity we utilize a multi-level backoff model with additional syntactic annotations that allow for better discrimination over previous STSG formulations. We compare our approach to T3 (Cohn and Lapata, 2009), a recent STSG implementation, as well as two state-of-the-art phrase-based sentence simplifiers on a corpus of aligned sentences from English and Simple English Wikipedia. Our new approach performs significantly better than T3, similarly to human simplifications for both simplicity and fluency, and better than the phrasebased simplifiers for most of the evaluation metrics.	algorithm;backoff;evaluation function;language model;level of detail;sensitivity and specificity;text simplification;transducer;unified framework;wikipedia	Dan Feblowitz;David Kauchak	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-21.942379768421922	-76.92558908424849	77468
5a985cf8981fa04c0997fc364654378b145ab9d9	opinion analysis for ntcir8 at postech		We describe an opinion analysis system developed for a Multilingual Opinion Analysis Task at NTCIR8. Given a topic and relevant newspaper articles, our system determines whether a sentence in the articles has an opinion. If so, we then extract the holder of the opinion. In the opinion judgment task, we constructed a phrase-level opinion expression extractor from sentence-level annotated corpus. In opinion holder extraction task, we used the probability that the word is appeared in the opinion holder and a dependency relationship between the word and the verb of the sentence.	randomness extractor	Hun-Young Jung;Jungi Kim;Jong-Hyeok Lee	2010			newspaper;natural language processing;verb;extractor;sentence;political science;artificial intelligence	NLP	-25.27535070565995	-69.19985937943748	77508
ce11821f416638dcb240be3b7445ab9cb6cbec90	a simplified chinese parser with factored model		This paper presents our work for participation in the 2012 CIPS-ParsEval shared task of Simplified Chinese parsing. We adopt a factored model to parse the Simplified Chinese. The factored model is one kind of combined structure between PCFG structure and dependency structure. It mainly uses an extremely effective A* parsing algorithm which enables to get a more optimal solution. Throughout this paper, we use TCT Treebank as experimental data. TCT mainly consists of binary trees, with a few single-branch trees. The final experiment result demonstrates that the head propagation table improves the parsing performance. Finally, we describe the implementation of the system we used as well as analyze our experiment result SC_F1 from 43% up to 63% and the LC_F1 is about 92% we have achieved.	algorithm;binary tree;machine learning;parser;software propagation;stochastic context-free grammar;the coroner's toolkit;treebank	Qiuping Huang;Liangye He;Derek F. Wong;Lidia S. Chao	2012			natural language processing;speech recognition;computer science;bottom-up parsing;top-down parsing;algorithm	NLP	-21.80067318862702	-76.18968714069065	77544
7c8de2abe8024de05d00095447d918b21960095d	key2vec: automatic ranked keyphrase extraction from scientific articles using phrase embeddings		Keyphrase extraction is a fundamental task in natural language processing that facilitates mapping of documents to a set of representative phrases. In this paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles. Specifically, we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Evaluations are performed on benchmark datasets producing state-of-the-art results.	benchmark (computing);keyword extraction;natural language processing;pagerank;scientific literature	Debanjan Mahata;John Kuriakose;Rajiv Ratn Shah;Roger Zimmermann	2018			natural language processing;computer science;phrase;artificial intelligence;ranking	NLP	-27.607079341199505	-67.183855480977	77632
1ced220ba6ed5f70a91c4d9bd1e2437909630751	detection of computer-generated papers using one-class svm and cluster approaches		The paper presents a novel methodology intended to distinguish between real and artificially generated manuscripts. The approach employs inherent differences between the human and artificially generated wring styles. Taking into account the nature of the generation process, we suggest that the human style is essentially more “diverse” and “rich” in comparison with an artificial one. In order to assess dissimilarities between fake and real papers, a distance between writing styles is evaluated via the dynamic dissimilarity methodology. From this standpoint, the generated papers are much similar in their own style and significantly differ from the human written documents. A set of fake documents is captured as the training data so that a real document is expected to appear as an outlier in relation to this collection. Thus, we analyze the proposed task in the context of the one-class classification using a one-class SVM approach compared with a clustering base procedure. The provided numerical experiments demonstrate very high ability of the proposed methodology to recognize artificially generated papers.	computer-generated holography	Renata Avros;Zeev Volkovich	2018		10.1007/978-3-319-96133-0_4	machine learning;pattern recognition;computer science;support vector machine;artificial intelligence;outlier;writing style;training set;text mining;cluster analysis	Vision	-23.54330863967043	-67.2291656197807	77655
0226565f6745710c4e705852da7705384d77c97e	exploiting named entity classes in ccg surface realization	broad coverage surface realization;ccg surface realization;certain multi-word nes;class labels yield;language model;state-of-the-art bleu score;openccg realizer;class information;significant rise;entity class;largest quality increase;bleu score increase corresponds	Legend Condition Expansion LM baseline-LM: word 3g+ pos 3g*stag 3g HT baseline Hypertagger LM4 LM with 4g word LMC LM with class-rep model interpolated LM4C LM with both HTC HT with classes on nodes as extra feats Corpus Condition %Exact %Complete BLEU Uncollapsed LM+HT 29.27 84.02 0.7900 (98.6% LM4+HT 29.14 83.61 0.7899 coverage) LMC+HT 30.64 83.70 0.7937 LM4C+HT 30.85 83.65 0.7946 Partly collapsed LM+HT 28.28 82.48 0.7917 (98.6% LM4+HT 28.68 82.54 0.7929 coverage) LMC+HT 30.74 82.33 0.7993 LM4C+HT 31.06 82.33 0.7995 LM4C+HTC 32.01 83.17 0.8042 OpenCCG Realization Results (Section 00)	bleu;baseline (configuration management);interpolation;little man computer;named entity	Rajakrishnan Rajkumar;Michael White;Dominic Espinosa	2009			natural language processing;speech recognition;computer science;artificial intelligence;algorithm;language model	NLP	-21.737917031761985	-75.88607355147104	77770
368590e9bd4a4198a08d91e815d73b24b496842f	augmenting word space models for word sense discrimination using an automatic thesaurus	bag of words	This paper presents an algorithm for Word Sense Discrimination that divides the global representation of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not. We rely on the notion that words that are used in similar contexts will have the same or a closely related meaning, thus, given a target word, we group its dependency co-occurrences in a Word Space Model. Each cluster represents a distinct meaning or sense of that word. We experiment with augmenting the bag of words of each cluster of co-occurrences, the dictionary of sense definition, and augmenting both. Then we count the number of intersections of each word of the bag of clustered senses and the bag of the dictionary of senses following the Lesk method. We find an increase in recall and a decrease in precision when augmenting. However, the best resulting F-measure is for the option of augmenting the both dictionary of senses and the bag of words from the clusters.		Hiram Calvo	2008		10.1007/978-3-540-85287-2_10	natural language processing;speech recognition;computer science;communication	NLP	-26.154563600954386	-68.77257819785123	77773
4b240892615dfab719f79e413ca385cfc7b05a3a	praaline: an open-source system for managing, annotating, visualising and analysing speech corpora		In this system demonstration we present the latest developments of Praaline, an open-source software system for constituting and managing, manually and automatically annotating, visualising and analysing spoken language and multimodal corpora. We review the system’s functionality and design architecture, present current use cases and directions for future develop-	multimodal interaction;open-source software;software system;speech synthesis;text corpus	George Christodoulides	2018			natural language processing;artificial intelligence;computer science	HCI	-32.68780600611013	-76.49792488171141	77908
8ef9ce0e6943c1e16641aef636f98dc32976528f	benefits of the 'massively parallel rosetta stone': cross-language information retrieval with over 30 languages	latent semantic indexing	In this paper, we describe our experiences in extending a standard cross-language information retrieval (CLIR) approach which uses parallel aligned corpora and Latent Semantic Indexing. Most, if not all, previous work which follows this approach has focused on bilingual retrieval; two examples involve the use of FrenchEnglish or English-Greek parallel corpora. Our extension to the approach is ‘massively parallel’ in two senses, one linguistic and the other computational. First, we make use of a parallel aligned corpus consisting of almost 50 parallel translations in over 30 distinct languages, each in over 30,000 documents. Given the size of this dataset, a ‘massively parallel’ approach was also necessitated in the more usual computational sense. Our results indicate that, far from adding more noise, more linguistic parallelism is better when it comes to cross-language retrieval precision, in addition to the self-evident benefit that CLIR can be performed on more languages.	cross-language information retrieval;massively parallel processor array;parallel computing;parallel text;text corpus	Peter A. Chew;Ahmed Abdelali	2007				NLP	-30.30833038259372	-69.92246172377219	77980
df2f51f47afdfb6c9e9abc24879bcc75950c922e	modality expressions in japanese and their automatic paraphrasing	information retrieval;natural language;semantic similarity	It is important for future NLP systems to formulate the semantic equivalence (and more generally, the semantic similarity) of natural language expressions. In particular, paraphrasing, full text information retrieval, example-based MT and document compression technology require the effective equivalence criterion for linguistic expressions. In this paper, first, we discuss the meaning of Japanese sentence-final modality expressions (ME) and second, present equivalence rules for paraphrasing a string of MEs while preserving its meaning.	information retrieval;modality (human–computer interaction);natural language processing;regular expression;s-expression;semantic similarity;turing completeness	Toshifumi Tanabe;Kenji Yoshimura;Kosho Shudo	2001			equivalence (measure theory);semantic computing;semantic similarity;information retrieval;semantic equivalence;natural language processing;natural language;expression (mathematics);artificial intelligence;computer science	NLP	-30.026431657786787	-77.439841283692	78076
b2f7b103a6ce2ff9f3eb69f82fb8aa3c8d423067	arabic text categorization: a comparative study of different representation modes.		The quantity of accessible information on Internet is phenomenal, and its categorization remains one of the most important problems. A lot of work is currently focused on English rightly since; it is the dominant language of the Web. However, a need arises for the other languages, because the Web is each day more multilingual. The need is much more pressing for the Arabic language. Our research is on the categorization of the Arabic texts, its originality relates to the use of a conceptual representation of the text. For that we will use Arabic WordNet (AWN) as a lexical and semantic resource. To comprehend its effect, we incorporate it in a comparative study with the other usual modes of representation (bag of words and Ngrams), and we use different similarity measures. The results show the benefits and advantages of this representation compared to the more conventional methods, and demonstrate that the addition of the semantic dimension is one of the most promising approaches for the automatic categorization of Arabic texts.	bag-of-words model;categorization;document classification;internet;lexicon;n-gram;wordnet;world wide web	Zakaria Elberrichi;Karima Abidi	2012	Int. Arab J. Inf. Technol.		natural language processing;pattern recognition	NLP	-30.27283843040808	-66.87256761354239	78195
56e63b476607f5ed9e022294a29e438aab3177fa	hybrid simplification using deep semantics and machine translation		We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	interaction;level of detail;machine translation;natural language processing;parse tree;parsing;referring expression generation;surround sound;text simplification	Shashi Narayan;Claire Gardent	2014			natural language processing;speech recognition;computer science;linguistics;rule-based machine translation	NLP	-21.846907793593452	-77.41576234954492	78246
9bfb0956cf6da26b4e02e3a274ee7e469d9e46a3	tharawat: a vision for a comprehensive resource for arabic computational processing		In this paper, we present a vision for a comprehensive unified lexical resource for computational processing of Arabic with as many of its variants as possible. We will review the current state of the art for three existing resources and then propose a method to link them in addition to augment them in a manner that would render them even more useful for natural language processing whether targeting enabling technologies such as part of speech tagging or parsing, or applications such as Machine Translation, or Information Extraction. The unified lexical resource, Tharawat, meaning treasures, is an extension of our core unique resource Tharwa, which is a three way computational lexicon for Dialectal Arabic, Modern Standard Arabic, and English lemma correspondents. Tharawat will incorporate two other current resources namely SANA, our Arabic Sentiment Lexicon, and MuSTalAHAt, our Multiword Expression (MWE) version of Tharwa but instead of listing lemmas and their correspondents, it lists MWE and their correspondents. Moreover, we present a roadmap for incorporating links for Tharawat to existing English resources and corpora leveraging advanced machine learning techniques and crowd sourcing methods. Such resources are at the core of NLP technologies. Specifically, we believe that such a resource could lead to significant leaps and strides for Arabic NLP. Possessing them for a language such as Arabic could be quite impactful for the development of advanced scientific material and hence lead to an Arabic scientific and economic revolution.	computation	Mona T. Diab	2015		10.1007/978-3-319-18111-0_7	natural language processing;computer vision;speech recognition	Vision	-30.315441444862852	-74.82024115487053	78308
0889019b395890f57bfae3ce7d8391649ae68de4	word embedding based correlation model for question/answer matching		The large scale of Q&A archives accumulated in community based question answering (CQA) servivces are important information and knowledge resource on the web. Question and answer matching task has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, a Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding. Given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs, while it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method’s promising	archive;crusader: no remorse;experiment;parallel text;question answering;scott continuity;user experience;wireless experimental centre;word embedding	Yikang Shen;Wenge Rong;Nan Jiang;Baolin Peng;Jie Tang;Zhang Xiong	2017			natural language processing;machine learning;pattern recognition;data mining;information retrieval	AI	-23.55252817430329	-68.4902011122742	78349
b8ad481ff93bcdb7298cb75fac709bf8f0ac7875	ensemble of feature sets and classification methods for stance detection		Stance detection is the task of automatically determining the author’s favorability towards a given target. However, the target may not be explicitly mentioned in the text and even someone may refer some positive opinions to against the target, which make the task more difficult. In this paper, we describe an ensemble framework which integrates various feature sets and classification methods, and does not consist any handcrafted templates or rules to help stance detection. We submit our solution to NLPCC 2016 shared task: Detecting Stance in Chinese Weibo (Task A), which is a supervised task towards five targets. The official results show that our solution of the team “CBrain” achieves one 1st place and one 2nd place on these targets, and the overall ranking is 4th out of 16 teams. Our code is available at https://github.com/ jacoxu/2016NLPCC Stance Detection.		Jiaming Xu;Suncong Zheng;Jing Shi;Yiqun Yao;Bo Xu	2016		10.1007/978-3-319-50496-4_61	machine learning;pattern recognition;data mining	NLP	-22.289840589689305	-69.63027975769708	78438
816a8ceddde6b1ce35a470efd51b665030a480b2	enumerative series in spanish: formalization and automatic detection		Analysis and formalization are presented about the enumerative series structure in Spanish for subsequent computational implantation. The enumerative series is a textual construct composed by a matrix, an enumerator and an enumeration. Each element of an enumeration is called ‘enumerating’; all elements are related to an “enumeratheme”. Enumeratings and enumeratheme establish a kind of hypernym-hyponym relationship (e.g. in “the days Monday, Tuesday, and Wednesday”; “days” is the enumeratheme and “Monday”, “Tuesday”, and “Wednesday”, the enumeratings). According to this description, a formalization is achieved allowing computational implantation for the detection of enumerations and enumerative series. To accomplish this objective, the NooJ program is used, and the methodology is tested on a corpus of Wikipedia entries related to the medical field, reaching 100% precision, 52.50% recall, and 68.65% F measure.		Walter Koza	2016		10.1007/978-3-319-55002-2_11	discrete mathematics;matrix (mathematics);enumeration;algorithm;mathematics	NLP	-27.182953891293902	-71.44344133809659	78465
303df576947198a7029ff384270750bdb8520521	language independent ner using a maximum entropy tagger	named entity recognition;named entity;maximum entropy	Named Entity Recognition ( NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.	brill tagger;display resolution;encode;entropy (information theory);gaussian blur;information;named entity;overfitting;principle of maximum entropy;smoothing;sparse matrix	James R. Curran;Stephen Clark	2003		10.3115/1119176.1119200	natural language processing;speech recognition;computer science;principle of maximum entropy;pattern recognition;statistics	NLP	-21.65371195888817	-75.48911417384572	78536
6a09510c1b2d791e82c0c5286a9e99801bdec25f	supertagging for a statistical hpsg parser for spanish	parsing;spanish;supertagging;hpsg	We created a supertagger for the Spanish language aimed at disambiguating the HPSG lexical frames for the verbs in a sentence. The supertagger uses a CRF model and achieves an accuracy of 83.58i¾ź% for the verb classes on the test set. The tagset contains 92 verb classes, extracted from a Spanish HPSG-compatible annotated corpus that was created by automatically transforming the Ancora Spanish corpus. The verb tags include information about the arguments structure and syntactic categories of the arguments, so they can be easily translated into HPSG lexical entries.	head-driven phrase structure grammar;parser	Luis Chiruzzo;Dina Wonsever	2015		10.1007/978-3-319-25789-1_3	natural language processing;speech recognition;computer science;linguistics	NLP	-26.28072673195103	-75.71143811949246	78546
9d08213ede54c4e205d18b4400288831af918ec8	headline generation based on statistical translation	problem result;statistical machine translation;practical approach;target document;document summary;headline generation;statistical translation;statistical model;concise language;extractive summarization technique;ideal summarization system;source document	Extractive summarization techniques cannotgeneratedocumentsummaries shorterthan a single sentence,something that is often required. An ideal summarizationsystem would understandeachdocumentand generatean appropriatesummarydirectly from the resultsof that understanding.A more practicalapproachto this problemresults in the use of an approximation: viewing summarizationas a problem analogousto statisticalmachinetranslation. The issuethenbecomesoneof generatinga targetdocumentin a more conciselanguagefrom a sourcedocumentin a moreverboselanguage.This paperpresentsresultson experiments using this approach,in which statistical modelsof the term selectionand termorderingarejointly appliedto producesummariesin astylelearnedfrom a trainingcorpus.	approximation;experiment;statistical machine translation	Michele Banko;Vibhu O. Mittal;Michael J. Witbrock	2000			natural language processing;statistical model;speech recognition;multi-document summarization;computer science;automatic summarization;information retrieval	NLP	-27.95231261029118	-67.26564437761044	78640
2f3d5a1edf8a6ec497ad6eaedeebc287db9a8bca	an empirical evaluation of probabilistic lexicalized tree insertion grammars	lexicalized counterpart;n-gram model;probabilistic context-free grammars;non-hierarchical n-gram model;nonlexicalized counterpart;improved parsing performance;pltigs display;best aspect;probabilistic lexicalized tree insertion;empirical evaluation;language modeling capability	We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its non-lexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs. 1 Introduction There are many advantages to expressing a grammar in a lexicalized form, where an observable word of the language is encoded in each grammar rule. First, the lexical words help to clarify ambiguities that cannot be resolved by the sentence structures alone. For example, to correctly attach a prepositional phrase, it is often necessary to consider the lexical relationships between the head word of the prepositional phrase and those of the phrases it might modify. Second, lexicalizing the grammar rules increases computational efficiency because those rules that do not contain any observed words can be pruned away immediately. The Lexicalized Tree Insertion Grammar formalism (LTIG) has been proposed as a way to lexicalize context-free grammars (Schabes guidance; Joshua Goodman for his PCFG code; Lillian Lee and the three anonymous reviewers for their comments on the paper. and Waters, 1994). We now apply a prob-abilistic variant of this formalism, Probabilis-tic Tree Insertion Grammars (PLTIGs), to natural language processing problems of stochas-tic parsing and language modeling. This paper presents two sets of experiments, comparing PLTIGs with non-lexicalized Probabilistic Context-Free Grammars (PCFGs) (Pereira and Schabes, 1992) and non-hierarchical N-gram models that use the right branching bracketing heuristics (period attaches high) as their parsing strategy. We show that PLTIGs can be induced from partially bracketed data, and that the resulting trained grammars can parse unseen sentences and estimate the likelihood of their occurrences in the language. The experiments are run on two corpora: the Air Travel Information System (ATIS) corpus and a subset of the Wall Street Journal TreeBank corpus. The results show that the lexicalized nature of the formalism helps our induced PLTIGs to converge faster and provide a better language model than PCFGs while maintaining comparable parsing qualities. Although N-gram models still slightly out-perform PLTIGs on language modeling, they lack high level structures needed for parsing. Therefore, PLTIGs have combined the best of two worlds: the language modeling capability of N-grams and the parse quality of …	automatic transmitter identification system (television);context-free language;converge;experiment;formal grammar;grams;heuristic (computer science);high-level programming language;information system;insertion sort;language model;n-gram;natural language processing;observable;parsing;probabilistic automaton;semantics (computer science);stochastic context-free grammar;text corpus;the wall street journal;treebank	Rebecca Hwa	1998			natural language processing;speech recognition;computer science;machine learning;empirical research	NLP	-21.368810688787114	-78.53428056577424	78692
33336037b83d0fad4290e4199a9f9020a03043ce	semantic similarity for aspect-based sentiment analysis		In the second part of the article the methods for main subtasks of aspectbased sentiment analysis are described. The method for explicit aspect term extraction relies on the vector space of distributed representations of words. The term polarity detection method is based on use of pointwise mutual information and semantic similarity measure. Results from SentiRuEval workshop for automobiles and restaurants domains are given. Proposed methods achieved good results in several key subtasks. In aspect term polarity detection task and sentiment analysis of whole review on aspect categories methods showed the best result for both domains. In the aspect term categorization task our method was placed at the second position. And for explicit aspect term extraction the first result obtained for the restaurant domain according to partial match evaluation criteria.	automobiles;categories;categorization;f1 score;matching;machine learning;mathematical morphology;pointwise mutual information;rule (guideline);semantic similarity;sentiment analysis;similarity measure;terminology extraction;polarity	Pavel Blinov;Evgeniy V. Kotelnikov	2015	Russian Digital Libraries Journal			NLP	-25.095528230349192	-69.04792045502492	78746
3316a4ecfa4e91b6846d119b51e583447a837aa5	age-related temporal phrases in spanish and french	people s age;multilingual comparison;temporal expressions	This paper reports research on temporal expressions. The analyzed phrases include a common temporal expression for a period of years reinforced by an adverb of time. We found that some of those phrases are age-related expressions. We analyzed samples obtained from the Internet for Spanish and French to determine appropriate annotations for marking up text and possible translations. We present the results for a group of selected classes.	binary prefix;compiler;degree of parallelism;internet;item unique identification;nec shun-ei;parallel computing;server name indication;temporal expressions;textual entailment	Sofía N. Galicia-Haro;Alexander F. Gelbukh	2011		10.1007/978-3-642-25324-9_32	natural language processing;computer science	NLP	-28.499993153148996	-76.32096658224968	78816
4a88fb26f3b63ea6d175940d4f3290c12d71e52b	training parsers on partial trees: a cross-language comparison		We present a study that compares data-driven dependency par sers obtained by means of annotation projection between lan guage pairs of varying structural similarity. We show how the partial de pendency trees projected from English to Dutch, Italian and German can be exploited to train parsers for the target languages. We eval uate the parsers against manual gold standard annotations a nd find that the projected parsers substantially outperform our heuristic baseline by 9–25% UAS, which corresponds to a 21–43% reducti on in error rate. A comparative error analysis focuses on how the projected ta rget language parsers handle subjects, which is especially interesting for Italian as an instance of a pro-drop language. For Dutch, we f urther present experiments with German as an alternative so urce language. In both source languages, we contrast standard baseline par s rs with parsers that are enhanced with the predictions fro m large-scale LFG grammars through a technique of parser stacking, and show th at improvements of the source language parser can directly l ad to similar improvements of the projected target language parser.	baseline (configuration management);bitext word alignment;compiler;error analysis (mathematics);eval;experiment;fork (software development);heuristic;lexical functional grammar;parsing;sl (complexity);stacking;structural similarity;treebank;unmanned aerial vehicle	Kathrin Spreyer;Lilja Øvrelid;Jonas Kuhn	2010			natural language processing;word error rate;structural similarity;parsing;heuristic;artificial intelligence;computer science;german;rule-based machine translation;pattern recognition;annotation	NLP	-22.24367766595518	-76.80061180115692	78860
9f336e2d91408a40ded1aa347b14f95f280867a3	flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding		Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (Ter-Bleu)/2 points. We also provide a detailed experimental and qualitative analysis of the results.	baseline (configuration management);experiment;interaction;machine translation;sparse matrix	Martin Cmejrek;Haitao Mi;Bowen Zhou	2013			computer science;theoretical computer science;machine learning;data mining	NLP	-19.439962608539048	-74.73480536053673	78903
0121c0ee20886bc59586f45c1e470fd151e39b16	exploring nlp web apis for building arabic systems		Natural language processing (NLP) is the branch of Artificial Intelligence that is concerned with enabling computers understand human languages. Implementing new NLP tools that effectively and efficiently process Arabic is not an easy task, usually such tools face challenges related to NLP various tasks. However, with the movement of many NLP companies to provide their NLP services via Web APIs, building NLP systems that can benefit from such APIs is becoming a reality. This paper will explore the available NLP Web APIs that supports Arabic language. It will also discuss their strengths and weaknesses and provide suggestion for future use.	application programming interface;artificial intelligence;computer;natural language processing	Sharefah A. Al-Ghamdi;Joharah Khabti;Hend Suliman Al-Khalifa	2017	2017 Twelfth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2017.8244649	arabic;computer science;web api;natural language processing;pragmatics;semantics;strengths and weaknesses;artificial intelligence	NLP	-32.17595682219019	-72.91463978206626	79076
14bb1010730a8a360f4d622041943ca1444a9e32	evaluating natural language generated database records	database record;respective application;various natural language processing;qualitatively measure;evaluation technique;project murasaki;inevitable task;text understanding system;evaluation technique quantitatively;expected output	With the onslaught of various natural language processing (NLP) systems and their respective applications comes the inevitable task of determining a way in which to compare and thus evaluate the output of these systems. This paper focuses on one such evaluation technique that originated from the text understanding system called Project MURASAKI. This evaluation technique quantitatively and qualitatively measures the match (or distance) from the output of one text understanding system to the expected output of another.	natural language	Rita McCardell Doerr	1990			natural language processing;speech recognition;computer science;data mining;information retrieval	DB	-30.710173127620624	-71.756079308821	79077
0cf1327ba6be1cbf99fd7bd85017c4f515e0bcbe	word or phrase? learning which unit to stress for information retrieval		The use of phrases in retrieval models has been proven to be helpful in the literature, but no particular research addresses the problem of discriminating phrases that are likely to degrade the retrieval performance from the ones that do not. In this paper, we present a retrieval framework that utilizes both words and phrases flexibly, followed by a general learning-to-rank method for learning the potential contribution of a phrase in retrieval. We also present useful features that reflect the compositionality and discriminative power of a phrase and its constituent words for optimizing the weights of phrase use in phrase-based retrieval models. Experimental results on the TREC collections show that our proposed method is effective.	algorithm;emoticon;experiment;gradient descent;information retrieval;interpolation;learning to rank;text retrieval conference;world wide web	Young-In Song;Jung-Tae Lee;Hae-Chang Rim	2009			natural language processing;visual word;cognitive models of information retrieval;computer science;phrase search;machine learning;pattern recognition;term discrimination;information retrieval;learning to rank	Web+IR	-24.38953282688644	-68.42320196776267	79115
8eb61f36e6f29a000f6f1dc02cc579f102fa41fe	document analysis and retrieval tasks in scientific digital libraries		Machine Learning (ML) algorithms have opened up new possibilities for the acquisition and processing of documents in Information Retrieval (IR) systems. Indeed, it is now possible to automate several labor-intensive tasks related to documents such as categorization and entity extraction. Consequently, the application of machine learning techniques for various large-scale IR tasks has gathered significant research interest in both the ML and IR communities. This tutorial provides a reference summary of our research in applying machine learning techniques to diverse tasks in Digital Libraries (DL). Digital library portals are specialized IR systems that work on collections of documents related to particular domains. We focus on open-access, scientific digital libraries such as CiteSeerx, which involve several crawling, ranking, content analysis, and metadata extraction tasks. We elaborate on the challenges involved in these tasks and highlight how machine learning methods can successfully address these challenges.	algorithm;categorization;citeseerx;computer science;digital library;information retrieval;library (computing);library portal;machine learning;named-entity recognition;portals;semi-supervised learning;semiconductor industry;unsupervised learning	Sujatha Das Gollapalli;Cornelia Caragea;Xiaoli Li;C. Lee Giles	2014		10.1007/978-3-319-25485-2_1	document clustering	Web+IR	-33.036787727948486	-66.87436373910911	79318
0211991ad87137d013341d59903bf88c862c955b	readerbench learns dutch: building a comprehensive automated essay scoring system for dutch language		Automated Essay Scoring has gained a wider applicability and usage with the integration of advanced Natural Language Processing techniques which enabled in-depth analyses of discourse in order capture the specificities of written texts. In this paper, we introduce a novel Automatic Essay Scoring method for Dutch language, built within the Readerbench framework, which encompasses a wide range of textual complexity indices, as well as an automated segmentation approach. Our method was evaluated on a corpus of 173 technical reports automatically split into sections and subsections, thus forming a hierarchical structure on which textual complexity indices were subsequently applied. The stepwise regression model explained 30.5% of the variance in students’ scores, while a Discriminant Function Analysis predicted with substantial accuracy (75.1%) whether they are high or low performance students.	automated essay scoring;complexity index;discriminant;natural language processing;stepwise regression;text corpus	Mihai Dascalu;Wim Westera;Stefan Ruseti;Stefan Trausan-Matu;Hub Kurvers	2017		10.1007/978-3-319-61425-0_5	artificial intelligence;machine learning;computer science;stepwise regression;complexity index;discriminant function analysis;automated essay scoring	NLP	-30.89260894317571	-72.9701455885343	79415
925391b71af0d693c04c9279b3325fcf34196730	prototypical opinion holders: what we can learn from experts and analysts		In order to automatically extract opinion holders, we propose to harness the contexts of prototypical opinion holders, i.e. common nouns, such as experts or analysts, that describe particular groups of people whose profession or occupation is to form and express opinions towards specific items. We assess their effectiveness in supervised learning where these contexts are regarded as labeled training data and in rule-based classification which uses predicates that frequently co-occur with mentions of the prototypical opinion holders. Finally, we also examine in how far knowledge gained from these contexts can compensate the lack of large amounts of labeled training data in supervised learning by considering various amounts of actually labeled training sets.	linear classifier;logic programming;multimodal interaction;supervised learning;technical support	Michael Wiegand;Dietrich Klakow	2011			knowledge management;data mining	ML	-24.705307331715275	-71.2643332126002	79561
c3c9cbea694ada2cf6bc477336f6e065aa4785d0	metadata enrichment of multi-disciplinary digital library: a semantic-based approach		In the scientific digital libraries, some papers from different research communities can be described by community-dependent keywords even if they share a semantically similar topic. Articles that are not tagged with enough keyword variations are poorly indexed in any information retrieval system which limits potentially fruitful exchanges between scientific disciplines. In this paper, we introduce a novel experimentally designed pipeline for multi-label semantic-based tagging developed for open-access metadata digital libraries. The approach starts by learning from a standard scientific categorization and a sample of topic tagged articles to find semantically relevant articles and enrich its metadata accordingly. Our proposed pipeline aims to enable researchers reaching articles from various disciplines that tend to use different terminologies. It allows retrieving semantically relevant articles given a limited known variation of search terms. In addition to achieving an accuracy that is higher than an expanded query based method using a topic synonym set extracted from a semantic network, our experiments also show a higher computational scalability versus other comparable techniques. We created a new benchmark extracted from the open-access metadata of a scientific digital library and published it along with the experiment code to allow further research in the topic.	artificial intelligence;babelnet;benchmark (computing);categorization;computation;computational resource;data curation;digital curation;digital library;elasticsearch;experiment;gene ontology term enrichment;information retrieval;library (computing);machine learning;multi-label classification;ontology (information science);pipeline (computing);preprocessor;query expansion;scalability;semantic network;source-to-source compiler;synonym ring;text mining	Hussein T. Al-Natsheh;Lucie Martinet;Fabrice Muhlenbach;Fabien Rico;Djamel A. Zighed	2018		10.1007/978-3-030-00066-0_3	data mining;topic model;scalability;synonym;categorization;digital library;metadata;multi-label classification;semantic network;computer science	Web+IR	-32.39208101084969	-66.84248408469821	79638
7d9e2d6f71f6a56ca55b591954f8865a3fe6e142	supervised learning based approach to aspect based sentiment analysis	supervised learning;support vector machines;training;portable computers;feature extraction;classification algorithms;sentiment analysis	"""Aspect base sentiment analysis is a very popular concept in the machine learning era which is under the research domain still at the movement. This research mainly consist of the way of exploring the sentiment analysis based on the trained data set to provide the positive, negative and neutral reviews for different products in the marketing world. Most of the existing approaches for opinion mining are based on word level analysis of texts and are able to detect only explicitly expressed opinions. In aspect-based sentiment analysis (ABSA) the aim is to identify the aspects of entities and the sentiment expressed for each aspect. The ultimate goal is to be able to generate summaries listing all the aspects and their overall polarity. For this research mainly natural language and machine learning techniques are used. To train the application for the given data sets SVM (support vector machine) and ME (Maximum Entropy) classification algorithms have been used. Differentiation of the performance of the each algorithm will be analyzed through this research using the proven technologies available in the world like """"Re call"""", """"F-Measure"""" and Accuracy."""	algorithm;entity;f1 score;machine learning;natural language;sentiment analysis;supervised learning;support vector machine	Nipuna Upeka Pannala;Chamira Priyamanthi Nawarathna;J. T. K. Jayakody;Lakmal Rupasinghe;Kesavan Krishnadeva	2016	2016 IEEE International Conference on Computer and Information Technology (CIT)	10.1109/CIT.2016.107	support vector machine;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;data mining;supervised learning;sentiment analysis	ML	-21.5355756434655	-67.49646264674583	79727
5698dbb05d948639170ce893dc2440710b389b47	plagiarism detection based on structural information	stopwords;n grams;or phrases;plagiarism detection	In this paper a novel method for detecting plagiarized passages in document collections is presented. In contrast to previous work in this field that uses mainly content terms to represent documents, the proposed method is based on structural information provided by occurrences of a small list of stopwords (i.e., very frequent words). We show that stopword n-grams are able to capture local syntactic similarities between suspicious and original documents. Moreover, an algorithm for detecting the exact boundaries of plagiarized and source passages is proposed. Experimental results on a publicly-available corpus demonstrate that the performance of the proposed approach is competitive when compared with the best reported results. More importantly, it achieves significantly better results when dealing with difficult plagiarism cases where the plagiarized passages are highly modified by replacing most of the words or phrases with synonyms to hide the similarity with the source documents.	algorithm;grams;n-gram;sensor;text corpus	Efstathios Stamatatos	2011		10.1145/2063576.2063754	n-gram;natural language processing;computer science;data mining;world wide web;information retrieval;stop words	AI	-28.786470614927453	-67.5073604029549	79747
387bb47ec7e80df00d86d0adba29a0b01d1e376a	providing internet access to portuguese corpora: the ac/dc project	conferenceobject	"""In this paper we report on the activity of the project Computational Processing of Portuguese (Processamento computacional do português) in what concerns providing access to Portuguese corpora through the Internet. One of its activities, the AC/DC project (Acesso a corpora/Disponibilização de Corpora, roughly """"Access and Availability of Corpora"""") allows a user to query around 40 million words of Portuguese text. After describing the aims of the service, which is still being subject to regular improvements, we focus on the process of tagging and parsing the underlying corpora, using a Constraint Grammar parser for Portuguese."""	computation;constraint grammar;internet access;parsing;text corpus	Diana Santos;Eckhard Bick	2000			computer science;database;multimedia;world wide web	NLP	-32.85917344348365	-74.91864751439009	79895
100ad1b2fd481d01db2f0507f3bfca228166ecc4	evaluation of analogical proportions through kolmogorov complexity	google;search engine;journal article;kolmogorov complexity;analogical proportion;common sense analogies	In this paper, we try to identify analogical proportions, i.e., statements of the form ‘‘a is to b as c is to d’’, expressed in linguistic terms. While it is conceivable to use an algebraic model for testing proportions such as ‘‘2 is to 4 as 5 is to 10’’, or even such as ‘‘read is to reader as lecture is to lecturer’’, there is no algebraic framework to support statements such as ‘‘engine is to car as heart is to human’’ or ‘‘wine is to France as beer is to England’’, helping to recognize them as meaningful analogical proportions. The idea is then to rely on text corpora, or even on the Web itself, where one may expect to find the pragmatics and the semantics of the words, in their common use. In that context, in order to attach a numerical value to the ‘‘analogical ratio’’ corresponding to the phrase ‘‘a is to b’’, we start from the works of Kolmogorov on complexity theory. This is the basis for a universal measure of the information content of a word a, or of a word a with respect to another one b, which, in practice, is estimated in a statistical manner. We investigate the link between a purely logical, recently introduced view of analogical proportions and its counterpart based on Kolmogorov theory. The criteria proposed for testing candidate proportions fit with the expected properties (symmetry, central permutation) of analogical proportions. This leads to a new computational method to define, and ultimately to try to detect, analogical proportions in natural language. Experiments with classifiers based on these ideas are reported, and results are rather encouraging with respect to the recognition of common sense linguistic analogies. The approach is also compared with existing works on similar problems. 2011 Elsevier B.V. All rights reserved.	analogical modeling;approximation;computation;computational complexity theory;database;experiment;kolmogorov complexity;lexicon;microsoft word for mac;natural language;numerical analysis;preprocessor;self-information;text retrieval conference;text corpus;world wide web	Meriam Bayoudh;Henri Prade;Gilles Richard	2012	Knowl.-Based Syst.	10.1016/j.knosys.2011.06.022	computer science;artificial intelligence;machine learning;algorithm;search engine;statistics	AI	-28.679017833976157	-70.42911522012693	79922
46afa0bee8568a22a7abfcd5827ed8094bcbcdce	a transformation-based approach to argument labeling		This paper presents the results of applying transformation-based learning (TBL) to the problem of semantic role labeling. The great advantage of the TBL paradigm is that it provides a simple learning framework in which the parallel tasks of argument identification and argument labeling can mutually influence one another. Semantic role labeling nevertheless differs from other tasks in which TBL has been successfully applied, such as part-of-speech tagging and named-entity recognition, because of the large span of some arguments, the dependence of argument labels on global information, and the fact that core argument labels are largely arbitrary. Consequently, some care is needed in posing the task in a TBL framework.	named-entity recognition;part-of-speech tagging;programming paradigm;semantic role labeling	Derrick Higgins	2004			artificial intelligence;natural language processing;machine learning;computer science	NLP	-19.635118389292916	-72.79235926833114	79992
ebafe5bc20c4c6813c5367bc3de25313f28430a9	approaching wordnets through a structural point of view	semantic network;point of view;natural language processing	The analogy of a semantic network to hypertext has long been recognized, and semantic networks have been considered as a logical model of hypertext – especially for those hypertexts with typed nodes and links. Moreover, wordnets form the most representative type of semantic networks in the field of Natural Language Processing and semantics in particular. It is obvious that hypertext and wordnets share many common points regarding their fundamental principles and the objectives towards which they both aim. This paper expresses our initial thoughts towards incorporating the Balkan WordNet in Callimachus CB-OHS, as such systems can conveniently support structure. We strongly believe that such tasks can be addressed by using already implemented domain abstractions along with a new set of behaviors.ions along with a new set of behaviors.	hypertext;natural language processing;oracle http server;semantic network;wordnet	Dimitris Avramidis;Maria Kyriakopoulou;Manolis Tzagarakis;Sofia Stamou;Dimitris Christodoulakis	2002		10.1007/3-540-44872-1_7	natural language processing;semantic computing;computer science;theoretical computer science;communication	AI	-32.6566837576528	-70.63306169282217	79994
08240b716eb2d63d78998e738531502283901b89	co-training and self-training for word sense disambiguation	word sense disambiguations;sense classifiers;optimal parameter settings;bootstrapping	This paper investigates the application of cotraining and self-training to word sense disambiguation. Optimal and empirical parameter selection methods for co-training and self-training are investigated, with various degrees of error reduction. A new method that combines cotraining with majority voting is introduced, with the effect of smoothing the bootstrapping learning curves, and improving the average performance.	algorithm;best, worst and average case;co-training;mathematical optimization;smoothing;word sense;word-sense disambiguation	Rada Mihalcea	2004			natural language processing;computer science;machine learning;pattern recognition	NLP	-22.406570931696457	-71.24463161962632	80012
6efe0d86f1b97ad402b4c6a17fe0e0cfc403a6d7	potsdam: semantic dependency parsing by bidirectional graph-tree transformations and syntactic parsing		We present the Potsdam systems that participated in the semantic dependency parsing shared task of SemEval 2014. They are based on linguistically motivated bidirectional transformations between graphs and trees and on utilization of syntactic dependency parsing. They were entered in both the closed track and the open track of the challenge, recording a peak average labeled F1 score of 78.60.	bidirectional transformation;f1 score;parsing;semeval	Zeljko Agic;Alexander Koller	2014			natural language processing;computer science;bottom-up parsing;s-attributed grammar;linguistics;programming language;top-down parsing	NLP	-24.586185918904864	-75.33927088306791	80013
eb61a58282d891d3a7c75ed92a4d099e4457fb2a	collecting fluency corrections for spoken learner english		We present crowdsourced collection of error annotations for transcriptions of spoken learner English. Our emphasis in data collection is on fluency corrections, a more complete correction than has traditionally been aimed for in grammatical error correction research (GEC). Fluency corrections require improvements to the text, taking discourse and utterance level semantics into account: the result is a more naturalistic, holistic version of the original. We propose that this shifted emphasis be reflected in a new name for the task: ‘holistic error correction’ (HEC). We analyse crowdworker behaviour in HEC and conclude that the method is useful with certain amendments for future work.	crc-based framing;crowdsourcing;error detection and correction;holism	Andrew Caines;Emma Flint;Paula Buttery	2017			natural language processing;linguistics;fluency;computer science;artificial intelligence	NLP	-27.935591054341177	-76.11088116511904	80026
710822be03043849bc016acc987695a3b8b34ecf	event extraction using distant supervision		Distant supervision is a successful paradigm that gathers training data for information extraction systems by automatically aligning vast databases of facts with text. Previous work has demonstrated its usefulness for the extraction of binary relations such as a person’s employer or a film’s director. Here, we extend the distant supervision approach to template-based event extraction, focusing on the extraction of passenger counts, aircraft types, and other facts concerning airplane crash events. We present a new publicly available dataset and event extraction task in the plane crash domain based on Wikipedia infoboxes and newswire text. Using this dataset, we conduct a preliminary evaluation of four distantly supervised extraction models which assign named entity mentions in text to entries in the event template. Our results indicate that joint inference over sequences of candidate entity mentions is beneficial. Furthermore, we demonstrate that the SEARN algorithm outperforms a linear-chain CRF and strong baselines with local inference.	algorithm;conditional random field;crostata;database;information extraction;named entity;programming paradigm;wikipedia	Kevin Reschke;Martin Jankowiak;Mihai Surdeanu;Christopher D. Manning;Daniel Jurafsky	2014			named entity;speech recognition;information extraction;data mining;inference;computer science;training set;crash	NLP	-22.371933932134894	-70.91294063369742	80185
e0e2571465f78067f7bcb4c83817eb34234460ef	in codice ratio: machine transcription of medieval manuscripts		Our project, In Codice Ratio, is an interdisciplinary research initiative for analyzing content of historical documents conserved in the Vatican Secret Archives (VSA). As most of such documents are digitized as images, Machine Transcription is both an enabler to the application of Knowledge Discovery techniques, as well as a useful tool to the paleographer for speeding up the transcription process. Our approach involves a convolutional neural network to recognize characters, statistical language models to compose and rank word transcriptions, and crowdsourcing for scalable training data collection. We have conducted experiments on pages from the medieval manuscript collection known as the Vatican Registers. Our results show that almost all the considered words can be transcribed without significant spelling errors.		Serena Ammirati;Donatella Firmani;Marco Maiorino;Paolo Merialdo;Elena Nieddu	2019		10.1007/978-3-030-11226-4_15	scalability;convolutional neural network;information retrieval;knowledge extraction;language model;transcription (linguistics);spelling;training set;computer science;crowdsourcing	NLP	-33.66324899908331	-72.6913699455253	80212
83c7147acbf21479934091f6de7fdcab7f87460d	minimum error training of log-linear translation models		Recent work on training of log-linear interpolation models for statistical machine translation reported performance improvements by optimizing parameters with respect to translation quality, rather than to likelihood oriented criteria. This work presents an alternative and more direct training procedure for log-linear interpolation models. In addition, we point out the subtle interaction between log-linear models and the beam search algorithm. Experimental results are reported on two Chinese-English evaluation sets, C-Star 2003 and Nist 2003, by using a statistical phrase-based model derived from Model 4. By optimizing parameters with respect to the BLUE score, performance relative improvements by 9.6% and 2.8% were achieved, respectively.	beam search;cross-validation (statistics);linear interpolation;linear model;log-linear model;search algorithm;statistical machine translation	Mauro Cettolo;Marcello Federico	2004			interpolation;log-linear model;machine translation;nist;mathematics;artificial intelligence;pattern recognition;beam search;phrase	NLP	-20.934553538465245	-77.73618482160653	80293
9fe996f1e6281fbbb43f0a30dc46bb0d9df515a7	embedded controlled language to facilitate information extraction from egov policies	information extraction;text mining;e government;controlled natural language;natural language processing	The goal of this paper is to propose a system that can extract formal semantic knowledge representation from natural language eGov policies. We present an architecture that allows for extracting Controlled Natural Language (CNL) statements from heterogeneous natural language texts with the ability to support multilinguality. The approach is based on the concept of embedded CNLs.	compute node linux;controlled natural language;embedded system;information extraction;knowledge representation and reasoning	Hazem Safwat;Normunds Gruzitis;Ramona Enache;Brian Davis	2015		10.1145/2837185.2837238	natural language processing;language identification;natural language programming;universal networking language;question answering;natural language user interface;computer science;linguistics;programming language;temporal annotation;information extraction	AI	-32.38858009213841	-71.33899686862408	80297
cb4b1ff1c58fb4a04362c5f625e6915d6f3b4255	analyzing the semantic types of claims and premises in an online persuasive forum		Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of understanding what makes a message persuasive. Premises are annotated with the three types of persuasive modes: ethos, logos, pathos, while claims are labeled as interpretation, evaluation, agreement, or disagreement, the latter two designed to account for the dialogical nature of our corpus. We aim to answer three questions: 1) can humans reliably annotate the semantic types of argument components? 2) are types of premises/claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in persuasive messages than in nonpersuasive messages?	crowdsourcing;darpa grand challenge;error message;expect;persuasive technology;recurrent neural network;special number field sieve;text corpus	Christopher Hidey;Elena Musi;Alyssa Hwang;Smaranda Muresan;Kathy McKeown	2017			computer science;natural language processing;artificial intelligence;premises	NLP	-22.416327644048987	-67.79513276706865	80369
79b4eecbb20169395d747f25b11cc2b81f773435	a galician textual corpus for morphosyntactic tagging with application to text-to-speech synthesis		This paper will present the morphosintactic tagger and the corpus of contemporary written Galician which are being employed in the development of the Galician version of our tex-to-speech synthesizer. Their quality and accuracy make them useful for speech technology applications and turn them into possible references for further investigation and research projects about Galician language. In essence, the tagger assigns automatically the morphosyntactic categories and other additional labels to the words in the corpus by resorting to a combination of both a reduced (although highly reliable) set of rules, and a stochastic language model that employs class n-grams whose probabilities are trained using the corpus itself. A bootstrapping technique is employed for tagging the texts contained in the corpus: a small amount of text is initially tagged automatically making use of a reduced set of linguistic rules and then, gathering together the results obtained at this stage of the process (after the manual revision of the tagging), an initial statistical model is built. The tagging process may be said to consist essentialy of a number of consecutive automatic-tagging stages that enclose: the use of the latest version of the statistical model, the manual revision, and the subsequent updating of the stochastic model with the correctly tagged text.	brill tagger;grams;language model;n-gram;probabilistic automaton;speech synthesis;speech technology;statistical model;tag (metadata)	Lorena Seijo Pereiro;Ana Martínez Ínsua;Francisco Méndez Pazó;Francisco Campillo Díaz;Eduardo Rodríguez Banga	2004			artificial intelligence;natural language processing;stochastic modelling;speech recognition;bootstrapping;computer science;galician language;statistical model;speech technology;language model;speech synthesis	NLP	-24.62829702071669	-78.54120014480532	80437
76d3d111d24391a2fe153e16b1a4d120d9bb7c3d	deeppurple: estimating sentence semantic similarity using n-gram regression models and web snippets	semantic similarity;semantic similarity computation;sentence semantic similarity;regression model;sentence length;lexical semantic similarity;web snippet;sentence level;n-gram regression model;modified mutual information metric;lexical match;semeval test set;shorter sentence	We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length. Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set.	approximation algorithm;computation;image scaling;language-independent specification;mutual information;n-gram;nonlinear system;parallel text;semeval;semantic similarity;simple features;test set;text corpus	Nikos Malandrakis;Elias Iosif;Alexandros Potamianos	2012			natural language processing;semantic similarity;computer science;pattern recognition;information retrieval	NLP	-25.717478026912655	-68.40833312361364	80635
2fee0cd36ed7dab81d69577b9e560ac2abdc99fa	baseline results for the clef 2007 medical automatic annotation task		This paper provides baseline results for the medical automatic annotation task of CLEF 2007. Therefore, the algorithms initially used for the corresponding tasks in 2005 and 2006 are applied, using the same parameterization. Three classifiers based on global image features are used and combined within a nearest neighbor approach. In 2007, a hierarchical code is introduced to describe the image contents, with the evaluation scheme allowing a finer granularity of the classification accuracy. We therefore evaluate some techniques for estimating the confidence in the classifier decision, which stop or alter classifier reports at code levels with uncertain classifier reports.	algorithm;baseline (configuration management);linear classifier	Mark Oliver Güld;Thomas Martin Deserno	2007			computer science;machine learning;pattern recognition;data mining	NLP	-23.501883494160154	-70.48419171738261	80681
470567a118eb65599f777a5e4a6f7271a1f9d9fc	using grammars for text classification	text classification	In this contribution we present our experiments with using grammars for text classification. Approaches usually used are based on statistical methods working with term frequency. We investigate short texts (stock exchange news) more deeply in that we analyze the structure of sentences and context of used phrases. Results are used for predicting market movements coming from the hypotheses that news move markets.	document classification;experiment;modality (human–computer interaction);refinement (computing);tf–idf	Petr Kroha;Thomas Reichel	2007			text graph;computer science	NLP	-22.170646314932096	-67.64274329456666	80871
f44fb2e17b7a046beef71a7e5c166f2061ad8bf1	million-scale derivation of semantic relations from a manually constructed predicate taxonomy		We manually created a semantic taxonomy called Phased Predicate Template Taxonomy (PPTT) that covers 12,023 predicate templates (i.e., predicates with one argument slot like “rescue X”) and derived from it various semantic relations between these templates on a million-instance scale (70%-80% precision level). The derived relations include entailment (e.g., rescue X⊃X is alive), happens-before (e.g., buy X⇒drink X), and a novel relation type anomalous obstruction (e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT’s design and the use of statistical methods.	coherence (physics);robertson–seymour theorem	Motoki Sano;Kentaro Torisawa;Julien Kloetzer;Chikara Hashimoto;István Varga;Jong-Hoon Oh	2014			natural language processing;syntactic predicate;algorithm	NLP	-27.940272959684542	-70.34438682792008	81043
3d6b479960fa36ed8b14b2cd55c3fcce1b3aafb0	citation clustering for identifying research contribution		The h-index is an index that measures productivity and citation impact of the published work but it has been criticized because it does not consider context of citation and reason behind citation. This indicates that there is a need for an improved h-index by a new approach which includes important citations received by a paper instead of the whole list of citations. Citation classification is an emerging area of research that categorizes citations based on the purpose behind the citation. To perform citation classification there is need of a standard set of classes called as classification scheme. Such standard scheme is not available so we aim to generate a citation classification scheme automatically i.e. by using hierarchical clustering. The clustering is performed by using similarity vectors. The main contribution of this research is to generate similarity distance matrix of keywords and verbs extracted from the citation sentences with the help of WordNet.	cluster analysis;comparison and contrast of classification schemes in linguistics and metadata;distance matrix;hierarchical clustering;similarity learning;wordnet	Madhumita Satish Jadhav;Jyoti Dhas;Deepali Joshi;Namrata Jadhavrao;Sayali Kadam	2015	JCP	10.17706/jcp.10.6.406-411	computer science;data science;data mining;information retrieval	ML	-26.196640916133664	-66.3661724956916	81185
8fd0ba753a54f06a652f9849f6f936c01bc630d8	incorporating conditional random fields and active learning to improve sentiment identification	active learning;conditional random fields;customer reviews;sentiment analysis	Many machine learning, statistical, and computational linguistic methods have been developed to identify sentiment of sentences in documents, yielding promising results. However, most of state-of-the-art methods focus on individual sentences and ignore the impact of context on the meaning of a sentence. In this paper, we propose a method based on conditional random fields to incorporate sentence structure and context information in addition to syntactic information for improving sentiment identification. We also investigate how human interaction affects the accuracy of sentiment labeling using limited training data. We propose and evaluate two different active learning strategies for labeling sentiment data. Our experiments with the proposed approach demonstrate a 5%-15% improvement in accuracy on Amazon customer reviews compared to existing supervised learning and rule-based methods.		Kunpeng Zhang;Yusheng Xie;Yi Yang;Aaron Sun;Hengchang Liu;Alok N. Choudhary	2014	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2014.04.005	natural language processing;computer science;machine learning;pattern recognition;active learning;conditional random field;sentiment analysis	NLP	-20.143017540252483	-68.68024072184163	81493
44829f7fb0663d19e8f06b436f2cdeb595a1c7bd	a comparison of wordnet and roget's taxonomy for measuring semantic similarity	upper bound;semantic similarity	This paper presents the results of using Roget's International Thesaurus as the taxonomy in a semantic similarity measurement task. Four similarity metrics were taken from the literature and applied to Roget's. The experimental evaluation suggests that the traditional edge counting approach does surprisingly well (a correlation of r=0.88 with a benchmark set of human similarity judgements, with an upper bound of r=0.90 for human subjects performing the same task.)	benchmark (computing);roget's thesaurus;semantic similarity;ti-92 series;wordnet	Michael McHale	1998	CoRR		natural language processing;semantic similarity	NLP	-27.219637312911075	-67.81083527906011	81601
ff88763625a87f8d7a5b34ae6e860418d060cd56	vaakkriti: sanskrit tokenizer		Machine Translation has evolved tremendously in the recent time and stood as center of research interest for many computer scientists. Developing a Machine Translation system for ancient languages is much more fascinating and challenging task. A detailed study of Sanskrit language reveals that its well-structured and finely organized grammar has affinity for automated translation systems. This paper provides necessary analysis of Sanskrit Grammar in the perspective of Machine Translation and also provides one of the possible solution for Samaas Vigraha(Compound Dissolution).	affinity analysis;computer scientist;lexical analysis;machine translation	Aasish Pappu;Ratna Sanyal	2008			artificial intelligence;natural language processing;lexical analysis;computer science;machine translation;sanskrit;sanskrit grammar;grammar	NLP	-31.47085621389343	-75.63766121069936	81690
aa1438dcaa61b4600c41d02ffa07d51f37473abe	comet: integrating different levels of linguistic modeling for meaning assessment		This paper describes the CoMeT system, our contribution to the SemEval 2013 Task 7 challenge, focusing on the task of automatically assessing student answers to factual questions. CoMeT is based on a meta-classifier that uses the outputs of the sub-systems we developed: CoMiC, CoSeC, and three shallower bag approaches. We sketch the functionality of all sub-systems and evaluate their performance against the official test set of the challenge. CoMeT obtained the best result (73.1% accuracy) for the 3-way unseen answers in Beetle among all challenge participants. We also discuss possible improvements and directions for future research.	semeval;test set	Niels Ott;Ramon Ziai;Michael Hahn;Walt Detmar Meurers	2013			simulation;artificial intelligence	NLP	-21.393818911444328	-70.27807206436441	81698
5aa46a1295ef7bf60578b8e0563fd4e7add4e715	improving search and retrieval in digital libraries by leveraging keyphrase extraction systems		In this tutorial, we will focus on recent developments in the keyphrase extraction task using research papers as a case study. In particular, we will discuss a wide range of keyphrase extraction models ranging from the representative supervised approaches such as KEA and GenEx to more recent ones that make use of the advances in artificial intelligence. Beyond introducing the outstanding approaches in this domain, we will discuss how keyphrases can significantly improve the search and retrieval of information in digital libraries and hence, leads to an improved organization, search, retrieval, and recommendation of scientific documents. Participants will learn about existing approaches, challenges and future trends in the keyphrase extraction task, and how they can be applied to digital library applications.	artificial intelligence;digital library;information extraction;library (computing)	Wei Jin;Corina Florescu	2018		10.1145/3197026.3201778	information extraction;information retrieval;digital library;computer science;ranging	AI	-33.13474314828829	-66.69010933297044	81723
91af39b4e7423da9bde6350b6e0ab4351ad2e142	review: open-domain question answering from large text collections, by marius pasca			question answering	Kiril Ribarov	2004	Prague Bull. Math. Linguistics		natural language processing;artificial intelligence;information retrieval;question answering;computer science	NLP	-30.959245781133447	-76.86703967995628	81754
33afb65c190107e9b439596383ebdcd02f015cb8	automatic word sense disambiguation using cooccurrence and hierarchical information	word sense disambiguation	We review in detail here a polished version of the systems with which we participated in the Senseval2 competition English tasks (all words and lexical sample). It is based on a combination of selectional preference measured over a large corpus and hierarchical information taken from WordNet, as well as some additional heuristics. We use that information to expand sense glosses of the senses in WordNet and compare the similarity between the contexts vectors and the word sense vectors in a way similar to that used by Yarowsky and Schuetze. A supervised extension of the system is also discussed. We provide new and previously unpublished evaluation over the SemCor collection, which is two orders of magnitude larger than SENSEVAL-2 collections as well as comparison with baselines. Our systems scored first among unsupervised systems in both tasks. We note that the method is very sensitive to the quality of the characterizations of word senses; glosses being much better than training examples.	gloss (annotation);heuristic;lexical definition;supervised learning;unsupervised learning;word sense;word-sense disambiguation;wordnet	David Fernández-Amorós;Ruben Heradio;José Antonio Cerrada;Carlos Cerrada Somolinos	2010		10.1007/978-3-642-13881-2_6	natural language processing;speech recognition;semeval;computer science	NLP	-25.130724173557212	-71.75930722344694	81771
fd1fe61d4fe6b311a7d0b49721c92cdcf3377313	building a knowledge based summarization system for text data mining		This paper provides details on building a knowledge based automatic summarization system for mining text data. The knowledge based system mines text data on documents and webpages to create abstractive summaries by generalizing new concepts, deriving main topics, and creating new sentences. The knowledge based system makes use of the domain knowledge provided by Cyc development platform that consists of the world’s largest knowledge base and one of the most powerful inference engines. The system extracts syntactic structures and semantic features by employing natural language processing techniques and Cyc knowledge base and reasoning engine. The system creates a summary of the given documents in three stages: knowledge acquisition, knowledge discovery, and knowledge representation for human readers. The knowledge acquisition derives syntactic structure of each sentence in the documents and maps their words and their syntactic relationships into Cyc knowledge base. The knowledge discovery abstracts novel concepts and derives main topics of the documents by exploring the ontology of the mapped concepts and by clustering the concepts. The knowledge representation creates new English sentences to summarize the documents. This system has been implemented and integrated with Cyc knowledge based system. The implementation encodes a process consisting seven stages: syntactic analysis, mapping words to Cyc, concept propagation, concept weights and relations accumulation, topic derivation, subject identification, and new sentence generation. The implementation has been tested on various documents and webpages. The test performance data suggests that such a system could benefit from running on parallel and distributed computing platforms. The test results showed that the system is capable of creating new sentences that include abstracted concepts not explicitly mentioned in the original documents and that contain information synthesized from different parts of the documents to compose a summary.	data mining;text corpus	Andrey Timofeyev;Ben Choi	2018		10.1007/978-3-319-99740-7_8	knowledge extraction;automatic summarization;knowledge representation and reasoning;data mining;domain knowledge;knowledge acquisition;knowledge-based systems;knowledge base;inference engine;computer science	ML	-31.20678036080433	-69.88348589403219	81812
67656fb207a136bcf975c55d4b231165534ead21	impact of less skewed distributions on efficiency and effectiveness of biomedical relation extraction	skewed distribution;machine learning;biomedical text mining;relation extraction	Like in other NLP tasks, it has been claimed that advances of machine learning (ML) based approaches to relation extraction (RE) are hampered by the imbalanced distribution of positive and negative instances in the annotated training data. Usually, the number of negative instances is much larger than that of the positive ones and such skewness also exists in the test data. In this paper, we aim at addressing the problem of imbalanced distribution by automatically curbing less informative negative instances. We propose some criteria for identifying such instances and incorporate them in an existing state-of-the-art RE approach. Empirical results on 5 benchmark biomedical corpora show that our proposed approach improves both recall and F1 scores. At the same time, there is a large drop in the number of negative instances and in execution runtime as well. Title and Abstract in Italian L’Impatto di Distribuzioni Meno Squilibrate sull’Efficienza e l’Efficacia dell’Estrazione di Relazioni Biomediche Come per altri compiti di Trattamento Automatico del Linguaggio, si è sostenuto che i progressi degli approcci all’estrazione di relazioni basati su apprendimento automatico sono ostacolati dalla distribuzione squilibrata dei casi positivi e negativi nei dati di addestramento annotati. Generalmente, il numero di istanze negative è molto più grande del numero di quelli positivi e tale squilibrio esiste anche nei dati di test. In questo articolo, ci si propone di affrontare il problema della distribuzione squilibrata eliminando automaticamente le istanze negative meno informative. Proponiamo alcuni criteri per individuare tali casi e inserirli in un approccio all’estrazione di relazioni con prestazioni allo stato dell’arte. I risultati empirici su 5 corpora biomedici di riferimento mostrano che l’approccio proposto migliora sia la recall sia il punteggio di F1. Allo stesso tempo, c’è una diminuzione nel numero di istanze negative e anche nel tempo di esecuzione.	baseline (configuration management);benchmark (computing);f1 score;information;lenstra–lenstra–lovász lattice basis reduction algorithm;linear algebra;machine learning;naruto shippuden: clash of ninja revolution 3;natural language processing;pixel density;relationship extraction;stumbleupon;tali'zorah;test data;text corpus;unique name assumption;whole earth 'lectronic link	Md. Faisal Mahbub Chowdhury;Alberto Lavelli	2012			computer science;machine learning;pattern recognition;data mining	NLP	-22.710826280793853	-71.28803714283504	81824
92aa05fb2b8f1ec1d82c3f0581434b92518d4f70	multilingual adaptations of a reusable information extraction tool		In this demo we will present GATE, an architecture and framework for language engineering, and ANNIE, an information extraction system developed within it. We will demonstrate how ANNIE has been adapted to perform NE recognition in different languages, including Indic and Slavonic languages as well as Western European ones, and how the resources can be reused for new applications and languages.	gate;information extraction;overhead (computing);software deployment;software portability;text corpus;word lists by frequency	Diana Maynard;Hamish Cunningham;Kalina Bontcheva	2003				NLP	-30.983212577431587	-75.53387304438785	82058
6a0ce6ccf4a8719a440280fa7b56ae2fa08f668f	exploring the umls: a rough sets based theoretical framework	information theory;unified medical language system;mathematics	The Unified Medical Language System (UMLS) [1] has a unique and leading position in the evolution of thesauri and metathesauri. Features that set it apart are: its composition from more than fifty component health care vocabularies; the sophisticated UMLS ontology linking the Metathesaurus with structures such as the Semantic Network and the SPECIALIST lexicon; and the high level of social collaboration invested in its construction and growth. It is our thesis that in order to successfully harness such a complex vocabulary for text retrieval we need sophisticated methods derived from a deeper understanding of the UMLS system. Thus we propose a theoretical framework based on the theory of rough sets, that supports the systematic and exploratory investigation of the UMLS Metathesaurus for text retrieval. Our goal is to make it more feasible for individuals such as patients and health care professionals to access relevant information at the point of need.	document retrieval;health care;high-level programming language;lexicon;ontology;patients;rough set;semantic network;social collaboration;thesaurus;unified medical language system;vocabulary	Padmini Srinivasan	1999	Proceedings. AMIA Symposium		information retrieval;natural language processing;health care;ontology;vocabulary;unified medical language system;semantic network;lexicon;computer science;rough set;artificial intelligence;social collaboration	Web+IR	-33.3377188210548	-69.4492802094203	82125
16cce78c000ab10446a795104f619960d0440316	applying pairwise ranked optimisation to improve the interpolation of translation models		In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system. One way of doing this is to build separate translation models from each data set and linearly interpolate them, and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set. In this work, rather than optimising for this indirect measure, we directly optimise for BLEU on the tuning set and show improvements in average performance over two data sets and 8 language pairs.	bleu;best, worst and average case;linear interpolation;mathematical optimization;perplexity;statistical machine translation	Barry Haddow	2013			computer science;machine learning;pattern recognition;data mining	NLP	-20.53680803691197	-77.50845537755865	82338
a7ee24c8b6b30a2111dedd913b70b38266543c53	measuring the dynamic relatedness between chinese entities orienting to news corpus	dynamic relatedness measure;co occurrence statistics;news corpus	The related applications are limited due to the static characteristics on existing relatedness calculation algorithms. We proposed a method aiming to efficiently compute the dynamic relatedness between Chinese entity-pairs, which changes over time. Our method consists of three components: using cooccurrence statistics method to mine the co-occurrence information of entities from the news texts, inducing the development law of dynamic relatedness between entity-pairs, taking the development law as basis and consulting the existing relatedness measures to design a dynamic relatedness measure algorithm. We evaluate the proposed method on the relatedness value and related entity ranking. Experimental results on a dynamic news corpus covering seven domains show a statistically significant improvement over the classical relatedness measure.	algorithm;definition;entity;world wide web	Zhishu Wang;Jing Yang;Xin Lin	2012		10.1007/978-3-642-31537-4_49	natural language processing;computer science;data mining	AI	-26.385076867316428	-66.79183484401133	82347
7ac0d41cb6b96f7a5a34ea224b34cc06d873451a	person name spotting by combining acoustic matching and lda topic models		In this article, we are interested in spoken term detection task, with a particular focus on Person Name (PN) spotting in automatic speech recognition (ASR) system outputs. We propose a two-step method that combines an acoustic matching based on a Phoneme Confusion Network (PCN) with a semantic rescoring based on the Latent Dirichlet Allocation (LDA) models. The first module allows to find, in the PCN, potential PN candidates in speech segments, while the second is in charge of ranking the competing PN, according to a LDA topic model. The proposed LDA-based approach outperforms significantly the baseline system based on a search in the ASR phoneme lattice, obtaining a F-measure score of 77.04% on PN detection.	acoustic cryptanalysis;baseline (configuration management);latent dirichlet allocation;program composition notation;speech recognition;topic model	Grégory Senay;Benjamin Bigot;Richard Dufour;Georges Linarès;Corinne Fredouille	2013			natural language processing;speech recognition;pattern recognition	NLP	-23.609219677215503	-72.0525384181236	82389
b56bf3d3d9e6e710464d6e51bee25c956a31adf0	improving authorship attribution: optimizing burrows' delta method	word frequency;delta method;text classification;similarity measure	Burrows’ Delta Method (Burrows, 2002) is a leading method of authorship attribution. It can be used to shortlist potential authors from a list or to even identify potential authors. The technique has been extended by Hoover (2004a, 2006). In this investigation, we look at the choice of words for the word vector used, the size of the word vector, the similarity measure and the impact of corpus choice on the accuracy of text classification. Our results show a word frequency vector of between 200 and 300 words give the most accurate results (Aldridge, 2007). We also demonstrate a dramatic improvement in accuracy by adapting Burrows’ Delta to the cosine similarity measure. Additionally, our results indicate areas where the word vector can be optimized still further for more accurate results.	angularjs;cosine similarity;document classification;euclidean distance;high frequency content measure;mathematical optimization;optimizing compiler;similarity measure;stylometry;text corpus;word embedding;word lists by frequency	Peter W. H. Smith;W. Aldridge	2011	Journal of Quantitative Linguistics	10.1080/09296174.2011.533591	delta method;speech recognition;computer science;machine learning;pattern recognition;mathematics;word lists by frequency;linguistics;statistics	NLP	-26.198093683131386	-67.87860409301494	82501
b20164f38129ab1fbad4792cfe24d2e15620a48d	finding canonical forms for historical german text		Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any technique or system requiring reference to a fixed lexicon accessed by orthographic form. This paper presents two methods for mapping unknown historical text types to one or more synchronically active canonical types: conflation by phonetic form, and conflation by lemma instantiation heuristics. Implementation details and evaluation of both methods are provided for a corpus of historical German verse quotation evidence from the digital edition of the Deutsches Wörterbuch.	heuristic (computer science);lexicon;natural language processing;orthographic projection;text corpus;universal instantiation;verse protocol	Bryan Jurish	2008			text types;phonetic form;conflation;lexicon;natural language processing;lemma (mathematics);canonical form;artificial intelligence;german;digital edition;computer science	NLP	-31.32364056783335	-75.04683828781704	82584
0f9427a1caed8e0ad79d37bacd769c0bcc3ebab3	automatic extraction of morphological information from botanical collections	information extraction;statistical model;botanical collection;pattern recognition;knowledge base	Specific morphological information is often used by users to search botanical collections. However, traditional systems based on statistical models are often not effective for such search. This study automatically extracts morphological information from botanical collections using an adapted and enhanced information extraction system. Experimental results indicate this approach is promising. This study also indicates that this approach is generalizable to similar collections in the same domain and even to different domains with adaptation of the pattern recognition and knowledge base in the new domain.	information extraction;knowledge base;pattern recognition;statistical model	Xiaoya Tang	2008		10.1145/1378889.1379015	natural language processing;statistical model;knowledge base;computer science;data mining;information extraction;information retrieval	NLP	-30.638537548914517	-68.6239495059705	82589
20ed61b930f2f496d14377dc9c772b42cfe29a22	combined word alignments	combined word alignment;bitext correspondences identification;hypotheses testing approach;individual aligner;different method;word alignment system	We briefly describe a word alignment system that combines two different methods in bitext correspondences identification. The first one is a hypotheses testing approach (Gale and Church, 1991; Melamed, 2001; Tufiş 2002) while the second one is closer to a model estimating approach (Brown et al., 1993; Och and Ney, 2000). We show that combining the two aligners the results are significantly improved as compared to each individual aligner.	bitext word alignment;parallel text;symbian;word lists by frequency	Dan Tufis;Radu Ion;Alexandru Ceausu;Dan Stefanescu	2005			speech recognition;computer science;artificial intelligence;algorithm	NLP	-22.930683889782944	-77.65070559589944	82617
74433227462eb03a04c4059257213000b7881607	language-specific models in multilingual topic tracking	multilingual;tdt;classification;arabic;crosslingual;topic tracking;machine translation;native language	Topic tracking is complicated when the stories in the stream occur in multiple languages. Typically, researchers have trained only English topic models because the training stories have been provided in English. In tracking, non-English test stories are then machine translated into English to compare them with the topic models. We propose a native language hypothesis stating that comparisons would be more effective in the original language of the story. We first test and support the hypothesis for story link detection. For topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream. We compare different methods of incrementally building such native language topic models.	topic model	Leah S. Larkey;Fangfang Feng;Margaret E. Connell;Victor Lavrenko	2004		10.1145/1008992.1009061	natural language processing;speech recognition;computer science;arabic;machine translation	NLP	-23.012752127312385	-74.55490267426204	82625
d2880900e45db82dc87de9b6b3aad2f97f6fcfa0	the story picturing engine---a system for automatic text illustration	story picturing;user study;image database;lexical referencing;large scale;statistical analysis;mutual reinforcement;markov chain;image retrieval	We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.	database;unsupervised learning;usability testing;wordnet	Dhiraj Joshi;James Ze Wang;Jia Li	2006	TOMCCAP	10.1145/1126004.1126008	natural language processing;markov chain;image retrieval;computer science;world wide web;information retrieval;statistics	Vision	-29.608471127272352	-68.28836140507099	82639
8ae9fb593e3647bb19f9471cedac18dd9f7bcbce	automatic alignment of persian and english lexical resources: a structural-linguistic approach.		Cross-lingual mapping of linguistic resources such as corpora, ontologies, lexicons and thesauri is very important for developing cross-lingual (CL) applications such as machine translation, CL information retrieval and question answering. Developing mapping techniques for lexical ontologies of different languages is not only important for inter-lingual tasks but also can be implied to build lexical ontologies for a new language based on existing ones. In this paper we propose a two-phase approach for mapping a Persian lexical resource to Princeton's WordNet. In the first phase, Persian words are mapped to WordNet synsets using some heuristic improved linguistic approaches. In the second phase, the previous mappings are evaluated (accepted or rejected) according to the structural similarities of WordNet and Persian thesaurus. Although we applied it to Persian, our proposed approach, SBU methodology is language independent. As there is no lexical ontology for Persian, our approach helps in building one for this language too.	heuristic;information retrieval;lexicon;machine translation;ontology (information science);question answering;synonym ring;text corpus;thesaurus;two-phase commit protocol;wordnet	Rahim Dehkharghani;Mehrnoush Shamsfard	2009			linguistics;persian;computer science	NLP	-28.872218159958628	-71.50518971183494	82652
b9df9fc1486e45957c40b53b0765f409daf60bf9	approche hybride pour le résumé automatique de textes. application à la langue arabe. (hybrid approach for the automatic abstract of texts. application with the arab language)		"""This thesis falls within the framework of Natural Language Processing. The problems of automatic summarization of Arabic documents which was approached, in this thesis, are based on two points. The first point relates to the criteria used to determine the essential content to extract. The second point focuses on the means to express the essential content extracted in the form of a text targeting the user potential needs. In order to show the feasibility of our approach, we developed the """"L.A.E"""" system, based on a hybrid approach which combines a symbolic analysis with a numerical processing. The evaluation results are encouraging and prove the performance of the proposed hybrid approach.These results showed, initially, the applicability of the approach in the context of mono documents without restriction as for their topics (Education, Sport, Science, Politics, Interaction, etc), their content and their volume. They also showed the importance of the machine learning in the phase of classification and selection of the sentences forming the final extract."""		Mohamed Hédi Mâaloul	2012				AI	-27.11623799531059	-77.26726591251817	82803
fa67fbf4f6ce2e7a2da7a005a241ec432862258a	link-topic model for biomedical abbreviation disambiguation	svmsupport vector machine;biomedical abbreviation disambiguation;cuiconcept unique identifiers;memaximum entropy;umlsunified medical language system;me;wsdword sense disambiguation;umls;meshmedical subject headings;word sense disambiguation;latent dirichlet allocation;global abbreviation;medical subject headings;unified medical language system;lda;cui;mesh;semantic link;svm;wsd;support vector machine;ldalatent dirichlet allocation;maximum entropy;topic model;concept unique identifiers	INTRODUCTION The ambiguity of biomedical abbreviations is one of the challenges in biomedical text mining systems. In particular, the handling of term variants and abbreviations without nearby definitions is a critical issue. In this study, we adopt the concepts of topic of document and word link to disambiguate biomedical abbreviations.   METHODS We newly suggest the link topic model inspired by the latent Dirichlet allocation model, in which each document is perceived as a random mixture of topics, where each topic is characterized by a distribution over words. Thus, the most probable expansions with respect to abbreviations of a given abstract are determined by word-topic, document-topic, and word-link distributions estimated from a document collection through the link topic model. The model allows two distinct modes of word generation to incorporate semantic dependencies among words, particularly long form words of abbreviations and their sentential co-occurring words; a word can be generated either dependently on the long form of the abbreviation or independently. The semantic dependency between two words is defined as a link and a new random parameter for the link is assigned to each word as well as a topic parameter. Because the link status indicates whether the word constitutes a link with a given specific long form, it has the effect of determining whether a word forms a unigram or a skipping/consecutive bigram with respect to the long form. Furthermore, we place a constraint on the model so that a word has the same topic as a specific long form if it is generated in reference to the long form. Consequently, documents are generated from the two hidden parameters, i.e. topic and link, and the most probable expansion of a specific abbreviation is estimated from the parameters.   RESULTS Our model relaxes the bag-of-words assumption of the standard topic model in which the word order is neglected, and it captures a richer structure of text than does the standard topic model by considering unigrams and semantically associated bigrams simultaneously. The addition of semantic links improves the disambiguation accuracy without removing irrelevant contextual words and reduces the parameter space of massive skipping or consecutive bigrams. The link topic model achieves 98.42% disambiguation accuracy on 73,505 MEDLINE abstracts with respect to 21 three letter abbreviations and their 139 distinct long forms.		Seonho Kim;Juntae Yoon	2015	Journal of biomedical informatics	10.1016/j.jbi.2014.12.013	natural language processing;support vector machine;computer science;machine learning;pattern recognition;data mining;database;unified medical language system;information retrieval	NLP	-29.799427737333325	-66.59864018605903	82862
e799ea22eecbae6243ffe013d2469f9a339f9d5a	lexical and textual resources for sense recognition and description		It is common knowledge that the creation of language resources for Language Engineering (LE) applications is a time-consuming, and hence expensive, enterprise. From this knowledge stems the demand for the re-usability of resources, which always remains essential. In this paper we will, however, concentrate on another, complementary, aspect, namely that of combining and extending existing resources by a variety of means and with a minimum of manual interaction. The resources to be discussed below consist of (i) a large lexical database, (ii) a formalized computational lexicon, and (iii) a sense-tagged corpus for Swedish. Some results concerning the semi-automatic annotation of the corpus and examples of a variety of phenomena analysed, such as compounding, will also be given. The annotation has been performed within the framework of the SemTag project, while part of this material has been successfully used in the SENSEVAL-2 exercise. In addition to these three resources, it can be added the background material of the Swedish Language Bank (some hundred million words) that forms the basis for the creation of (i) and partly (ii). Having been developed at our department, the lexical resources can easily be accessed, and, more importantly, can be systematically improved where necessary. It should be noted that this type of work requires close cooperation between specialists in lexicography and language technology.	language technology;lexical database;lexicography;lexicon;semiconductor industry;text corpus;usability	Jerker Järborg;Dimitrios Kokkinakis;Maria Toporowska Gronostaj	2002			lexical functional grammar;language technology;natural language processing;artificial intelligence;lexical database;common knowledge;computer science;lexicon;lexicography;annotation	NLP	-30.042920008617784	-73.69693546050107	82972
14999d313ad3b476cf2b7d0d5e24f1de0a5d4bcf	automatic segmentation of texts and corpora	in vocabulary;automatic segmentation;growth analysis	Segmentation of large textual corpora is one of the major questions asked of literary studies. We present a combination of two relevant methods. F irst, vocabulary growth analysis highlights the main discontinuities in a work. Seco nd, these results are supplemented with the analysis of variations in vocabulary diversity with in corpora. A segmentation algorithm, associated with a test of validity, indicates the o ptimal succession in distinct stages. This method is applied to Racine's works and those of va rious other works in French. Résumé Le découpage des grands corpus de textes est l'une des questions cruciales posées aux études littéraires. Il est proposé une double méthode. L'a nalyse de la croissance du vocabulaire (typetoken ratio) met en lumière les principaux changeme nts de rythme. Ces résultats sont complétés par l'étude de la diversité du vocabulair e. Un algorithme de segmentation, associé à un test de validité, indique le découpage optimal. L méthode est appliquée aux œuvres de Racine, Corneille et aux discours du Général de Gau lle.	algorithm;cranial electrotherapy stimulation;linear algebra;succession;text corpus;vocabulary	Cyril Labbé;Dominique Labbé;Pierre Hubert	2004	Journal of Quantitative Linguistics	10.1080/0929617042000314958	natural language processing;speech recognition;computer science;linguistics	NLP	-27.442571109117512	-76.83005323516416	83000
458f729741a0004912029c31b8e93815572c2853	using lexical and relational similarity to classify semantic relations	compound noun interpretation task;semantic similarity;lexical similarity;relational similarity;word pair similarity;distinct type;word pair;relational model;certain nlp task;semantic relation;individual word;noun	Many methods are available for computing semantic similarity between individual words, but certain NLP tasks require the comparison of word pairs. This paper presents a kernel-based framework for application to relational reasoning tasks of this kind. The model presented here combines information about two distinct types of word pair similarity: lexical similarity and relational similarity. We present an efficient and flexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state-ofthe-art results on a compound noun interpretation task.	algorithm;expectation propagation;kernel (operating system);lexicon;natural language processing;relational model;semantic similarity;string kernel;syntactic predicate;unsupervised learning	Diarmuid Ó Séaghdha;Ann A. Copestake	2009			natural language processing;noun;semantic similarity;relational model/tasmania;computer science;pattern recognition;linguistics;similarity heuristic;dishin	NLP	-25.524470834347635	-69.91177924675914	83050
41fa9f1ad5d69f6f6b57f7dbc9a95d00bcfdd3e7	improving discriminative sequential learning with rare--but--important associations	association mining;information extraction;large dataset;learning model;data mining;statistical learning;named entity recognition;association rule;conditional random field;feature selection;discriminative sequential learning;natural language processing;text segmentation	Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various non--independent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data--driven approach that can deal with such hard--to--predict data instances by discovering and emphasizing rare--but--important associations of statistics hidden in the training data. Mined associations are then incorporated into these models to deal with difficult examples. Experimental results of English phrase chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.	association rule learning;c traps and pitfalls;conditional random field;information extraction;machine learning;mined;mined-out;named-entity recognition;natural language processing;phrase chunking;shallow parsing	Xuan Hieu Phan;Minh Le Nguyen;Tu Bao Ho;Susumu Horiguchi	2005		10.1145/1081870.1081906	text segmentation;association rule learning;computer science;machine learning;pattern recognition;data mining;feature selection;conditional random field;information extraction;discriminative model	NLP	-21.27726293591745	-71.61952590737114	83075
800b895525ef1acc68df8b8b3ba96a08beb7a588	relation extraction of medical concepts using categorization and sentiment analysis		In healthcare services, information extraction is the key to understand any corpus-based knowledge. The process becomes laborious when the annotation is done manually for the availability of a large number of text corpora. Hence, future automated extraction systems will be essential for groups of experts such as doctors and medical practitioners as well as non-experts such as patients, to ensure enhanced clinical decision-making for improving healthcare systems. Such extraction systems can be developed using medical concepts and concept-related features as the part of a structured corpus. The latter can assist in assigning the category and sentiment to each of the medical concepts and their lexical contexts. These categories and sentiment assignments constitute semantic relations of medical concepts, with their context, represented by sentences of the corpus. This paper presents a new domain-based knowledge lexicon coupled with a machine learning approach to extract semantic relations. This is done by assigning category and sentiment of the medical concepts and contexts. The categories considered in this research, are diseases, symptoms, drugs, human_anatomy, and miscellaneous medical terms, whereas sentiments are considered as positive and negative. The proposed assignment systems are developed on the top of WordNet of Medical Event (WME) lexicon. The developed lexicon provides medical concepts and their features, namely Parts-Of-Speech (POS), gloss (descriptive explanation), Similar Sentiment Words (SSW), affinity score, gravity score, polarity score, and sentiment. Several well-known supervised classifiers, including Naïve Bayes, Logistic Regression, and support vector-based Sequential Minimal Optimization (SMO) have been applied to evaluate the developed systems. The proposed approaches have resulted in a concepts clustering application by identifying the semantic relations of concepts. The application provides potential exploitation in several domains, such as medical ontologies and recommendation systems.	affinity analysis;baseline (configuration management);categorization;cluster analysis;domain-specific language;gloss (annotation);information extraction;lexicon;logistic regression;machine learning;naive bayes classifier;norm (social);ontology (information science);ontology components;program optimization;recommender system;relationship extraction;sentiment analysis;sequential minimal optimization;supervised learning;text corpus;windows media encoder;wordnet	Anupam Mondal;Erik Cambria;Dipankar Das;Amir Hussain;Sivaji Bandyopadhyay	2018	Cognitive Computation	10.1007/s12559-018-9567-8	information extraction;artificial intelligence;pattern recognition;data mining;computer science;categorization;sentiment analysis;ontology (information science);relationship extraction;wordnet;lexicon;text corpus	AI	-23.65450566809021	-67.54821524798322	83088
03a85df7e4435715fa5fc85a50802d64d9191974	simplified processing of elliptic and anaphoric utterances in a train timetable information retrieval dialogue system	dialogue system;representacion conocimientos;service information;information retrieval;man machine dialogue;semantics;base connaissance;semantica;semantique;recherche information;servicio informacion;dialogo hombre maquina;base conocimiento;recuperacion informacion;information service;information system;knowledge representation;representation connaissances;systeme information;dialogue homme machine;sistema informacion;knowledge base	This paper presents a simplified approach to processing of ambiguous requests in spoken dialogues in information train timetable service systems. The simplified processing of ambiguous and incomplete utterances is based on a simple representation of the semantics of analyzed utterances by specially constructed frames, on the special representation and storage of facts extracted from the previous steps of dialogue (dialogue path, or dialogue history), and on the creation of the simple knowledge base containing the reasoning rules linking the meaning detected in the analyzed user's utterance with the facts stored in the dialogue path or in the system knowledge base. Some aspects of the implementation of utterance internal representing frames completion and their evaluation are discussed in the concluding part of this paper.	anaphora (linguistics);dialog system;dialog tree;information retrieval;schedule	Václav Matoušek	2000		10.1007/3-540-45323-7_67	natural language processing;knowledge base;computer science;artificial intelligence;semantics;linguistics;information system;algorithm	NLP	-30.62569644641239	-70.87704855164387	83469
50eb6c746d90e276baa846f0dc104d6281010132	relax - extractino of semantic relations in biographical contexts		The automatic extraction of biographical information from business news is a complex task. This approach deals with the characterization of the so called biographical relations by means of local grammars. In order to provide a well-founded proceeding, it is necessary to give a complete and accurate definition of the notion “relation” seen here as a social relationship between human beings or expressing the binding of person to a biographical event, such as birth, marriage, divorce, professional career etc. This paper outlines the linguistic background and modeling method of our system RELAX doing semantic RELAtion eXtraction and linking persons to their corresponding biographical information. Results show that software applications like question-answering systems can successfully deal with this method and efficiently retrieve semantic knowledge out of text corpora. MOTS-CLÉS : extraction d’informations biographiques, relations sémantiques, grammaires locales, entités nommées, questions-réponses.	linear programming relaxation;question answering;relax ng;relation (database);relationship extraction;text corpus	Michaela Geierhos;Olivier Blanc;Sandra Bsiri	2008	TAL			NLP	-31.11745229078378	-71.18609347650542	83534
d878a67b2ef6a0a5dec72db15291f12419040ab1	using web images as additional training resource for the discriminative generalized hough transform	manuals;filtering;training;computational modeling;shape;transforms;adaptation models	Many algorithms in computer vision, e.g., for object localization, are supervised and need annotated training data. One approach for object localization is the Discriminative Generalized Hough Transform (DGHT). It achieves state-of-the-art performance in applications like iris and epiphysis localization, if the amount and quality of training data is sufficient. This motivates techniques for extending the training corpus with limited manual effort. In this paper, we propose an active learning scheme to extend the training corpus by automatically and efficiently harvesting and selecting suitable Web images. We aim at improving localization performance, while reducing the manual supervision to a minimum. Our key idea is to estimate the benefit of a particular candidate Web image by analyzing its Hough space generated using an initial DGHT model. We show that our method performs similarly to a manual selection of Web images as well as a computationally intensive state-of-the-art approach.	algorithm;computer vision;generalised hough transform;internationalization and localization;ising model;text corpus	Alexander Oliver Mader;Hauke Schramm;Carsten Meyer	2016	2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2016.7821012	filter;computer vision;speech recognition;shape;computer science;machine learning;mathematics;computational model	Vision	-22.21224839226344	-73.80195833721656	83584
49812ddd936125bcfdd23965c0c56cc635fbf1a9	bengali question classification: towards developing qa system		This paper demonstrates the question classification step towards building a question answering system in Bengali. Bengali is an eastern Indo-Aryan language with about 230 million total speakers and one of the most spoken languages in the world. An important first step in developing a question answering system is to classify natural language question properly. In this work, we have studied suitable lexical, syntactic and semantic features to classify the Bengali question. As Bengali question classification is at early stage of development, so for simplicity we have proposed single-layer taxonomy which consists of only nine course-grained classes. We have also studied and categorized the interrogatives in Bengali language. The proposed automated classification work is based on various machine learning techniques. The baseline system based on Naïve Bayes classifier has achieved 80.65% accuracy. We have achieved up to 87.63% accuracy using decision tree classifier.	algorithm;baseline (configuration management);bengali input methods;categorization;category theory;decision tree;indo;lexicon;machine learning;naive bayes classifier;natural language;question answering;text corpus	Somnath Banerjee;Sivaji Bandyopadhyay	2012			natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.421887573405364	-70.66374190808389	83646
c939f456d64f14e598859384df8ff5a3fc3a5ac7	fully-automatic marker-based chunking in 11 european languages and counts of the number of analogies between chunks	conference paper	Analogy has been proposed as a possible principle for example-based machine translation. For such a framework to work properly, the training data should contain a large number of analogies between sentences. Consequently, such a framework can only work properly with short and repetitive sentences. To handle longer and more varied sentences, cutting the sentences into chunks could be a solution if the number of analogies between chunks is confirmed to be large. This paper thus reports counts of number of analogies using different numbers of chunk markers in 11 European languages. These experiments confirm that the number of analogies between chunks is very large: several tens of thousands of analogies between chunks extracted from sentences among which only very few analogies, if not none, were found.	algorithm;align (company);chunking (computing);example-based machine translation;experiment;lexicon;shallow parsing	Kota Takeya;Yves Lepage	2011			natural language processing;computer science;linguistics;algorithm	NLP	-24.852704832769383	-75.54038163180024	83747
15441fb7003d170b4710492a957bf5f78dd222e0	commonsense knowledge base completion		We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set. However, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as mediumconfidence tuples from ConceptNet.	bilinear filtering;commonsense knowledge (artificial intelligence);downstream (software development);entity;freebase;knowledge base;natural language processing;open mind common sense;utility functions on indivisible goods	Xiang Li;Aynaz Taheri;Lifu Tu;Kevin Gimpel	2016			tuple;artificial intelligence;machine learning;data mining	NLP	-19.743672031753125	-72.01480962146069	83759
3189d839e9f6ae7a042818e42864d88f4bdc94b3	adapting word prediction to subject matter without topic-labeled data	language modeling;statistical methods;topic modeling;statistical method;augmentative and alternative communication;word prediction;language model	Word prediction helps to increase communication rate when using Augmentative and Alternative Communication devices. Basic prediction systems offer topically inappropriate predictions for the context, thus we adapt the predictions to the topic of discourse. However, previous work has relied on texts that are grouped into topics by humans. In contrast, we avoid this restriction by treating each document as a topic. The results are comparable to human-labeled topics and also the method is applicable to unlabeled text.	subject matter expert turing test	Keith Trnka	2008		10.1145/1414471.1414556	natural language processing;speech recognition;computer science;topic model;language model	NLP	-24.01131734467535	-78.92953309238462	83771
a1a3069eeeda015a48be4f35c4072e24d455a72c	event lexical database: a semantic role labeling approach			lexical database;semantic role labeling	Nyuk Hiong Siaw;Narayanan Kulathuramaiyer;Bali Ranaivo-Malançon;Jane Labadin	2014		10.3233/978-1-61499-434-3-884	theoretical computer science;natural language processing;computer science;semantic role labeling;lexical database;artificial intelligence	NLP	-30.28839097503767	-77.2265612861141	83780
146862080f380645a59892ad93ea455c3f8850ee	knowledge-rich contexts discovery	machine readable dictionary;knowledge acquisition;semantic relations;knowledge base	Within large corpora of texts, Knowledge-Rich Contexts (KRCs) are a subset of sentences containing information that would be valuable to a human for the construction of a knowledge base. The entry point to the discovery of KRCs is the automatic identification of Knowledge Patterns (KPs) which are indicative of semantic relations. Machine readable dictionary serves as our starting point for investigating the types of knowledge embodied in definitions and some associated KPs. We then move toward corpora analysis and discuss issues of generality/specificity as well as KPs efficiency. We suggest an expansion of the lexical-syntactic definitions of KPs to include a semantic dimension, and we briefly present a tool for knowledge acquisition, SeRT, which allows user such flexible definition of KPs for automatic discovery of KRCs.	automatic identification and data capture;computation;definition;dictionary;entry point;human-readable medium;information extraction;knowledge acquisition;knowledge base;lexicon;logical connective;refinement (computing);sensitivity and specificity;text corpus;wordnet	Caroline Barrière	2004		10.1007/978-3-540-24840-8_14	natural language processing;knowledge base;computer science;artificial intelligence;knowledge-based systems;data mining;information retrieval	NLP	-32.54779985501898	-70.10485764189646	83827
60eac9d2dfe8bfcaafdb4205c34aa9ab7c105c27	word sense disambiguation by learning decision trees from unlabeled data	unlabeled data;decision tree;learning from unlabeled examples;selective sampling;word sense disambiguation;machine learning;committee learning	In this paper we describe a machine learning approach to word sense disambiguation that uses unlabeled data. Our method is based on selective sampling with committees of decision trees. The committee members are trained on a small set of labeled examples which are then augmented by a large number of unlabeled examples. Using unlabeled examples is important because obtaining labeled data is expensive and time-consuming while it is easy and inexpensive to collect a large number of unlabeled examples. The idea behind this approach is that the labels of unlabeled examples can be estimated by using committees. Using additional unlabeled examples, therefore, improves the performance of word sense disambiguation and minimizes the cost of manual labeling. Effectiveness of this approach was examined on a raw corpus of one million words. Using unlabeled data, we achieved an accuracy improvement up to 20.2%.	decision tree learning;machine learning;sampling (signal processing);word sense;word-sense disambiguation	Seong-Bae Park;Byoung-Tak Zhang;Yung Taek Kim	2003	Applied Intelligence	10.1023/A:1023812606045	semi-supervised learning;natural language processing;unsupervised learning;computer science;machine learning;decision tree;pattern recognition	AI	-19.85684233196091	-66.3993917043371	83874
d4da4eb4ed0b1b9edbadd44929e60256f337c360	arabic readability research: current state and future directions		Abstract We provide a perspective on the current state of Arabic readability assessment research with the objective of considering directions and opportunities for future research. We review and assess the current state of progress in Arabic readability assessment, briefly surveying research that has been performed on texts targeted at different populations: readers of Arabic as L1, adult readers in non-academic settings, and readers of Arabic as L2. Arabic readability assessment has followed trends in other languages, primarily English, but has faced challenges due to the specificities of Arabic and the relative scarcity of available corpora and tools, compared to languages with richer resources. We also consider whether readability assessment for Arabic should take into consideration the special situation of diglossia that exists in all Arab countries.		Violetta Cavalli-Sforza;Hind Saddiki;Naoual Nassiri	2018		10.1016/j.procs.2018.10.459	scarcity;natural language processing;data mining;arabic;readability;special situation;diglossia;computer science;artificial intelligence	NLP	-31.931588537329436	-73.89500561237286	83880
b519439f0777142c02d675647ca7c04d36cbc41b	predicting conjunct propagation and other extended stanford dependencies		In this work, we present a data-driven method to enhance syntax trees with additional dependencies as defined in the wellknown Stanford Dependencies scheme, so as to give more information about the structure of the sentence. This hybrid method utilizes both machine learning and a rule-based approach, and achieves a performance of 93.1% in F1-score, as evaluated using an existing treebank of Finnish. The resulting tool will be integrated into an existing Finnish parser and made publicly available at the address http://bionlp.utu.fi/.	baseline (configuration management);dependency grammar;f1 score;logic programming;machine learning;open content;parse tree;parsing;propbank;software propagation;syntactic predicate;treebank	Jenna Nyblom;Samuel Kohonen;Katri Haverinen;Tapio Salakoski;Filip Ginter	2013			natural language processing;speech recognition;computer science;algorithm	NLP	-22.08586657476314	-74.78734377754058	84150
ee20ed3b9435cb0bdd6d5444548e2a0611fd3695	a semantic retrieval framework for engineering domain knowledge	knowledge retrieval;query semantic extension;query semantic retrieval;ontology	In this paper, we propose a knowledge retrieval framework based on semantically annotated engineering ontology generated from domain documents. Particularly, we propose a scheme for build relations between ontology and domain documents. First, we build anthologies in engineering domain. Next, we transform the keywords into domain ontology concepts, and then find the synonyms of these keywords which are used as real queries to directly input into the query system. The semantic-based knowledge search and retrieval is then performed by ontology mapping and comparison. Using the semantic network of ontology, this system not only can conduct keyword-based retrieval, but also can understand the queries and answer questions by fuzzy inference based on domain ontology.		Xutang Zhang;Xiaofeng Chen;Xin Hou;Ting Zhuang	2011		10.1007/978-3-642-24728-6_66	upper ontology;ontology alignment;query expansion;bibliographic ontology;ontology inference layer;computer science;ontology;ontology;data mining;database;ontology-based data integration;owl-s;information retrieval;process ontology;suggested upper merged ontology	Web+IR	-32.1947583316527	-68.24672653573094	84168
e21d70efce81a1c0d199fb511e95aeb0afddde3c	explainable agreement through simulation for tasks with subjective labels		The field of information retrieval oftenworks with limited and noisy data in an attempt to classify documents into subjective categories, e.g., relevance, sentiment and controversy. We typically quantify a notion of agreement to understand the difficulty of the labeling task, but when we present final results, we do so using measures that are unaware of agreement or the inherent subjectivity of the task. We propose using user simulation to understand the effect size of this noisy agreement data. By simulating truth and predictions, we can understand the maximum scores a dataset can support: for if a classifier is doing better than a reasonable model of a human, we cannot conclude that it is actually better, but that it may be learning noise present in the dataset. We present a brief case study on controversy detection that concludes that a commonly-used dataset has been exhausted: in order to advance the state-of-the-art, more data must be gathered at the current level of label agreement in order to distinguish between techniques with confidence. ACM Reference Format: John Foley. 2018. Explainable Agreement through Simulation for Tasks with Subjective Labels. In Proceedings of ACM SIGIR 2018 Workshop on Learning from Limited or Noisy Data, Ann Arbor, Michigan, USA, July 12, 2018 (LND4IR	information retrieval;relevance;signal-to-noise ratio;simulation	John Foley	2018	CoRR		data mining;noisy data;computer science;classifier (linguistics);subjectivity;artificial intelligence;pattern recognition	NLP	-28.844734859234542	-69.1908496348859	84177
3b83fb47679ad6f0c834340829df112355a06d09	ancora-net: integración multilingüe de recursos lingüísticos semánticos	argument structure;computacion informatica;estructura argumental;filologias;semantics;semantica;recurso multilingue;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;lexical resource;recurso lexico;grupo a;multilingual resource;ciencias sociales;grupo b	AnCora-Net is a multilingual verbal lexicon built from the mapping of the Catalan and Spanish AnCora-Verb verbal lexicons into the English Unified Verb Index. The Unified Verb Index combines different sources of knowledge for English of wide coverage, which are a referent in semantic representation. The integration of our resources to the Unified Verb Index will enrich the contents of AnCora-Verb lexicons with semantic information coded for English. In the same way, the Unified Verb Index will be also related to equivalent verbs in another languages giving rise a multilingual resource that can be useful for comparative studies.	lexicon	Mariona Taulé;Oriol Borrega;Maria Antònia Martí	2011	Procesamiento del Lenguaje Natural		semantics;linguistics	NLP	-28.162910359862906	-77.07372051755816	84550
90d79a7b60b266af22c7e2f5fde8daffd260e3ba	using grammar-profiles to intrinsically expose plagiarism in text documents		Intrinsic plagiarism detection deals with the task of finding plagiarized sections in text documents without using a reference corpus. This paper describes a novel approach in this field by analyzing the grammar of authors and using sliding windows to find significant differences in writing styles. To find suspicious text passages, the algorithm splits a document into single sentences, calculates syntax grammar trees and builds profiles based on frequently used grammar patterns. The text is then traversed, where each window is compared to the document profile using a distance metric. Finally, all sentences that have a significantly higher distance according to a utilized Gaussian normal distribution are marked as suspicious. A preliminary evaluation of the algorithm shows very promising results.		Michael Tschuggnall;Günther Specht	2013		10.1007/978-3-642-38824-8_28	natural language processing;computer science;data science;world wide web	ML	-29.02404265325697	-67.49128109549974	84568
f1a73fea186a62321b460803f3305c3b5b05f3cd	neo-riemannian cycle detection with weighted finite-state transducers	monograph or book	This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finitestate transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications.	channel (communications);cycle detection;edit distance;franz lisp;ground truth;noisy channel model;noisy-channel coding theorem;sensor;substring;transducer	Jonathan Bragg;Elaine Chew;Stuart M. Shieber	2011			speech recognition;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics	NLP	-24.837150447365005	-77.65707533574187	84638
708124fa98ee6fda65a5a361a385b27c066b28f5	empirical methods for compound splitting	compound splitting;splitting rule;gold standard;parallel corpus;empirical method;nlp application;machine translation;german-english noun phrase translation;performance gain;compounded word;statistical mt system;noun phrase	Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.	bleu;natural language processing;one-to-one (data model);parallel text;statistical machine translation;text corpus	Philipp Koehn;Kevin Knight	2003			natural language processing;noun phrase;speech recognition;transfer-based machine translation;example-based machine translation;gold standard;computer science;linguistics;machine translation;empirical research	NLP	-22.338780119964063	-77.22695656164346	84872
1104841c25ed482bebd0cda518a20b08bc223fa3	randomized algorithms and nlp: using locality sensitive hash functions for high speed noun clustering		In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed.	asch conformity experiments;computer cluster;cyclic permutation;hpcc;information;locality of reference;natural language processing;question answering;random permutation;randomized algorithm;requirement;snapshot (computer storage);terascale (microarchitecture);time complexity;word sense;word-sense disambiguation	Deepak Ravichandran;Patrick Pantel;Eduard H. Hovy	2005			noun;computer science;theoretical computer science;machine learning;linguistics;locality preserving hashing;randomized algorithm;algorithm;locality-sensitive hashing	NLP	-30.460961836104076	-69.75124381928336	85118
b5c038772fe777f721b5c26574dea208f9c1e464	time expression analysis and recognition using syntactic token types and general heuristic rules		Extracting time expressions from free text is a fundamental task for many applications. We analyze time expressions from four different datasets and find that only a small group of words are used to express time information and that the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach named SynTime1 for time expression recognition. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related token regular expressions. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a lightweight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text from different domains and different text types. Experiments on benchmark datasets and tweets data show that SynTime outperforms state-of-the-art methods.	benchmark (computing);brill tagger;experiment;heuristic;logic programming;modifier key;real-time computing;regular expression	Xiaoshi Zhong;Aixin Sun;Erik Cambria	2017		10.18653/v1/P17-1039	natural language processing;artificial intelligence;computer science;machine learning;syntax;heuristic;security token	NLP	-24.32526853709082	-71.34786345395557	85147
0e32d3796ffbe07a40db593da15e68e5d036eb3a	bridging temporal context gaps using time-aware re-contextualization	temporal context;wikipedia;time aware re contextualization;complementarity	Understanding a text, which was written some time ago, can be compared to translating a text from another language. Complete interpretation requires a mapping, in this case, a kind of time-travel translation between present context knowledge and context knowledge at time of text creation. In this paper, we study time-aware re-contextualization, the challenging problem of retrieving concise and complementing information in order to bridge this temporal context gap. We propose an approach based on learning to rank techniques using sentence-level context information extracted from Wikipedia. The employed ranking combines relevance, complimentarity and time-awareness. The effectiveness of the approach is evaluated by contextualizing articles from a news archive collection using more than 7,000 manually judged relevance pairs. To this end, we show that our approach is able to retrieve a significant number of relevant context information for a given news article.	archive;bridging (networking);learning to rank;relevance;wikipedia	Andrea Ceroni;Nam Khanh Tran;Nattiya Kanhabua;Claudia Niederée	2014		10.1145/2600428.2609526	natural language processing;computer science;complementarity;data mining;brand;context model;world wide web;information retrieval	NLP	-30.1570153772472	-67.18472647782343	85206
f03f11ec6f2177fbf9bfb2ec072d2f7409fda1bf	em-based hybrid model for bilingual terminology extraction from comparable corpora	hybrid model	In this paper, we present an unsupervised hybrid model which combines statistical, lexical, linguistic, contextual, and temporal features in a generic EMbased framework to harvest bilingual terminology from comparable corpora through comparable document alignment constraint. The model is configurable for any language and is extensible for additional features. In overall, it produces considerable improvement in performance over the baseline method. On top of that, our model has shown promising capability to discover new bilingual terminology with limited usage of dictionaries.	baseline (configuration management);dictionary;expectation–maximization algorithm;experiment;terminology extraction;text corpus;unsupervised learning	Lianhau Lee;AiTi Aw;Min Zhang;Haizhou Li	2010			natural language processing;speech recognition;computer science	NLP	-21.58670672808219	-75.89201055313302	85243
d125dc9b34a8559e755a18e06fc0f7f39f6bedba	a composed confidence measure for automatic face recognition in uncontrolled environment	confidence measure;face recognition;scale invariant feature transform sift;multi layer perceptron;czech news agency	This paper is focused on automatic face recognition in order to annotate people in photographs taken in completely uncontrolled environment. Recognition accuracy of the current approaches is not sufficient in this case and it is thus beneficial to improve the results. We would like to solve this issue by proposing a novel confidence measure method to identify the incorrectly classified examples at the output of our classifier. The proposed approach combines two measures based on the post rior probability and two ones based on the predictor features in a supervised way. The experiments show that the proposed approach is very efficient, because it detects almost all erroneous examples.	experiment;facial recognition system;kerrison predictor;statistical classification;uncontrolled format string	Pavel Král;Ladislav Lenc	2014		10.5220/0004926202300237	facial recognition system;computer science;artificial intelligence;machine learning;pattern recognition;data mining;multilayer perceptron	Vision	-20.113589212318885	-69.19703347654378	85468
52f53c0b4e5c33afefa1b7d2705b9dd548b320ed	the itc-irst smt system for iwslt 2006	statistical machine translation;word alignment;system development	This paper describes the statistical machine translation system developed at ITC-irst for the evaluation campaign of the International Workshop on Spoken Language Translation 2005. The system exploits two search passes: the first pass is performed by a beam-search decoder which generates an n-best list of translations, the second by a simple re-scoring algorithm. The two passes apply log-linear phrase-based models with an increasing number of feature functions. Runs have been submitted under the supplieddata and manual-transcription conditions for three language pairs: Chinese-to-English, Japanese-to-English and Arabicto-English. Moreover, the Japanese-to-English system has been also employed under the ASR first-best condition. Significant improvements are reported by exploiting alternative word-alignments, and by using novel feature functions in the re-scoring step.	algorithm;automated system recovery;beam search;log-linear model;statistical machine translation;transcription (software)	Boxing Chen;Roldano Cattoni;Nicola Bertoldi;Mauro Cettolo;Marcello Federico	2005			natural language processing;speech recognition;computer science;communication	NLP	-22.24318630125471	-77.2852909269921	85477
bedcfa4c3cbd2611dcad82a8e151623fe2bf8817	an intelligent lexicon for contextual word sense discrimination	lexical semantics;semantics;multi expert architecture;lexical knowledge representation;word sense discrimination;knowledge representation;written text understanding;natural language processing	This paper presents a lexical model dedicated to the semanticrepresentation and interpretation of individual words inunrestricted text, where sense discrimination is difficult toassess. We discuss the need of a lexicon including local inferencemechanisms and cooperating with as many other knowledge sources(about syntax, semantics and pragmatics) as possible. We suggest a’minimal‘ representation (that is, the smallest representationpossible) acting as a bridge between a conceptual representation andthe microscopic sense variations of lexical semantics. We describean interpretation method providing one or many alternativecandidate(s) to the word, as representatives of its meaning in thesentence (and text).	denotational semantics;lexicon;word sense;word-sense induction	Violaine Prince	1997	Applied Intelligence	10.1023/A:1008245812391	natural language processing;statistical semantics;lexical semantics;lexical item;computer science;semantics;lexical choice;computational semantics	AI	-32.775235791289624	-79.80102883726605	85503
dd8b3c34b6e4f80fffd054bf1b1e5396eaf5fee9	e-law module supporting lawyers in the process of knowledge discovery from legal documents		E-law module is the web application which works mainly as the set of information retrieval and extraction tools dedicated for the lawyers. E-law module consists of following tools: (1) document search engine; (2) context oriented search engine plugin; (3) legal phrase oriented machine translation; (4) document metatagger; (5) verdict finder. Machine translation, document meta-tagger and verdict finder tools are available for the general public. Other tools are restricted and are accessible after logging into the module.	brill tagger;information retrieval;machine translation;web application;web search engine	Marek Kozlowski;Maciej Kowalski;Maciej Kazula	2015			knowledge management;knowledge extraction;computer science	NLP	-31.711551737810133	-68.52335804522514	85601
321c7b58eff9c1d07c6f9912a196f7738f1e2e08	log-linear models for word alignment	bilingual dictionary coverage;ibm translation model;alignment probability;log-linear model;ibm model;source language sentence;statistical alignment model;target language sentence;pos correspondence;word alignment;log linear model	We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models.	baseline (configuration management);bilingual dictionary;bitext word alignment;brown corpus;compiler;data structure alignment;experiment;fo (complexity);linear model;log-linear model;mathematical model;microsoft word for mac;search algorithm;serial digital video out;statistical machine translation;statistical model;supervised learning;text corpus;unified model;unsupervised learning	Yang Liu;Qun Liu;Shouxun Lin	2005			natural language processing;speech recognition;computer science;machine learning;pattern recognition;log-linear model	NLP	-21.0772332268653	-77.03399514201004	85840
d4d3c385bdde1687a3c5708c69bdebc4ebf0ddf8	the bnc parsed with rasp4uima	t technology;british national corpus	We have integrated the RASP system with the UIMA framework (RASP4UIMA) and used this to parse the XML-encoded version of the British National Corpus (BNC). All original annotation is preserved, and parsing information, mainly in the form of grammatical relations, is added in an XML format. A few specific adaptations of the system to give better results with the BNC are discussed briefly. The RASP4UIMA system is publicly available and can be used to parse other corpora or document collections, and the final parsed version of the BNC will be deposited with the Oxford Text Archive. 1. The British National Corpus The British National Corpus (BNC), a 100-million-word balanced sample of written (90%) and spoken (10%) English produced in the UK in the period 1960–93, was first released in 1995 and has since seen a variety of uses in lexicography and linguistics, natural language processing and artificial intelligence. With the third edition (Burnard, 2007), the corpus moved from a custom SGML format to standard XML and Unicode, which makes it compatible with available tools and more readily exploitable. 2. Parsing and Metadata The BNC contains word and sentence boundaries as well as part-of-speech tags (Leech, Garside & Bryant, 1994), but no parsing information and thus no facility to search for or otherwise make use of grammatical relations between words, which have proven useful in many applications. Various groups of people have parsed the corpus throughout the years using different tools and approaches. However, most, if not all, have simply removed all ‘extraneous’ mark-up from the corpus before parsing, which is not entirely satisfactory since we lose, e.g., the distinction between titles and running text, formatting information, named entities and multi-word expressions, not to mention metadata including genre and provenance of texts and spoken data. (In addition, white-space modifications for tokenisation purposes will, if employed, cause further divergence from the original.) It seems to us that the only adequate solution is to keep the original mark-up intact and add new elements and attributes to indicate parsing information. The RASP system (Briscoe, Carroll & Watson, 2006) is a domain-independent, robust parsing system for English which is free for research purposes. It was, in common with other extant publically-available parsers, designed for plain-text input and has only limited ability to handle XMLstyle mark-up natively. It would be possible, of course, to enhance RASP to handle arbitrary XML, but we chose instead the more flexible option of integrating its different parts into an existing analysis framework able to handle XML. UIMA, the Unstructured Information Management Architecture (Ferrucci & Lally, 2004), originated at IBM Research from a need to process initially unstructured data, mainly natural-language documents, with a sequence of complementary tools. A well-defined architecture allows ‘mixing and matching’ of components without worrying about interfacing issues: each part adds new structured information in a way which makes it immediately available as input for the remainder of the processing chain. UIMA accepts modules written in Java and C++ and is currently being developed as a project in the Apache incubator (Apache, 2007).	apache uima;artificial intelligence;bowyer–watson algorithm;british national corpus;c++;carroll morgan (computer scientist);ferranti mark 1;ibm research;information management;java;lexicography;named entity;natural language processing;oxford text archive;parsing;part-of-speech tagging;random-access stored-program machine;standard generalized markup language;text corpus;unicode;xml	Øistein E. Andersen;Julien Nioche;Ted Briscoe;John A. Carroll	2008			natural language processing;speech recognition;computer science;information retrieval	NLP	-33.170864282940435	-74.82713823566874	85993
ac9e57635838d4020994abda405cb51f1913c6e2	automatic enrichment of terminological resources: the iate rdf example		Terminological resources have proven necessary in many organizations and institutions to ensure communication between experts. However, the maintenance of these resources is a very time-consuming and expensive process. Therefore, the work described in this contribution aims to automate the maintenance process of such resources. As an example, we demonstrate enriching the RDF version of IATE with new terms in the languages for which no translation was available, as well as with domain-disambiguated sentences and information about usage frequency. This is achieved by relying on machine translation trained on parallel corpora that contains the terms in question and multilingual word sense disambiguation performed on the context provided by the sentences. Our results show that for most languages translating the terms within a disambiguated context significantly outperforms the approach with randomly selected	gene ontology term enrichment;inter-active terminology for europe;machine translation;parallel text;randomness;resource description framework;text corpus;word sense;word-sense disambiguation	Mihael Arcan;Elena Montiel-Ponsoda;John P. McCrae;Paul Buitelaar	2018			artificial intelligence;rdf;world wide web;natural language processing;machine translation;terminology;computer science	NLP	-29.729153035451755	-71.5550773495725	86045
8050392594208552194d746d2e8de1ab6c083147	automated extraction and clustering of requirements glossary terms		A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.	algorithm;cluster analysis;glossary;natural language;relevance;requirement;software requirements;subject matter expert turing test;terminology extraction	Chetan Arora;Mehrdad Sabetzadeh;Lionel C. Briand;Frank Zimmer	2017	IEEE Transactions on Software Engineering	10.1109/TSE.2016.2635134	computer science;subject-matter expert;glossary;natural language;data mining;terminology;software;ambiguity;software requirements specification;information retrieval;cluster analysis	SE	-30.489706567069618	-70.24539806920701	86165
1e9350da6284a24e3bdc4ffeec4aba2d121155eb	noun paraphrasing based on a variety of contexts	conference paper	We paraphrase nouns along the contexts of sentence input on the basis of a variety of contexts obtained from a large-scale corpus. The proposed method only uses the number of types of context, not word frequency or cooccurrence frequency features. This method is based on the notion that paraphrase candidates appear more commonly with target words in the same context. The results of our experiment demonstrate that the approach can produce more appropriate paraphrases than approaches based on co-occurrence frequency and pointwise mutual information.	pointwise mutual information;word lists by frequency	Tomoyuki Kajiwara;Kazuhide Yamamoto	2014			natural language processing;computer science;linguistics;communication	NLP	-25.540146471952323	-71.6170698685622	86171
9d4380b313bf5ae6dbd70f2c05f5fa4760069b03	amritacen_nlp @ fire 2015 language identification for indian languages in social media text		The progression of social media contents, similar like Twitter and Facebook messages and blog post, has created, many new opportunities for language technology. The user generated contents such as tweets and blogs in most of the languages are written using Roman script due to distinct social culture and technology. Some of them using own language script and mixed script. The primary challenges in process the short message is identifying languages. Therefore, the language identification is not restricted to a language but also to multiple languages. The task is to label the words with the following categories L1, L2, Named Entities, Mixed, Punctuation and Others This paper presents the AmritaCen_NLP team participation in FIRE2015-Shared Task on Mixed Script Information Retrieval Subtask 1: Query Word Labeling on language identification of each word in text, Named Entities, Mixed, Punctuation and Others which uses sequence level query labelling with Support Vector Machine. CCS Concepts • Theory of computation~Support vector machines • Computing methodologies~Natural language Processing • Information systems~Information extraction • Humancentered computing~Social tagging systems	blog;color gradient;entity;information extraction;information retrieval;language identification;language technology;microsoft word for mac;natural language processing;social media;support vector machine;theory of computation	Rahul Venkatesh Kumar;M. Anand Kumar;K. P. Soman	2015			language identification;linguistics;social media;computer science	NLP	-31.05531400519176	-76.7739972434614	86262
ab2eded52d3a9c5d45d18562c7acb3512d58f4ee	clinical text analysis using machine learning methods	conditional random fields natural language processing semantic evaluation clinical text analysis;semantics;text analysis;testing;clinical text analysis;conditional random fields;unified modeling language learning artificial intelligence medical information systems natural language processing text analysis;training data;feature extraction;unified modeling language;semantic evaluation;unified modeling language semantics natural language processing feature extraction testing text analysis training data;conditional random field clinical text analysis semeval semantic evaluation natural language processing nlp clinical disorder named entity recognition unified medical language system umls semantic group normalization umls concept unique identifier cui supervised machine learning disorder named entities;natural language processing	SemEval (Semantic Evaluation) is an annual workshop where attendees participate in a series of evaluations (competitions) of computational semantic analysis for natural language processing (NLP). The series evaluations include 10-20 tasks each year. In this paper we present our entry to the SemEval-2014 Task 7 on the Analysis of Clinical Text evaluation. The main aim of this task is to analyze large amounts of clinical data and to find the mentions of clinical disorders. This task consists of two sub tasks: a) named entity recognition i.e, identifying disorder concepts that belong to Unified Medical Language System (UMLS) semantic group; and b) normalization, i.e, mapping mentions of disorders to UMLS Concept Unique Identifier (CUI). In this paper, we present a supervised machine learning system for prediction of disorder named entities based on the conditional random field model. The data set provided by the Task 7 organizer was used to evaluate our model.	conditional random field;electronic organizer;emoticon;machine learning;named entity;natural language processing;semeval;semantic analysis (computational);supervised learning;unique identifier	Krishna Prasad Chodey;Gongzhu Hu	2016	2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2016.7550908	natural language processing;language identification;unified modeling language;training set;semantic computing;universal networking language;semeval;feature extraction;computer science;machine learning;data mining;semantic compression;database;semantics;software testing;temporal annotation;conditional random field;information retrieval;sentiment analysis	NLP	-23.301118046616384	-69.74829851139762	86297
30c6ce423db5ca40969b57d96ba029089946c4e6	correlation-based intrinsic evaluation of word vector representations		We introduce QVEC-CCA—an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources. We show that QVECCCA scores are an effective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity.	approximation algorithm;downstream (software development);natural language processing;treebank;word embedding	Yulia Tsvetkov;Manaal Faruqui;Chris Dyer	2016		10.18653/v1/W16-2520	natural language processing;pattern recognition	NLP	-21.09613330740316	-72.71553558470426	86337
387fdb95febc1eccb7fd371cd92803813f9b1d3a	adoption op verbal and visual dialogue behaviour in document handling systems				Bodo Arndt	1991			natural language processing;multimedia;computer science;artificial intelligence	NLP	-32.692017007901384	-79.12646263719668	86373
03a61b231b97bcc806a69ff8b42d768780242ed4	domain-specific entity extraction from noisy, unstructured data using ontology-guided search	information extraction;ontology guided search;hidden markov model;text analysis;electronic health record;context model;conditional random field;general motors;domain specificity;language model;structured data;language models	Domain-specific knowledge is often recorded by experts in the form of unstructured text. For example, in the medical domain, clinical notes from electronic health records contain a wealth of information. Similar practices are found in other domains. The challenge we discuss in this paper is how to identify and extract part names from technicians repair notes, a noisy unstructured text data source from General Motors’ archives of solved vehicle repair problems, with the goal to develop a robust and dynamic reasoning system to be used as a repair adviser by service technicians. In the present work, we discuss two approaches to this problem. We present an algorithm for ontology-guided entity disambiguation that uses existing knowledge sources, such as domain-specific taxonomies and other structured data. We illustrate its use in the automotive domain, using GM parts ontology and the unit structure of repair manuals text to build context models, which are then used to disambiguate mentions of part-related entities in the text. We also describe extraction of part names with a small amount of annotated data using hidden Markov models (HMM) with shrinkage, achieving an f-score of approximately 80%. Next, we used linear-chain conditional random fields (CRF) in order to model observation dependencies present in the repair notes. Using CRF did not lead to improved performance, but a slight improvement over the HMM results was obtained by using a weighted combination of the HMM and CRF models.	algorithm;archive;conditional random field;data model;entity;f1 score;hidden markov model;markov chain;named-entity recognition;noisy text;ontology (information science);reasoning system;signal-to-noise ratio;taxonomy (general);text corpus;textual case-based reasoning;word-sense disambiguation	Sergey Bratus;Anna Rumshisky;Alexy Khrabrov;Rajendra Magar;Paul Thompson	2011	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-011-0149-5	natural language processing;computer vision;speech recognition;data model;computer science;artificial intelligence;data science;machine learning;pattern recognition;data mining;context model;conditional random field;statistics;language model	NLP	-30.499969048457753	-70.2884364173004	86485
5192c8f9b054a3de3a427c649787aa3e54b42e39	corpus creation for new genres: a crowdsourced approach to pp attachment	computer science	This paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowdsourcing judgments. We develop and present a system to extract prepositional phrases and their potential attachments from ungrammatical and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from Amazon’s Mechanical Turk service. Our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text, and this semi-automated strategy holds promise for similar annotation projects in new genres.	amazon mechanical turk;attachments;blog;crowdsourcing;ibm notes;parsing;pipeline (computing);screenshot;semiconductor industry;text corpus;the turk;word-sense disambiguation	Mukund Jha;Jacob Andreas;Kapil Thadani;Sara Rosenthal;Kathleen McKeown	2010			natural language processing;speech recognition;computer science;artificial intelligence;linguistics	NLP	-30.34459297678898	-73.23108265501816	86541
f14cefed7a67fb92c93ff830b014e01484e16136	learning to predict distributions of words across domains		Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word’s predominant meaning changes. However, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain partof-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current stateof-the-art methods, while requiring no task-specific customisations.	distributional semantics;domain adaptation;experiment;natural language processing;part-of-speech tagging;partial least squares regression;unsupervised learning	Danushka Bollegala;David J. Weir;John A. Carroll	2014			natural language processing;computer science;theoretical computer science;machine learning	NLP	-20.162042587930713	-73.04879886679254	86573
57f1414540a5bb9e5f4971d4859c4d0583807b63	a naïve bayes classifier for shakespeare's second-person pronoun		In order to investigate in explicit detail the way that yand thpronouns alternate in the Shakespearean corpus, I have undertaken a collocational analysis of the full corpus of Shakespeare’s 37 plays and found that (1) second-person pronouns can be disambiguated based on context alone, (2) ypronouns seem to be used in more formal situations or when an inferior is addressing a social better, and (3) the thpronoun is reserved for addressing peers, servants, or other familiar personages. Through the Python Natural Language Toolkit (Bird et al., 2009, Natural Language Processing with Python. Sebastopol, CA: O’Reilly Media), I implemented a Naı̈ve Bayes classifier that in effect treats each occurrence of a second-person pronoun as a black box that must be resolved into either a ypronoun or a thpronoun based only on the surrounding words. Using tenfold cross-validation, the classifier achieves an accuracy of 78.3% when fellow thand ypronouns are excluded from the context and 88.0% when we allow fellow thand ypronouns to assist in classification. Most interesting, however, are the context words that prove most informative in categorizing the pronouns. Significantly, the words most useful in classifying a pronoun as a ypronoun include high-register words such as lordship, madam, lords, and sir. After a group of conjugated second-person verbs like art and wert, the words most associated with thpronouns are words such as torment, nuncle, lesser, and villain. The ability to discriminate between forms based only on context confirms the hypothesis that the two classes of second-person pronoun are indeed used distinctly in the Shakespearean corpus. The list of words most helpful in making that distinction strongly suggests a difference in formality. We can also gain additional insight into the plays by examining some of the unexpected words that collocate with either one form or the other. .................................................................................................................................................................................	black box;categorization;collocation;computation;computational linguistics;cross-validation (statistics);information;lords of the realm ii;mike lesser;naive bayes classifier;natural language toolkit;natural language processing;python;torment: tides of numenera;vocabulary;dialog	Kyle Mahowald	2012	LLC	10.1093/llc/fqr045	naive bayes classifier;natural language processing;pronoun;artificial intelligence;computer science	NLP	-26.308112459620915	-73.691672795501	86642
c1a8fc39ff20968f63ad1ca0ba09a9a68e7e9741	usaar at semeval-2016 task 13: hyponym endocentricity.		This paper describes our submission to the SemEval-2016 Taxonomy Extraction Evaluation (TExEval-2) Task. We examine the endocentric nature of hyponyms and propose a simple rule-based method to identify hypernyms at high precision. For the food domain, we extract lists of terms from the Wikipedia lists of lists by using the name of each list as the endocentric head and treating all terms in the extracted tables as the hyponym of the endocentric head. Our submission achieved competitive results in taxonomy construction and ranked top in hypernym identification when evaluated against gold standard taxonomies and also in manual evaluation of novel relations not covered by the gold standard taxonomies.	flynn's taxonomy;logic programming;semeval;taxonomy (general);wikipedia	Liling Tan;Francis Bond;Josef van Genabith	2016		10.18653/v1/S16-1203	artificial intelligence;natural language processing;computer science;semeval	NLP	-25.717039312293878	-69.59201021376572	86666
af25e2079e2db22fe49b73666b2690f8c3187123	improbable morphological forms in a computational lexicon		In the construction of a computational lexicon, one of the problems is how to handle cases where words have a partial morphological paradigm. In this paper we will describe this problem and sketch how we implemented a system for capturing the degree to which forms should be considered improbable. Also, we will describe how our results can be used in language applications.	lexicon;programming paradigm	Kristin Hagen;Lars Nygaard	2005			natural language processing;lexicon;computer science;artificial intelligence	NLP	-30.114200621185148	-79.33156605616033	86798
321823814d77418154ba537bce1a8104452fd1c3	semantic interpretation and knowledge extraction	inferences;knowledge extraction;system performance;semantic interpretation;knowledge acquisition;wordnet	A system that extracts knowledge from texts is presented. It is also indicated how the inferences necessary for the extraction of knowledge can be acquired by the system from sentences entered by users. The knowledge acquisition component is grounded on a semantic interpreter of English based on an enhanced WordNet. An evaluation of the system performance is included.	semantic interpretation	Fernando Gomez;Carlos Segami	2007	Knowl.-Based Syst.	10.1016/j.knosys.2006.07.002	natural language processing;wordnet;semantic interpretation;computer science;artificial intelligence;knowledge-based systems;knowledge extraction;information retrieval	NLP	-30.04051302107968	-70.59502493790426	86918
1a2684280c447851be1456881f3621d07fda769e	a pinch of humor for short-text conversation: an information retrieval approach		The paper describes a work in progress on humorous response generation for short-text conversation using information retrieval approach. We gathered a large collection of funny tweets and implemented three baseline retrieval models: BM25, the query term reweighting model based on syntactic parsing and named entity recognition, and the doc2vec similarity model. We evaluated these models in two ways: in situ on a popular community question answering platform and in laboratory settings. The approach proved to be promising: even simple search techniques demonstrated satisfactory performance. The collection, test questions, evaluation protocol, and assessors’ judgments create a ground for future research towards more sophisticated models.	baseline (configuration management);information retrieval;named entity;named-entity recognition;parsing;question answering	Vladislav Blinov;Kirill Mishchenko;Valeria Bolotova;Pavel Braslavski	2017		10.1007/978-3-319-65813-1_1	artificial intelligence;syntax;natural language processing;conversation;information retrieval;parsing;computer science;question answering;in situ;named-entity recognition	NLP	-26.950377805342537	-72.3021223850882	86967
88dc4e620a0f277e59916d05c15f58a7f65dce24	english morphological analysis with machine-learned rules	conference paper	This paper expounds an algorithm for morphological analysis of English language. The algorithm consists of two closely related components: morphological rule learning and morphological analyzing. The morphological rules are obtained through statistical learning from wordlist, with particular morphological features of English language taken into consideration. The procedure of morphological analysis considers two types of ambiguities: intersectional ambiguity and combinatory ambiguity. The procedure also considers the order of wordform formation in the language. Experiment shows that the algorithm performs distinctively compared to other algorithms. Keyword: Morphological analysis; statistical learning; intersectional ambiguity; combinatory ambiguity; wordform formation order	algorithm;ambiguity function;combinatory logic;experiment;f1 score;futures studies;indo;machine learning;markov chain;morphological parsing;text segmentation;transformational grammar;unsupervised learning;word-sense disambiguation	Xuri Tang	2006			computer science;artificial intelligence;communication;algorithm	NLP	-26.15619069086719	-77.52269474941494	87014
aa8c4507d6fbdd9aead29be3cefb91db6341a7df	multi-relation modeling on multi concept extraction lig participation at imageclefmed.		This paper presents the LIG contribution to the CLEF 2008 medical retrieval task (i.e. ImageCLEFmed). The main idea behind our contribution is to incorporate knowledge in the language modeling approach to information retrieval (IR). On ImageCLEFmed our model makes use of the textual part of the corpus and of the medical knowledge found in the Unified Medical Language System (UMLS) knowledge sources. Last year, we used UMLS to create a conceptual representation for each sentence in the corpus, and proposed a language modeling approach on these representations. The use of a conceptual representation allows the system to work at a more abstract semantic level, which solves some of the information retrieval problems, as the one of terminological variation. We also used different concept extraction methods, and tested how to combine these extraction methods on queries. This year, we have extended our previous method in two ways: first, we have used, in addition to relations derived from UMLS, co-occurrence relations; second, we have combined concept extraction methods not only on queries, but also on documents. In this paper, we first detail some IR approaches that use advanced	information retrieval;language model	Loïc Maisonnasse;Éric Gaussier;Jean-Pierre Chevallet	2008			information retrieval;language model;unified medical language system;computer science;sentence	NLP	-30.49356813617212	-68.66801527330365	87058
fea5eee26c6cd9c7f1591e9ab83fc59cb346678b	finding associations between people		Associations between people and other concepts are common in text and range from distant to close connections. This paper discusses and justifies the need to consider subtypes of the generic relation ASSOCIATION. Semantic primitives are used as a concise and formal way of specifying the key semantic differences between subtypes. A taxonomy of association relations is proposed, and a method based on composing previously extracted relations is used to extract subtypes. Experimental results show high precision and moderate recall.	complete (complexity);discriminative model;semantic similarity;taxonomy (general)	Eduardo Blanco;Dan I. Moldovan	2012			data mining	NLP	-27.394023211415234	-70.68804367249602	87369
3e9e424e54c10392752f439ae8aa190a8d266cf1	an improved tag dictionary for faster part-of-speech tagging		Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi’s tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi’s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging—more than 100,000 tokens per second in Perl.	data dictionary;fastest;part-of-speech tagging;perl;semi-supervised learning;semiconductor industry	Robert Moore	2015			computer science;data mining;world wide web;information retrieval	NLP	-21.39693800218267	-74.56180767002687	87377
1375a19dd4429a064d7fee959ff07d495ba8c06c	semi-markov phrase-based monolingual alignment		We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state of the art.	conditional random field;discriminative model;experiment;frank rosenblatt;jason;manifold alignment;markov chain;natural language processing;question answering;semiconductor industry;text corpus;travis ci;dbase	Xuchen Yao;Benjamin Van Durme;Chris Callison-Burch;Peter Clark	2013			speech recognition;computer science;pattern recognition;data mining	NLP	-21.186412651372557	-75.97423094577664	87440
34e1c23ccf3bbff0199c39c6c845cce866a91f78	predicting word embeddings variability		Neural word embeddings models (such as those built with word2vec) are known to have stability problems: when retraining a model with the exact same hyperparameters, words neighborhoods may change. We propose a method to estimate such variation, based on the overlap of neighbors of a given word in two models trained with identical hyperparameters. We show that this inherent variation is not negligible, and that it does not affect every word in the same way. We examine the influence of several features that are intrinsic to a word, corpus or embedding model and provide a methodology that can predict the variability (and as such, reliability) of a word representation in a semantic vector space.	heart rate variability;microsoft word for mac;spatial variability;text corpus;word2vec	Benedicte Pierrejean;Ludovic Tanguy	2018			natural language processing;machine learning;computer science;artificial intelligence	NLP	-20.0420732714752	-73.15118141831068	87476
7a289d3e2fb49b502c190b50b2eb616f113e0250	big data text-oriented benchmark creation for hadoop	benchmark testing dictionaries information management data handling data storage systems patents web pages text mining	Massive-scale Big Data analytics is representative of a new class of workloads that justifies a rethinking of how computing systems should be optimized. This paper addresses the need for a set of benchmarks that system designers can use to measure the quality of their designs and that customers can use to evaluate competing systems offerings with respect to commonly performed text-oriented workflows in Hadoop™. Additions are needed to existing benchmarks such as HiBench in terms of both scale and relevance. We describe a methodology for creating a petascale data-size text-oriented benchmark that includes representative Big Data workflows and can be used to test total system performance, with demands balanced across storage, network, and computation. Creating such a benchmark requires meeting unique challenges associated with the data size and its often unstructured nature. To be useful, the benchmark also needs to be sufficiently generic to be accepted by the community at large. Here, we focus on a text-oriented Hadoop workflow that consists of three common tasks: categorizing text documents, identifying significant documents within each category, and analyzing significant documents for new topic creation.	apache hadoop;benchmark (computing);big data	Anne E. Gattiker;Fadi H. Gebara;H. Peter Hofstee;J. D. Hayes;A. Hylick	2013	IBM Journal of Research and Development	10.1147/JRD.2013.2240732	computer science;data science;operating system;data mining;database	DB	-32.7529249442729	-66.42314902879075	87535
6c51e87de5e1d0e1acf089f385ec3778ca279b99	contextual semantic parsing using crowdsourced spatial descriptions		We describe a contextual parser for the Robot Commands Treebank, a new crowdsourced resource. In contrast to previous semantic parsers that select the most-probable parse, we consider the different problem of parsing using additional situational context to disambiguate between different readings of a sentence. We show that multiple semantic analyses can be searched using dynamic programming via interaction with a spatial planner, to guide the parsing process. We are able to parse sentences in near linear-time by ruling out analyses early on that are incompatible with spatial context. We report a 34% upper bound on accuracy, as our planner correctly processes spatial context for 3,394 out of 10,000 sentences. However, our parser achieves a 96.53% exactmatch score for parsing within the subset of sentences recognized by the planner, compared to 82.14% for a non-contextual parser.	attachments;bottom-up parsing;brute-force search;crowdsourcing;dialog system;dynamic programming;lexicon;nl (complexity);planner;question answering;shrdlu;shift-reduce parser;statistical model;time complexity;treebank;vocabulary;word-sense disambiguation	Kais Dukes	2014	CoRR		natural language processing;parser combinator;speech recognition;computer science;bottom-up parsing;parsing;s-attributed grammar;programming language;top-down parsing	NLP	-23.41993054943461	-78.63910047681857	87561
5d01fb02a12f544e5e9e8495b7db3171f60bcb0e	frequent term distribution measures for dataset profiling		"""We motivate the need for dataset profiling in the context of evaluation, and show that textual datasets differ in ways that challenge assumptions about the applicability of techniques. We set out some criteria for useful profiling measures. We argue that distribution patterns of frequent words are useful in profiling genre, and report on a series of experiments with χ based measures on the TIPSTER collection, and on textual intranet data. Findings show substantial differences in the distribution of very frequent terms across datasets. 1 Computing Department 2 Statistics Department Evaluation and Dataset Profiles There is a substantial literature to suggest that the characteristics of a particular corpus or dataset (including genre) will influence the behaviour and performance of Language Engineering (LE) and Information Retrieval (IE) applications and techniques in significant ways. Standard textbooks state, for instance, that keyword based retrieval works better on long documents (Jurafsky and Martin 2000), and that some techniques, such as LSA, work better on datasets with heterogenous vocabulary (Manning and Schuetze 1999). Stemming improves performance for short documents (Krovetz 1993), but not in general (Harman 1991). Recently, Barbu and Mitkov (2001) have pointed out the impact of the evaluation corpus for anaphora resolution algorithms, and Donaway et al (2000) pursue a related argument for automatic summarization. However, the precise nature of this dependency between dataset and performance remains vague in the absence of established methodologies and measures for profiling datasets. Evaluations of systems and techniques are reported without reference to the characteristics of the collections on which they were performed. Yet, making profiling information available would have several methodological and practical benefits. It would add a dimension to the significance of evaluation results, which could be interpreted in the context of different collections. It would also help researchers and developers in estimating the distance between the type of dataset used for development and evaluation of a system or technique, and the type of dataset on which it is deployed in a practical setting. In this paper, we set out some criteria for useful measures, and develop one such measure which aims to profile the degree of heterogeneity in the distribution of very frequent terms in different collections. We first formulate a “homogeneity” assumption, which we defeat, for each dataset, by means of an experimental regime based on the χ test (with p-value). Experiments profiled the TIPSTER sub-collections, and a dataset harvested from the Open University intranet. We conclude with a brief evaluation of our measure against the initial criteria. Developing measures What makes a good measure? To be useful, profiling measures have to meet both practical and methodological requirements. Ideally (i) the (collection of) measures have to profile testable features that are relevant to a range of language processing, search and retrieval techniques; (ii) they have to be sufficiently diverse and fine grained to allow complex profiles that reflect combinations of a range of relevant features; (iii) they have to be cheap to implement and run, so they can be used in practical development, over large datasets. Why measure very frequent term distribution? The question arises which features to measure. Term distribution patterns for high frequency words are a good starting point. Frequency based measures are cheap to implement (criterion iii). Regarding criteria (i) and (ii), term distribution patterns have been associated with fine grained genre and language modelling (Kilgariff 1997; Rose et al 1997), and with a number of techniques relevant to information retrieval. Katz (1996), for instance, argues convincingly for the identification of (high frequency) function and (rarer) content words with specific distribution patterns. The performance of established retrieval and categorisation techniques improves where stop-word identification takes account of collection specific term distributions (Wilbur and Sirotkin 1992, Yang et al. 1996). Finally, where they are function words, very frequent terms provide large amounts of evidence in almost any textual dataset, and allow a readily available point of comparison between collections. Some initial sampling we conducted shows that datasets do differ in ways which challenge assumptions about frequent term distribution. The 50 most frequent terms in each of the TIPSTER datasets contain several examples of domain-dependent non-function words. Table 1, for instance, lists a short description of each dataset. The occurrence of """"software"""" in position 21 of the ZF frequency list, and """"invention"""" in position 26 in the PAT frequency list are a case in point."""	algorithm;anaphora (linguistics);automatic summarization;categorization;experiment;information retrieval;intranet;language model;profiling (computer programming);requirement;sampling (signal processing);stemming;text corpus;vagueness;vocabulary;word lists by frequency;yang;zermelo–fraenkel set theory	Anne N. De Roeck;Avik Sarkar;Paul H. Garthwaite	2004			computer science;data mining;database;information retrieval	NLP	-28.474605509157602	-67.85775823186319	87654
3d1a22ae2b905c2d0334719be57d3619e1d6eaac	automated story capture from conversational speech	text processing;knowledge management;classification;text classification;symposia;storytelling;interviewing;speech recognition;algorithms;extraction;automation	While storytelling has long been recognized as an important part of effective knowledge management in organizations, knowledge management technologies have generally not distinguished between stories and other types of discourse. In this paper we describe a new type of technological support for storytelling that involves automatically capturing the stories that people tell to each other in conversations. We describe our first attempt at constructing an automated story extraction system using statistical text classification and a simple voting scheme. We evaluate the performance of this system and demonstrate that useful levels of precision and recall can be obtained when analyzing transcripts of interviews, but that performance on speech recognition data is not above what can be expected by chance. This paper establishes the level of performance that can be obtained using a straightforward approach to story extraction, and outlines ways in which future systems can improve on these results and enable a wide range of knowledge socialization applications.	document classification;knowledge management;precision and recall;randomness;socialization;speech recognition;statistical model	Andrew S. Gordon;Kavita Ganesan	2005		10.1145/1088622.1088649	extraction;speech recognition;interview;biological classification;computer science;artificial intelligence;automation;multimedia;world wide web	AI	-26.355975122802892	-80.19113728069024	87713
53be605d8150be6a038fa0c2bd9f26ffcf938ee4	vcu-tsa at semeval-2016 task 4: sentiment analysis in twitter		The aim of this paper is to produce a methodology for analyzing sentiments of selected Twitter messages, better known as Tweets. This project elaborates on two experiments carried out to analyze the sentiment of Tweets from SemEval-2016 Task 4 Subtask A and Subtask B. Our method is built from a simple unigram model baseline with three main feature enhancements incorporated into the model: 1) emoticon retention, 2) word stemming, and 3) token saliency calculation. Our results indicate an increase in classification accuracy with the addition of emoticon retention and word stemming, while token saliency shows mixed performance. These results elucidate a possible classification feature model that could aid in the sentiment analysis of Twitter feeds and other microblogging environments.	baseline (configuration management);emoticon;experiment;feature model;language model;sentiment analysis;stemming	Gerard Briones;Kasun Amarasinghe;Bridget T. McInnes	2016			natural language processing;artificial intelligence;sentiment analysis;computer science;semeval	NLP	-21.405488920616385	-69.32254983815812	87771
d84d4d494f58a98331bb2c77f66301e2112ffe49	kxtractor: an effective biomedical information extraction technique based on mixture hidden markov models	information extraction;hidden markov model;semi structured data;machine learning;inductive learning;part of speech;protein protein interaction	We present a novel information extraction (IE) technique, KXtractor, which combines a text chunking technique and Mixture Hidden Markov Models (MiHMM). KXtractor overcomes the problem of the single Part-Of-Speech (POS) HMMs with modeling the rich representation of text where features overlap among state units such as word, line, sentence, and paragraph. KXtractor also resolves issues with the traditional HMMs for IE that operate only on the semi-structured data such as HTML documents and other text sources in which language grammar does not play a pivotal role. We compared KXtractor with three IE techniques: 1) RAPIER, an inductive learning-based machine learning system, 2) a Dictionary-based extraction system, and 3) single POS HMM. Our experiments showed that KXtractor outperforms these three IE systems in extracting protein-protein interactions. In our experiments, the F-measure for KXtractor was higher than for RAPIER, a dictionary-based system, and single POS HMM respectively by 16.89%, 16.28%, and 8.58%. In addition, both precision and recall of KXtractor are higher than those systems.	hidden markov model;information extraction;markov chain	Min Song;Il-Yeol Song;Xiaohua Hu;Robert B. Allen	2005	Trans. Computational Systems Biology	10.1007/11567752_5	protein–protein interaction;natural language processing;semi-structured data;speech recognition;part of speech;computer science;machine learning;pattern recognition;information extraction;hidden markov model	Logic	-23.861841857034293	-72.94499379721702	87851
c9c96c4c41a3c9fbc8a32ac3726f866861c5dd6f	results and perspectives of the göttingen project on quantitative linguistics		"""This article reports on the state and perspectives of the Gottingen Quantitative Linguistics Project, which is, in the first place, concerned with investigations of word-length distributions in texts and also considers a number of further topics. The study of word length distributions goes back to investigations by Čebanov, Fucks, Grotjahn and others, and has been conducted, from the very beginning, in close cooperation with G. Altmann (Bochum) on the theoretical basis provided by Altmann, Grotjahn, Kohler, and Wimmer in several recent publications. Notes on all relevant aspects of the Gottingen project can be found on the Internet (URL: http:// www.gwdg.de/~kbest/projekt.htm); an interim report has been included in Best and Altmann (1996). THEORETICAL FOUNDATIONS It is a basic assumption of every serious attempt at construing a theory of language that Bunge is right in his statement """"Everything abides by laws"""" (Bunge, 1977, p. 17). A natural consequence of this assumption is the search for the laws underlying linguistic phenomena. One of the possible objects of investigation which can be studied with this aim in a relatively simple (albeit time-consuming) way is the frequency distribution of words of different lengths in complete texts. When the Gottingen project started, the following results were available as a point of depature: The first investigation on this topic seems to be a paper of Cebanov (1947) which obviously remained unknown in western linguistics for a long time. Studying texts of Indo-European languages, Cebanov proposed the displaced Poisson distribution as a good model for word length distributions in these languages. Fucks 1 Altmann (1988, p. 58) mentions (with reference to Piotrowski, Bektaev, & Piotrowskaja, 1985, pp. 254ff) that S.G. Čebanov obtained the same result as Fucks. I'm very grateful to S.V. Čebanov (St. Petersburg) for providing a copy of S.G. Čebanov's paper and some further information. (1955a, 1995b, 1956), studying some Indo-European and non-Indo-European languages also considered the displaced Poisson distribution to be a good model for natural languages in general (neglecting differences of style). He successfully applied it to text corpora in eight languages. Only one of these languages, Arabic, showed substantial deviations, which Fucks (1956, p. 15) explained by corpus problems. Another attempt at setting up a theory of word-length distribution was made by Grotjahn (1982). He criticises Fucks' assumption that """"the individual events are independent of each other and occur with a constant probability"""". Grotjahn argues: It certainly would be a far more realistic assumption that every individual word follows a displaced Poisson distribution ..., but that the probabilities of individual words are not the same but vary depending on factors, such as (linguistic) context, change of topic, etc. This means that the parameter 0 of the displaced Poisson distribution itself has to be regarded as a random variable"""" (Grotjahn 1982, p. 55). By choosing a Gamma distribution for 0 , Grotjahn obtains the composed * Address correspondence to: Karl-Heinz Best, Im Siebigsfeld 17, D-37115 Duderstadt, Germany. E-mail: kbest@gwdg.de. http://www.gwdg.de/~kbest/projekt.htm. D ow nl oa de d by [ Fl or id a St at e U ni ve rs ity ] at 0 2: 30 2 1 O ct ob er 2 01 4"""	indo;microsoft word for mac;nl (complexity);natural language;project xanadu;text corpus;theory	Karl-Heinz Best	1998	Journal of Quantitative Linguistics	10.1080/09296179808590122	computer science;artificial intelligence;mathematics;linguistics	Theory	-31.195440066578414	-73.86643147287931	87862
820969282ae23b4187acfc6bcd6df94f89f52d66	multi-language hypotheses ranking and domain tracking for open domain dialogue systems		Hypothesis ranking (HR) is an approach for improving the accuracy of both domain detection and tracking in multi-domain, multi-turn dialogue systems. This paper presents the results of applying a universal HR model to multiple dialogue systems, each of which are using a different language. It demonstrates that as the set of input features used by HR models are largely language independent a single, universal HR model can be used in place of language specific HR models with only a small loss in accuracy (average absolute gain of +3.55% versus +4.54%), and also such a model can generalise well to new unseen languages, especially related languages (achieving an average absolute gain of +2.8% in domain accuracy on held out locales fr-fr, es-es, it-it; an average of 66% of the gain that could be achieve by training language specific HR models). That the latter is achieved without retraining significantly eases expansion of existing dialogue systems to new locales/languages.	cma-es;dialog system	Paul A. Crook;Jean-Philippe Robichaud;Ruhi Sarikaya	2015			pattern recognition;absolute gain;retraining;machine learning;ranking;computer science;artificial intelligence	NLP	-21.0193459572891	-72.48727757327889	87893
8aa250e8b3484fbe3b32c4ea4342621e4889b872	improving statistical machine translation using shallow linguistic knowledge	iterative refinement;statistical machine translation;system performance;word alignment;computational linguistics;linguistique informatique;language model	We describe methods for improving the performance of statistical machine translation (SMT) between four linguistically different languages, i.e., Chinese, English, Japanese, and Korean by using morphosyntactic knowledge. For the purpose of reducing the translation ambiguities and generating grammatically correct and fluent translation output, we address the use of shallow linguistic knowledge, that is: (1) enriching a word with its morphosyntactic features, (2) obtaining shallow linguistically-motivated phrase pairs, (3) iteratively refining word alignment using filtered phrase pairs, and (4) building a language model from morphosyntactically enriched words. Previous studies reported that the introduction of syntactic features into SMT models resulted in only a slight improvement in performance in spite of the heavy computational expense, however, this study demonstrates the effectiveness of morphosyntactic features, when reliable, discriminative features are used. Our experimental results show that word representations that incorporate morphosyntactic features significantly improve the performance of the translation model and language model. Moreover, we show that refining the word alignment using fine-grained phrase pairs is effective in improving system performance. 2006 Elsevier Ltd. All rights reserved.	analysis of algorithms;bitext word alignment;computation;data structure alignment;language model;statistical machine translation	Young-Sook Hwang;Andrew M. Finch;Yutaka Sasaki	2007	Computer Speech & Language	10.1016/j.csl.2006.06.007	natural language processing;speech recognition;transfer-based machine translation;computer science;computational linguistics;linguistics;machine translation;rule-based machine translation;language model	NLP	-22.025227830696934	-76.86959890543248	87918
6268daca3037d6cb40f8193b8636aa97efe1287f	chinese spelling check evaluation at sighan bake-off 2013		This paper introduces an overview of Chinese Spelling Check task at SIGHAN Bake-off 2013. We describe all aspects of the task for Chinese spelling check, consisting of task description, data preparation, performance metrics, and evaluation results. This bake-off contains two subtasks, i.e., error detection and error correction. We evaluate the systems that can automatically point out the spelling errors and provide the corresponding corrections in students’ essays, summarize the performance of all participants’ submitted results, and discuss some advanced issues. The hope is that through such evaluation campaigns, more advanced Chinese spelling check techniques will be emerged.	benchmark (computing);error detection and correction;evaluation function;natural language processing;performance evaluation;spell checker	Shih-Hung Wu;Chao-Lin Liu;Lung-Hao Lee	2013			natural language processing;speech recognition;computer science	NLP	-31.178594749954033	-72.86734037147401	87921
50cd5faa23a89e2ca45d0d60601415347e5f96cb	ntt statistical machine translation for iwslt 2006		We present the NTT translation system that is experimented for the evaluation campaign of “International Workshop on Spoken Language Translation (IWSLT).” The system consists of two primary components: a hierarchical phrase-bas d statistical machine translation system and a reranking sys tem. The former is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using nonterminals. The latter uses a modified voted perceptron approach with large number of features. Experiments showed that our hierarchical phrase-based model outperformed a conventional phrase-based model. In addition, our reranki ng algorithm further boosted the performance.	algorithm;context-free grammar;perceptron;statistical machine translation;terminal and nonterminal symbols	Taro Watanabe;Jun Suzuki;Hajime Tsukada;Hideki Isozaki	2006			perceptron;machine translation;machine translation software usability;spoken language;synchronous context-free grammar;natural language processing;speech recognition;phrase;computer science;rule-based machine translation;example-based machine translation;artificial intelligence	NLP	-22.54220379895769	-78.12124462644823	88218
18032e83d99598097bc9b4b2c63c8c6350989433	document clustering in heterogeneous corpora			cluster analysis;text corpus	Romaric Besançon;Anne-Laure Daquo	2015	Document Numérique	10.3166/dn.18.2-3.81-100	document clustering	NLP	-25.045795957585508	-67.71652365568262	88395
9cad956793e3c2ee76d0dd836f24073977512c61	automating the structural markup process in the conversion of print documents to electronic texts.				Casey Palowitch;Darin Stewart	1995			natural language processing;computer science;multimedia;information retrieval	DB	-31.668149472750407	-76.61865982912026	88446
0ef5b4487d4beb4894af34f410a514db23feed7f	learning word sense distributions, detecting unattested senses and identifying novel senses using topic models		Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions. This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory.	inventory;parallel text;parsing;sensor;spectral density estimation;text corpus;unsupervised learning;web services for devices;word sense;word-sense disambiguation	Jey Han Lau;Paul Cook;Diana McCarthy;Spandana Gella;Timothy Baldwin	2014			natural language processing;speech recognition;semeval;computer science	NLP	-26.050430424186263	-71.83246808615128	88481
b420b5a98af0a78531a0e9d5b8627d2840cfc2dd	a deep learning solution to named entity recognition		Identifying Named Entities is vital for many Natural Language Processing (NLP) applications. Much of the earlier work for identifying named entities focused on using handcrafted features and knowledge resources (feature engineering). This is a barrier for resource-scarce languages as many resources are not readily available. Recently, Deep Learning techniques have been proposed for various NLP tasks requiring little/no hand-crafted features and knowledge resources, instead the features are learned from the data. Many proposed deep learning solutions for Named Entity Recognition (NER) still rely on feature engineering as opposed to feature learning. However, it is not clear whether the deep learning system or the engineered features are responsible for the positive results reported. This is in contrast with the goal of deep learning systems i.e., to learn the features from the data itself. In this study, we show that a feature learned deep learning system is a viable solution to NER task. We test our deep learning systems on CoNLL English and Spanish NER datasets. Our system is able to give comparable results with the existing state-of-the-art feature engineered systems for English. We report the best performance of 89.27 F-Score for English when comparing with systems which do not use any handcrafted features or knowledge resources. Evaluation of our trained system on out-of-domain data indicate that the results are promising with the reported results. Our system when tested on Spanish NER achieves the best reported F-Score of 82.59 indicating its applicability to other languages.	best practice;deep learning;experiment;f1 score;feature engineering;feature learning;mit engineering systems division;named entity;natural language processing;unsupervised learning;way to go	V. Rudra Murthy;Pushpak Bhattacharyya	2016		10.1007/978-3-319-75477-2_30	natural language processing;convolutional neural network;computer science;deep learning;recurrent neural network;feature engineering;named-entity recognition;feature learning;artificial intelligence	AI	-20.099090201504044	-71.12291283667085	88552
054d29ffe5d54e4b6e28a3935850f08aaeb2e934	cost optimization in crowdsourcing translation: low cost translations made even cheaper		Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentenceby-sentence basis, and fit a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost.	crowdsourcing;linear model;principle of good enough;shadow volume;statistical machine translation	Mingkun Gao;Wei Xu;Chris Callison-Burch	2015			natural language processing;artificial intelligence;computer science;crowdsourcing	NLP	-23.64723987144302	-77.10502626083696	88579
718d4796ef518ab54d0b4436d80a1afe5c7a3a39	elirf-upv en tass 2018: análisis de sentimientos en twitter basado en aprendizaje profundo (elirf-upv at tass 2018: sentiment analysis in twitter based on deep learning)		This paper describes the participation of the ELiRF research group of the Universitat Politècnica de València at TASS2018 Workshop which is a satellite event of the XXXIV edition of the International Conference of the Spanish Society for Natural Language Processing. We describe the approaches used for “Sentiment Analysis at Tweet level” and “Aspect-based Sentiment Analysis” tasks, the results obtained and a discussion of these results. Our participation has focused primarily on exploring different approaches of Deep Learning and we have achieved competitive results in the addressed tasks.	deep learning;general motors en-v;natural language processing;sentiment analysis;tass times in tonetown	José-Ángel González;Ferran Plà;Lluís-F. Hurtado	2018			sentiment analysis;data science;deep learning;computer science;artificial intelligence	Robotics	-21.53620259341378	-69.62827902897885	88609
e2db9f9afd22e423c0deea75e4940df622a62185	named entity transliteration with sequence-to-sequence neural network		Named Entities are often rare words, and their transliteration across languages has been a challenging task. In this paper, we study a novel technique that segments a named entity into a sequence sub-words or characters. We propose to learn the transliteration mechanism using a sequence-to-sequence neural network. Applying the proposed technique to personal named transliteration on LDC dataset, we show impressive results with more than 10 BLEU score improvement over the competing statistic method on the same corpus.	artificial neural network;bleu;linguistic data consortium;named entity	Zhongwei Li;Chng Eng Siong;Haizhou Li	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300621	pattern recognition;bleu;named entity;artificial neural network;statistic;artificial intelligence;computer science;transliteration	NLP	-19.617630296313994	-75.6893526449133	88673
3ecf77a0ba78dce458add49cd4eadea57bebe344	authorship attribution using variable length part-of-speech patterns	authorship attribution;part-of-speech tags;stylometry;variable length sequential patterns	Identifying the author of a book or document is an interesting research topic having numerous real-life applications. A number of algorithms have been proposed for the automatic authorship attribution of texts. However, it remains an important challenge to find distinct and quantifiable features for accurately identifying or narrowing the range of likely authors of a text. In this paper we propose a novel approach for authorship attribution, which relies on the discovery of variable-length sequential patterns of parts of speech to build signatures representing each author’s writing style. An experimental evaluation using 10 authors and 30 books, consisting of 2,615,856 words, from Project Gutenberg was carried. Results show that the proposed approach can accurately classify texts most of the time using a very small number of variable-length patterns. The proposed approach is also shown to perform better using variable-length patterns than with fixed-length patterns (bigrams or trigrams).	algorithm;antivirus software;bigram;blog;book;experiment;point of sale;real life;stylometry;trigram;variable-length code	Yao Jean Marc Pokou;Philippe Fournier-Viger;Chadia Moghrabi	2016		10.5220/0005710103540361	computer science;part of speech;machine learning;natural language processing;attribution;artificial intelligence	NLP	-23.948262061875255	-66.35474612321642	88697
70553687a7fd9369474edc22971e11cecd5514f7	clustering words with the mdl principle		"""We address the probhml of automaticMly constructing a thesaurus by clustering words based on corpus data. We view this problem as that of estimating a joint distribution over the (:artesian product of a parti t ion of a set of nouns and a partition of a set of verbs, and propose a learning a.lgorithm based on the Mininmm Description Length (MDL) Principle for such estimation. We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering, and found that the former outperforms the latter. ~¢Ve also evaluated the method by conducting pp-a t tachment disambiguation experiments using an automaticMly constructed thesaurus. Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation. 1 I n t r o d u c t i o n Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. have been proposed (Hindle, 1990; Brown et al., 1992; Pereira et al., 1993; Tokunaga et al., 1995). The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand, b) do away with subjectivity inherent in a hand made thesaurus, and c) make it easier to adapt a natural language processing system to a new domain. In this paper, we propose a new method for automatic construction of thesauri. Specifically, we view the problem of automatically clustering words as that of estimating a joint distributiofl over the Cartesian product of a partition of a set of nouns (in general, any set of words) and a parti t ion of a set of w:rbs (in general, any set of words), and propose an est.imation *Real World Computing Partership algorithm using simulated annealing with an energy function based on the bl inimum Description Length (MDL) Principle. The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics. As a method of statisticM estimation MDL is guaranteed to be near optimal. We empiricMly evMuated the effectiveness of our method. In particular, we compared the performance of an MDL-based sinm]ated anuealilag Mgorithm in hierarchical word clustering against. that of one based on the Maximum Likelihood Estimator (MLE, for short). We found that the MDL-based method performs better than the MLE-based method. We also evaluated our method by conducting pp-a t tachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved. Since some words never occur in a corpus, and thus cannot be reliably classified by a method solely based on corpus data, we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation. We conducted some experiments in order to test the effectiveness of this strategy. Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage 1 of our disambiguation method, while maintaining high accuracy e. 2 The Problem Sett ing A method of constructing a thesaurus based on corpus data usually consists of the following three steps: (i) Extract co-occurrence data (e.g. case frame data, adjacency data) fl'om a corpus, (ii) Starting from a single class (or each word composing its own class), divide (or merge) word classes 1 ~Cover~tge' refers to the proportion (in percentage) of test data for which the disambiguat.ion method can make a decision. 2'Accuracy' refers to the success rate, given that the disambiguation method makes a decision. b a s e d Oll the co-occurrence da ta using 8Ollle Sill> ilarity (distance) measure. (The former apl)roach is called 'divisive', the latter 'agglomerat ive ' . ) (iii) Repeat step (ii) until some s topping condition is met, to construct a thesaurus (tree). The method we propose here consists of the same three st.eps. Suppose available to us are frequency da ta (cooccurrence data.) between verbs and their case slot. values extracted from a corpus (step (i)). We then view the problem of clustering words as tha t of es t imat ing a probabilistic model (representing a. probabil i ty distr ibution) tllat generates such da ta We assume that the target model can be defined in the following way. First, we define a noun part i t ion """"PA. ~ over a given set of nouns ..'V"""" and a verb partioll """"Pv over a given set. of verbs 12. A noun par t i t ion is any set T'-~ satisfying """"P,~ C 2 H, Wc~e'&v('i = A/ and VCi, (..) E 7)A.', Ci 0 (/j = O. A verb part i t ion 7)v is defined analogously. In this paper, we call a member of a noun part i t ion 'a, llOUll cluster ' , and a nlenlbe, r of a verb partit ion a ~verb cluster' . We refer to a member of the Cartesian product of a noun part i t ion and a verb part i t ion ( C """"P:v x """"Pv ) simply as 'a cluster ' . We then define a probabilistic model (a joint distribution), writ ten I ' (C , , (:v), where random variable C,, assumes a value fl'om a fizcd nouu part i t ion ~PX, and C~. a va.lue from a fixed verb part i t ion 7)v. Within a given cluster, we assume thai each element is generated with equal probability, i.e., P(c,,,c~,) v . , E c,,,v,,, E c,,, P(,,,,,,) I C . x < ,1 (t) In this paper, we assume that the observed data are generaied by a model belonging to the class of models just de.scribed, and select a model which best explains the data.. As a result of this, we obtain both noun clusters and verb clusters. This problem set t ing is based on the intuit.lye assumption that similar words occur in the sa.me context with roughly equal likelihood, as is made explicit in equation (l) . Thus selecting a model which best explains the given da ta is equivalent to finding the most appropria te classification of words base(t on their co-occurrence. 3 C l u s t e r i n g w i t h M D L We now turn to the question of what. s t ra tegy (or criterion) we should employ for est imating the best model. Our choice is the MDL (Minimum Description I,ength) principle (tlissanen, 1989), a well-known principle of da ta compression and statistical estimation from inforlnation theory. MDI, st ipulates tha t the best probability model for given data is tha t model which requires the least cod(: length ['or encoding of the model itself, as well as the giwql da ta relative to it a. We refer to the code length for the model aWe refer /.he interested reader to eli aml Abe, 1!195) for explana.tion of ra.tionals behind using the as ' the model description h 'ngth ' and that for tile da ta ' the da ta description length."""" We apply MDI, to the problem of est imating a model consisting of a pair of part i t ions as described above. In this context, a model with less clusters tends to be simpler (in t.erms of the number of parameters) , but also tends to have a poorer fit. to the data. In contrast , a model with more clusters is more complex, but tends to have a better fit to the data. Thus, there is a trade-off relationship between the simplicity of a model and the goodness of fit to the data. The model description length quantifies the simplicity (complexity) of a model, and the da ta description length quantifies the tlt. to the data. According to MDL, the model which minimizes the sum total of the two types of description lengths should be selected. In what follows, we will describe in detail how the description length is to be calculated in our current context, as well as our silnulated annealing algori thm based on MI)L. 3.1 C a l c u l a t i n g D e s c r i p t i o n L e n g t h We will now describe how the description length for a model is calculated, lh'call tha t each model is specified by the Cartesian product of a part i t ion of nouns and a part i t ion of verbs, and a number of parameters for them. Here we let /,', denote the size of the noun parti t ion, and /q, the size of the verb parti t ion. Tiien, there are k , . k~ , 1 free parameters in a model. Given a model M and da ta k', its total description length L ( J / ) 4 is COlnputed as the suni of the model description length L .... d( ' l t ) , the description length of its parameters I;~,,,,.(M), and da ta description length Ld,~t(M). (We often refer to Lm.od(.'l.]) qLpar (:'~l) as the model description length). Namely, L(:~'I) = L,,~o(~(:~I) + L>.,,.(:~I) + L ~ , ( M ) (2) We employ the %inary noun clustering method ' , in which k,, is fixed at IVt and we are to dechle whether k,~ -1 or k,,. = 2, which is then to be applied recursiw~ly to the clusters thus obtained. This is as if we view the noutls as entities a.nd the verbs as features and cluster the entities based on their feat.ures. Since there are 2Pv'I subsets of the set of llottns .~, and for each 'b inary ' noun partition we have two different subsets (a special case of which is when one subset is A 'r and the other the empty set 0), the number of possible binary noml part i t ions is 2tAq/2 = 21~'l-J. Thus for each I)inary noun parti t ion we need log 21a""""l-t = i3jI _ 1 bit.s 5 to describe it. 6 Ilenee L ..... a(M) is calculated MI)L principle in natural language processing. ~L(M) depends on .';, but we will leave ,5' implicit. 5Throughout the paper 'log' denotes the logarit.hnt to the base 2. 6 For further explanation, see (Quinlan and Rivest, 1989)."""	algorithm;binary number;cartesian closed category;cluster analysis;computer cluster;corpus linguistics;data compression;emoticon;entity;estdomains;estimation theory;experiment;information theory;mdl (programming language);mathematical optimization;medium-dependent interface;minimum description length;model m keyboard;multiple document interface;natural language processing;ross quinlan;simulated annealing;statistical model;stellar classification;test data;text corpus;thesaurus (information retrieval);word lists by frequency;word-sense disambiguation	Hang Li;Naoki Abe	1996			natural language processing;speech recognition;mathematics;linguistics	NLP	-25.55922699810075	-73.86878042582337	88737
9adaa779a327893ecc2b4f9d757c2e776174c212	automatic terminology support in computer-aided and machine translation.	machine translation			Heinz-Dirk Luckhardt	1990			computer-assisted translation;synchronous context-free grammar;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;machine translation software usability	EDA	-29.673668852761296	-78.08321879150662	88756
1a2c35e8c549049615b9bb22462456ae01e9c84e	adaptive context-based term (re)weighting: an experiment on single-word question answering		Term weighting is a crucial task in many Information Retrieval applications. Common approaches are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms. The Context Function is learned by examples, so that its implementation is mostly automatic. The algorithm was successfully tested on a data set of crossword clues, which represent a case of Single-Word Question Answering.	algorithm;crossword;information retrieval;natural language;question answering	Marco Ernandes;Giovanni Angelini;Marco Gori;Leonardo Rigutini;Franco Scarselli	2006				Web+IR	-26.55188238553986	-68.72572935177504	88792
3f40a9564c3b45930a5f6b6db89d02ff1b8cf4cf	sussx: wsd using automatically acquired predominant senses	current paper;base method;predominant sense;finegrained structure;topic domain specilisation;sense inventory;particular track	We introduced a method for discovering the predominant sense of words automatically using raw (unlabelled) text in (McCarthy et al., 2004) and participated with this system in SENSEVAL3. Since then, we worked on further developing ideas to improve upon the base method. In the current paper we target two areas where we believe there is potential for improvement. In the first one we address the finegrained structure of WordNet’s (WN) sense inventory (i.e. the topic of the task in this particular track). The second issue we address here, deals with topic domain specilisation of the base method. Error analysis tought us that the method is sensitive to the fine-grained nature of WN. When two distinct senses in the WN sense inventory are closely related, the method often has difficulties discriminating between the two senses. If, for example, sense 1 and sense 7 for a word are closely related, choosing sense 7 in stead of sense 1 has serious consequences if you are using a first-sense heuristic (considering the highly skewed distribution of word senses). We expect that applying our method on a coarser grained sense inventory might help us resolve some of the more unfortunate errors. (Magnini et al., 2002) have shown that information about the domain of a document is very useful for WSD. This is because many concepts are specific to particular domains, and for many words their most likely meaning in context is strongly correlated to the domain of the document they appear in. Thus, since word sense distributions are skewed and depend on the domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system.	expect;heuristic;microsoft word for mac;strongly correlated material;web services for devices;word lists by frequency;word sense;wordnet	Rob Koeling;Diana McCarthy	2007		10.3115/1621474.1621542	natural language processing;speech recognition;computer science;communication	NLP	-26.92923145009224	-71.82593586811598	88844
d53020ae40927a84766f068efe1bc372a4320d54	linguistic features of genre and method variation in translation: a computational perspective		In this paper we describe the use of text classification methods to investigate genre and method variation in an English German translation corpus. For this purpose we use linguistically motivated features representing texts using a combination of part-of-speech tags arranged in bigrams, trigrams, and 4-grams. The classification method used in this paper is a Bayesian classifier with Laplace smoothing. We use the output of the classifiers to carry out an extensive feature analysis on the main difference between genres and methods of translation.	additive smoothing;algorithm;bayesian network;bigram;brown corpus;computation;document classification;f1 score;language industry;machine translation;naive bayes classifier;need to know;part-of-speech tagging;point of sale;postediting;text corpus;trigram	Ekaterina Lapshinova-Koltunski;Marcos Zampieri	2017	CoRR		naive bayes classifier;machine learning;trigram;natural language processing;artificial intelligence;additive smoothing;computer science;linguistics;bigram;pattern recognition (psychology)	NLP	-23.520458517647338	-73.86596063327049	88915
bba5ac4a40da9f8f21b34119195f94782909b260	just-in-time grammar.	rule based;syntactic analysis;just in time	In this paper, we attempt to explain why rule-based syntactic analysis systems have not so far worked as well as they could. This will motivate our approach, which is based on a narrowing of the focus of the rules, resulting in more accurate, less ambiguous, more efficient parsing and relative ease of acquisition. These rules are word-based, rather than construction-based, and are used to produce “Just in Time” grammars for specific inputs. We combine this grammar with an error detection and repair module that is enabled primarily by the dependency-directed capabilities of the basic parsing engine. Our approach is most closely related to the XTAG philosophy (Schabes, et. al., 1988). We discuss the similarities and differences between XTAG and our work while further affirming the benefits of lexicalized grammars..	attribute grammar;context-free grammar;error detection and correction;just-in-time compilation;logic programming;parsing	Stephen Beale;Sergei Nirenburg;Marjorie McShane	2003			natural language processing;id/lp grammar;generative grammar;categorial grammar;syntax;operator-precedence grammar;regular grammar;syntactic predicate;affix grammar;regular tree grammar;stochastic grammar;phrase structure rules;emergent grammar;linguistics;relational grammar;attribute grammar;word grammar;adaptive grammar;mildly context-sensitive grammar formalism;head-driven phrase structure grammar	NLP	-25.52795885034252	-76.64087762669956	89109
aa1c69b48c5da4d47c9b5ba4ff093c0cf4ea7df9	attention-based lstm-cnns for uncertainty identification on chinese social media texts		Uncertainty identification is an important semantic processing task, which is crucial to the quality of information in terms of factuality in many techniques, e.g. topic detection, question answering. Especially in social media, the texts are written informally which are widely used in many applications, so the factuality has become a premier concern. However, existing approaches that still rely on lexical cues suffer greatly from the casual or word-of-mouth peculiarity of social media, in which the cue phrases are often expressed in sub-standard form or even omitted from sentences. To tackle these problems, this paper proposes the attention-based LSTM-CNNs for the uncertainty identification on social media texts, named ALUNI. ALUNI incorporates attention-based LSTM networks to represent the semantics of words, and convolutional neural networks to capture the most important semantics of uncertainty for identification. Experiments are conducted on both Chinese Weibo and news datasets, and 78.19% and 73.95% of F1-measure scores are achieved with 11% and 3% improvement over the baseline, respectively.	artificial neural network;baseline (configuration management);convolutional neural network;f1 score;long short-term memory;natural language processing;question answering;social media	Binyang Li;Kaiming Zhou;Wei Gao;Xu Han;Linna Zhou	2017	2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2017.8304349	natural language processing;task analysis;convolutional neural network;semantics;semantic memory;feature extraction;information quality;question answering;social media;artificial intelligence	NLP	-19.548506412198783	-69.44641867654707	89135
fbfbee5fd3bbb5e1295bf04e2139f38c27c7f782	influence of preprocessing on dependency syntax annotation: speed and agreement		When creating a new resource, preprocessing the source texts before annotation is both ubiquitous and obvious. How the preprocessing affects the annotation effort for various tasks is for the most part an open question, however. In this paper, we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline. We also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.1	baseline (configuration management);meaning–text theory;parsing;preprocessor;text corpus	Arne Skjærholt	2013			natural language processing;computer science;data mining;information retrieval	NLP	-24.785860994071584	-74.11141841837079	89237
05fb5bc730664b6af9cea78d38d2f3ffaf498c05	yet another suite of multilingual nlp tools		This paper presents the current development of a multilingual suite for Natural Language Processing. It consists of a sentence chunker, a tokenizer, a PoS-tagger, a dictionary-based lemmatizer and a Named Entity Recognizer (both for enamex and numex expressions). The architecture of the pipeline and the main resources used for its development are described. Besides, the PoS-tagger and Named Entity Recognizer are evaluated against several state-of-the-art systems. The experiments performed in Portuguese and English show that, in spite of its simplicity, our system competes with some well known tools for NLP. It is entirely written in Perl and distributed under a GPL license.	brill tagger;dictionary;expect;experiment;finite-state machine;lemmatisation;lexical analysis;lexicon;logic programming;named entity;natural language processing;part-of-speech tagging;perl;pipeline (computing);point of sale;text corpus;yet another	Marcos García;Pablo Gamallo	2015		10.1007/978-3-319-27653-3_7	natural language processing;speech recognition;computer science;linguistics;programming language;world wide web	NLP	-30.04403762387178	-74.95673733964232	89247
f78b0c56442380c4cd5df674bd20fab745ff9c28	semantics in text processing. step 2008 conference proceedings				Johan Bos;Rodolfo Delmonte	2008				HPC	-31.383636452013917	-77.09093755941521	89308
1d0f25dad832a100f395ec42d0b9b6cc33c23f38	blast: a tool for error analysis of machine translation output	sprakteknologi sprakvetenskaplig databehandling;datavetenskap datalogi;language technology computational linguistics;computer science	We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part of MT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.	blast;biological anthropology;error analysis (mathematics);floor and ceiling functions;graphical user interface;machine translation;open-source software;usability	Sara Stymne	2011			natural language processing;speech recognition;transfer-based machine translation;computer science;machine learning	NLP	-33.08591529269987	-77.5959140762741	89409
7be5151cfbcb547eb255195a2a4c9f8e25fa6602	card-660: a reliable evaluation framework for rare word representation models				Mohammad Taher Pilehvar;Dimitri Kartsaklis;Victor Prokhorov;Nigel Collier	2018			artificial intelligence;natural language processing;machine learning;computer science	NLP	-31.335209838373377	-77.80286894346148	89417
ca3c5d52d0c586d259f64fbf13a1b01b2cf4d2e1	lys at clef replab 2014: creating the state of the art in author influence ranking and reputation classification on twitter		This paper describes our participation at RepLab 2014, a competitive evaluation for reputation monitoring on Twitter. The following tasks were addressed: (1) categorisation of tweets with respect to standard reputation dimensions and (2) characterisation of Twitter profiles, which includes: (2.1) identifying the type of those profiles, such as journalist or investor, and (2.2) ranking the authors according to their level of influence on this social network. We consider an approach based on the application of natural language processing techniques in order to take into account part-of-speech, syntactic and semantic information. However, each task is addressed independently, since they respond to different requirements. The official results confirm the competitiveness of our approaches, which achieve the 2nd place, tied in practice with the 1st place, at the author ranking task; and 3rd place at the reputation dimensions classification tasks.	bag-of-words model;categorization;machine learning;natural language processing;olami–feder–christensen model;parsing;part-of-speech tagging;preprocessor;requirement;social network	David Vilares;Miguel Hermo;Miguel A. Alonso;Carlos Gómez-Rodríguez;Jesús Vilares	2014			public relations;engineering;data mining;world wide web	NLP	-21.727744275698203	-68.7052861648319	89418
ee6e6e9b763009150dc2ab9397c99a7b74723643	automatic extraction of events from textual requirements specification	document summarization;uml;information filtering;text analysis;biology;data mining;dynamic behaviour;computer architecture;artificial neural networks;domain independent tool event automatic extraction textual requirements specification event partitioning approach object oriented analysis event extraction tools java english natural language;systems analysis;events;natural language;text analysis information filtering java natural language processing systems analysis;data mining natural languages ontologies computer science information technology java testing natural language processing unified modeling language writing;ontologies;functional requirement;object oriented analysis;requirement specification;use case;event extraction;natural language processing;context;domain specificity;document summarization events event extraction natural language processing dynamic behaviour uml;java	Events give important information about the behavior of a system in a summarized form. In the past, events have played an important role in breaking the functional requirements of the system in the “Event Partitioning Approach”. Our previous work has shown that Events can be a starting point in Object-Oriented Analysis of requirements. Every event triggers a Use Case in the system, hence should get a priority in identifying and analyzing requirements over Use Cases. In any system there is plethora of events happening, some are important to be recorded, while others are to be ignored. Moreover, there are various perspectives to define events. Thus, it becomes important to have an automated process that could help not only in extracting events but also analyze and classify them into various types. A study on various existing event extraction tools shows that they are either domain specific or take events as actions that occur at a particular time. There is no tool which extracts events that represent system behavior and at the same time gives a result that can be reused for application in multiple domains. This paper presents, a domain independent tool, developed in JAVA that automates the process of Extraction, Analysis and Classification of Events from Textual Requirements expressed in English as a Natural Language. This tool also assists the analysts in further refining identified events and to add some new events in the application domain. Tool has been tested on several case studies from different domains and has given very promising results.	application domain;brill tagger;computer performance;diagram;event partitioning;functional requirement;java;lexical analysis;named entity;natural language;unified modeling language	Sandeep Kumar Singh;Sangeeta Sabharwal;J. P. Gupta;Reetesh Gupta	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393565	object-oriented analysis and design;natural language processing;use case;unified modeling language;systems analysis;text mining;computer science;ontology;data mining;database;natural language;java;functional requirement;artificial neural network	SE	-30.04198294562084	-67.99954577116112	89453
25f88f41b0b3a3931a5f8e806e83925d2ad1a4cb	online handwritten script recognition	handwritten data retrieval;multilingual documents;handheld devices;handwritten data analysis;automatic identification;cyrillic script;devnagari script;roman script;feature extraction;hebrew script;han script;online operation;arabic script;automatic transcription;handwriting recognition;online handwritten script recognition;document image processing	Automatic identification of handwritten script facilitates many important applications such as automatic transcription of multilingual documents and search for documents on the Web containing a particular script. The increase in usage of handheld devices which accept handwritten input has created a growing demand for algorithms that can efficiently analyze and retrieve handwritten data. This paper proposes a method to classify words and lines in an online handwritten document into one of the six major scripts: Arabic, Cyrillic, Devnagari, Han, Hebrew, or Roman. The classification is based on 11 different spatial and temporal features extracted from the strokes of the words. The proposed system attains an overall classification accuracy of 87.1 percent at the word level with 5-fold cross validation on a data set containing 13,379 words. The classification accuracy improves to 95 percent as the number of words in the test sample is increased to five, and to 95.5 percent for complete text lines consisting of an average of seven words.	algorithm;automatic identification and data capture;cerebrovascular accident;extraction;han unification;mind;mobile device;multilingualism;online and offline;regular expression;transcription (software);world wide web;biologic segmentation;triangulation	Anoop M. Namboodiri;Anil K. Jain	2004	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.10009	natural language processing;speech recognition;feature extraction;computer science;machine learning;pattern recognition;mobile device;handwriting recognition;cross-validation	Web+IR	-24.548463834980595	-72.00372039479284	89471
2e2f501f9cc4484977e5f08a4adb7c9740ea5648	emotinet: a knowledge base for emotion detection in text built on the appraisal theories	action chain;emotinet;emotion ontology;appraisal theories;self reported affect;emotion detection;knowledge base	The automatic detection of emotions is a difficult task in Artificial Intelligence. In the field of Natural Language Processing, the challenge of automatically detecting emotion from text has been tackled from many perspectives. Nonetheless, the majority of the approaches contemplated only the word level. Due to the fact that emotion is most of the times not expressed through specific words, but by evoking situations that have a commonsense affective meaning, the performance of existing systems is low. This article presents the EmotiNet knowledge base - a resource for the detection of emotion from text based on commonsense knowledge on concepts, their interaction and their affective consequence. The core of the resource is built from a set of self-reported affective situations and extended with external sources of commonsense knowledge on emotion-triggering concepts. The results of the preliminary evaluations show that the approach is appropriate for capturing and storing the structure and the semantics of real situations and predict the emotional responses triggered by actions presented in text.	knowledge base;theory	Alexandra Balahur;Jesús M. Hermida;Andrés Montoyo;Rafael Muñoz	2011		10.1007/978-3-642-22327-3_4	natural language processing;knowledge base;computer science;artificial intelligence;commonsense knowledge	NLP	-27.90021049336809	-71.65152573950233	89608
0879ec901057b366c3221ab69ec81def04a30fc5	chinese spelling check system based on tri-gram model		This paper presents our system in the Chinese spelling check (CSC) task of SIGHAN-8 Bake-Off. Given a sentence, our systems are designed to detect and correct the spelling error. As we know, CSC is still a hot topic today and it is an open problem yet. N-gram language modeling (LM) is widely used in CSC, since its simplicity and power. We present a model based on joint bi-gram and trigram LM and Chinese word segmentation. Besides, we apply dynamic programming to increase efficiency and employ smoothing technique to address the sparseness of the n-gram in training data. The evaluation results show the utility of our CSC system.	additive model;additive smoothing;algorithm;computation;direction finding;dynamic programming;language model;n-gram;neural coding;smoothing;test set;text segmentation;trigram	Qiang Huang;Peijie Huang;Xinrui Zhang;Weijian Xie;Kaiduo Hong;Bingzhou Chen;Lei Huang	2014		10.3115/v1/W14-6827	spelling;gram;pattern recognition;computer science;artificial intelligence	NLP	-21.349052568353557	-77.40241038913358	89737
dbc7160df3660559da022a2b60aea0f9c87e8cbd	a parallel algorithm for statistical multiword term extraction from very large corpora	statistical extraction;random access memory;large corpora;text mining;system analysis and design;cloud;multiword terms;servers;cloud text mining large corpora multiword terms statistical extraction parallel processing;parallel processing cloud computing algorithm design and analysis servers data models random access memory system analysis and design;algorithm design and analysis;parallel processing;cloud computing;data models	"""Multi-word Relevant Expressions (REs) can be defined as sequences of words (n grams) with strong semantic meaning, such as """"ice melting"""" and """"Ministère des Affaires Étrangères"""", useful in Information Retrieval, Document Clustering or Classification and Indexing of Documents. The need of extracting REs in several languages led research on statistical approaches rather than symbolic methods, since the former allow language-independence. Based on the assumption that REthe LocalMaxs algorithms have strong cohesion between their consecutive n-grams, the LocalMaxs algorithm is a language independent approach that extracts REs. Apart from its good precision, this extractor is time-consuming, being inoperable for Big Data if implemented in a sequential manner. This paper presents the first parallel and distributed version of this algorithm, achieving almost linear speedup and sizeup when processing corpora up to 1 billion words, using up to 54 virtual machines in a public cloud. This parallel version of the algorithm explores the statistical knowledge of the n grams in the corpus, to promote the locality of the references."""	aggregate data;analysis of algorithms;big data;boolean expression;cpu cache;cloud computing;cluster analysis;data structure;experiment;grams;information retrieval;locality of reference;n-gram;operability;parallel algorithm;randomness extractor;scalability;speedup;terminology extraction;text corpus;tree accumulation;virtual machine	Carlos Gonçalves;Joaquim F. Silva;José C. Cunha	2015	2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems	10.1109/HPCC-CSS-ICESS.2015.72	natural language processing;parallel processing;text mining;parallel computing;cloud computing;computer science;operating system;database;information retrieval	HPC	-30.714152249826245	-69.58286787178312	89888
652ad8c2029aaeb212dea89ab1fec8f9017b2fd7	similarity between term senses in a lexical network		Typed lexical relations between terms are indispensable for the tasks realized in NLP, but collecting lexical information is a difficult process. Indeed, when done manually, it requires the competence of experts and the duration can be prohibitive. When done automatically, the results can be biased by the chosen corpus of texts. The approach we present here consists in having people take part in a collective project by offering them a playful application accessible on the web. From an already existing base of terms, the players themselves thus build the lexical network, by supplying associations which are validated only by an agreeing pair of users. Furthermore, these typed relations are weighted according to the number of pairs of users who provide them. We then approach the question of word usage determination for a term, by searching relations between this term and its neighbours in the network, before introducing the notion of similarity between these different word usages. We are thus able to build the tree of word usages for a term. Finally, we briefly present the realization and the first obtained results. MOTS-CLÉS : traitement automatique du langage naturel, réseau lexical, relations typées pondérées, sens d’usage d’un terme, jeu en ligne.	natural language processing;relation (database)	Mathieu Lafourcade;Alain Joubert	2009	TAL		lexical chain;artificial intelligence;computer science;pattern recognition	NLP	-27.545664136620182	-71.40708618148396	89987
310b8073bc022b2a4ac90b6b6060a72225fe18a7	constructing specialised corpora through analysing domain representativeness of websites	site web;langue de specialite;traitement automatique des langues naturelles;specialised corpus;search engine;boilerplate removal;reconnaissance de mot;virtual corpus;linguistique de corpus;language for specific purposes;internet;corpus;web derived corpus;word recognition;website ranking;corpus linguistics;web corpus;corpus construction;term recognition;natural language processing;large text collections	The role of the Web for text corpus construction is becoming increasingly significant. However, the contribution of the Web is largely confined to building a general virtual corpus or low quality specialised corpora. In this paper, we introduce a new technique called SPARTAN for constructing specialised corpora from the Web by systematically analysing website contents. Our evaluations show that the corpora constructed using our technique are independent of the search engines employed. In particular, SPARTAN-derived corpora outperform all corpora based on existing techniques for the task of term recognition.	download;edgehtml;experiment;f1 score;information extraction;precision and recall;sensitivity and specificity;spartan;terminology extraction;text corpus;vocabulary;web crawler;web page;web search engine;world wide web	Wilson Wong;Wei Liu;Mohammed Bennamoun	2011	Language Resources and Evaluation	10.1007/s10579-011-9141-4	natural language processing;the internet;speech recognition;word recognition;computer science;corpus linguistics;linguistics;information retrieval;search engine	NLP	-28.787353900290455	-73.90824204889923	90037
3d036f8d3d5d19fe1464c7f85a57a81cd6293154	an annotated corpus of quoted opinions in news articles		Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.	computable function;cyclic redundancy check;point of view (computer hardware company);text corpus	Timothy O'Keefe;James R. Curran;Peter Ashwell;Irena Koprinska	2013			multimedia;world wide web;information retrieval	AI	-29.006688897409052	-70.46212414564057	90215
6c033b3d4ad911804a5945c75c80f227e40eb752	an attempt to automatic thesaurus construction from an ordinary japanese language dictionary	machine readable form;ordinary japanese language dictionary;important problem;mechanical extraction;synonym relation;automatic thesaurus construction;definition sentence;superordinate-hyponym relation;hierarchical relation;pilot system;shinmeikai kokugojiten	How to obta in h i e r a r ch i ca l r e l a t i o n s ( e . g , superord ina te -hyponym relation, synonym r e l a t i o n ) is one of the most important problems for thesaurus cons t ruc t ion , A p i lo t system for ex t r ac t ing these r e l a t i o n s au tomat ica l ly from an ordinary Japanese language d i c t i ona ry (Shinmeikai Kokugojiten, published hy Sansei-do, in machine readable form) i s given. The f e a t u r e s of the definition sentences in the d i c t iona ry , the mechanical ex t r ac t i on of the h i e r a r ch i ca l r e l a t i o n s and the es t ima t ion of the r e s u l t s are d i scussed .	dictionary;human-readable medium;original net animation;thesaurus	Hiroaki Tsurumaru;Toru Hitaka;Sho Yoshida	1986			natural language processing;speech recognition;machine-readable dictionary;computer science;linguistics	AI	-30.590013280113542	-79.02299648585577	90357
0f310daf6fc761c3bf1649ebab37f8c2e92a857e	do characters abuse more than words?		Although word and character n-grams have been used as features in different NLP applications, no systematic comparison or analysis has shown the power of character-based features for detecting abusive language. In this study, we investigate the effectiveness of such features for abusive language detection in user-generated online comments, and show that such methods outperform previous state-of-theart approaches and other strong baselines.	baseline (configuration management);deep learning;grams;language identification;n-gram;natural language processing;regular expression;sensor;text-based (computing);user-generated content	Yashar Mehdad;Joel R. Tetreault	2016				NLP	-21.48953773550884	-68.10462518624563	90400
a0ba35a287ee89f0a9830f6a7e21e9631f836ed7	training lstm-rnn with imperfect transcription: limitations and outcomes		Bidirectional LSTM-RNN have become one of the standard methods for sequence learning, especially in the context of OCR due to its ability to process unsegmented data and its inherent statistical language modeling [5]. It has recently been shown that training LSTM-RNNs even with imperfect transcriptions can lead to improved transcription results [7, 14]. The statistical nature of the LSTM's inherent language modeling can compensate for some of the errors in the ground truth and learn the correct temporal relations. In this paper we systematically explore the limits of the LSTM's language modeling ability by comparing the impact of imperfect transcriptions with various hand crafted error types and real erroneous data created through segmentation and clustering. We show that training LSTM-RNN with imperfect transcriptions can produce useful OCR models even if the ground truth error is up to 20%. Further we show that it can compensate for some handcrafted error types with error rates of up to 40% almost perfectly.	cluster analysis;ground truth;language model;long short-term memory;medical transcription;optical character recognition;random neural network;transcription (software)	Martin Jenckel;Syed Saqib Bukhari;Andreas Dengel	2017		10.1145/3151509.3151527	transcription (linguistics);cluster analysis;sequence learning;imperfect;language model;ground truth;pattern recognition;computer science;artificial intelligence	NLP	-23.08123045197956	-78.60678117445823	90599
be103f7cd975ea13e98e10ea419666923c89b665	filling the gap: semi-supervised learning for opinion detection across domains	domain transfer;sparse data situation;best result;semi-supervised learning;target domain training data;domain adaptation;opinion detection;blog data;absolute improvement;maximum absolute gain	We investigate the use of Semi-Supervised Learning (SSL) in opinion detection both in sparse data situations and for domain adaptation. We show that co-training reaches the best results in an in-domain setting with small labeled data sets, with a maximum absolute gain of 33.5%. For domain transfer, we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16% over the supervised approach with target domain training data.	baseline (configuration management);blog;co-training;domain adaptation;lexicon;sl (complexity);semi-supervised learning;semiconductor industry;sparse matrix;supervised learning	Ning Yu;Sandra Kübler	2011			semi-supervised learning;computer science;machine learning;pattern recognition;data mining	NLP	-19.783118281701032	-66.3716841525953	90834
388c4c4203b58912186ea56e0f3adf99b3ecea6e	an impact analysis of features in a classification approach to irony detection in product reviews		Irony is an important device in human communication, both in everyday spoken conversations as well as in written texts including books, websites, chats, reviews, and Twitter messages among others. Specific cases of irony and sarcasm have been studied in different contexts but, to the best of our knowledge, only recently the first publicly available corpus including annotations about whether a text is ironic or not has been published by Filatova (2012). However, no baseline for classification of ironic or sarcastic reviews has been provided. With this paper, we aim at closing this gap. We formulate the problem as a supervised classification task and evaluate different classifiers, reaching an F1-measure of up to 74 % using logistic regression. We analyze the impact of a number of features which have been proposed in previous research as well as combinations of them.	baseline (configuration management);book;closing (morphology);f1 score;irony;logistic regression;machine learning;supervised learning	Konstantin Buschmeier;Philipp Cimiano;Roman Klinger	2014			speech recognition;computer science;artificial intelligence;communication	NLP	-21.490280045246628	-68.20069784404696	90837
d8302795ff8676612c324f973ac595cc2f8ff364	si3trenn and si3silb: using the sisisi word analysis system for pre-hyphenation and syllable counting in german documents	recuento;lenguaje natural;legibility;comptage;syllabe;text;analisis estadistico;preprocesor;latex;preprocesseur;preprocessor;langage naturel;enumeration counting;text analysis;tratamiento lenguaje;texte;denombrement;probabilistic approach;contaje;analyse texte;syllable;statistical analysis;language processing;enfoque probabilista;approche probabiliste;natural language;analyse statistique;traitement langage;counting;legibilidad;lisibilite;silaba;texto	We present two applications of a word analysis system for the German language: pre-hyphenation of documents in various formats, and counting the syllables of all words of a document. The Si3Trenn preprocessor provides pre-hyphenation for file formats allowing for soft hyphens (currently: plain text, LTEX, RTF). It applies reliable, senseconveying hyphenation (SiSiSi) to each word of the input text and inserts soft hyphens directly into the text. The resulting document can be processed as usual; soft hyphens will be used for hyphenation at the end of lines if appropriate. The Si3Silb syllable counter is a helpful tool for the statistical analysis of texts, e.g. in readability studies.	counterfactual conditional;experiment;html;hyphenation algorithm;java;latex;microsoft word for mac;personal digital assistant;preprocessor;syllable	Gabriele Kodydek;Martin Schönhacker	2003		10.1007/978-3-540-39398-6_10	natural language processing;text mining;speech recognition;computer science;linguistics;natural language;syllable;preprocessor;counting	NLP	-25.66546658012183	-78.32014243230074	90858
6a3911ab7d62aa1f5e337be583c931a257ac577f	effectiveness of syntactic information for document classification	nombre propio;nom propre;coreano;fiabilidad;reliability;inverse document frequency;noun;noun phrase;heuristic method;weighting;verbe;metodo heuristico;term frequency;ponderacion;classification;proper noun;korean;voting;coreen;fiabilite;verbo;voto;methode heuristique;ponderation;document classification;vote;clasificacion;verb	This paper compares effectiveness of document classification algorithms for a highly inflectional/derivational language that forms monolithic compound noun terms, like Korean. The system is composed of three phases: (1) a Korean morphological analyser called HAM [10], (2) compound noun phrase analysis and extraction of terms whose syntactic categories are noun, proper noun, verb, and adjective, and (3) various document classification algorithms based on preferred class score heuristics. We focus on the comparison of document classification methods including a simple voting method, and preferred class score heuristics employing two factors, namely ICF (inverse class frequency) and IDF (inverse document frequency) with/without term frequency weighting. In addition, this paper compares algorithms that use different class feature sets filtered by four syntactic categories. Compared to the results of algorithms that are not using syntactic information for class feature sets, the algorithms using syntactic information for class feature sets shows performance differences in this paper by -3.3% 4.7%. Of the 20 algorithms that were tested, the algorithms, PCSIDF FV (i.e. Filtering Verb Terms) and Weighted PCSIDF FV, show the best performance (74.2% of F-measurement ratio). In the case of the Weighted PCSICF algorithm, the use of syntactic information for selection of class feature sets decreased the performance on document classification by 1.3 3.3%.		Kyongho Min;William H. Wilson	2003		10.1007/978-3-540-24581-0_85	natural language processing;speech recognition;computer science;mathematics;tf–idf	NLP	-26.663405313081437	-77.12690345205391	90868
43e24832ac3e5081c263d88b1071578b7b9ef4ec	split and rephrase		We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labelers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning.1 Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.	benchmark (computing);dbpedia;experiment;level of detail;machine translation;multi-source;natural language generation;natural language processing;parsing;preprocessor;rico;text corpus;text simplification	Shashi Narayan;Claire Gardent;Shay B. Cohen;Anastasia Shimorina	2017			machine learning;balanced sentence;artificial intelligence;computer science;machine translation;natural language processing;parsing;preprocessor;sentence	NLP	-24.18445520638462	-74.76491024893251	91027
6e974a062d44ef202ce63577e1e78c587b51b977	identification and handling of intensifiers for enhancing accuracy of urdu sentiment analysis				Neelam Mukhtar;Mohammad Abid Khan;Nadia Chiragh;Shah Nazir	2018	Expert Systems	10.1111/exsy.12317	data mining;computer science;sentiment analysis;urdu	HCI	-30.342711764860653	-76.60681322668357	91054
0ea79868e86ca47bd5d0c732afdb2a26bbec3b1a	the gidoc prototype	natural language processing	Transcription of handwritten text in (old) documents is an important, time-consuming task for digital libraries. In this paper, an efficient interactivepredictive transcription prototype called GIDOC (Gimp-based Interactive transcription of old text DOCuments) is presented. GIDOC is a first attempt to provide integrated support for interactive-predictive page layout analysis, text line detection and handwritten text transcription. It is based on GIMP and uses advanced techniques and tools for language and handwritten text modelling. Results are given on a real transcription task on a 764-page Spanish manuscript	digital library;edge detection;fast software encryption;feature extraction;floating-point unit;gimp;gnu;hidden markov model;language model;library (computing);medical transcription;preprocessor;prototype;serial digital video out;transcription (software)	Nicolás Serrano;Lionel Tarazón;Daniel Pérez;Oriol Ramos Terrades;Alfons Juan-Císcar	2010			prototype	HPC	-32.81376896084723	-75.42852121097674	91092
57f1be50af57c17fa56d7addafd32871b96cdd52	conditioned unification for natural language processing	natural language;natural language processing	"""A]]STV~ACT This paper prescnLs what wc c.all a condiLiol'md unification, a r'm'w meLhod of unificatiol'~ for processing natural languages. The key idea is to annotate Lhe patterns wiLh a cerLcdn sort of conditions, so that they carry abundant inforrnation. ']'his met.bed t.ransmits inforrnaLion frorn one pattern to another more efl'icienLly Lhan proecdurc aLLachmenLs, in which information cortLaincd in the procedure is embcddcd in the progranl rather Lhan dirccl./y aLLachcd Lo paLL(ms Coupled wilt techniques in forrnal linguistics> i]]orcovcr, conditiorled unification serves most. types o1"""" opcrations for natlu'ai ]ar/guage processil'q~,."""	han unification;natural language processing	Kôiti Hasida	1986		10.3115/991365.991388	natural language processing;language identification;natural language programming;computer science;linguistics;natural language;programming language	DB	-30.6009747697228	-78.0794031929205	91116
dcc918ac1178abae6875286530d276ebb2d4c2bb	dimensions of deep grammar validation		In order to arrive at a more disciplined approach to the sustained development of linguistically rich grammars, I present a methodology for grammar validation, identifying principal dimensions of the task, and illustrating the application of the method for one release cycle of the open-source English Resource Grammar.	open-source software;software release life cycle	Dan Flickinger	2005			natural language processing;speech recognition;linguistics	NLP	-29.210519828273302	-78.77185502283874	91152
c9beb9d101fe4a47715c2dd4a4a31537051a1175	multifunctional computational lexicon of contemporary portuguese: an available resource for multitype applications		This paper presents some aspects of the first Portuguese frequency lexicon extracted from a corpus of large dimensions. The Multifunctional Computational Lexicon of Contemporary Portuguese (henceforth MCL) rised from the necessity of filling a gap existent in the studies of the contemporary Portuguese. Until recently, the frequency lexicons of Portuguese were of very small dimensions, such as Português Fundamental, which is constituted by 2.217 words extracted from a 700.000 word corpus and the Frequency Dictionary of Portuguese Words based on a literary corpus of 500.000 words. We describe here the main steps taken for collecting the lexical and frequency data and some of the major problems that arouse in the process. The resulting lexicon is a freely available reliable resource for several types of applications.	computation;dictionary;lexicon;macintosh common lisp;multi-function printer;word lists by frequency	Florbela Barreto;Raquel Amaro	2004			artificial intelligence;natural language processing;linguistics;computer science;lexicon;portuguese	NLP	-30.62334883000124	-75.17402499574594	91260
c3b2795a965643462a065eabc0fd617f23bb1854	building information extraction system based on computing domain ontology	knowledge based system;information extraction;semantic relation;domain ontology	In this paper, we present an Information Extraction (IE) system, which is built from unstructured text based on Computing domain ontology. The IE system comprises four sequential processing steps: preprocessing, topic identifier, building domain specific ontology and extracting information from text corpus. The first two steps perform generic Natural Language Processing (NLP) and Machine Learning tasks, while the last two phases are application-specific and require a thorough understanding of the application domain. Furthermore, the paper focuses on evaluating the IE IEsystem by selected methods. One of these methods that we introduced here, is comparative. Comparative evaluation performed in this study use of Key Exchange Algorithm with the same corpus to contrast results. Results generated by such experiments show that this IE system outperforms Key Exchange Algorithm, respectably.	algorithm;application domain;experiment;identifier;information extraction;key exchange;machine learning;natural language processing;ontology (information science);preprocessor;text corpus;text-based (computing)	Chien D. C. Ta;Tuoi Phan Thi	2014		10.1145/2684200.2684329	natural language processing;computer science;ontology;data mining;information retrieval	NLP	-30.80820825736985	-71.32773501426942	91468
8a7db2720d7711a4d65579030c4c7c2b899d08b4	evaluating unsupervised learning for natural language processing tasks	unsupervised natural language processing;future work;in-context evaluation;evaluation paradigm unsuitable;unsupervised part-of-speech;annotated data;natural language processing task;case study;unsupervised learning method;primary advantage	The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. The primary advantage of these methods is that they do not require annotated data to learn a model. However, this advantage makes them difficult to evaluate against a manually labeled gold standard. Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. Instead, we argue that the rarely used in-context evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.	downstream (software development);information;natural language processing;part-of-speech tagging;preprocessor;programming paradigm;text corpus;unsupervised learning	Andreas Vlachos	2011			natural language processing;unsupervised learning;computer science;artificial intelligence;machine learning;data mining	NLP	-26.32518798194343	-72.84810420609993	91492
5eaab80b6be7357653dbcabcdf2595468760d034	relation guided bootstrapping of semantic lexicons	relationship constraint;supervised knowledge;substantial amount;open relationship;lexicon growth;expert-crafted semantic constraint;state-of-the-art bootstrapping system;semantic lexicon;extracts lexicon;negative category;semantic drift;category	State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.	algorithm;carlson's theorem;interpreter (computing);lexicon;relationship extraction;supervised learning;the australian;unsupervised learning	Tara McIntosh;Lars Yencken;James R. Curran;Timothy Baldwin	2011			natural language processing;bootstrapping;computer science;pattern recognition;data mining	NLP	-21.993259821557192	-71.65894860732824	91511
66ae28c4e58ec6376632c760529f4b030ee4252d	noise-robust morphological disambiguation for dialectal arabic		User-generated text tends to be noisy with many lexical and orthographic inconsistencies, making natural language processing (NLP) tasks more challenging. The challenging nature of noisy text processing is exacerbated for dialectal content, where in addition to spelling and lexical differences, dialectal text is characterized with morpho-syntactic and phonetic variations. These issues increase sparsity in NLP models and reduce accuracy. We present a neural morphological tagging and disambiguation model for Egyptian Arabic, with various extensions to handle noisy and inconsistent content. Our models achieve about 5% relative error reduction (1.1% absolute improvement) for full morphological analysis, and around 22% relative error reduction (1.8% absolute improvement) for part-of-speech tagging, over a state-of-the-art baseline.	approximation error;baseline (configuration management);computer multitasking;deep learning;long short-term memory;natural language processing;noisy text;orthographic projection;part-of-speech tagging;space mapping;sparse matrix;window function;word-sense disambiguation	Nasser Zalmout;Alexander Erdmann;Nizar Habash	2018			arabic;natural language processing;artificial intelligence;computer science	NLP	-20.4129163704497	-74.42497293493294	91516
c0f1047fe0f95c367184d494e78bb07b11ee3608	recognizing semantic relations within polish noun phrase: a rule-based approach		The paper1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora.	experiment;logic programming;ontology components;text corpus	Pawel Kedzia;Marek Maziarz	2013			nominalization;noun phrase;determiner phrase	AI	-25.57526557509329	-69.82701941418122	91542
2c69e02b319ea809725aa28462c2fbbde91cf20f	mixed language query disambiguation	contextual word voting;average query translation accuracy;baseline accuracy;primary language;mixed language query;monolingual query;mixed language query disambiguation;secondary language;baseline feature;1-best contextual word;nearest neighbor	We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only. A mixed language query consists of words in a primary language and a secondary language. Our method translates the query into monolingual queries in either language. Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor. Average query translation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy	baseline (configuration management);word-sense disambiguation	Pascale Fung;Xiaohuo Liu;Chi Shun Cheung	1999			natural language processing;query optimization;web query classification;speech recognition;computer science;pattern recognition;k-nearest neighbors algorithm;query language	NLP	-24.876282582358225	-71.60249560511875	91726
5299c4c50b1ac8a085e5cc7839b9d5b49736ef34	construction and implementation of the quantifier-noun collocation knowledge base for proofreading based on multiple knowledge sources	knowledge acquisition;quantifier-noun collocation;quantifier-noun collocation knowledge base;semantic collocation	"""Due to specific semantic constraints between quantifiers and nouns and the frequent phenomena of quantifier-noun collocation error in real text, this paper proposes a new model of extracting quantifier-noun collocation and a new representation method of knowledge base. In this paper, by rules and statistical methods and the use of """"The Grammatical Knowledge-base of Contemporary Chinese"""" and the """"People's Daily"""" corpus resources, word-level quantifier-noun collocation knowledge base was established. On this basis, by analyzing the characteristics of the nouns and combining semantic class attributes in the """"The Semantic Knowledge-base of Contemporary Chinese"""", semantic-level quantifier-noun collocation knowledge base was established. The experiment results show that the accuracy of checking quantifier-noun collocation errors reaches 77.38% by using quantifier-noun collocation knowledge base built by this method, and the scale of this knowledge base is expanded effectively. © 2013 Springer-Verlag."""	collocation;knowledge base;quantifier (logic)	Lin Jiang;Yangsen Zhang;Jun Guan;Zhenlei Du	2013		10.1007/978-3-642-45185-0_62	computer science;knowledge management;theoretical computer science;data mining	NLP	-29.494559882938677	-70.09901157064995	91768
7adafd5cd75fd3b14f01a1e4768697072397cbba	exona results for oaei 2015		This paper presents the results of EXONA in the Ontology Alignment Evaluation Initiative (OAEI) 2015. EXONA is an automatic instance-based ontology alignment systems in which we parse ontology as rst step. In the second step, we index instances of the rst ontology. These indexed instances will be applied for the querying phase. In the last step, our system aligns instances based by aggregating score of di erent terminological matchers. We rst describe the overall framework of our matching System (EXONA) then we detail the techniques used in the framework for instance matching. Last, we give a thorough analysis on our results and discuss some future work on our system. It's our rst participation in the OAEI instance matching, the results are good in terms of recall, precision and F-measure. 1 Presentation of the system Ontology matching is a key interoperability enabler for the semantic web, as well as a useful tactic in data integration tasks. Knowledge about one object may be contained in multiple and di erent knowledge bases. Therefore, a lot of work has already been built to obtain more complete knowledge about things existing in di erent domains. This is in order to exceed the area of divergence obstacle, by creating cross-domain knowledge. Accordingly, it's strongly recommended to focus on the more active element of ontology which it called instance. Many instances matching approaches have been proposed, and among which is ours. In fact, our system is proposed for large scale instance matching. It operates on three successive modules, namely : transformation, indexation and correspondence. Transformation consists in transforming separately both of knowledge bases on an exploitable form and then creating our own instance object as a profession object. Indexation is the process of indexing instances of knowledge base; only instances of the source base knowledge	correspondence problem;f1 score;instance (computer science);interoperability;knowledge base;ontology alignment;parsing;semantic web	Syrine Damak;Hazem Souid;Marouen Kachroudi;Sami Zghal	2015				AI	-30.265957290814434	-66.23893541681929	91803
1c7aac6212d52e23f0e54ea7c5b6ea91d34dae06	the isi/usc mt system		The ISI/USC machine translation system is a statistical system based on a phrase translation model that is trained on bilingual parallel data. This translation model is combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the log-linear model are set by an automatic parameter-tuning method. The system described here has been developed for translating news text, and is a simplified version of the one we participated with in the NIST 2004 MT evaluation. We give a brief overview of the components of the system and discuss its performance at IWSLT. 1. The ISI/USC Machine Translation System Our machine translation system uses a log-linear model to combine several different knowledge sources into a direct model of translation. The 12 different models used to score hypothesized translations are given in Table 1. We also give more in-depth descriptions of the major components. 1.1. Translation Model At the core of the system is the alignment template translation model, which learns many-to-many mappings between word sequences from parallel bilingual data. A sentence is translated by segmenting a source-language sentence into phrases, translating these phrases with the ones observed in the training data, and reordering the target-language phrases. More details about the alignment template approach to machine translation used here are given in [1], [2]. For the IWSLT evaluation for Chineseand Japanese-toEnglish, we trained the alignment template system on the 20,000 lines of bilingual basic travel expressions provided by the organizers. For the “additional” evaluation condition for Chinese, we used 6 of the allowed corpora provided by LDC. For the “unrestricted” evaluation condition for Chinese, we used 167M words of parallel news and political data obtained from LDC in addition to the provided data. When mixing the provided in-domain data with out-of-domain data, the in-domain data was weighted by a factor of 5, and was resegmented with the LDC segmenter. 1Now at Google, Inc. 1.2. Language Model A smoothed trigram model was also used to score hypothesized translations. We used the SRI Language Modelling Toolkit to train a language model smoothed with Kneser-Ney discounting. For all of the evaluation conditions, a language model was trained on the English half of the parallel corpus used for alignment-template training. For the “additional” and “unrestricted” evaluation conditions, an additional language model was used that was trained on 800M words of monolingual news text. Each language model is considered an independent information source, and is weighted separately in the global log-linear model. 1.3. Minimum Error Rate Training The individual model weights of the log-linear model are set using a parameter tuning procedure that minimizes the error rate of a given evaluation function (such as the BLEU score) on a held-out test corpus. Setting model weights in order to minimize the error of the function used for testing has been shown to provide better results than maximumlikelihood training [3]. For this evaluation, we optimize parameters to achieve the best performance with respect to the BLEU score. We split the provided development data into two equally sized corpora that were used separately for minimum error training and testing.	bleu;evaluation function;information sciences institute;information source;language model;linear model;linguistic data consortium;log-linear model;metal mt;machine translation;many-to-many;parallel text;smoothed analysis;smoothing;template processor;text corpus;trigram;unified soil classification system	Ignacio Thayer;Emil Ettelaie;Kevin Knight;Daniel Marcu;Dragos Stefan Munteanu;Franz Josef Och;Quamrul Tipu	2004			computer science	NLP	-21.223539973499665	-77.8916049774064	91873
1816c5119e13937daf61f9820b274c36071f0ebe	chinese new word identification: a latent discriminative model with global features	new word identiflcation;new words pos tagging;conditional random flelds;hidden semi-crf;global fragment features	Chinese new words are particularly problematic in Chinese natural language processing. With the fast development of Internet and information explosion, it is impossible to get a complete system lexicon for applications in Chinese natural language processing, as new words out of dictionaries are always being created. The procedure of new words identification and POS tagging are usually separated and the features of lexical information cannot be fully used. A latent discriminative model, which combines the strengths of Latent Dynamic Conditional Random Field (LDCRF) and semi-CRF, is proposed to detect new words together with their POS synchronously regardless of the types of new words from Chinese text without being pre-segmented. Unlike semi-CRF, in proposed latent discriminative model, LDCRF is applied to generate candidate entities, which accelerates the training speed and decreases the computational cost. The complexity of proposed hidden semi-CRF could be further adjusted by tuning the number of hidden variables and the number of candidate entities from the Nbest outputs of LDCRF model. A new-word-generating framework is proposed for model training and testing, under which the definitions and distributions of new words conform to the ones in real text. The global feature called “Global Fragment Features” for new word identification is adopted. We tested our model on the corpus from SIGHAN-6. Experimental results show that the proposed method is capable of detecting even low frequency new words together with their POS tags with satisfactory results. The proposed model performs competitively with the state-of-the-art models.	algorithmic efficiency;artificial intelligence;brown corpus;computational complexity theory;conditional random field;dictionary;discriminative model;entity;enumerated type;feature selection;hidden variable theory;high-level programming language;information explosion;internet;lexicon;machine translation;n-gram;natural language processing;natural language understanding;part-of-speech tagging;phrase chunking;protologism;semiconductor industry;sensor;shallow parsing;text segmentation;trigram	Xiao Sun;Degen Huang;Hai-Yu Song;Fuji Ren	2011	Journal of Computer Science and Technology	10.1007/s11390-011-9411-z	natural language processing;speech recognition;computer science;machine learning;pattern recognition;conditional random field	AI	-22.87464788600357	-75.58136441639309	91876
e313940d6836df1653bb765fb2e96da77d7500e0	poor man's ocr post-correction: unsupervised recognition of variant spelling applied to a multilingual document collection		The accuracy of Optical Character Recognition (OCR) is sets the limit for the success of subsequent applications used in text analyzing pipeline. Recent models of OCR postprocessing significantly improve the quality of OCR-generated text but require engineering work or resources such as human-labeled data or a dictionary to perform with such accuracy on novel datasets. In the present paper we introduce a technique for OCR post-processing that runs off-the-shelf with no resources or parameter tuning required. In essence, words which are similar in form that are also distributionally more similar than expected at random are deemed OCR-variants. As such it can be applied to any language or genre (as long as the orthography segments the language at the word-level). The algorithm is illustrated and evaluated using a multilingual document collection and a benchmark English dataset.	algorithm;archive;benchmark (computing);dictionary;optical character recognition;python;unsupervised learning;video post-processing	Harald Hammarström;Shafqat Mumtaz Virk;Markus Forsberg	2017		10.1145/3078081.3078107	orthography;optical character recognition;spelling;speech recognition;artificial intelligence;computer science;pattern recognition	NLP	-21.931129794729387	-75.61929738547829	91881
cb2023f243a50ed84e045135baec85363f274201	discussion note: bad news for anyone? - a reply to abbott	filologias;article letter to editor;linguistica;grupo a			Bart Geurts	2002	J. Semantics	10.1093/jos/19.2.203	literature	PL	-29.59715128541437	-77.61897820857698	91955
4a65cb0562810d010e8ca125bd752b78c53a0866	an infinite hierarchical bayesian model of phrasal translation		Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.	baseline (configuration management);bayesian network;expectation propagation;experiment;machine translation;metropolis;metropolis–hastings algorithm;newman's lemma;parallel text;recursion;sampling (signal processing);text corpus;transduction (machine learning)	Trevor Cohn;Gholamreza Haffari	2013			dynamic and formal equivalence;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;rule-based machine translation	NLP	-20.564743271593844	-76.66630709448297	92070
798e9f0480bf688e2ec548e56cbbdab23da1d33b	harnessing the expertise of 70, 000 human editors: knowledge-based feature generation for text categorization		Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation.	algorithm;apple open directory;bag-of-words model;categorization;chunking (computing);commonsense knowledge (artificial intelligence);directory (computing);document classification;focused crawler;image noise;information repository;information retrieval;knowledge base;knowledge-based systems;machine learning;map;mathematical induction;multiresolution analysis;natural language processing;ontology (information science);rm-odp;relevance feedback;semi-supervised learning;semiconductor industry;supervised learning;test set;web crawler;web page;word sense;word-sense disambiguation	Evgeniy Gabrilovich;Shaul Markovitch	2007	Journal of Machine Learning Research			AI	-29.83798977010632	-67.84063619450883	92095
2930c8037ed9b7b7a6353b313402e788ccf5765f	lexical bias in essay level prediction		Automatically predicting the level of nonnative English speakers given their written essays is an interesting machine learning problem. In this work I present the system balikasg that achieved the state-of-theart performance in the CAp 2018 data science challenge among 14 systems. I detail the feature extraction, feature engineering and model selection steps and I evaluate how these decisions impact the system’s performance. The paper concludes with remarks for future work.	data science;feature engineering;feature extraction;machine learning;model selection	Georgios Balikas	2018	CoRR		natural language processing;machine learning;model selection;artificial intelligence;feature extraction;feature engineering;computer science	NLP	-20.813959243439584	-70.85347729140362	92128
8b74a718629fed469fd95f14b9ff0e31740afa6e	a dutch component for a multilingual systemic text generation system	institutional repositories;fedora;vital;cumulant;vtls;text generation;ils	This paper describes how a fully new linguistic component, a Dutch grammar fragment, has been added to an existing multilingual systemic generation system. The system has an architecture based on functional typology which allows the cumulative addition of other languages without revising the overall system and with much re-use of resources across languages. We have indeed found that despite the possible expectation that divergences in linguistic resources would occur early in the systemic classification and that sharing would therefore be minimal, this is not the case. The functional organization of the systemic network does appear to offer considerable re-use of resources.		Liesbeth Degand	1993		10.1007/3-540-60800-1_39	natural language processing;speech recognition;data science;statistics;cumulant	NLP	-29.147973121058648	-74.66633828110308	92209
b8a8c1a2b5d9a3d9ebffc5543016994af6007811	enrichir et raisonner sur des espaces sémantiques pour l'attribution de mots-clés (enriching and reasoning on semantic spaces for keyword extraction) [in french]		Enriching and reasoning on semantic spaces for keyword extraction This article presents a multi-modular hybrid system for extraction of keywords from corpus of scientific articles. System is multi-modular because it integrates components executing transformations on 1) morphosyntactic level (lemmatization and chunking) 2) semantic level (Reflected Random Indexing), as well as upon more 3) « pragmatic » aspects of processed documents, modeled by production rules. The system is hybrid because it was able to address both tracks of DEFT2012 competition – a «reduced search-space» scenario of Track 1, whose objective was to map the content of a scientific article upon one among the members of a « terminological list » ; as well as more « real-life » scenario of Track2 within which no list was associated to documents contained in the corpus. In both Tracks, the system hereby presented has obtained the an F-score of 0.9488 for the Track1, and 0.5874 for the Track2. MOTS-CLÉS : Extraction de mots-clés, Espaces sémantiques, RRI, Réseau bayésien, Règles de production, Chunking.	bibliothèque de l'école des chartes;hybrid system;keyword extraction;lemmatisation;random indexing;real life;scientific literature;semantic web;shallow parsing;text corpus	Adil El Ghali;Daniel Hromada;Kaoutar El Ghali	2012				NLP	-33.226152322713695	-73.03386094618385	92337
0229c0eb39efed90db6691469daf0bb7244cf649	text classification and named entities for new event detection	named entities;vector space;document representation;text classification;topic detection and tracking;new event detection;named entity	New Event Detection is a challenging task that still offers scope for great improvement after years of effort. In this paper we show how performance on New Event Detection (NED) can be improved by the use of text classification techniques as well as by using named entities in a new way. We explore modifications to the document representation in a vector space-based NED system. We also show that addressing named entities preferentially is useful only in certain situations. A combination of all the above results in a multi-stage NED system that performs much better than baseline single-stage NED systems.	baseline (configuration management);document classification;named entity;named-entity recognition	Giridhar Kumaran;James Allan	2004		10.1145/1008992.1009044	vector space;computer science;pattern recognition;data mining;information retrieval	Web+IR	-25.398195516774404	-67.48159186684164	92341
393158606639f86f4d3ca2e89d15bd38b59577e1	transliteration for resource-scarce languages	cross lingual information retrieval;cross entropy;transliteration;prefix based partial match ppm;rule based system;rule based;resource scarce languages;character sequence modeling	Today, parallel corpus-based systems dominate the transliteration landscape. But the resource-scarce languages do not enjoy the luxury of large parallel transliteration corpus. For these languages, rule-based transliteration is the only viable option. In this article, we show that by properly harnessing the monolingual resources in conjunction with manually created rule base, one can achieve reasonable transliteration performance. We achieve this performance by exploiting the power of Character Sequence Modeling (CSM), which requires only monolingual resources. We present the results of our rule-based system for Hindi to English, English to Hindi, and Persian to English transliteration tasks. We also perform extrinsic evaluation of transliteration systems in the context of Cross Lingual Information Retrieval. Another important contribution of our work is to explain the widely varying accuracy numbers reported in transliteration literature, in terms of the entropy of the language pairs and the datasets involved.	entropy (information theory);information retrieval;logic programming;parallel text;rule-based system;text corpus;unified extensible firmware interface	Manoj Kumar Chinnakotla;Om P. Damani;Avijit Satoskar	2010	ACM Trans. Asian Lang. Inf. Process.	10.1145/1838751.1838753	rule-based system;natural language processing;speech recognition;transliteration;computer science;linguistics;cross entropy	NLP	-29.846629046768687	-74.90890165967298	92349
37bd2b50f66a281ac3466423f60bf1dc876725ec	image sentiment prediction based on textual descriptions with adjective noun pairs		We aim to predict the sentiment related information reflected in images based on SentiBank, which is a library including Adjective Noun Pair (ANP) concept detectors for image sentiment analysis. Instead of using only ANP responses in images as mid-level features, we make full use of the ANPs’ textual sentiment. We first give each ANP concept in SentiBank a sentiment value by adding together the textual sentiment value of the adjective and that of the noun. Having detected the presence of ANPs in an image, we define an image sentiment value by computing the weighted sum of the textual sentiment values of ANPs describing this image with corresponding ANP responses as weights. On the one hand, we adopt a one-dimension logistic regression model to predict the sentiment orientation according to the image sentiment value. On the other hand, we use the ANP responses detected in an image as mid-level representations to train a regularized logistic regression classifier for sentiment prediction. We finally employ a late fusion algorithm to combine the prediction results from the two schemes. Experimental results reveal that the proposed method which takes into account the textual sentiment of ANPs improves the performance of SentiBank based image sentiment prediction.	algorithm;automatic image annotation;digital image;logistic regression;sensor;sentiment analysis;weight function	Zuhe Li;Yangyu Fan;Weihua Liu;Fengqin Wang	2016	Multimedia Tools and Applications	10.1007/s11042-016-4310-5	natural language processing;speech recognition;pattern recognition	AI	-19.931538366082172	-68.21745300853867	92393
740b3014c6cb661aa1a0adf1bc0c685888a7ec8a	in question answering, two heads are better than one	ensemble method;average precision;machine learning;statistical techniques;natural language processing;question answering;knowledge base	Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora. The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques. We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.	algorithm;baseline (configuration management);ensemble learning;experiment;information retrieval;machine learning;multi-agent system;multi-source;natural language processing;question answering;test set;text corpus	Jennifer Chu-Carroll;Krzysztof Czuba;John M. Prager;Abraham Ittycheriah	2003		10.3115/1073445.1073449	natural language processing;knowledge base;question answering;computer science;machine learning;data mining;information retrieval	AI	-25.073887846372312	-69.636595340061	92440
491ac2f4e8fa6ddeb5cc40bbe1425abd67091501	chinese liwc lexicon expansion via hierarchical classification of word embeddings with sememe attention		Linguistic Inquiry and Word Count (LIWC) is a word counting software tool which has been used for quantitative text analysis in many fields. Due to its success and popularity, the core lexicon has been translated into Chinese and many other languages. However, the lexicon only contains several thousand of words, which is deficient compared with the number of common words in Chinese. Current approaches often require manually expanding the lexicon, but it often takes too much time and requires linguistic experts to extend the lexicon. To address this issue, we propose to expand the LIWC lexicon automatically. Specifically, we consider it as a hierarchical classification problem and utilize the Sequence-to-Sequence model to classify words in the lexicon. Moreover, we use the sememe information with the attention mechanism to capture the exact meanings of a word, so that we can expand a more precise and comprehensive lexicon. The experimental results show that our model has a better understanding of word meanings with the help of sememes and achieves significant and consistent improvements compared with the state-of-the-art methods. The source code of this paper can be obtained from https://github.com/thunlp/Auto CLIWC.	experiment;lexicon;programming tool	Xiangkai Zeng;Cheng Yang;Cunchao Tu;Zhiyuan Liu;Maosong Sun	2018			machine learning;computer science;artificial intelligence;lexicon;sememe	AI	-25.339491547182078	-73.66268459934082	92629
ae784de9f4b45740d7fd4ae67ff8aaab393cb7af	improved error recovery in generated lr parsers			lr parser;parsing	Bostjan Slivnik;Bostjan Vilfan	2004	Informatica (Slovenia)		machine learning;lr parser;artificial intelligence;computer science	Theory	-23.40018187686327	-77.04916527881693	92646
b48195cf17f2fce1b6acb15c7126aede22360d40	a rule-based approach to prepositional phrase attachment disambiguation	corpus-based approach;rule-based approach;present result;phrase attachment disambiguation;new corpus-based approach;rule based	I:n this paper, we describe a new corpus-based approach to prepositional phrase a t t achment disambiguation, and present results colnparing peffo> mange of this a lgori thm with other corpus-based approaches to this problem.	attachments;stellar classification;word-sense disambiguation	Eric Brill;Philip Resnik	1994			rule-based system;natural language processing;noun phrase;speech recognition;computer science;determiner phrase;linguistics	NLP	-26.790685277634065	-77.2723996201244	92817
6766a707e783600e4ab96558268f0840129209dd	frame selection in parsing	inference rule	The problem of frame selection in parsing is discussed, with the focus on the selection of a frame for texts which contain highly ambiguous or vague words. An approach to frame selection is presented which involves the use of a small number of general inference rules, in conjunction with a hierarchically-organized conceptual memory. This is in contrast to various other methods, which rely either on disambiguation rules stored in the dictionary definitions of ambiguous words, or on previously-activated frames to guide the selection of new frames. The selection of frames for vague or ambiguous words using these previous methods is shown to be problematic. The method presented here does not suffer from these same problems because of the hierarchical organization of memory and the general inference rules used.	dictionary;parsing;vagueness;word-sense disambiguation	Steven L. Lytinen	1984			natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;rule of inference	NLP	-24.92777957055223	-76.19898355994741	92841
da39d99189605f63dedbd8d271590914fb1fbbce	iscas at multilingual opinion analysis task.	information retrieval;iterative algorithm;em algorithm	The paper presents our work in the multilingual opinion analysis task in NTCIR7 in Simplified Chinese. In detecting opinionated sentences, an EM algorithm was proposed to extract the sentiment words based on the sentimental dictionary, and then an iterative algorithm was used to estimate the score of the sentiment words and the sentences. In detecting relevant sentences, we solve this problem by analogizing the task to the traditional information retrieval task. The difficulty lies in that some sentence is relevant to the topic even if there are no key words hit in it. In this situation, we use a pseudo feedback and query extension method to refine the result. The evaluation results and the result analysis will also be presented.	dictionary;expectation–maximization algorithm;extension method;information retrieval;iterative method;sensor	Yunping Huang;Yulin Wang;Le Sun	2008			natural language processing;computer science;data science;information retrieval	NLP	-24.751908686369713	-67.21088679445157	92853
6bb16d186d8bbc7c81a11dae30c585842c547ab1	generation by inverting a semantic parser that uses statistical machine translation	statistical machine translation;natural language;natural language generation;machine translation	This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages.	hybrid system;natural language generation;parser;statistical machine translation;wasp	Yuk Wah Wong;Raymond J. Mooney	2007			computer-assisted translation;natural language processing;speech recognition;transfer-based machine translation;universal networking language;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;natural language;programming language;machine translation software usability	NLP	-28.294025449215127	-79.42026723948982	92918
7b2fe603b2c82cf422a5826490e4d2d12525d93d	influence of target reader background and text features on text readability in bangla: a computational approach		In this paper, we have studied the effect of two important factors influencing text readability in Bangla: the target reader and text properties. Accordingly, at first we have built a novel Bangla readability dataset of 135 documents annotated by 50 readers from two different backgrounds. We have identified 20 different features that can affect the readability of Bangla texts; the features were divided in two groups, namely, „classic‟ and „non-classic‟. Preliminary correlation analysis reveals that text features have varying influence on the text hardness stated by the two groups. We have employed support vector machine (SVM) and support vector regression (SVR) techniques to model the reading difficulties of Bangla texts. In addition to developing different models targeted towards different type of readers, separate combinations of features were tested to evaluate their comparative contributions. Our study establishes that the perception of text difficulty varies largely with the background of the reader. To the best of our knowledge, no such work on text readability has been recorded earlier in Bangla.	computation;computational model;graph minor;statistical classification;support vector machine	Manjira Sinha;Tirthankar Dasgupta;Anupam Basu	2014			natural language processing;speech recognition;computer science;information retrieval	NLP	-25.062052098483154	-69.62839606966646	93313
1d96af5ed8d5f55b8467945310e6dfa431d0ebfb	chinese web scale linguistic datasets and toolkit		The web provides a huge collection of web pages for researchers to study natural languages. However, processing web scale texts is not an easy task and needs many computational and linguistic resources. In this paper, we introduce two Chinese parts-of-speech tagged web-scale datasets and describe tools that make them easy to use for NLP applications. The first is a Chinese segmented and POS-tagged dataset, in which the materials are selected from the ClueWeb09 dataset. The second is a Chinese POS n-gram corpus extracted from the POS-tagged dataset. Tools to access the POS-tagged dataset and the POS n-gram corpus are presented. The two datasets will be released to the public along with their tools. 中文網路規模語言資料集和工具 網際網路提供研究人員巨量網頁進行自然語言處理研究,但是處理網路規模的文本不是件 簡單的工作,而是需要大量的計算和語言資源。在本文中,我們介紹兩種加上中文詞性標 記的網路規模資料集,以及易於將這些資源運用於自然語言處理應用的工具。第一種資料 集選自於ClueWeb09的中文語料,並經過中文斷詞和詞性標記。第二種資料集是由上述詞 性標記資料集中擷取中文詞性n-gram,所建立的語料庫。我們同時也提出搜尋詞性標記資 料集和詞性n-gram語料庫的工具,這兩種資料集連同工具將提供研究人員使用。	n-gram;natural language processing;parsing;point of sale;scalability;sentiment analysis;text corpus;web page;world wide web	Chi-Hsin Yu;Hsin-Hsi Chen	2012			natural language processing;computer science;data science;data mining;information retrieval	NLP	-31.099616946215484	-68.92874128655285	93519
2686ca44cec04f58e23c461d6ce7b82baecdd123	an ordering of terms based on semantic relatedness	term expansion;semantic order;lexical resource;term selection method;semantic similarity measure;semantic relatedness measure;statistical measure;term expansion method;weight term;term weighting;semantic relatedness;information retrieval;semantic similarity;natural language processing	Term selection methods typically employ a statistical measure to filter or weight terms. Term expansion for IR may also depend on statistics, or use some other, non-metric method based on a lexical resource. At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods.	algorithm;natural language processing;semantic similarity;word sense;word-sense disambiguation	Peter Wittek;Sándor Darányi;Chew Lim Tan	2009			natural language processing;semantic similarity;semantic computing;semeval;semantic search;computer science;semantic compression;linguistics;information retrieval	NLP	-27.198291336309772	-67.32628265909044	93627
f4ccd5e4550e907225411f0880d3591755709dac	towards acquisition of taxonomic inference (short paper)		On a pilot corpus of Dutch medical encyclopedia texts, we focus on the mechanism of taxonomic inference that involves the extraction of co-hyponym terms, and the taxonomic or domain-specific lexicosemantic relation in the form of a textual hypothesis, which coordinates these. An initial set of inference elements can be acquired by syntactic and semantic alignment, additionally driven by document structure. From the terms and the related hypothesis we can harvest lexical patterns, which are linked to annotated domain concepts. The aim of the process is to learn inference patterns and apply the system to short as well as unstructured documents where fewer or no discourse-level cues are available, in order to acquire new co-hyponyms linked to their coordinating term via a specified relation.		Piroska Lendvai	2009			bioinformatics;pattern recognition;statistics	NLP	-33.02693799284265	-69.39609473932116	93845
7abacfff3882ed552e7a72f4dabbf341852f87a8	unifying multiple semantic intentions for a syntactic construct				Chingmin Jim Lo	1984	Comput. J.	10.1093/comjnl/27.1.83	natural language processing;computer science;artificial intelligence	Logic	-31.962885646988276	-79.21196575282936	93850
b45f6b3f29316cd761fd1f4c6e433135590d0ebc	semi-supervised methods for improving keyword search of unseen terms		We present a semi-supervised language modeling technique to improve search performance on words without training data. Probabilities estimated from automatic transcripts of a large corpus of in-domain audio are added to an existing LM. Requiring only pronunciations and indomain audio, our method achieves 70% of the possible gain without any supervised data or development data. This method is contrasted with techniques using additional resources. If human effort is available, we also present a transcription regime to efficiently close the gap.	language model;semi-supervised learning;semiconductor industry;supervised learning;text corpus;transcription (software)	Scott Novotney;Ivan Bulyko;Richard M. Schwartz;Sanjeev Khudanpur;Owen Kimball	2012			artificial intelligence;training set;pattern recognition;computer science;language model	NLP	-20.24190964178091	-78.6003895457639	93857
d6e880a66432444aafd5330e336e589dc8ac25ac	learning to rank semantic coherence for topic segmentation		Topic segmentation plays an important role for discourse parsing and information retrieval. Due to the absence of training data, previous work mainly adopts unsupervised methods to rank semantic coherence between paragraphs for topic segmentation. In this paper, we present an intuitive and simple idea to automatically create a “quasi” training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our algorithm is able to achieve competitive performance over strong baselines on several real-world datasets.	algorithm;artificial neural network;baseline (configuration management);convolutional neural network;experiment;information retrieval;learning to rank;parsing;semantic similarity;text segmentation	Liang Wang;Sujian Li;Yajuan Lü;Houfeng Wang	2017			machine learning;learning to rank;computer science;segmentation;artificial intelligence;pattern recognition;coherence (physics)	NLP	-20.147179504637837	-72.36816354125432	93959
2134407e2edf2f791c8898a88872cbd277f68052	discourse-level annotation over europarl for machine translation: connectives and pronouns	statistical machine translation;annotation;440 french related languages;discourse connectives;pronouns;840 french related literatures;parallel corpora	This paper describes methods and results for the annotation of two discourse-level phenomena, connectives and pronouns, over a multilingual parallel corpus. Excerpts from Europarl in English and French have been annotated with disambiguation information for connectives and pronouns, for about 3600 tokens. This data is then used in several ways: for cross-linguistic studies, for training automatic disambiguation software, and ultimately for training and testing discourse-aware statistical machine translation systems. The paper presents the annotation procedures and their results in detail, and overviews the first systems trained on the annotated resources and their use for machine translation.	logical connective;parallel text;statistical machine translation;word-sense disambiguation	Andrei Popescu-Belis;Thomas Meyer;Jeevanthi Liyanapathirana;Bruno Cartoni;Sandrine Zufferey	2012		10.7892/boris.78674	natural language processing;speech recognition;example-based machine translation;computer science;linguistics;java annotation	NLP	-28.671103429641832	-76.39316624242488	93981
f9f68bfb52907f9d5c09f1d923aec6c30f525d2d	speech and text analysis for multimodal addressee detection in human-human-computer interaction.		The necessity of addressee detection arises in multiparty spoken dialogue systems which deal with human-human-computer interaction. In order to cope with this kind of interaction, such a system is supposed to determine whether the user is addressing the system or another human. The present study is focused on multimodal addressee detection and describes three levels of speech and text analysis: acoustical, syntactical, and lexical. We define the connection between different levels of analysis and the classification performance for different categories of speech and determine the dependence of addressee detection performance on speech recognition accuracy. We also compare the obtained results with the results of the original research performed by the authors of the Smart Video Corpus which we use in our computations. Our most effective meta-classifier working with acoustical, syntactical, and lexical features reaches an unweighted average recall equal to 0.917 showing almost a nine percent advantage over the best baseline model, though this baseline classifier additionally uses head orientation data. We also propose a universal meta-model based on acoustical and syntactical analysis, which may theoretically be applied in different domains.	baseline (configuration management);computation;dialog system;human–computer interaction;metamodeling;multimodal interaction;parsing;speech recognition	Oleg Akhtiamov;Maxim Sidorov;Alexey A. Karpov;Wolfgang Minker	2017		10.21437/Interspeech.2017-501	speech recognition;natural language processing;computer science;artificial intelligence;text mining	NLP	-23.108982282576495	-70.3553750188064	94169
aa7d500fcd17e5e98fbe44ea888603daab9e7c96	context and humor: understanding amul advertisements of india		Contextual knowledge is the most important element in understanding language. By contextual knowledge we mean both general knowledge and discourse knowledge i.e. knowledge of the situational context, background knowledge and the co-textual context [10]. In this paper, we will discuss the importance of contextual knowledge in understanding the humor present in the cartoon based Amul advertisements in India. In the process, we will analyze these advertisements and also see if humor is an effective tool for advertising and thereby, for marketing. These bilingual advertisements also expect the audience to have the appropriate linguistic knowledge which includes knowledge of English and Hindi vocabulary, morphology and syntax. Different techniques like punning, portmanteaus and parodies of popular proverbs, expressions, acronyms, famous dialogues, songs etc are employed to convey the message in a humorous way. The present study will concentrate on these linguistic cues and the required context for understanding wit and humor.	computation;computational humor;knowledge representation and reasoning;mathematical morphology;type punning;vocabulary	Radhika Mamidi	2017	CoRR		artificial intelligence;morphology (linguistics);natural language processing;hindi;syntax;advertising;general knowledge;meaning (linguistics);computer science;expression (mathematics);vocabulary	NLP	-33.18524803446755	-79.80526622345995	94281
1bf590e16c434580d0267af93dc93d5c257e035a	eco and onto.pt: a flexible approach for creating a portuguese wordnet automatically	information extraction;lexical ontology;clustering;wordnet;semantic relations	A wordnet is an important tool for developing natural language processing applications for a language. However, most wordnets are handcrafted by experts, which limits their growth. In this article, we propose an automatic approach to create wordnets by exploiting textual resources, dubbed ECO. After extracting semantic relation instances, identified by discriminating textual patterns, ECO discovers synonymy clusters, used as synsets, and attaches the remaining relations to suitable synsets. Besides introducing each step of ECO, we report on how it was implemented to create Onto.PT, a public lexical ontology for Portuguese. Onto.PT is the result of the automatic exploitation of Portuguese dictionaries and thesauri, and it aims to minimise the main limitations of existing Portuguese lexical knowledge bases.	dictionary;natural language processing;ontology components;pro tools;synonym ring;thesaurus;wordnet	Hugo Gonçalo Oliveira;Paulo Gomes	2014	Language Resources and Evaluation	10.1007/s10579-013-9249-9	natural language processing;wordnet;computer science;data mining;cluster analysis;information extraction;information retrieval	NLP	-30.7635357629409	-68.65780399490413	94326
2cfd15f991d4199d225e5e0d0e32c6ce742cc31a	what's in a name?	nature chemical biology; science	This paper describes experiments on identifying the language of a single name in isolation or in a document written in a different language. A new corpus has been compiled and made available, matching names against languages. This corpus is used in a series of experiments measuring the performance of general language models and names-only language models on the language identification task. Conclusions are drawn from the comparison between using general language models and names-only language models and between identifying the language of isolated names and the language of very short document fragments. Future research directions are outlined.	compiler;experiment;language identification;language model;text corpus	Stasinos Konstantopoulos	2017	FASEB journal : official publication of the Federation of American Societies for Experimental Biology	10.1096/fj.170801ufm		NLP	-29.581606271103592	-74.91210485203048	94362
ade04cff22fdb40e7456d4d7ed2c7665a08164f9	semantically expanding questions for supervised automatic classification	statistical approach;noun;classification;linguistic analysis;machine learning;question answering system;term weighting;semantic expansion;feature selection;automatic classification	Responding correctly to a question given a large collection of textual data is not an easy task. There is a need to perceive and recognize the question at a level that permits to detect some constraints that the question imposes on possible answers. The question classification task is used in Question Answering systems. This deduces the type of expected answer, to perform a semantic classification to the target answer. The purpose is to provide additional information to reduce the gap between answer and question to match them. An approach to ameliorate the effectiveness of classifiers focusing on the linguistic analysis (semantic, syntactic and morphological) and statistical approaches guided by a layered semantic hierarchy of fine grained questions types. This work also proposes two methods of questions expansion. The first finds for each word synonyms matching its contextual sense. The second one adds a high representation ”hypernym” for the noun. Various representation features of documents, term weighting and diverse machine learning algorithms are studied. Experiments conducted on actual data are presented. Of interest is the improvement in the precision of the classification of questions.		Ali Harb;Michel Beigbeder;Jean-Jacques Girardot	2009		10.1007/978-3-642-04957-6_32	noun;biological classification;computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval	NLP	-26.455276533508997	-70.41152110444217	94391
6a48f542c025ef38fb108f6354eecdb41ea098fc	what is wrong with style transfer for texts?		A number of recent machine learning papers work with an automated style transfer for texts and, counter to intuition, demonstrate that there is no consensus formulation of this NLP task. Different researchers propose different algorithms, datasets and target metrics to address it. This short opinion paper aims to discuss possible formalization of this NLP task in anticipation of a further growing interest to it.	algorithm;hoc (programming language);machine learning;natural language processing;parallel text;text corpus	Alexey Tikhonov;Ivan P. Yamshchikov	2018	CoRR			NLP	-26.41631779603062	-72.79991994941166	94700
935c83c4d9ca351fd5fa89a01d4afd8b868018a6	using topic sentiment sentences to recognize sentiment polarity in chinese reviews		An approach to recognizing sentiment polarity in Chinese reviews based on topic sentiment sentences is presented. Considering the features of Chinese reviews, we firstly identify the topic of a review using an n-gram matching approach. To extract candidate topic sentiment sentences, we compute the semantic similarity between a given sentence and the ascertained topic and meanwhile determine whether the sentence is subjective. A certain number of these sentences are then selected as representatives according to their semantic similarity value with relation to the topic. The average value of the representative topic sentiment sentences is calculated and taken as the sentiment polarity of a review. Experiment results show that the proposed method is feasible and can achieve relatively high precision.	compiler;error analysis for the global positioning system;lexicon;linux test project (ltp);n-gram;nan;off topic;semantic similarity;sentiment analysis	Jiang Yang;Min Hou	2010			natural language processing;artificial intelligence;computer science	NLP	-23.422060397694715	-66.72618612594782	94788
64df8ae57d77fa81459e090ec9a38b6c2bb3509d	short text classification using semantic random forest		Using traditional Random Forests in short text classification revealed a performance degradation compared to using them for standard texts. Shortness, sparseness and lack of contextual information in short texts are the reasons of this degradation. Existing solutions to overcome these issues are mainly based on data enrichment. However, data enrichment can also introduce noise. We propose a new approach that combines data enrichment with the introduction of semantics in Random Forests. Each short text is enriched with data semantically similar to its words. These data come from an external source of knowledge distributed into topics thanks to the Latent Dirichlet Allocation model. Learning process in Random Forests is adapted to consider semantic relations between words while building the trees. Tests performed on search-snippets using the new method showed significant improvements in the classification. The accuracy has increased by 34% compared to traditional Random Forests and by 20% compared to MaxEnt.	algorithm;decision tree;document classification;elegant degradation;exploit (computer security);feature selection;gene ontology term enrichment;image noise;latent dirichlet allocation;neural coding;ontology components;principle of maximum entropy;random forest;relevance;unbalanced circuit	Ameni Bouaziz;Christel Dartigues-Pallez;Célia da Costa Pereira;Frédéric Precioso;Patrick Lloret	2014		10.1007/978-3-319-10160-6_26	natural language processing;pattern recognition;information retrieval	AI	-23.238913733342898	-66.3197730667891	94866
4879ded89a518ccad9b4237f93ccf589507aaaf2	domain ontology construction from biomedical text.	information extraction;cardiology;unified medical language system;ontology learning;domain ontology;natural language processing;domain specificity	NLM's Unified Medical Language System (UMLS) is a very large ontology of biomedical and health data. In order to be used effectively for knowledge processing, it needs to be customized to a specific domain. In this paper, we present techniques to automatically discover domain-specific concepts, discover relationships between these concepts, build a context map from these relationships, link these domain concepts with the best-matching concept identifiers in UMLS using our context map and UMLS concept trees, and finally assign categories to the discovered relationships. This specific domain ontology of terms and relationships using evidential information can serve as a basis for applications in analysis, reasoning and discovery of new relationships. We have automatically built an ontology for the Nuclear Cardiology domain as a testbed for further enhancing our techniques.	identifier;netware loadable module;ontology (information science);testbed	Saurav Sahay;Baoli Li;Ernest V. Garcia;Eugene Agichtein;Ashwin Ram	2007			natural language processing;upper ontology;open biomedical ontologies;bibliographic ontology;computer science;ontology;data mining;unified medical language system;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-33.39780633621227	-68.70845057511085	94944
9338ce31c8bc1ad239c1f78c95991bb2858c1d66	domain adaptation for named entity recognition using crfs		In this paper we explain how we created a labelled corpus in English for a Named Entity Recognition (NER) task from multi-source and multi-domain data, for an industrial partner. We explain the specificities of this corpus with examples and describe some baseline experiments. We present some results of domain adaptation on this corpus using a labelled Twitter corpus (Ritter et al., 2011). We tested a semi-supervised method from (Garcia-Fernandez et al., 2014) combined with a supervised domain adaptation approach proposed in (Raymond and Fayolle, 2010) for machine learning experiments with CRFs (Conditional Random Fields). We use the same technique to improve the NER results on the Twitter corpus (Ritter et al., 2011). Our contributions thus consist in an industrial corpus creation and NER performance improvements.	baseline (configuration management);conditional random field;domain adaptation;experiment;machine learning;multi-source;named entity;named-entity recognition;semi-supervised learning;semiconductor industry	Tian Tian;Marco Dinarelli;Isabelle Tellier;Pedro Miguel Dias Cardoso	2016			crfs;natural language processing;artificial intelligence;domain adaptation;computer science;named-entity recognition	NLP	-22.005147022195626	-70.69750617956848	95304
77a58130de71778bceb1b26b391600daa02c5fcd	learning a motor grammar of iconic gestures		In this paper, we present a computational investigation into the compositionality of iconic gestures by trying to learn a motor grammar. We propose a grammar formalism that learns (1) the salient, invariant features of single movement segments (motor primitives) and (2) the hierarchical organization of these segments in complex gesturing. The formalism is applied to a dataset of natural iconic gestures. The extracted structure reveals compositional patterns of iconic gesturing.	cognitive science;formal grammar;german research centre for artificial intelligence;logic programming;reification (computer science);semantics (computer science);whole earth 'lectronic link	Amir Sadeghipour;Stefan Kopp	2014			natural language processing;linguistics;communication	NLP	-30.456161565961413	-79.84830688792812	95484
9fbf313fa837c386cb0337c1848e67de8be99faf	sense-level semantic clustering of hashtags		We enhance the accuracy of the currently available semantic hashtag clustering method, which leverages hashtag semantics extracted from dictionaries such as Wordnet and Wikipedia. While immune to the uncontrolled and often sparse usage of hashtags, the current method distinguishes hashtag semantics only at the word-level. Unfortunately, a word can have multiple senses representing the exact semantics of a word, and, therefore, word-level semantic clustering fails to disambiguate the true sense-level semantics of hashtags and, as a result, may generate incorrect clusters. This paper shows how this problem can be overcome through sense-level clustering and demonstrates its impacts on clustering behavior and accuracy.	algorithm;cluster analysis;crowdsourcing;google translate;hashtag;semantic similarity;sparse matrix;uncontrolled format string;urban dictionary;web search engine;wikipedia;wordnet	Ali Javed;Byung Suk Lee	2016		10.1007/978-3-319-55209-5_1	semantics;cluster analysis;wordnet;artificial intelligence;pattern recognition;computer science	AI	-25.32400319565005	-67.21245188542079	95507
8d25a4cdfaf2d4b7f7147a3dbefd5620b7ac1e9b	albanian part-of-speech tagging: gold standard and evaluation		In this paper, we present a gold standard corpus for Albanian part-of-speech tagging and perform evaluation experiments with different statistical taggers. The corpus consists of more than 31,000 tokens and has been manually annotated with a medium-sized tagset that can adequately represent the syntagmatic aspects of the language. We provide mappings from the full tagset to both the original Google Universal Part-of-Speech Tags and the variant used in the Universal Dependencies project. We perform experiments with different taggers on the full tagset as well as on the coarser tagsets and achieve accuracies of up to 95.10%.	experiment;google summer of code;part-of-speech tagging;text corpus	Besim Kabashi;Thomas Proisl	2018			natural language processing;artificial intelligence;speech recognition;computer science;gold standard;part-of-speech tagging	NLP	-28.44865192124457	-74.72019537556984	95573
468700831a80743c0935d18d7c6cf38360349544	uniza system for the spoken web search task at mediaeval2013		In this paper, we present an approach to detect spoken keywords according to a given query, as part of the MediaEval benchmark. The proposed approach is based on a concept of modelling the speech query as a concatenation of language-independent quasiphoneme models, which are derived by unsupervised clustering on various audio data. Since only an initial version of the system is presented, issues concerning further system improvements are also discussed.	benchmark (computing);cluster analysis;concatenation;language-independent specification	Roman Jarina;Michal Kuba;Róbert Gubka;Michal Chmulik;Martin Paralic	2013			natural language processing;speech recognition;computer science;communication	AI	-27.932612066299598	-68.78596186991074	95602
319083576ecc0422c9c429a09a5a990eba292d40	a sense-based translation model for statistical machine translation		The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation.	bleu;bag-of-words model;baseline (configuration management);brill tagger;compiler;experiment;language model;nist (metric);satisfiability modulo theories;statistical machine translation;test data;topic model;wafer-scale integration;web services for devices;word sense;word-sense disambiguation;word-sense induction	Deyi Xiong;Min Zhang	2014			natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;semeval;word error rate;computer science;rule-based machine translation	NLP	-20.592363635076946	-75.61336321338617	95763
a8492c76ea47b71583a662ec2e791158df9d8995	joint training for neural machine translation models with monolingual data		Monolingual data have been demonstrated to be helpful in improving translation quality of both statistical machine translation (SMT) systems and neural machine translation (NMT) systems, especially in resource-poor or domain adaptation tasks where parallel data are not rich enough. In this paper, we propose a novel approach to better leveraging monolingual data for neural machine translation by jointly learning source-to-target and target-to-source NMT models for a language pair with a joint EM optimization method. The training process starts with two initial NMT models pre-trained on parallel data for each direction, and these two models are iteratively updated by incrementally decreasing translation losses on training data. In each iteration step, both NMT models are first used to translate monolingual data from one language to the other, forming pseudo-training data of the other NMT model. Then two new NMT models are learnt from parallel data together with the pseudo training data. Both NMT models are expected to be improved and better pseudo-training data can be generated in next step. Experiment results on Chinese-English and English-German translation tasks show that our approach can simultaneously improve translation quality of source-to-target and target-to-source models, significantly outperforming strong baseline systems which are enhanced with monolingual data for model training including back-translation.	algorithm;baseline (configuration management);domain adaptation;iteration;mathematical optimization;neural machine translation;semi-supervised learning;semiconductor industry;statistical machine translation	Zhirui Zhang;Shujie Liu;Mu Li;Ming Zhou;Enhong Chen	2018			machine learning;machine translation;computer science;domain adaptation;artificial intelligence;training set	AI	-19.155072922857467	-75.69863993157988	96031
97a33b96de848ba808031adeff52697ecdefb863	using other learner corpora in the 2013 nli shared task		• Parsed with Stanford parser • Binary features • Word bigrams • Mixed POS/function trigrams • Frequency cutoff feature selection • SVM classifier • One-versus-all subclassifiers • C parameter: 1 • The 2013 Native Language Identification Shared Task (Tetreault et al. 2013) • Participated in all three tasks: one closed-training, two open-training • The new TOEFL-11 learner essay corpus (Blanchard et al. 2013) • Well controlled, but limited in scope? • Our focus is on building robust models • Use of cross-corpus evaluation (Brooke and Hirst 2012; Bykh and Meurers 2012)	bigram;feature selection;language identification;native-language identification;parsing;text corpus;trigram;word lists by frequency	Julian Brooke;Graeme Hirst	2013			natural language processing;speech recognition;computer science	NLP	-22.501143079361388	-70.76467805612795	96281
ad220711ccd040167b6392f3a4cab36ed27db4cc	chinese maximal noun phrase parsing based on cascaded conditional random fields	layer by layer;trees mathematics grammars natural languages random processes;noun phrase;training;natural languages;testing;conditional random fields;trees mathematics;knowledge engineering aerospace engineering natural language processing computer aided instruction testing information retrieval performance analysis;data mining;phrase structure tree;maximal noun phrase;grammars;chinese maximal noun phrase parsing;random processes;conditional random field;phrase structure tree cascaded conditional random fields maximal noun phrase conditional random fields;phrase structure tree chinese maximal noun phrase parsing cascaded conditional random fields phrases recognition;labeling;data models;tagging;cascaded conditional random fields;phrases recognition;knowledge engineering	This paper proposes an approach for Chinese Maximal Noun Phrase parsing based on Cascaded Conditional Random Fields. In this approach, the parse tree of Chinese Maximal Noun Phrase is constructed layer by layer. The Chinese chunks are first recognized by the lower Conditional Random Fields model, then the result is passed as input to the higher model for recognition of phrases, the process of recognizing phrases is continued until no new phrases are discovered. Post-processing rules are constructed between the lower and higher models to modify the erroneous recognition of Chinese chunks, and finally the phrase structure tree of the Chinese Maximal Noun Phrase is constructed. In open test, our Chinese Maximal Noun Phrase parser achieves F1-score of 92.02%.	conditional random field;f1 score;information retrieval;machine translation;maximal set;microcom networking protocol;multiple encryption;np (complexity);natural language processing;parse tree;phrase structure rules;shallow parsing;sparse matrix;video post-processing	Dongfeng Cai;Xin Liu;Qiaoli Zhou;Na Ye	2009	2009 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2009.5313768	natural language processing;noun phrase;speech recognition;computer science;pattern recognition;determiner phrase	NLP	-22.557701116487696	-74.9690721665984	96325
620a86f6c439f452098453694f6fae4d20b1a4cc	constructing word-sense association networks from bilingual dictionary and comparable corpora		A novel thesaurus named a word-sense association network is proposed for the first time. It consists of nodes representing word senses, each of which is defined as a set consisting of a word and its translation equivalents, and edges connecting topically associated word senses. This word-sense association network is produced from a bilingual dictionary and comparable corpora by means of a newly developed fully automatic method. The feasibility and effectiveness of the method were demonstrated experimentally by using the EDR English-Japanese dictionary together with Wall Street Journal and Nihon Keizai Shimbun corpora. The word-sense association networks were applied to word-sense disambiguation as well as to a query interface for information retrieval.	bilingual dictionary;bluetooth;experiment;information retrieval;japanese dictionary;natural language processing;synonym ring;text corpus;the wall street journal;thesaurus;word sense;word-sense disambiguation;wordnet	Hiroyuki Kaji;Osamu Imaichi	2004			word sense;speech recognition;artificial intelligence;natural language processing;machine-readable dictionary;computer science;bilingual dictionary	NLP	-27.72868023209907	-69.8976649854296	96472
ec400a7768ca9d829b3ec668cebf2c66997b7723	combining multiword units into a hidden markov model for part-of-speech tagging			hidden markov model;markov chain;part-of-speech tagging	Jae-Hoon Kim	1997			hidden markov model;part-of-speech tagging;computer science;artificial intelligence;pattern recognition	NLP	-24.86395942838695	-78.83313542208776	96489
4291fd9987414f546fafa477edd718dffdcb2ebd	cross-lingual morphological tagging for low-resource languages		Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embeddingbased model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.	baseline (configuration management);brill tagger;downstream (software development);experiment;formal language;hidden markov model;lossy compression;morphological parsing;natural language processing;parallel text;parsing expression grammar;part-of-speech tagging;scalability	Jan Buys;Jan A. Botha	2016	CoRR	10.18653/v1/P16-1184	natural language processing;speech recognition;computer science;machine learning	NLP	-20.96589704367138	-75.6613418117014	96603
754d2456d500091d0531c95a2e0c790c0cc50505	bootstrapped named entity recognition for product attribute extraction	product attribute extraction;bootstrapped ner system;informative context;supervised ner;output normalized result;new brand;entity recognition;short listing title;output normalized attribute value;grammatical structure;information extraction	We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay’s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.	algorithm;bootstrapping (compilers);comparison of programming languages (string functions);conditional random field;hidden markov model;information extraction;n-gram;named entity;principle of maximum entropy;randomness extractor;shoes;substring;text corpus;viterbi decoder	Duangmanee Putthividhya;Junling Hu	2011			speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-24.157022520403917	-70.80937232812103	96631
6b20d895f45099dcc79270424c9f5efab543aebc	automatic generation of question answer pairs from noisy case logs	training;semantics;noise measurement;hidden markov models;segmentation performance question answer pairs case logs mining frequently asked questions faq knowledge repository qa pairs discovery question answering system hidden markov model hmm latent dirichlet allocation model lda model semantic similarity problem statement segments conditional random field;syntactics;viterbi algorithm;context;hidden markov models semantics noise measurement syntactics training viterbi algorithm context;question answering information retrieval data mining hidden markov models	In a customer support scenario, a lot of valuable information is recorded in the form of `case logs'. Case logs are primarily written for future references or manual inspections and therefore are written in a hasty manner and are very noisy. In this paper, we propose techniques that exploit these case logs to mine real customer concerns or problems and then map them to well written knowledge articles for that enterprise. This mapping results into generation of question-answer (QA) pairs. These QA pairs can be used for a variety of applications such as dynamically updating the frequently-asked-questions (FAQs), updating the knowledge repository etc. In this paper we show the utility of these discovered QA pairs as training data for a question-answering system. Our approach for mining the case logs is based on a composite model consisting of two generative models, viz, hidden Markov model (HMM) and latent Dirichlet allocation (LDA) model. The LDA model explains the long-range dependencies across words due to their semantic similarity and HMM models the sequential patterns present in these case logs. Such processing results in crisp `problem statement' segments which are indicative of the real customer concerns. Our experiments show that this approach finds crisp problem-statements in 56% of the cases and outperforms other alternate methods for segmentation such as HMM, LDA and conditional random field (CRF). After finding these crisp problem-statements, appropriate answers are looked up from an existing knowledge repository index forming candidate QA pairs. We show that considering only the problemstatement segments for which the answers can be found further improves the segmentation performance to 82%. Finally, we show that when these QA pairs are used as training data, the performance of a question-answering system can be improved significantly.	care-of address;conditional random field;customer support;ecosystem;experiment;hidden markov model;information retrieval;latent dirichlet allocation;markov chain;question answering;semantic similarity;software quality assurance;viz: the computer game	Jitendra Ajmera;Sachindra Joshi;Ashish Verma;Amol Mittal	2014	2014 IEEE 30th International Conference on Data Engineering	10.1109/ICDE.2014.6816671	natural language processing;viterbi algorithm;computer science;noise measurement;pattern recognition;data mining;database;semantics;markov model	DB	-24.24833892423421	-67.01839379292606	96779
63d4a515b60e9843d77853a807a36b2b0823290e	constituency parsing of complex noun sequences in hindi	constituency parsing;complex noun sequence;compound noun;bracketing;genitives	A complex noun sequence is one in which a head noun is recursively modified by one or more bare nouns and/or genitives Constituency analysis of complex noun sequence is a prerequisite for finding dependency relation semantic relation between components of the sequence. Identification of dependency relation is useful for various applications such as question answering, information extraction, textual entailment, paraphrasing.#R##N##R##N#In Hindi, syntactic agreement rules can handle to a large extent the parsing of recursive genitives Sharma, 2012[12].This paper implements frequency based corpus driven approaches for parsing recursive genitive structures that syntactic rules cannot handle as well as recursive compound nouns and combination of gentive and compound noun sequences. Using syntactic rules and dependency global algorithm, an accuracy of 92.85% is obtained.	parsing	Arpita Batra;Soma Paul;Amba Kulkarni	2014		10.1007/978-3-642-54906-9_23	natural language processing;noun;nominalization;speech recognition;computer science;bracketing;linguistics	NLP	-26.494862183342985	-76.06978902600008	96793
35343f5c4a2531060b6985a21d26bfc9144a3c90	towards identifying hindi/urdu noun templates in support of a large-scale lfg grammar	inproceedings	Complex predicates (CPs) are a highly productive predicational phenomenon in Hindi and Urdu and present a challenge for deep syntactic parsing. For CPs, a combination of a noun and light verb express a single event. The combinatorial preferences of nouns with one (or more) light verb is useful for predicting an instance of a CP. In this paper, we present a semi-automatic method to obtain noun groups based on their co-occurrences with light verbs. These noun groups represent the likelihood of a particular noun-verb combination in a large corpus. Finally, in order to encode this in an LFG grammar, we propose linking nouns with templates that describe preferable combinations with light verbs.	algorithm;encode;k-means clustering;lexical functional grammar;natural language processing;parsing;semiconductor industry;text corpus;wordnet	Sebastian Sulger;Ashwini Vaidya	2014		10.3115/v1/W14-5501	natural language processing;noun;speech recognition;computer science;linguistics	NLP	-23.97398982204446	-74.24415250466868	96964
a6d4d2aade9d728035751c8c759139d7b258553e	using target-language information to train part-of-speech taggers for machine translation	statistical approach;hidden markov model;hidden markov models language modeling;qualite;rule based;language modeling;baum welch;modele de langage;hidden markov models;traduction automatique;etiquetage automatique;quality;target language;part of speech tagging;ambiguity resolution;expectation maximization algorithm;part of speech;source language;rule based machine translation;parallel corpora;language model;machine translation;tagging	Although corpus-based approaches to machine translation (MT) are growing in interest, they are not applicable when the translation involves less-resourced language pairs for which there are no parallel corpora available; in those cases, the rule-based approach is the only applicable solution. Most rule-based MT systems make use of part-of-speech (PoS) taggers to solve the PoS ambiguities in the source-language texts to translate; those MT systems require accurate PoS taggers to produce reliable translations in the target language (TL). The standard statistical approach to PoS ambiguity resolution (or tagging) uses hidden Markov models (HMM) trained in a supervised way from hand-tagged corpora, an expensive resource not always available, or in an unsupervised way through the Baum-Welch expectation-maximization algorithm; both methods use information only from the language being tagged. However, when tagging is considered as an intermediate task for the translation procedure, that is, when the PoS tagger is to be embedded as a module within an MT system, information from the TL can be (unsupervisedly) used in the training phase to increase the translation quality of the whole MT system. This paper presents a method to train HMM-based PoS taggers to be used in MT; the new method uses not only information from the source language (SL), as general-purpose methods do, but also information from the TL and from the remaining modules of the MT system in which the PoS tagger is to be embedded. We find that the translation quality of the MT system embedding a PoS tagger trained in an unsupervised manner through this new method is clearly better than that of the same MT system embedding a PoS tagger trained through the Baum-Welch algorithm, and comparable to that obtained by embedding a PoS tagger trained in a supervised way from hand-tagged corpora.	baum–welch algorithm;brill tagger;compiler;embedded system;expectation–maximization algorithm;general-purpose markup language;hidden markov model;logic programming;machine translation;markov chain;parallel text;part-of-speech tagging;sl (complexity);text corpus;transform, clipping, and lighting;welch's method	Felipe Sánchez-Martínez;Juan Antonio Pérez-Ortiz;Mikel L. Forcada	2008	Machine Translation	10.1007/s10590-008-9044-3	natural language processing;speech recognition;expectation–maximization algorithm;part of speech;computer science;baum–welch algorithm;pattern recognition;linguistics;rule-based machine translation;hidden markov model;language model	NLP	-24.266568824654396	-78.67595559495594	97160
d8b6eba43c9d89ba2dfad0e7c1ca286bec0ac619	an algorithm of identifying semantic arguments of a verb from structured data		We discuss a method for identifying semantic arguments of a verb from a sentence. It differs from existing methods by an unique feature that represents all semantic arguments of a verb in a syntactic parse tree. The feature is a path in which at least one of the children of a node is a root of a subtree that associates with a semantic argument. Experiments on WSJ data from Penn TreeBank and PropBank show that our method achieves an average of precision 92.3% and an average of recall 94.2% on identifying semantic arguments of over six hundred verbs.	algorithm;graphical model;lexicon;parse tree;parsing;propbank;the wall street journal;tree (data structure);treebank	Minhua Huang;Robert M. Haralick	2011			natural language processing;semantic role labeling;computer science;pattern recognition;linguistics;selection	NLP	-23.150133805156518	-73.92345279636034	97448
eb3b3ee3cbc08fff52b7f1000179e5be5da44443	adaptive hter estimation for document-specific mt post-editing		We present an adaptive translation quality estimation (QE) method to predict the human-targeted translation error rate (HTER) for a document-specific machine translation model. We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis. We show the effectiveness of such features in both classification and regression of MT quality. By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences. Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase in human translators’	adaptive grammar;coefficient;field research;postediting;quadratic equation;statistical machine translation	Fei Huang;Jianming Xu;Abraham Ittycheriah;Salim Roukos	2014			speech recognition;computer science;artificial intelligence;machine learning;data mining	AI	-19.903848712062153	-78.39343499744577	97519
a17d545972b413c6122f9c40b41ee4c13365a6c9	arabic diacritic restoration approach based on maximum entropy models	word error rate;diacritique;arabic diacritic restoration;diacritic;vowelization;modern standard arabic;vowelizalion;arabic;part of speech tagging;error rate;maximum entropy model;arabe;computational linguistics;finite state transducer;high performance;linguistique informatique;maximum entropy	In modern standard Arabic and in dialectal Arabic texts, short vowels and other diacritics are omitted. Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic. Scripts without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting. In this paper we present a maximum entropy approach for restoring short vowels and other diacritics in an Arabic document. The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segment-based and part-of-speech tag features. The combination of these feature types leads to a high-performance diacritic restoration model. Using a publicly available corpus (LDC’s Arabic Treebank Part 3), we achieve a diacritic error rate of 5.1%, a segment error rate 8.5%, and a word error rate of 17.3%. In case-ending-less setting, we obtain a diacritic error rate of 2.2%, a segment error rate of 4.0%, and a word error rate of 7.2%. We also show in this paper a comparison of our approach to previously published techniques and we demonstrate the effectiveness of this technique in restoring diacritics in different kind of data such as the dialectal Iraqi Arabic scripts. 2008 Elsevier Ltd. All rights reserved.	alloy analyzer;circuit restoration;exception handling;experiment;finite-state transducer;lexicon;linguistic data consortium;parsing;part-of-speech tagging;point of sale;principle of maximum entropy;simulation;statistical model;treebank;word error rate	Imed Zitouni;Ruhi Sarikaya	2009	Computer Speech & Language	10.1016/j.csl.2008.06.001	natural language processing;speech recognition;word error rate;computer science;principle of maximum entropy;computational linguistics;linguistics	NLP	-25.35561832057557	-77.42713627791235	97617
419794ac69424b3cd66127c909dde325fe88c463	a framework for analyzing semantic change of words across time	data visualisation;history;natural language processing;text analysis;nlp approaches;change visualization;contrastive-pair level;diachronic corpora;exploratory analysis;historical language corpora;historical texts;language evolution;semantic change analysis;sentiment orientation level;word evolution;computational etymology;historical linguistics;language change;word meaning evolution	Recently, large amounts of historical texts have been digitized and made accessible to the public. Thanks to this, for the first time, it became possible to analyze evolution of language through the use of automatic approaches. In this paper, we show the results of an exploratory analysis aiming to investigate methods for studying and visualizing changes in word meaning over time. In particular, we propose a framework for exploring semantic change at the lexical level, at the contrastive-pair level, and at the sentiment orientation level. We demonstrate several kinds of NLP approaches that altogether give users deeper understanding of word evolution. We use two diachronic corpora that are currently the largest available historical language corpora. Our results indicate that the task is feasible and satisfactory outcomes can be already achieved by using simple approaches.	natural language processing;text corpus	Adam Jatowt;Kevin Duh	2014	IEEE/ACM Joint Conference on Digital Libraries		natural language processing;speech recognition;time–frequency analysis;historical linguistics;semeval;sparse matrix;computer science;cultural diversity	NLP	-30.552451998929623	-74.3284027970881	97649
9e1d569c6ddf86548883f0925ce3ac4620503774	abstractive cross-language summarization via translation model enhanced predicate argument structure fusing	integer linear programming;ieee transactions;integer linear programming abstractive cross language summarization predicate argument structure translation model;speech;speech enhancement;english to chinese cross language summarization abstractive cross language multidocument summarization enhanced predicate argument structure fusing single sentence selection source language documents machine translation system bilingual concepts source side predicate argument structures source side pas bilingual pas elements integer linear programming salience quality translation quality;feature extraction ieee transactions speech hurricanes speech enhancement integer linear programming;feature extraction;hurricanes;natural language processing document handling integer programming language translation linear programming linguistics	Cross-language multidocument summarization is the task to generate a summary in a target language e.g., Chinese from a collection of documents in a different source language e.g., English. Previous methods such as the extractive and compressive algorithms focus only on single sentence selection and compression, which cannot make full use of the similar sentences containing complementary information. Furthermore, the translation model knowledge is not fully explored in previous approaches. To address these two problems, we propose in this paper an abstractive cross-language summarization framework. First, the source language documents are translated into target language with a machine translation system. Then, the method constructs a pool of bilingual concepts and facts represented by the bilingual elements of the source-side predicate-argument structures PAS and their target-side counterparts. Finally, new summary sentences are produced by fusing bilingual PAS elements with the integer linear programming algorithm to maximize both of the salience and translation quality of the PAS elements. The experimental results on English-to-Chinese cross-language summarization demonstrate that our proposed method outperforms the state-of-the-art extractive systems in both automatic and manual evaluations.	algorithm;automatic summarization;compiler;integer programming;linear programming;machine translation;regular expression	Jiajun Zhang;Yu Zhou;Chengqing Zong	2016	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2016.2586608	rouge;natural language processing;speech recognition;integer programming;feature extraction;tropical cyclone;computer science;speech;automatic summarization;machine learning;linguistics	NLP	-22.355067078252	-76.43485090726281	97724
1bd7f7e5350c25d40534d93b1923e4278a057458	cyber hate classification: 'othering' language and paragraph embedding		Hateful and offensive language (also known as hate speech or cyber hate) posted and widely circulated via the World Wide Web can be considered as a key risk factor for individual and societal tension linked to regional instability. Automated Web-based hate speech detection is important for the observation and understanding trends of societal tension. In this research, we improve on existing research by proposing different data mining feature extraction methods. While previous work has involved using lexicons, bags-of-words or probabilistic parsing approach (e.g. using Typed Dependencies), they all suffer from a similar issue which is that hate speech can often be subtle and indirect, and depending on individual words or phrases can lead to a significant number of false negatives. This problem motivated us to conduct new experiments to identify subtle language use, such as references to immigration or job prosperity in a hateful context. We propose a novel u0027Othering Lexiconu0027 to identify these subtleties and we incorporate our lexicon with embedding learning for feature extraction and subsequent classification using a neural network approach. Our method first explores the context around othering terms in a corpus, and identifies context patterns that are relevant to the othering context. These patterns are used along with the othering pronoun and hate speech terms to build our u0027Othering Lexiconu0027. Embedding algorithm has the superior characteristic that the similar words have a closer distance, which is helpful to train our classifier on the negative and positive classes. For validation, several experiments were conducted on different types of hate speech, namely religion, disability, race and sexual orientation, with F-measure scores for classifying hateful instances obtained through applying our model of 0.93, 0.95, 0.97 and 0.92 respective.	aggregate data;algorithm;baseline (configuration management);experiment;framing (world wide web);instability;lexicon;n-gram;outgroup (cladistics);parsing;semantic similarity;social media;social network;unrest;world wide web	Wafa Alorainy;Pete Burnap;Han Liu;Matthew Williams	2018	CoRR		machine learning;artificial intelligence;paragraph;natural language processing;voice activity detection;parsing;lexicon;probabilistic logic;feature extraction;computer science;classifier (linguistics);instability	NLP	-20.844829239784083	-67.92825420824497	97799
3d323cd8b20c9db363ec2729f28cfb831f487f12	engkoo: mining the web for language learning	statistical machine translation;language learning;system level;chinese user;currently engkoo;web page;massive set;nlp technology;cross language retrieval;mining translation knowledge	This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Specifically, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together at the same time covering the most up-to-date terms as captured by the net.	dictionary;internet;lexicon;natural language processing;parallel computing;real-time computing;real-time web;software system;statistical machine translation;web mining;web page;world wide web	Matthew R. Scott;Xiaohua Liu;Ming Zhou	2011			natural language processing;speech recognition;computer science;machine learning;linguistics;world wide web	NLP	-30.69763014820938	-67.64369359935924	97874
5953b5b5794c2758c2f110c7d5190ddffa5bbbec	automatic feature engineering for italian question answering systems		In this paper, we propose automatic feature engineering for Italian QA systems. Our approach only requires a shallow syntactic representation of the questions and the answer passages. We apply Support Vector Machines using tree kernels to such trees for automatically generating relational syntactic patters, which significantly improve on BM25 retrieval models.	feature engineering;question answering;software quality assurance;support vector machine	Antonio E. Uva;Alessandro Moschitti	2015			support vector machine;data mining;artificial intelligence;machine learning;syntax;question answering;tree kernel;feature engineering;computer science	NLP	-22.25996575618141	-74.005523780362	97922
10116d82a8438c78877a8a142be47c4ee8662138	methods for intrinsic plagiarism detection and author diarization		The paper investigates methods for intrinsic plagiarism detection and author diarization. We developed a plagiarism detection method based on constructing an author style function from features of text sentences and detecting outliers. We adapted the method for the diarization problem by segmenting author style statistics on text parts, which correspond to different authors. Both methods were tested on the PAN-2011 collection for the intrinsic plagiarism detection and implemented for the PAN-2016 competition on author diarization.	discrepancy function;expectation–maximization algorithm;f1 score;sensor;speaker diarisation	Mikhail P. Kuznetsov;Anastasia Motrenko;Rita Kuznetsova;Vadim V. Strijov	2016			instrumental and intrinsic value;information retrieval;speaker diarisation;speech recognition;computer science;plagiarism detection	ML	-23.109092711312492	-71.16590853922769	97934
50b71cc473b26e5985f24c8a0313fc3b44b1a7e0	semantic web evaluation challenges		The Open Knowledge Extraction (OKE) challenge is aimed at promoting research in the automatic extraction of structured content from textual data and its representation and publication as Linked Data. We designed two extraction tasks: (1) Entity Recognition, Linking and Typing and (2) Class Induction and entity typing. The challenge saw the participations of four systems: CETUS-FOX and FRED participating to both tasks, Adel participating to Task 1 and OAK@Sheffield participating to Task 2. In this paper we describe the OKE challenge, the tasks, the datasets used for training and evaluating the systems, the evaluation method, and obtained results.	linked data;open knowledge;semantic web;structured content;text corpus;typing	Fabien L. Gandon;Elena Cabrio;Milan Stankovic;Antoine Zimmermann	2015		10.1007/978-3-319-25518-7	computer science;social semantic web;data mining;semantic web stack;world wide web;information retrieval	NLP	-32.369695019793944	-66.72996367593102	98009
73cf712691a4836c754e593db9e61e9f88bad5df	peking: building semantic dependency graphs with a hybrid parser		This paper is a description of our system for SemEval-2015 Task 18: Broad-Coverage Semantic Dependency Parsing. We implement a hybrid parser which benefits from both transition-based and graph-based parsing approaches. In particular, the tree approximation method is explored to take advantage of wellstudied tree parsing techniques. Evaluation on multilingual data sets demonstrates that considerably good semantic analysis can be automatically built by applying state-of-the-art data-driven parsing techniques.	approximation;hybrid system;parser;semeval	Yantao Du;Fan Zhang;Xun Zhang;Weiwei Sun;Xiaojun Wan	2015			natural language processing;computer science;database;programming language	NLP	-21.632509883553617	-76.02047476404216	98043
29e9d4774c5e70c9e68deb517fafc559ea1825dd	cross-lingual discourse relation analysis: a corpus study and a semi-supervised classification system		We present a cross-lingual discourse relation analysis based on a parallel corpus with discourse information available only for one language. First, we conduct a corpus study to explore differences in discourse organization between Chinese and English, including differences in information packaging, implicit/explicit discourse expression divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification.	discourse relation;logical connective;machine learning;parallel text;semi-supervised learning;semiconductor industry;supervised learning;text corpus	Junyi Jessy Li;Marine Carpuat;Ani Nenkova	2014			natural language processing;linguistics	NLP	-25.030747447102776	-72.8806094644513	98057
118d590c74b3bcedc4cd879e76f158f61e09b316	building a non-trivial paraphrase corpus using multiple machine translation systems		We propose a novel sentential paraphrase acquisition method. To build a wellbalanced corpus for Paraphrase Identification, we especially focus on acquiring both non-trivial positive and negative instances. We use multiple machine translation systems to generate positive candidates and a monolingual corpus to extract negative candidates. To collect nontrivial instances, the candidates are uniformly sampled by word overlap rate. Finally, annotators judge whether the candidates are either positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs.	machine translation;text corpus;wake-on-ring	Yui Suzuki;Tomoyuki Kajiwara;Mamoru Komachi	2017		10.18653/v1/P17-3007	artificial intelligence;computer-assisted translation;computer science;natural language processing;machine translation;machine translation software usability;paraphrase;speech recognition;rule-based machine translation;example-based machine translation	NLP	-25.49155246767349	-73.90336904846662	98077
cafe95b0fb526b325d6c6e35905321afd379485b	learning thesaurus relations from distributional features		In distributional semantics words are represented by aggregated context features. The similarity of words can be computed by comparing their feature vectors. Thus, we can predict whether two words are synonymous or similar with respect to some other semantic relation. We will show on six different datasets of pairs of similar and non-similar words that a supervised learning algorithm on feature vectors representing pairs of words outperforms cosine similarity between vectors representing single words. We compared different methods to construct a feature vector representing a pair of words. We show that simple methods like pairwise addition or multiplication give better results than a recently proposed method that combines different types of features. The semantic relation we consider is relatedness of terms in thesauri for intellectual document classification. Thus our findings can directly be applied for the maintenance and extension of such thesauri. To the best of our knowledge this relation was not considered before in the field of distributional semantics.	algorithm;cosine similarity;distributional semantics;document classification;feature vector;ontology components;supervised learning;thesaurus (information retrieval)	Rosa Tsegaye Aga;Christian Wartena;Lucas Drumond;Lars Schmidt-Thieme	2016			supervised learning;natural language processing;distributional semantics;cosine similarity;computer science;multiplication;feature vector;pairwise comparison;pattern recognition;artificial intelligence;document classification	NLP	-25.724089430601307	-66.75180209025437	98319
0335e19dd883b1b398150cba9daee152d1c20c4f	parameterization of the input in training the hvs semantic parser	hidden vector state;automatic generation;feature vector	The aim of this paper is to present an extension of the hidden vector state semantic parser. First, we describe the statistical semantic parsing and its decomposition into the semantic and the lexical model. Subsequently, we present the original hidden vector state parser. Then, we modify its lexical model so that it supports the use of the input sequence of feature vectors instead of the sequence of words. We compose the feature vector from the automatically generated linguistic features (lemma form and morphological tag of the original word). We also examine the effect of including the original word into the feature vector. Finally, we evaluate the modified semantic parser on the Czech Human-Human train timetable corpus. We found that the performance of the semantic parser improved significantly compared with the baseline hidden vector state parser.	human visual system model;parser	Jan Svec;Filip Jurcícek;Ludek Müller	2007		10.1007/978-3-540-74628-7_54	natural language processing;parser combinator;speech recognition;feature vector;computer science;glr parser;pattern recognition;recursive descent parser	NLP	-20.379010218276036	-80.00958510790835	98460
55507038af2368e19a279bb9e6b906f4f87b66d2	unsupervised word alignment with arbitrary features	word alignment	We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.	bitext word alignment;data structure alignment;discriminative model;generative model;log-linear model;statistical machine translation	Chris Dyer;Jonathan H. Clark;Alon Lavie;Noah A. Smith	2011			natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-20.02058051327556	-75.94396356208622	98632
6d1e84076d76fe26c366905180c137112f1e550e	alignment algorithms for learning to read aloud	benchmark data set;popular custom;domain-specific parameter;alignment algorithm;error rate;hill climbing	A complete system of learning spelling-tophoneme conversion of English words consists of three major processes: alignment, mapping learning, and grapheme generation. Such a system can be used to construct prototypes of reading machines for English or other lan­ guages quickly and automatically. This paper focusses on the alignment process, which is crit­ ical to mapping learning and grapheme genera­ t ion. We present several novel alignment algo­ r i thms which learn alignment wi thout supervi­ sion. The basic alignment algori thm is a hi l l cl imbing algori thm. Several improvements of it are studied and tested. In addit ion, a method that overcomes the pi t fa l l in hi l l-cl imbing al­ gorithms is designed. Our best alignment al­ gor i thm produces very impressive results: only 0.5% of error rate.	backpropagation;best practice;complex systems;computation;d programming language;dectalk;de morgan's laws;decision tree;id3 algorithm;international conference on machine learning;linear algebra;local area transport;mathematical induction;morgan;north american mesoscale model;reverse mathematics;ross quinlan;search algorithm;sion's minimax theorem;speech synthesis;think aloud protocol;ical	Charles X. Ling;Handong Wang	1997			machine learning;word error rate;learning to read;hill climbing;computer science;artificial intelligence	AI	-21.150554340135532	-78.80924171910779	98658
777f65deb3bab61f453c97b28990b056c78493e2	automatic extraction of hyponyms from japanese newspapers. using lexico-syntactic patterns		"""We describe a method to automatically extract hyponyms from Japanese newspapers. First, we discover patterns which can extract hyponyms of a noun, such as """"A nado-no B (B such as A)"""", then we apply the patterns to the newspaper corpus to extract instances. The procedure works best to extract hyponyms of concrete things in the middle of the word hierarchies. The precision is 49-87 percent depending on the patterns. We compare the extracted hyponyms and those associated by humans. We find that the popular words in the associative concept dictionary are likely to be found in the corpus but also many additional hyponyms can be extracted from 32 years of newspaper articles."""	dictionary;lexico;software patent;text corpus;world wide web	Maya Ando;Satoshi Sekine;Shun Ishizaki	2004			syntax;noun;natural language processing;artificial intelligence;newspaper;associative property;lexico;computer science;hierarchy	NLP	-27.056069067314372	-73.06723261938049	98691
a7aaf74cd9f8afb24560efc0477362ad22219b18	fishing for exactness	exact test;likelihood ratio;statistical natural language processing;statistical method;natural language;chi square test;sparse data	Statistical methods for automatically identifying de pendent word pairs i e dependent bigrams in a cor pus of natural language text have traditionally been performed using asymptotic tests of signi cance This paper suggests that Fisher s exact test is a more ap propriate test due to the skewed and sparse data sam ples typical of this problem Both theoretical and experimental comparisons between Fisher s exact test and a variety of asymptotic tests the t test Pearson s chi square test and Likelihood ratio chi square test are presented These comparisons show that Fisher s exact test is more reliable in identifying dependent word pairs The usefulness of Fisher s exact test ex tends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language The experiment presented in this paper was performed using PROC FREQ of the SAS System	bigram;minimum fisher information;natural language processing;sas;sparse matrix;stochastic grammar;turing test	Ted Pedersen	1996	CoRR		pearson's chi-squared test;p-value;speech recognition;score test;chi-square test;sparse matrix;likelihood-ratio test;computer science;machine learning;exact statistics;exact test;fisher's exact test;natural language;statistics	ML	-24.028809002778054	-79.36186334762661	98718
1603ac6b0c31ade2a55095e23fc26ea7cc49ebc4	discovering author groups using a b-compact graph-based clustering		Identifying the authorship either of an anonymous or a doubtful document constitutes a cornerstone for automatic forensic applications. Moreover, it is a challenging task for both humans and computers. Clustering documents according to the linguistic style of the authors who wrote them has been a task little studied by the research community. In order to address this problem, PAN Evaluation Framework has become the first effort to promote the development of the author clustering. This article proposes a graph-based method, specifically βcompact clustering, for discovering the groups of documents written by the same author. The β-compact algorithm is based on the analysis of the similarity between documents and they belong to the same group as long as the similarity between them exceeds the threshold β and it is the maximum similarity with respect to other documents. In our proposal we evaluated different linguistic features and similarity measures presented in previous works of authorship analysis task. The training dataset was used to determine the best value of β parameter for each language. The result of the experiments was encouraging.	algorithm;cluster analysis;computer cluster;experiment;stylometry	Yasmany García-Mondeja;Daniel Castro-Castro;Vania Lavielle-Castro;Rafael Muñoz	2017			cluster analysis;artificial intelligence;graph;pattern recognition;computer science	NLP	-25.902798915068196	-66.29819681449624	98739
ca3ea1fdf578449b8689badc437dc4e0e85a3477	de la représentation de l'interlocuteur vers un modèle utilisateur formel pour le dialogue personne-machine	natural language dialogue;dialogue;natural language;interlocutor model;user model	This paper proposes the formalization and implementation of user model. It is based on an interlocutor's model in natural language dialogue system. Experimental studies on interlocutor's representation and, particularly, on user's familiarization with the system were used for the results analyses. The analysis of the results has shown the more the user is familiarized the more he gives relevant information for research in his first utterances, and with the same number of words. The formalization proposed concerns (1) user's expertise or familiarization level, and (2) the detection of this level. The extended user model allows the system to adapt and thus, to contribute to increase usability and its efficiency in the information research processes.	dialog system;linear algebra;natural language;usability;user modeling	Philippe Bretier;Ludovic Le Bigot;Franck Panaget;David Sadek	2004		10.1145/1148613.1148618	natural language processing;computer science;linguistics;communication	NLP	-33.600537079946264	-79.4459887203705	98794
70c65702f0761d3722b53fb1b8356affc8da53cc	term recognition using conditional random fields	biomedical terms;probability;general term;conditional random fields;biomedical terms term recognition conditional random fields machine learning syntactic features;machine learning;syntactic features;syntactics;probability learning artificial intelligence natural language processing;conditional random field;novel term tracking term recognition conditional random fields syntactic function general term;syntactic function;learning artificial intelligence;novel term;term recognition;natural language processing;tracking	A machine learning framework, Conditional Random fields (CRF), is constructed in this study, which exploits syntactic information to recognize biomedical terms. Features used in this CRF framework focus on syntactic information in different levels, including parent nodes, syntactic functions, syntactic paths and term ratios. A series of experiments have been done to study the effects of training sizes, general term recognition and novel term recognition. The experiment results show that features as syntactic paths and term ratios can achieve good precision of term recognition, including both general terms and novel terms. However, the recall of novel term recognition is still unsatisfactory, which calls for more effective features to be used. All in all, as this research studies in depth the uses of some unique syntactic features, it is innovative in respect of constructing machine learning based term recognition system.	conditional random field;experiment;machine learning;terminology extraction	Xing Zhang;Yan Song;Alex Chengyu Fang	2010	Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)	10.1109/NLPKE.2010.5587809	natural language processing;feature;computer science;machine learning;pattern recognition	NLP	-24.562931364053984	-69.37231406300549	98965
6af77763848f6efba4003459cf902540fb13dc2e	thunlp at tac kbp 2013 in entity linking		Entity Linking is to link a name string from plain-text documents to the corresponding entry in given knowledge base. In this paper we demonstrate our entity linking system for TAC KBP 2011 Track. Our system implements pairwise and listwise learning to rank methods to create a ranking list of candidates with several kinds of features, including context similarity, term frequency, key entity extraction and WikiPage information. We participate in entity linking and cross-lingual entity linking task. We use random forest to validate the top 1 candidate recommended by our system. We achieve a performance of 72.9% F1 measure for both two tasks.	cluster analysis;entity linking;f1 score;information retrieval;kilobyte;knowledge base;learning to rank;named-entity recognition;random forest;relevance;tf–idf	Yan Wang;Yankai Lin;Zhiyuan Liu;Maosong Sun	2011				NLP	-26.202535203794977	-67.42851676562991	99114
cf10c692ad1b9183777680387710a2e01da01c7c	grammar transfer in a second order recurrent neural network	second order;in vocabulary;recurrent neural network;finite state machine;neural network	It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis.	artificial neural network;categorial grammar;finite-state machine;linear discriminant analysis;recurrent neural network;turing completeness;vocabulary	Michiro Negishi;Stephen Jose Hanson	2001			natural language processing;synchronous context-free grammar;speech recognition;computer science;recurrent neural network;machine learning;time delay neural network;finite-state machine;second-order logic;artificial neural network	AI	-20.011417220824764	-79.99529599658332	99207
b8e25c66778e6f7ce85d411c17db5f91dc753a00	pattern-based distinction of paradigmatic relations for german nouns, verbs, adjectives		This paper implements a simple vector space model relying on lexico-syntactic patterns to distinguish between the paradigmatic relations synonymy, antonymy and hypernymy. Our study is performed across word classes, and models the lexical relations between German nouns, verbs and adjectives. Applying nearest-centroid classification to the relation vectors, we achieve a precision of 59.80%, which significantly outperforms the majority baseline (χ, p<0.05). The best results rely on large-scale, noisy patterns, without significant improvements from various pattern generalisations and reliability filters. Analysing the classification shows that (i) antonym/synonym distinction is performed significantly better than synonym/hypernym distinction, and (ii) that paradigmatic relations between verbs are more difficult to predict than paradigmatic relations between nouns or adjectives.	baseline (configuration management);lexico;paradigm	Sabine Schulte im Walde;Maximilian Köper	2013		10.1007/978-3-642-40722-2_19	noun;german nouns;computational linguistics;vector space model;synonym;artificial intelligence;mathematics;pattern recognition	NLP	-25.207328225454837	-71.50211259072056	99321
9e31d11d05ce389833766dfd3c68a56d5029b6bc	language use matters: analysis of the linguistic structure of question texts can characterize answerability in quora		Quora is one of the most popular community Q&A sites of recent times. However, many question posts on this Q&A site often do not get answered. In this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. Our central finding is that the way users use language while writing the question text can be a very effective means to characterize answerability. This characterization helps us to predict early if a question remaining unanswered for a specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). Notably, features representing the language use patterns of the users are most discriminative and alone account for an accuracy of 74.18%. We also compare our method with some of the similar works (Dror et al., Yang et al.) achieving a maximum improvement of ∼ 39% in terms of accuracy.	baseline (configuration management);precision and recall;yang	Suman Kalyan Maity;Aman Kharb;Animesh Mukherjee	2017			natural language processing;computer science;linguistics	NLP	-25.434585325425235	-70.9451261350255	99334
0902df2ebe6a4019b94835283b7b6d79b4417806	generalized agreement for bidirectional word alignment		While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.	bitext word alignment;constrained optimization;data structure alignment;expectation–maximization algorithm;loss function;sequence alignment;viterbi algorithm	Chunyang Liu;Yang Liu;Maosong Sun;Huan-Bo Luan;Heng Yu	2015			speech recognition;machine learning;pattern recognition	NLP	-20.4621066919228	-76.45060296941031	99501
940272400f0eb2904cb23b7d5001a13e1fb7ad62	measuring tagging performance of a joint language model	indexing terms;part of speech tagging;spoken language processing;language model	Predicting syntactic information in a joint language model (LM) has been shown not only to improve the model at its main task of predicting words, but it also allows this information to be passed to other applications, such as spoken language processing. This raises the question of just how accurate the syntactic information predicted by the LM is. In this paper, we present a joint LM designed not only to scale to large quantities of training data, but also to be able to utilize fine-grain syntactic information, as well as other features, such as morphology and prosody. We evaluate the accuracy of our model at predicting syntactic information on the POS tagging task against state-ofthe-art POS taggers and on perplexity against the ngram model.	language model;mathematical morphology;n-gram;part-of-speech tagging;perplexity;semantic prosody	Denis Filimonov;Mary P. Harper	2009			natural language processing;speech recognition;index term;computer science;linguistics;language model	NLP	-21.901632715442997	-76.39651589518066	99650
11b1d4f739a29f66b2381e07c35e46582d6e48e4	multilex, a pipelined lexical analyzer	pipelined lexical analyzer		lexical analysis	Timothy W. Bickmore;Robert E. Filman	1997	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(199701)27:1%3C25::AID-SPE70%3E3.0.CO;2-W	computer science;theoretical computer science;parsing;pattern matching;database;programming language;regular expression	NLP	-30.104418452215857	-78.46363407534965	99690
6bf2bbd3580bd6564683d21796bb6674c64d55cb	interactive medical word sense disambiguation with instance and feature labeling			word sense;word-sense disambiguation	Yue Wang;Kai Zheng;Hua Xu;Qiaozhu Mei	2017			word-sense disambiguation;pattern recognition;artificial intelligence;computer science	NLP	-30.357762751485318	-77.01977499083091	99724
a928ee872e4b91d8a32bd6ee1abde15f2b7e508d	deep belief network based semantic taggers for spoken language understanding		This paper investigates the use of deep belief networks (DBN) for semantic tagging, a sequence classification task, in spoken language understanding (SLU). We evaluate the performance of the DBN based sequence tagger on the well-studied ATIS task and compare our technique to conditional random fields (CRF), a state-of-the-art classifier for sequence classification. In conjunction with lexical and named entity features, we also use dependency parser based syntactic features and part of speech (POS) tags [1]. Under both noisy conditions (output of automatic speech recognition system) and clean conditions (manual transcriptions), our deep belief network based sequence tagger outperforms the best CRF based system described in [1] by an absolute 2% and 1% F-measure, respectively.Upon carrying out an analysis of cases where CRF and DBN models made different predictions, we observed that when discrete features are projected onto a continuous space during neural network training, the model learns to cluster these features leading to its improved generalization capability, relative to a CRF model, especially in cases where some features are either missing or noisy.	artificial neural network;automatic transmitter identification system (television);bayesian network;brill tagger;brown corpus;conditional random field;deep belief network;f1 score;named entity;natural language understanding;speech recognition;whole earth 'lectronic link	Anoop Deoras;Ruhi Sarikaya	2013			natural language processing;speech recognition;linguistics	NLP	-21.982211965475962	-75.65700476085681	99928
bfa2b3203618c7639fa2b568842515deada35695	clinical information extraction via convolutional neural network		We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) con-volutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.	artificial neural network;baseline (configuration management);convolutional neural network;deep learning;information extraction;multilayer perceptron;naruto shippuden: clash of ninja revolution 3;part-of-speech tagging;quad flat no-leads package	Peng Li;Heng Huang	2016	CoRR		computer science;machine learning;pattern recognition;data mining	AI	-19.144978984075518	-72.18169197652273	100091
63c4419c454a4ce48aa6bf87b3eb734d1bb1bee4	unsupervised feature-rich clustering		Unsupervised clustering of documents is challenging because documents can conceivably be divided across multiple dimensions. Motivated by prior work incorporating expressive features into unsupervised generative models, this paper presents an unsupervised model for categorizing textual data which is capable of utilizing arbitrary features over a large context. Utilizing locally normalized log-linear models in the generative process, we offer straightforward extensions to the standard multinomial mixture model that allow us to effectively utilize automatically derived complex linguistic, statistical, and metadata features to influence the learned cluster structure for the desired task. We extensively evaluate and analyze the model’s capabilities over four distinct clustering tasks: topic, perspective, sentiment analysis, and Congressional bill survival, and show that this model outperforms strong baselines and state-of-the-art models.	categorization;cluster analysis;generative model;linear model;log-linear model;mixture model;multinomial logistic regression;sentiment analysis;software feature;supervised learning;text corpus	Vladimir Eidelman	2012			computer science;machine learning;pattern recognition;data mining	NLP	-19.285654190947344	-67.81942035308292	100098
44e84a8eb378813aa6f27d4e09341fc96e6b910e	building a hierarchical annotated corpus of urdu: the urdu.kon-tb treebank	pos;hybrid;urdu;treebank;phrase	This work aims at the development of a representative treebank for the South Asian language Urdu. Urdu is a comparatively under resourced language and the development of a reliable treebank for Urdu will have significant impact on the state-of-the-art for Urdu language processing. In URDU.KON-TB treebank described here, a POS tagset, a syntactic tagset and a functional tagset have been proposed. The construction of the treebank is based on an existing corpus of 19 million words for the Urdu language. Part of speech (POS) tagging and annotation of a selected set of sentences from different sub-domains of this corpus is in process manually and the work performed till to date is presented here. The hierarchical annotation scheme we adopted has a combination of a phrase structure (PS) and a hybrid dependency structure (HDS).	holographic data storage;language identification;machine learning;natural language processing;parsing;part-of-speech tagging;pattern matching;phrase structure rules;stochastic context-free grammar;terabyte;treebank;word-sense disambiguation	Qaiser Abbas	2012		10.1007/978-3-642-28604-9_6	natural language processing;speech recognition;hybrid;computer science;treebank;linguistics;point of sale	NLP	-29.50022718258595	-75.88180716389438	100244
0c74b326e706b9b41de06ae564c40bcba28f1620	learning features from co-occurrences: a theoretical analysis		Representing a word by its co-occurrences with other words in context is an effective way to capture the meaning of the word. However, the theory behind remains a challenge. In this work, taking the example of a word classification task, we give a theoretical analysis of the approaches that represent a word X by a function f(P (C|X)), where C is a context feature, P (C|X) is the conditional probability estimated from a text corpus, and the function f maps the co-occurrence measure to a prediction score. We investigate the impact of context feature C and the function f . We also explain the reasons why using the co-occurrences with multiple context features may be better than just using a single one. In addition, based on the analysis, we propose a hypothesis about the conditional probability on zero probability events.	algorithm design;distributional semantics;map;semantic similarity;text corpus	Yanpeng Li	2018			machine learning;natural language processing;conditional probability;computer science;pattern recognition;text corpus;feature learning;artificial intelligence	AI	-19.792261461917445	-69.8791833971325	100401
e1f9b170646983d2f93efd9231f70b174689518a	semi-automatic detection of cross-lingual marketing blunders based on pragmatic label propagation in wiktionary		We introduce the task of detecting cross-lingual marketing blunders, which occur if a trade name resembles an inappropriate or negatively connotated word in a target language. To this end, we suggest a formal task definition and a semi-automatic method based the propagation of pragmatic labels from Wiktionary across sense-disambiguated translations. Our final tool assists users by providing clues for problematic names in any language, which we simulate in two experiments on detecting previously occurred marketing blunders and identifying relevant clues for established international brands. We conclude the paper with a suggested research roadmap for this new task. To initiate further research, we publish our online demo along with the source code and data at http://uby.ukp.informatik.tu-darmstadt.de/blunder/.	compiler;dictionary;error analysis (mathematics);experiment;jruby;knowledge-based systems;language-independent specification;multitier architecture;orthographic projection;semiconductor industry;sensor;simulation;software propagation;substring;text corpus	Christian M. Meyer;Judith Eckle-Kohler;Iryna Gurevych	2016			natural language processing;trade name;computer science;source code;artificial intelligence;publication;marketing	NLP	-30.25539501917912	-73.24724066050916	100436
7d376f038f6ccd1d254716ca0b25ca87b667ab3a	a brief introduction to natural language processing for non-linguists	language comprehension;lenguaje natural;morphologie;reconocimiento lenguaje;linguistique;reconnaissance langage;langage naturel;tratamiento lenguaje;computational method;language recognition;morphology;linguistica;phonology;language processing;natural language;traitement langage;fonologia;comprension lenguaje;comprehension langage;phonologie;morfologia;natural language processing;linguistics	This chapter introduces the field of natural language processing to the computer scientist or logician outside of the field. We first discuss some concepts from the field of linguistics, which focuses on the language half of the NLP equation, then move on to some of the common computational methods used to process and understand language. No previous knowledge of NLP is assumed.		Cynthia A. Thompson	1999		10.1007/3-540-40030-3_2	natural language processing;language identification;language and communication technologies;natural language programming;universal networking language;question answering;morphology;object language;computer science;computational linguistics;natural language;temporal annotation;language technology;phonology	AI	-31.468487295786467	-75.71933303848823	100496
b2b42f7c433ff220fda916a620ebd57e35229b97	can domain adaptation be handled as analogies?		Aspect identification in user generated texts by supervised text classification might suffer degradation in performance when changing to other domains than the one used for training. For referring to aspects such as quality, price or customer services the vocabulary might differ and affect performance. In this paper, we present an experiment to validate a method to handle domain shifts when there is no available labeled data to retrain. The system is based on the offset method as used for solving word analogy problems in vector semantic models such as word embedding. Despite of the fact that the offset method indeed found relevant analogues in the new domain for the classifier initial selected features, the classifiers did not deliver the expected results. The analysis showed that a number of words were found as analogues for many different initial features. This phenomenon was already described in the literature as 'default words' or 'hubs'. However, our data showed that it cannot be explained in terms of word frequency or distance to the question word, as suggested.	document classification;domain adaptation;elegant degradation;vocabulary;word embedding;word lists by frequency	Núria Bel;Joel Pocostales	2018				AI	-20.02458876377801	-72.83269731174406	100576
4fe0a2f6aec54b142015f01d1d12a6b645330ef7	the probability distribution of textual vocabulary in the english language		AbstractThe probability of textual vocabulary is defined as the combined probabilities of the individual lemmas occurring in a text, which sum to 1 in the text but normally less than 1 in another different text. If the text is expanded the probability of the original textual vocabulary would be smaller than 1 in the expanded text. However, the present study reveals that as the text expands continually, instead of monotonically decreasing, the probability of the original textual vocabulary quickly reaches a point from which it stabilizes despite further expansion of the text. In addition, the probability of the textual vocabulary of a text occurring in other texts is not affected by the length of the texts in which they occur. Mathematical models are formulated capturing the distribution of the probability of textual vocabulary in the English language.	vocabulary	Fengxiang Fan;Yu Kyung Yang;Yaqin Wang	2016	Journal of Quantitative Linguistics	10.1080/09296174.2015.1071149	natural language processing;speech recognition;computer science;linguistics	NLP	-23.82389969302125	-79.98436976773516	100808
3e661cad3f16f62a38a2a74116cccad23edac97f	improvement of translation accuracy for the outlines of japanese statutes by splitting parenthesized expressions	parenthesized expressions japanese statutes japanese legal information sharing statistical machine translation system;systems engineering and theory;knowledge engineering systems engineering and theory;statistical machine translation legal information;law administration language translation;knowledge engineering	To globally share Japanese legal information, we translate the Outlines of Japanese statutes. These outlines are the official summaries of Japanese statutes and are useful to quickly understand their contents. In a previous statistical machine translation system for the outlines, we found that the training corpus consisted of both statutes and their outlines, including many long sentences that reduced the translation quality. To solve this problem, we shortened the length of sentences and focused on parenthesized expressions. In this paper, we propose a translation method that splits off parenthesized expressions from the sentences. Experimental result shows the effectiveness of our method.		Kouhei Okada;Yasuhiro Ogawa;Makoto Nakamura;Tomohiro Ohno;Katsuhiko Toyama	2015		10.1109/KSE.2015.61	natural language processing;speech recognition;example-based machine translation;computer science;artificial intelligence;knowledge engineering;data mining;linguistics;rule-based machine translation;algorithm	NLP	-24.932074521265346	-75.80152938574425	100831
23dcdb421a5dc1b8c0ebe1bfcbdd35a82b169118	a multifaceted evaluation of neural versus phrase-based machine translation for 9 language directions		We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.	computer performance;mechatronics;neural machine translation;programming paradigm	Antonio Toral;Víctor M. Sánchez-Cartagena	2017			dynamic and formal equivalence;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;rule-based machine translation	NLP	-21.278593214790604	-76.93680918256396	100877
310bdf0132415f0f9d6ef990f3218c97f53cade9	agile text mining for the 2014 i2b2/uthealth cardiac risk factors challenge	clinical natural language processing;information extraction;text mining	This paper describes the use of an agile text mining platform (Linguamatics' Interactive Information Extraction Platform, I2E) to extract document-level cardiac risk factors in patient records as defined in the i2b2/UTHealth 2014 challenge. The approach uses a data-driven rule-based methodology with the addition of a simple supervised classifier. We demonstrate that agile text mining allows for rapid optimization of extraction strategies, while post-processing can leverage annotation guidelines, corpus statistics and logic inferred from the gold standard data. We also show how data imbalance in a training set affects performance. Evaluation of this approach on the test data gave an F-Score of 91.7%, one percent behind the top performing system.		James Cormack;Chinmoy Nath;David Milward;Kalpana Raja;Siddhartha Reddy Jonnalagadda	2015	Journal of biomedical informatics	10.1016/j.jbi.2015.06.030	natural language processing;text mining;computer science;bioinformatics;artificial intelligence;data science;machine learning;pattern recognition;data mining;database;information extraction	ML	-23.70412004168781	-69.71917376341922	101099
8d4048c9b353b48e1f5534db9d77e5a620730e3d	extract interaction detection methods from the biological literature	computational biology bioinformatics;protein protein interaction;algorithms;pattern recognition automated;combinatorial libraries;protein interaction mapping;computational biology;computer appl in life sciences;information storage and retrieval;microarrays;bioinformatics	Considerable efforts have been made to extract protein-protein interactions from the biological literature, but little work has been done on the extraction of interaction detection methods. It is crucial to annotate the detection methods in the literature, since different detection methods shed different degrees of reliability on the reported interactions. However, the diversity of method mentions in the literature makes the automatic extraction quite challenging. In this article, we develop a generative topic model, the Correlated Method-Word model (CMW model) to extract the detection methods from the literature. In the CMW model, we formulate the correlation between the different methods and related words in a probabilistic framework in order to infer the potential methods from the given document. By applying the model on a corpus of 5319 full text documents annotated by the MINT and IntAct databases, we observe promising results, which outperform the best result reported in the BioCreative II challenge evaluation. From the promising experiment results, we can see that the CMW model overcomes the issues caused by the diversity in the method mentions and properly captures the in-depth correlations between the detection methods and related words. The performance outperforming the baseline methods confirms that the dependence assumptions of the model are reasonable and the model is competent for the practical processing.	baseline (configuration management);biocreative;body of uterus;database;flavor,mint;inference;interaction;text corpus;topic model	Hongning Wang;Minlie Huang;Xiaoyan Zhu	2009	BMC Bioinformatics	10.1186/1471-2105-10-S1-S55	protein–protein interaction;biology;dna microarray;computer science;bioinformatics;data science;data mining;algorithm	NLP	-23.65793400243342	-67.39834517974121	101140
e30b50e679b9286f4da753b1c69445ff7123a021	inducing morphemes using light knowledge	rewrite rule;morphological induction;hybrid approach;allomorphy;machine learning;computational linguistic;computational linguistics	Allomorphic variation, or form variation among morphs with the same meaning, is a stumbling block to morphological induction (MI). To address this problem, we present a hybrid approach that uses a small amount of linguistic knowledge in the form of orthographic rewrite rules to help refine an existing MI-produced segmentation. Using rules, we derive underlying analyses of morphs---generalized with respect to contextual spelling differences---from an existing surface morph segmentation, and from these we learn a morpheme-level segmentation. To learn morphemes, we have extended the Morfessor segmentation algorithm [Creutz and Lagus 2004; 2005; 2006] by using rules to infer possible underlying analyses from surface segmentations. A segmentation produced by Morfessor Categories-MAP Software v. 0.9.2 is used as input to our procedure and as a baseline that we evaluate against. To suggest analyses for our procedure, a set of language-specific orthographic rules is needed. Our procedure has yielded promising improvements for English and Turkish over the baseline approach when tested on the Morpho Challenge 2005 and 2007 style evaluations. On the Morpho Challenge 2007 test evaluation, we report gains over the current best unsupervised contestant for Turkish, where our technique shows a 2.5% absolute F-score improvement.	algorithm;american and british english spelling differences;baseline (configuration management);f1 score;michael creutz;morphological parsing;orthographic projection;rewrite (programming);rewriting;stumbleupon	Michael Tepper;Fei Xia	2010	ACM Trans. Asian Lang. Inf. Process.	10.1145/1731035.1731038	natural language processing;speech recognition;computer science;artificial intelligence;computational linguistics;machine learning;linguistics;allomorph	NLP	-23.917805823212113	-75.86421787398073	101177
cda689b95e4b9400b6df1aaa7e4bf53fb2b99c41	analysis of linguistic features for identifying information constituents of a concept		A concept is a unit of knowledge created by a unique combination of characteristics (ISO 1087-1, 2000). Each characteristic is an abstraction of a property of object(s). A term entry of encyclopedia is assumed to be a verbal designation of one concept, which is represented in its definition part. These characteristics are analyzed from a set of entries in an encyclopedia manually in disease subject field, whose characteristics are such as symptom, cause, and remedy. This set of characteristics recognizes properties of different diseases in the encyclopedia, in 81% precision. This work can be applied to automatic summarization or Q&A (question and answering).	automatic summarization	Haeseung Paik;Young-Soo Kang;Key-Sun Choi	2001			information retrieval;linguistics;natural language processing;computer science;artificial intelligence	NLP	-33.14029489311539	-69.64994609499524	101212
748df505207a6d425400633e06d6ac36f78393d5	the grec challenges 2010: overview and evaluation results	people entity;evaluation method;grec task;evaluation result;baseline system;neg task;human judge;coreference chain;people reference;grec tasks	There were threeGREC Tasks at Generation Challenges 2010: GREC-NER required participating systems to identify all people references in texts; for GRECNEG, systems selected coreference chains for all people entities in texts; and GRECFull combined theNER andNEG tasks, i.e. systems identified and, if appropriate, replaced references to people in texts. Five teams submitted 10 systems in total, and we additionally created baseline systems for each task. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges using preference strength judgements. This report presents the evaluation results, along with descriptions of the threeGREC tasks, the evaluation methods, and the participating systems.	baseline (configuration management);entity;intrinsic function	Anja Belz;Eric Kow	2010				NLP	-27.326145668940864	-68.78134242476712	101246
88308d4bb3f0c61bed09d17fb7493a9f2c441da2	specialized entailment engines: approaching linguistic aspects of textual entailment	tree edit distance;computational linguistic;textual entailment	Textual Entailment (TE), one of the current hot topics in Computational Linguistics, has been proposed as a task to address the problem of language variability. Since TE is due to the combination of different linguistic phenomena which interact among them in a complex way, this paper proposes to experiment the use of specialized entailment engines, each addressing a specific phenomenon relevant to entailment.	computation;computational linguistics;spatial variability;test engineer;textual entailment	Elena Cabrio	2009		10.1007/978-3-642-12550-8_31	natural language processing;textual entailment;computer science;linguistics;algorithm	NLP	-26.551808231801115	-72.10651624391821	101317
c1d0aa73a181a94cd57d5dc1f2ca16dcfd6c5515	corpus-based extension of semantic lexicons in large scale		ere has been an increased interest to acquire or extend, on a large-scale, highns. The methodology is usually corpus-driven. It is based on the (re-)use of urces of various types, and the application of cost effective ways to eliminate the i.e. derivational morphology, customization of off-the-shelf resources, statistical w parsing. This paper investigates how, and to what extent the flexibility and parser can be utilized to fully automatically achieve this goal. Our work is based t members of a semantic group are often surrounded by other members of the en a few category members we use parsed corpora to collect surrounding conother words that also belong to the same group. re has been an increased interest to use corpus-driven approaches to acquire xicons on large scale: (Grefenstette (1994); Dorr & Jones (1996); Hearst and ga et al. (1998); Lin (1998)). This paper investigates the use of a cost-effecacquisition bottleneck by exploiting the flexibility and robustness of a sysl parser. The parser uses fine-grained syntactic contexts for identifying rds and acquire large quantities of high quality general purpose semantic category members of a semantic group, we investigate whether it is possible urrounding contexts and identify other words, on a large scale, that also antic group. re is not to acquire the semantic lexicons from scratch, rather, to build on t our disposal. That is, lexical resources of high quality, manually produced tativelyinsufficientfor realistic large-scale tasks. Therefore, we focus our exploit inexpensive methods to progressively enrich the resources with sevclassified lexical units. the observation that members of a semantic group are often surrounded by ame group throughout a corpus. By a semantic group, we understand here for nd conjunctive phrases of the form: xa, xb, ..., xcor xa and xb, ..., and xc , ent-poor item, such as determiners and numerals, and a b cnouns or names. train this general observation by searching for particular types of phrases, of f particular semantic content provided by the available, limited semantic rces, then, are progressively enriched and applied in a bootstrapping manner racted from the corpora in order to classify as many as possible of the words e retrieved phrases. The level of the fine-grained syntactic analysis is made se of a robust parser developed by Abney (1997) in which Kokkinakis & (1999) have developed a large coverage grammar for written Swedish. The fer to are the Swedish SIMPLE lexicon ( Semantic Information for Multifunca) and gazeteers of person, location and organization names. Previous scale for Swedish (Kokkinakis et al. (2000)) have demonstrated that the task esources using syntactic information is feasible. Therefore we wanted to gnitude this can be done and evaluate, at least for some of the semantic e acquired semantic units. and important role in word acquisition. The use of syntax for generating	display resolution;galaxy morphological classification;jones calculus;jones polynomial;lexicon;parsing;text corpus;treebank	Dimitrios Kokkinakis;Maria Toporowska Gronostaj;Karin Warmenius	2001			computer science	NLP	-26.309779409353048	-75.298423171228	101336
51417b3bbef08140efe3f40ae1e005a654a8e39f	stochastic melody writing procedures: an analysis based approach				Jon Meinecke	1981			natural language processing;speech recognition;artificial intelligence;computer science	SE	-30.409090605895447	-79.18756352513027	101373
3797375691de703f058c7a3ef8ced7def4df2929	punct-an alternative verb semantic ontology representation		The goal is to build a verb ontology based on Indian grammatical tradition. We propose here an ontological structure to represent verbs in a language, which can be adapted across languages. This is an ongoing work and presently the method has been applied to develop ontologically informed etymon in English.	feature vector;language technology;misra c	Kavitha Rajan	2015			natural language processing;ontology;computer science;distributed computing;verb;artificial intelligence	NLP	-30.574164554629256	-76.71396286038191	101407
ff809bbbdd43e71d87fce0842cbc0a9f4980ec22	"""a note on k. bock's """"syntactic adjustment effect"""" problem"""	k. bock;syntactic adjustment effect		bundle adjustment	Ulrich Schade	1989		10.1007/978-3-642-74688-8_26		NLP	-29.299193978686795	-78.8480489501414	101491
071132f831c81abb9242821e540557aa22b4cb4a	dynamic programming method for analyzing conjunctive structures in japanese	long sentence;japanese sentence;prior part;renyoh chuushi-ho;dynamic programming method;conjunctive predicative clause;similar series;similar structure;posterior part;conjunctive noun phrase;conjunctive structure;noun phrase	"""Parsing a long sentence is very difficult, since long sentences often have conjunctions which result in ambiguities. If the conjunctive s tructures existing in a long sentence can be analyzed correctly, ambiguities can be reduced greatly and a sentence can be parsed in a high successful rate. Since the prior par t and the posterior par t of a conjunctive s t ructure have a similar s t ructure very often, finding two similar series of words is an essential point in solving this problem. Similarities of all pairs of words are calculated and then the two series of words which have the greatest sum of similarities are found by a technique of dynamic programming. We deal with not only conjunctive noun phrases, but also conjunctive predicative clauses created by """"Renyoh chuushi-ho"""". We will illustrate the effectiveness of this method by the analysis of 180 long Japanese sentences."""	dynamic programming;parsing	Sadao Kurohashi;Makoto Nagao	1992			natural language processing;noun phrase;speech recognition;boolean conjunctive query;linguistics;algorithm	NLP	-22.78330280893559	-78.76318282500162	101570
cb055fda317e5c16b76a0723687bfc2d1bf7cf45	computational exploration to linguistic structures of future: classification and categorization		English, like many languages, uses a wide variety of ways to talk about the future, which makes the automatic identification of future reference a challenge. In this research we extend Latent Dirichlet allocation (LDA) for use in the identification of future-referring sentences. Building off a set of hand-designed rules, we trained a ADAGRAD classifier to be able to automatically detect sentences referring to the future. Uni-bi-trigram and syntactic rule mixed feature was found to provide the highest accuracy. Latent Dirichlet Allocation (LDA) indicated the existence of four major categories of future orientation. Lastly, the results of these analyses were found to correlate with a range of behavioral measures, offering evidence in support of the psychological reality of the categories.	automatic identification and data capture;categorization;computation;high-level programming language;latent dirichlet allocation;phrase structure rules;stochastic gradient descent;trigram	Aiming Ni;Jinho D. Choi;Jason Shepard;Phillip Wolff	2015			natural language processing;computer science;machine learning;pattern recognition;categorization	NLP	-21.004305291158538	-68.06218000903051	101596
6d880029db2d8e40c3f8fe9b94b440f4a6ed1afd	exploiting noisy web data by oov ranking for low-resource keyword search	decoding;training;vocabulary;speech;noise measurement;logistics;keyword search	Spoken keyword search in low-resource condition suffers from out-of-vocabulary (OOV) problem and insufficient text data for language model (LM) training. Web-crawled text data is used to expand vocabulary and to augment language model. However, the mismatching between web text and the target speech data brings difficulties to effective utilization. New words from web data need an evaluation to exclude noisy words or introduce proper probabilities. In this paper, several criteria to rank new words from web data are investigated and are used as features for logistic regression. In the IV keyword case, top N words are selected to expand the vocabulary. In the OOV keyword case, all words are used for expansion but unigram probabilities are re-assigned by Zipf's law. On Swahili keyword search, after further text filtering and LM interpolation, this strategy is observed to outperform a strong and commonly used baseline method for data selection.	baseline (configuration management);interpolation;language model;logistic regression;n-gram;protologism;search algorithm;text corpus;vocabulary;zipf's law	Zhipeng Chen;Ji Wu	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918480	logistics;speech recognition;computer science;noise measurement;speech;machine learning;pattern recognition;keyword density;linguistics;information retrieval	NLP	-21.83127854710941	-79.61025275916407	101635
124e02d3f09d5c17f11676d81f90ca2bd6b05d2b	framework for the extraction of clausal mention movement events from the text using its meaning representation	event extraction system;event properties;event annotation;information retrieval;statistical methods;text analysis;statistical method;event detection;data mining;event argument;data mining event detection tellurium computer applications statistical analysis production filtering information retrieval logistics;text analysis information retrieval;statistical analysis;syntactic analysis;semantic web;event triggers;text meaning representation;tunneling magnetoresistance;clausal mention movement event;weapons;syntactic analysis clausal mention movement event text meaning representation event extraction system event triggers event argument event properties event annotation statistical methods	Given a text the proposed work is to identify clausal mention movement events in the text. As the existing event extraction systems do not incorporate the meaning of the text, there is a need for event extraction system that takes meaning of the text into account. The proposed work provides a framework for identifying events by considering the text meaning representation (TMR). The framework involves 4 phases, namely, identifying event triggers, event argument, event properties and annotation of event. Event triggers are recognized using statistical methods. Event arguments are identified from text meaning representation of the input text. Event properties are extracted from the outcome of syntactic analysis phase of TMR generating tool as well as TMR. This information is annotated with event extent so that it can be accessed by queries that need information about the events.	parsing;triple modular redundancy	S. Sangeetha;R. S. Thakur;Michael Arock	2009	2009 Second International Conference on Emerging Trends in Engineering & Technology	10.1109/ICETET.2009.107	natural language processing;computer science;complex event processing;data mining;information retrieval	NLP	-29.736365657043173	-67.83017229711677	101644
01b8ca1053b749c38dc58c4fdfec3c5668a9f491	comparative effectiveness between game-enhanced and pencil-and-paper english vocabulary learning approaches				Zhonggen Yu	2018	IJGCMS	10.4018/IJGCMS.2018040101	pencil (mathematics);multimedia;psychology;vocabulary	HCI	-32.17287226274442	-78.75740371513785	101775
1eb3f1143bb1441cbd1970dd69a3e490cf691d8e	approaches of anonymisation of an sms corpus	anonymisation process;sms corpus;human expert;anonymisation method;supervised approach;unsupervised approach	This paper presents two anonymisation methods to process an SMS corpus. The first one is based on an unsupervised approach called Seek&Hide. The implemented system uses several dictionaries and rules in order to predict if a SMS needs anonymisation process. The second method is based on a supervised approach using machine learning techniques. We evaluate the two approaches and we propose a way to use them together. Only when the two methods do not agree on their prediction, will the SMS be checked by a human expert. This greatly reduces the cost of anonymising the corpus.		Namrata Patel;Pierre Accorsi;Diana Inkpen;Cédric Lopez;Mathieu Roche	2013		10.1007/978-3-642-37247-6_7	computer science;artificial intelligence;data mining;world wide web	NLP	-23.812813912559427	-71.23630728857115	101858
09f3f80a9e0195d56a8f391d66c7070b4e8216d8	leveraging webpage classification for data object recognition	agent interaction;affective norms english words list;support vector machines;user interface;behavioural sciences computing;blog;text analysis;user to agent interaction;user profiling;recommender system;mood information services web sites internet support vector machines support vector machine classification diversity reception user interfaces voting interleaved codes;support vector machine based mood classifier;clustering;web sites;pattern classification;support vector machine;statistically weighted voting scheme;incremental web mining;mood flow analyzer;mood transitions;user model;mood transitions blog user to agent interaction support vector machine based mood classifier mood flow analyzer statistically weighted voting scheme affective norms english words list;web sites behavioural sciences computing pattern classification support vector machines text analysis	Mood classification for blogs is useful in helping user-to-agent interaction for a variety of applications involving the web, such as user modeling, recommendation systems, and user interface fields. It is challenging at the same time because of the diversity of the characteristics of bloggers, their experiences, and the way moods are expressed. As an attempt to handle the diversity, we combine multiple sources of evidence for a mood type. Support Vector Machine based Mood Classifier (SVMMC) is integrated with Mood Flow Analyzer (MFA) that incorporates commonsense knowledge obtained from the general public (i.e. ConceptNet), the Affective Norms English Words (ANEW) list, and mood transitions. In combining the two different approaches, we employ a statistically weighted voting scheme based on the Support Vector Machine (SVM). For evaluation, we have built a mood corpus consisting of manually annotated blogs, which amounts to over 4000 blogs. Our proposed method outperforms SVMMC by 5.68% in precision. The improvement is attributed to the strategy of choosing more trustable classification results in an interleaving fashion between the SVMMC and our MFA.	blog;commonsense knowledge (artificial intelligence);experience;forward error correction;open mind common sense;outline of object recognition;recommender system;support vector machine;user interface;user modeling;weatherstar;web page	Yassine Mrabet;Khaled Khelif;Rose Dieng	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.46	support vector machine;computer science;artificial intelligence;machine learning;data mining;world wide web;recommender system	ML	-21.97126067871197	-67.15627578818416	101907
40c156755235499d00c4f91741691a3acbe0bac7	improving the utility of mesh® terms using the topicalmesh representation	mesh;document retrieval;topic models;document classification;pubmed	OBJECTIVE To evaluate whether vector representations encoding latent topic proportions that capture similarities to MeSH terms can improve performance on biomedical document retrieval and classification tasks, compared to using MeSH terms.   MATERIALS AND METHODS We developed the TopicalMeSH representation, which exploits the 'correspondence' between topics generated using latent Dirichlet allocation (LDA) and MeSH terms to create new document representations that combine MeSH terms and latent topic vectors. We used 15 systematic drug review corpora to evaluate performance on information retrieval and classification tasks using this TopicalMeSH representation, compared to using standard encodings that rely on either (1) the original MeSH terms, (2) the text, or (3) their combination. For the document retrieval task, we compared the precision and recall achieved by ranking citations using MeSH and TopicalMeSH representations, respectively. For the classification task, we considered three supervised machine learning approaches, Support Vector Machines (SVMs), logistic regression, and decision trees. We used these to classify documents as relevant or irrelevant using (independently) MeSH, TopicalMeSH, Words (i.e., n-grams extracted from citation titles and abstracts, encoded via bag-of-words representation), a combination of MeSH and Words, and a combination of TopicalMeSH and Words. We also used SVM to compare the classification performance of tf-idf weighted MeSH terms, LDA Topics, a combination of Topics and MeSH, and TopicalMeSH to supervised LDA's classification performance.   RESULTS For the document retrieval task, using the TopicalMeSH representation resulted in higher precision than MeSH in 11 of 15 corpora while achieving the same recall. For the classification task, use of TopicalMeSH features realized a higher F1 score in 14 of 15 corpora when used by SVMs, 12 of 15 corpora using logistic regression, and 12 of 15 corpora using decision trees. TopicalMeSH also had better document classification performance on 12 of 15 corpora when compared to Topics, tf-idf weighted MeSH terms, and a combination of Topics and MeSH using SVMs. Supervised LDA achieved the worst performance in most of the corpora.   CONCLUSION The proposed TopicalMeSH representation (which combines MeSH terms with latent topics) consistently improved performance on document retrieval and classification tasks, compared to using alternative standard representations using MeSH terms alone, as well as, several standard alternative approaches.	abstract summary;anterior descending branch of left coronary artery;bag-of-words model;bridging (networking);decision trees;decision tree;document classification;document retrieval;encode;extraction;f1 score;grams;information retrieval;latent dirichlet allocation;logistic regression;machine learning;n-gram;precision and recall;pubmed;relevance;supervised learning;support vector machine;text corpus;tf–idf;trees (plant);citation;gram	Zhiguo Yu;Elmer V. Bernstam;Trevor Cohen;Byron C. Wallace;Todd R. Johnson	2016	Journal of biomedical informatics	10.1016/j.jbi.2016.03.013	natural language processing;document retrieval;computer science;machine learning;pattern recognition;data mining;database;topic model;information retrieval	Web+IR	-24.65479923282602	-68.59632808849551	101913
9e22b7639169182f0741b0eb86b5a2d5ad3af5c1	generating information-rich taxonomy from wikipedia	wikipedia;information rich taxonomy generation;informative relation work;avatar information rich taxonomy generation wikipedia hyponymy relation acquisition hypernym work james cameron informative relation work;rye;linguistics internet;james cameron;internet;hyponymy relation acquisition;avatars;cities and towns;electronic publishing;hypernym work;encyclopedias;encyclopedias electronic publishing internet avatars cities and towns educational institutions;avatar;linguistics	Even though hyponymy relation acquisition has been extensively studied, “how informative such acquired hyponymy relations are” has not been sufficiently discussed. We found that the hypernyms in automatically acquired hyponymy relations were often too vague or ambiguous to specify the meaning of their hyponyms. For instance, hypernym work is vague and ambiguous in hyponymy relations work/Avatar and work/The Catcher in the Rye. In this paper, we propose a simple method of generating intermediate concepts of hyponymy relations that can make such (vague) hypernyms more specific. Our method generates such an information-rich hyponymy relation as work / work by film director / work by James Cameron / Avatar from the less informative relation work/Avatar. Furthermore, the generated relation work by film director/Avatar can be paraphrased into a new relation movie/Avatar. Experiments showed that our method successfully acquired 2,719,441 enriched hyponymy relations with one intermediate concept with 0.853 precision and another 6,347,472 hyponymy relations with 0.786 precision.	avatar (computing);information;knowledge acquisition;thesaurus;vagueness;wikipedia	Ichiro Yamada;Chikara Hashimoto;Jong-Hoon Oh;Kentaro Torisawa;Kow Kuroda;Stijn De Saeger;Masaaki Tsuchida;Jun'ichi Kazama	2010	2010 4th International Universal Communication Symposium	10.1109/IUCS.2010.5666764	computer science;artificial intelligence;communication;world wide web	NLP	-30.30184362871281	-67.43820820517463	101985
0f2308a9dc7ea085ac409fc88cb66a07ce7bf8a6	characterizing the errors of data-driven dependency parsing models	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;computer and information sciences computer science;datavetenskap datalogi;computational linguistics;dependency parsing;language technology computational linguistics;computer science;data och informationsvetenskap	We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.	error analysis (mathematics);greedy algorithm;parsing	Ryan T. McDonald;Joakim Nivre	2007			natural language processing;computer science;bottom-up parsing;theoretical computer science;computational linguistics;machine learning;linguistics;dependency grammar	NLP	-21.821202123524134	-76.45743923432529	102157
8356d1ec5231156bc998357fffa98d0f4e3c9c35	discriminant models for word alignment		Word alignment aims to link each word of a translated sentence to its related words in the source sentence. Nowadays, Giza++ is the most used word alignment system. This toolkit implements the generative IBM models. Despite its popularity, several limitations remain. We thus propose to address this task using discriminative models (Maximum Entropy and Conditional Random Fields) which can easily make use of additional features. These models are evaluated in terms of Alignment Error Rate (AER) using two language pairs (French/english and Arabic English). Our results show that discriminant models are well suited for this task and that they can outperform IBM models. MOTS-CLÉS : modèles d’alignement mot à mot, maximum d’entropie, champs conditionnels aléatoires.	bitext word alignment;conditional random field;consistency model;data structure alignment;discriminant;discriminative model	Alexandre Allauzen;Guillaume Wisniewski	2009	TAL		speech recognition;artificial intelligence;pattern recognition;computer science;discriminant	NLP	-21.293124398131717	-76.12225731856542	102268
8f3f55c171633bc54af3e0572dc8f829fb7657c6	integration of neural networks and robust parsers in natural language understanding	neural network		artificial neural network;natural language understanding;parsing	Ying Cheng;Yves Normandin;Paul Fortier	1993			speech recognition;natural language processing;artificial neural network;parsing;natural language understanding;artificial intelligence;computer science	ML	-29.479515872806253	-79.34845173851147	102277
2bdddae00a333c0ed0c7dedf6442e3ece74905fc	text semantic mining model based on the algebra of human concept learning	text knowledge;web;semantic mining model;knowledge representation;algebra of human concept learning	Dealing with the large-scale text knowledge on the Web has become increasingly important with the development of the Web, yet it confronts with several challenges, one of which is to find out as much semantics as possible to represent text knowledge. As the text semantic mining process is also the knowledge representation process of text, this paper proposes a text knowledge representation model called text semantic mining model (TSMM) based on the algebra of human concept learning, which both carries rich semantics and is constructed automatically with a lower complexity. Herein, the algebra of human concept learning is introduced, which enables TSMM containing rich semantics. Then the formalization and the construction process of TSMM are discussed. Moreover, three types of reasoning rules based on TSMM are proposed. Lastly, experiments and the comparison with current text representation models show that the given model performs better than others.	concept learning;experiment;knowledge representation and reasoning;world wide web	Jun Zhang;Xiangfeng Luo;Xiang He;Chuanliang Cai	2011	IJCINI	10.4018/jcini.2011040105	natural language processing;knowledge representation and reasoning;text mining;computer science;artificial intelligence;data mining;information retrieval	AI	-32.095708043437	-69.26240338390102	102327
ce4f8892271552b0d4e0a219f66f44969e248b5f	tempowordnet for sentence time tagging	tempowordnet;sentence temporal classification;temporal ontology	In this paper, we propose to build a temporal ontology, which may contribute to the success of time-related applications. Temporal classifiers are learned from a set of time-sensitive synsets and then applied to the whole WordNet to give rise to TempoWordNet. So, each synset is augmented with its intrinsic temporal value. To evaluate TempoWordNet, we use a semantic vector space representation for sentence temporal classification, which shows that improvements may be achieved with the time-augmented knowledge base against a bag-of-ngrams representation.	knowledge base;n-gram;synonym ring;wordnet	Gaël Dias;Mohammed Hasanuzzaman;Stéphane Ferrari;Yann Mathet	2014		10.1145/2567948.2579042	natural language processing;computer science;data mining;information retrieval	NLP	-25.694557869731646	-70.30728058635576	102399
dbc534ec123274b8f5eb5588c6b3a9652a564a0d	melb-yb: preposition sense disambiguation using rich semantic features	wide variety;system entry;disambiguation task;maxent-based preposition sense disambiguation;test data;preposition sense disambiguation task;rich semantic feature;syntactic feature	This paper describes a maxent-based preposition sense disambiguation system entry to the preposition sense disambiguation task of the SemEval 2007. This system uses a wide variety of semantic and syntactic features to perform the disambiguation task and achieves a precision of 69.3% over the test data.	baseline (configuration management);collocation;principle of maximum entropy;semeval;test data;the australian;word-sense disambiguation;yottabyte	Patrick Ye;Timothy Baldwin	2007			semeval;bioinformatics;theoretical computer science;communication	NLP	-26.010885822531808	-71.68096364209278	102422
53205855dcb0d3b8bef45836db0cd84457cf6dba	explanatory opinions: to whom or what is all the fuzz about?		Exploiting sentiment relations to improve the accuracy of sentiment analysis has caught the interest of recent research. When expressing their opinions, users apply different sentence syntactic constructions styles. This analysis leverages on a sentiment lexicon that includes general sentiment words that characterize the overall sentiment towards the targeted named-entity. However, in most cases, target entities are themselves part of the sentiment lexicon, creating a loop from which it is difficult to infer the overall sentiment to the target entities. We propose the application of conditional random fields (CRF) to predict opinion target labels. More specifically, we exploit a set of opinion patterns to extend an opinion word lexicon and then propose to apply a CRF algorithm to detect the interactions between opinion expressions and opinion targets.		Filipa Peleja;Ioannis Arapakis;João Magalhães	2015				NLP	-20.450566607338178	-68.840423025902	102597
3e2fe566b7439676c6097dd1e097d78dccf4294e	argumentative link prediction using residual networks and multi-objective learning		We explore the use of residual networks for argumentation mining, with an emphasis on link prediction. We propose a method of general applicability, that does not rely on domain knowledge such as document or argument structure. We evaluate our method on a challenging dataset consisting of usergenerated comments collected from an online platform. Results show that our model outperforms an equivalent deep network and offers results comparable with state-of-the-art methods that rely on domain knowledge.	argumentation framework;baseline (configuration management);confusion matrix;structured prediction;word lists by frequency	Andrea Galassi;Marco Lippi;Paolo Torroni	2018				AI	-19.35066414411144	-67.39628711230833	102673
ee174eb321b6dd9e10939e22767318e1b67261be	reentrenamiento: aprendizaje semisupervisado de los sentidos de las palabras	maximum entropy	This paper presents re-training, a bootstrapping algorithm that automatically acquires semantically annotated data, ensuring high levels of precision. This algorithm uses a corpus-based system of word sense disambiguation that relies on maximum entropy probability models. The re-training method consists of the iterative feeding of training-classification cycles with new and high-confidence examples. The process relies on several filters that ensure the accuracy of the disambiguation by discarding uncertain classifications. This new method is inspired by co-training algorithms, but it makes stronger assumptions on when to assign a label to a linguistic context.	algorithm;bootstrapping (compilers);co-training;faceted classification;iterative method;teaching method;text corpus;word sense;word-sense disambiguation	Armando Suárez;Manuel Palomar;German Rigau	2005	Procesamiento del Lenguaje Natural		natural language processing;method;speech recognition;computer science;artificial intelligence;principle of maximum entropy;computational linguistics;applied linguistics;linguistics;bootstrapping	NLP	-24.360531470098994	-73.02268260964152	102691
945d67bb0e1f2f908a7221cb806904b31b8feec9	word embedding calculus in meaningful ultradense subspaces		We decompose a standard embedding space into interpretable orthogonal subspaces and a “remainder” subspace. We consider four interpretable subspaces in this paper: polarity, concreteness, frequency and part-of-speech (POS) subspaces. We introduce a new calculus for subspaces that supports operations like “−1 × hate = love” and “give me a neutral word for greasy” (i.e., oleaginous). This calculus extends analogy computations like “king−man+woman = queen”. For the tasks of Antonym Classification and POS Tagging our method outperforms the state of the art. We create test sets for Morphological Analogies and for the new task of Polarity Spectrum Creation.	computation;part-of-speech tagging;protologism;test set;word embedding	Sascha Rothe;Hinrich Schütze	2016			linear subspace;artificial intelligence;machine learning;word embedding;computer science	NLP	-20.12682880058264	-72.55968974574094	102777
afe8ee271bea7a180e27a7339f2cf638dde6b57b	deriving consensus for multi-parallel corpora: an english bible study		What can you do with multiple noisy versions of the same text? We present a method which generates a single consensus between multi-parallel corpora. By maximizing a function of linguistic features between word pairs, we jointly learn a single corpus-wide multiway alignment: a consensus between 27 versions of the English Bible. We additionally produce English paraphrases, word-level distributions of tags, and consensus dependency parses. Our method is language independent and applicable to any multi-parallel corpora. Given the Bible’s unique role as alignable bitext for over 800 of the world’s languages, this consensus alignment and resulting resources offer value for multilingual annotation projection, and also shed potential insights into the Bible itself.	algorithm;bitext word alignment;consensus (computer science);heuristic;language-independent specification;parallel text;scoring functions for docking;text corpus;vergence	Patrick Xia;David Yarowsky	2017			natural language processing;computer science;artificial intelligence	NLP	-21.199118301140317	-75.96949499912287	102828
5cf743a58b6f37984847c7ba555d64c3fde423bd	use of twitter data toward the development of an english and swahili question answering agent for the kenyan customer service market		The growth of internet and social media users in the African continent has expanded the channels by which organizations interact with their customers. Although there are multiple implementations of chatbots and other automated question answering systems in operation, their deployment in countries like Kenya has been limited due to implementation challenges arising from the presence of multiple languages (code mixing), underdeveloped training corpora and non-uniform spelling. In this note we describe the development of several components of a chatbot system intended to handle customer service queries in Kenya. We describe the collection and preparation of a custom training corpus developed from twitter data containing English and Swahili messages. The note covers the preprocessing steps to standardize the message format, along with the use of word embeddings for an initial categorization of queries intended to direct the future workflow of the chatbot system. Additionally, the work uses the corpus to measure the accuracy of word embeddings with KNN, and TF-IDF with and without N-Gram to predict correct response to customer questions based on similarity to previously seen questions. The preliminary work measures the ability of the system to preprocess customer inquiries, to broadly classify them, and to either provide an answer automatically or direct the query toward a human, discard the message, or seek further clarification.	categorization;internet;k-nearest neighbors algorithm;n-gram;preprocessor;question answering;social media;software agent;software deployment;text corpus;tf–idf;word embedding	Felix Kwizera;Isaac Markus;Purity Mugambi;Abdigani Diriye	2017		10.1145/3136560.3136587	message format;world wide web;implementation;chatbot;swahili;the internet;computer science;question answering;social media;workflow	NLP	-32.72952158399242	-72.50140934215848	102888
8f38e608251e085c818c9b148a46f7c1deb132bd	a comparison of summarization methods based on task-based evaluation		A task-based evaluation scheme has been adopted as a new method of evaluation for automatic text summarization systems. It evaluates the performance of a summarization system in a given task, such as information retrieval and text categorization. This paper compares ten different summarization methods based on information retrieval tasks. In order to evaluate the system performance, the subjects’ speed and accuracy are measured in judging the relevance of texts using summaries. We also analyze the similarity of summaries in order to investigate the similarity of the methods. Furthermore, we analyze what factors can affect evaluation results, and describe the problems that arose from our experimental design, in order to establish a better evaluation scheme.	automatic summarization;categorization;design of experiments;document classification;information retrieval;relevance	Hajime Mochizuki;Manabu Okumura	2000			speech recognition;automatic summarization;artificial intelligence;natural language processing;computer science	Web+IR	-27.74552085221092	-66.90456188933511	102923
082388e4ad2f8aec9a5fce77fd25d73c41613a35	samplerank training for phrase-based machine translation	output sentence;best model weight;samplerank proceed;samplerank training;document level feature;model weight;gain function;discriminative training;alternative algorithm;chosen gain function;statistical machine translation system;phrase-based machine translation	Statistical machine translation systems are normally optimised for a chosen gain function (metric) by using MERT to find the best model weights. This algorithm suffers from stability problems and cannot scale beyond 20-30 features. We present an alternative algorithm for discriminative training of phrasebased MT systems, SampleRank, which scales to hundreds of features, equals or beats MERT on both small and medium sized systems, and permits the use of sentence or document level features. SampleRank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function.	algorithm;multi-environment real-time;statistical machine translation	Barry Haddow;Abhishek Arun;Philipp Koehn	2011			speech recognition;computer science;machine learning;pattern recognition	NLP	-19.45941803855724	-77.542340787228	102967
6b5f7dd4f0f15557213f628933da0f780736c4db	translation oriented sentence level collocation identification and extraction		The technique to identify and extract collocations in a given sentence is very important to sentence understanding, analysing and translating. So we propose a sentence level collocation identification and extraction method which follows the traditional two phase collocation extraction model. In candidate generating phase, we use the dependency parsing results directly, while in the filtering phase, we propose to use the latest model of distributional semantics - word embedding based similarity to filter the noises. For each candidate, three word embedding based similarity rankings will be obtained and accordingly to decide if it is a real collocation. The experimental results show that the proposed filtering method performs better than the traditional well-known association measures. The comparison with the baseline system shows that the proposed method can retrieve more collocations with higher precision than the baseline, which is of significance to sentence related natural language processing tasks.	collocation	Xiaoxia Liu;Degen Huang	2017		10.1007/978-981-10-7134-8_8	word embedding;distributional semantics;dependency grammar;filter (signal processing);collocation;computer science;pattern recognition;collocation extraction;sentence;artificial intelligence	NLP	-26.395904609135343	-66.81710141366734	102971
301d8b27286b1e8105bbbf798c8a0aa10a2159ff	data interlinking through robust linkkey extraction		Links are important for the publication of RDF data on the web. Yet, establishing links between data sets is not an easy task. We develop an approach for that purpose which extracts weak linkkeys. Linkkeys extend the notion of a key to the case of different data sets. They are made of a set of pairs of properties belonging to two different classes. A weak linkkey holds between two classes if any resources having common values for all of these properties are the same resources. An algorithm is proposed to generate a small set of candidate linkkeys. Depending on whether some of the, valid or invalid, links are known, we define supervised and non supervised measures for selecting the appropriate linkkeys. The supervised measures approximate precision and recall, while the non supervised measures are the ratio of pairs of entities a linkkey covers (coverage), and the ratio of entities from the same data set it identifies (discrimination). We have experimented these techniques on two data sets, showing the accuracy and robustness of both approaches.	ambiguous name resolution;approximation algorithm;entity;experiment;many-to-many;one-to-one (data model);precision and recall;resource description framework;sequence alignment;supervised learning;turing completeness;weak ai	Manuel Atencia;Jérôme David;Jérôme Euzenat	2014		10.3233/978-1-61499-419-0-15	computer science;machine learning;data mining;information retrieval	ML	-30.254059317635402	-66.47567267871966	103222
3f0d4e3d0546e291c8d0ecd967f54f41bc23a385	dependency forest based word alignment		A hierarchical word alignment model that searches for k-best partial alignments on target constituent 1-best parse trees has been shown to outperform previous models. However, relying solely on 1-best parses trees might hinder the search for good alignments because 1-best trees are not necessarily the best for word alignment tasks in practice. This paper introduces a dependency forest based word alignment model, which utilizes target dependency forests in an attempt to minimize the impact on limitations attributable to 1-best parse trees. We present how k-best alignments are constructed over target-side dependency forests. Alignment experiments on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees.	approximation error;bitext word alignment;data structure alignment;experiment;graham scan;j.w. graham medal;machine translation;parse tree;parsing;word lists by frequency	Otsuki Hitoshi;Chenhui Chu;Toshiaki Nakazawa;Sadao Kurohashi	2016		10.18653/v1/P16-3002	speech recognition;computer science;machine learning	NLP	-20.799798540327537	-77.41851414011005	103348
a9f30a9fa0ef0f19fe8a462814ac6055f241161e	fuzzy clustering for semi-supervised learning --- case study: construction of an emotion lexicon	hard cluster;case study task;fuzzy clustering;emotion label;wordnet distance;wordnet affect lexicon;large emotion lexicon;semi-supervised learning;hard clustering;unsupervised fuzzy clustering	We consider the task of semi-supervised classifica tion: extending category labels from a small dataset of labeled exa mples to a much larger set. We show that, at least on our case study task, unsu pervised fuzzy clustering of the unlabeled examples helps in obtaining the hard clusters. Namely, we used the membership values obtained with fuzzy clusterin g as additional features for hard clustering. We also used these membership valu es to reduce the confusion set for the hard clustering. As a case study, we us e applied the proposed method to the task of constructing a large emotion lexicon by extending the emotion labels from the WordNet Affect lexicon using various features of words. Some of the features were extracted from the emotional stat ements of the freely available ISEAR dataset; other features were WordNet distance d the similarity measured via the polarity scores in the SenticNet resou rce. The proposed method classified words by emotion labels with high accura y.	baseline (configuration management);binary prefix;cluster analysis;data item;data point;exa;experiment;feature vector;fuzzy clustering;information quality;interplanet;lexicon;semi-supervised learning;semiconductor industry;similarity measure;supervised learning;text corpus;textual entailment;vocabulary;wordnet	Soujanya Poria;Alexander F. Gelbukh;Dipankar Das;Sivaji Bandyopadhyay	2012		10.1007/978-3-642-37807-2_7	natural language processing;machine learning;pattern recognition	NLP	-22.93497055710485	-70.57874609330737	103389
e4726c2701ae926526fff07a6ca4db7261280128	a hybrid framework for natural language processing of large data bases	information structure;inverted index;information retrieval;artificial intelligent;semantic net;natural language;indexation;procedural knowledge;natural language processing;large data	"""At the Information Retrieval Research Laboratory we are working on combining techniques in common use in information retrieval with concepts from artificial intelligence to provide enhanced processing of large data bases. Information retrieval research has provided the inverted file, which indexes the occurrence of each word in a text file. Our system will include a complete inverted index as an underlying representation of the natural language text and will impose additional information structures on that index. Specifically, the index will include: information about the semantic function of words at each of their occurrences, linguistic knowledge connecting the vocabulary nodes, """"fact nodes"""" attached to vocabulary nodes describing entities (e.g. people), and procedural knowledge needed to find facts and build fact nodes - attached to word nodes and functioning as word demons. The information structure will have some features of a semantic net, some of a network of frames. The procedural and linguistic knowledge attached to a vocabulary node will fill the function of an ATN segment that is attached to the sentence ATN when the associated word is recognized and will also be responsible for recognizing limited kinds of facts and attaching them as attributes to the appropriate entity."""	artificial intelligence;database;entity;html5 in mobile devices;information retrieval;inverted index;natural language processing;procedural programming;semantic network;vocabulary	Martha E. Williams;Scott E. Preece	1977	SIGART Newsletter	10.1145/1045283.1045330	natural language processing;inverted index;explicit semantic analysis;question answering;computer science;artificial intelligence;data mining;linguistics;procedural knowledge;natural language;information retrieval	Web+IR	-32.22277230682195	-70.17058619799373	103522
232a9e349ee0d6608edd86f1baa6a2088f5c3f41	the construction of a kind of chat corpus in chinese word segmentation	computers;manuals;natural language processing information retrieval;会议论文;manual annotation chats corpus chinese word segmentation;internet;dictionaries;information processing;nlpir chat corpus chinese word segmentation construction process automatic segmentation technology manual correction natural language processing information retrieval;manuals internet information processing buildings tagging dictionaries computers;buildings;tagging	In this thesis, we present a kind of chat corpus in Chinese word segmentation and we also present its construction process. This kind of chat corpus works in the way of combining application of automatic segmentation technology with the method of manual correction. Thereinto, the automatic segmentation is performed in the way of using the Natural Language Processing Information Retrieval (NLPIR). As to manual correction, errors from NLPIR will be categorized and some annotation suggestions will be put forward. Combining using these two methods above, our study, which is a preliminary study, could be very easy extended to other Chats texts. What's more, the corpus, which produced in our works, could provide a good standard for the research of Chinese word segmentation, especially in the part of dialogue.	categorization;information retrieval;microsoft word for mac;natural language processing;text corpus;text segmentation	Xia Yang;Peng Jin;Xingyuan Chen	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.196	natural language processing;the internet;speech recognition;information processing;computer science;law;information retrieval	NLP	-28.85104648652512	-75.67739558251085	103674
2719c51f752cc94dd6267cdd93d5c281102e5b43	automatic classification of german 'an' particle verbs		German particle verbs (PVs) are a challenge to theoretical and computational linguistics, as both of their parts (i.e., the particles and the base verbs) may be highly ambiguous (cf. Stiebels, 1996; Schulte im Walde, 2005; Lechler and Roßdeutscher, 2009; Springorum, 2009; 2011; among others). The current study works at the interface of theoretical and computational linguistics to explore the semantic properties of an particle verbs, i.e., German particle verbs with the particle an. Driven by a thorough analysis of the particle an from a theoretical point of view (Springorum, 2011a), we identify empirical features to perform an automatic semantic classification of the particle verbs. A focus of the study is on the questions (a) how we could transform the theoretical insights into empirical, corpus-based features, (b) to what extent we could replicate the theoretical classification by a machine learning approach, and (c) whether the computational analysis would in turn deepen our insights to the semantic properties of the PVs.	computation;computational linguistics;emoticon;machine learning;self-replicating machine	Sylvia Springorum;Sabine Schulte im Walde;Antje Roßdeutscher	2012			artificial intelligence;natural language processing;noun;semantic property;replicate;germanet;computational linguistics;computer science;particle;verb;phrase	NLP	-28.54466601613919	-75.0935561694405	103744
6595ad26cc67710f5d47bd2c7792b91a9300049d	backtracking-free dictionary access method for japanese morphological analysis	japanese morphological analysis;backtracking-free dictionary access method;morphological analysis;access method		backtracking;dictionary	Hiroshi Maruyama	1994			natural language processing;speech recognition;morphological analysis;computer science;access method	NLP	-29.721517256095183	-78.41951703690506	103915
8c573f5eb9d737c158293962e27e3bcee87729e5	design challenges in named entity transliteration		We analyze some of the fundamental design challenges that impact the development of a multilingual state-of-the-art named entity transliteration system, including curating bilingual named entity datasets and evaluation of multiple transliteration methods. We empirically evaluate the transliteration task using the traditional weighted finite state transducer (WFST) approach against two neural approaches: the encoder-decoder recurrent neural network method and the recent, non-sequential Transformer method. In order to improve availability of bilingual named entity transliteration datasets, we release personal name bilingual dictionaries mined from Wikidata for English to Russian, Hebrew, Arabic, and Japanese Katakana. Our code and dictionaries are publicly available1.	artificial neural network;bilingual dictionary;encoder;finite-state transducer;mined;named entity;recurrent neural network;transformer;wikidata	Yuval Merhav;Stephen Ash	2018			hebrew;arabic;transliteration;natural language processing;computer science;artificial intelligence;katakana;recurrent neural network;named entity;finite state transducer	NLP	-21.31949629021702	-74.48585405207699	103983
df8c2560ad94e7c13ad3e3e62cfc0ba2129c78cb	sumt: a framework of summarization and mt		We present a novel system combination of machine translation and text summarization which provides high quality summary translations superior to the baseline translation of the entire document. We first use supervised learning and build a classifier that predicts if the translation of a sentence has high or low translation quality. This is a reference-free estimation of MT quality which helps us to distinguish the subset of sentences which have better translation quality. We pair this classifier with a stateof-the-art summarization system to build an MT-aware summarization system. To evaluate summarization quality, we build a test set by summarizing a bilingual corpus. We evaluate the performance of our system with respect to both MT and summarization quality and, demonstrate that we can balance between improving MT quality and maintaining a decent summarization quality.	automatic summarization;baseline (configuration management);display resolution;emoticon;machine translation;statistical classification;supervised learning;test set;wikipedia	Houda Bouamor;Behrang Mohit;Kemal Oflazer	2013			rouge;computer science;automatic summarization;pattern recognition;data mining;information retrieval	NLP	-21.59977747138824	-66.94964071480163	104026
5f700497b2b9f306630bb5b4d7d6a84d7790a794	chinese word segmentation based on contextual entropy	statistical approach;conference paper;word segmentation;chinese word segmentation	Chinese is written without word delimiters so word segmentation is generally considered a key step in processing Chinese texts. This paper presents a new statistical approach to segment Chinese sequences into words based on contextual entropy on both sides of a bigram. It is used to capture the dependency with the left and right contexts in which a bigram occurs. Our approach tries to segment by finding the word boundaries instead of the words. Experimental results show that it is effective for Chinese word segmentation.	bigram;delimiter;markov chain;markov model;microsoft word for mac;text segmentation;unsupervised learning	Jin Hu Huang;David M. W. Powers	2003			natural language processing;speech recognition;segmentation-based object categorization;linguistics;speech segmentation;scale-space segmentation	NLP	-24.093954437677596	-77.8122986827117	104155
ee8c79b762d3d920067231b3c477afacc482f83b	some properties of a metalinguistic verbal system (in the metalanguage of the macmillan english dictionary's defining vocabulary)			defining vocabulary;dictionary	Sergey Andreev	2007	Glottometrics		natural language processing;metalanguage;linguistics;artificial intelligence;vocabulary;computer science	NLP	-30.366211613136535	-79.18693699065416	104178
1ac9590e51d8fe2fd788bb6953af731971c86859	retrieval experiments at morpho challenge 2008		Morpho Challenge 2008 hosted an extrinsic evaluation of morphological analysis that explored whether unsupervised morphology induction could benefit information retrieval. This paper presents results in alternative methods for word normalization using test sets from the Cross-Language Evaluation Forum (CLEF) ad-hoc collections. Preliminary results for the Morpho Challenge 2008 evaluation are consistent with these data. We found that: (1) rule-based stemming is effective in less morphologically complicated languages; (2) alternative methods for stemming such as unsupervised learning of morphemes and least common n-gram stemming are helpful; and, (3) full character n-gram indexing is the most effective form of tokenization in more morphologically complex languages.	hoc (programming language);information retrieval;logic programming;mathematical morphology;n-gram;stemming;tokenization (data security);unsupervised learning	Paul McNamee	2008			natural language processing;computer science;communication;algorithm	NLP	-28.79644650905563	-74.68591053464051	104193
b71e020c4c0ee896174d11a9a87d8941750dad22	annotating chinese collocations with multi information		This paper presents the design and construction of an annotated Chinese collocation bank as the resource to support systematic research on Chinese collocations. With the help of computational tools, the bi-gram and n-gram collocations corresponding to 3,643 headwords are manually identified. Furthermore, annotations for bi-gram collocations include dependency relation, chunking relation and classification of collocation types. Currently, the collocation bank annotated 23,581 bigram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus. Through statistical analysis on the collocation bank, some characteristics of Chinese bigram collocations are examined which is essential to collocation research, especially for Chinese.	algorithm;bigram;change control board;collocation extraction;computation;dependency relation;headword;n-gram;shallow parsing;stable model semantics	Ruifeng Xu;Qin Lu;Kam-Fai Wong;Wenjie Li	2007			natural language processing;computer science;linguistics;information retrieval	NLP	-29.707779061715417	-72.71083010170176	104349
5daed43ac6194bc30fae834f2891f10454656420	a neuro-evolutionary corpus-based method for word sense disambiguation	natural language processing neuro evolutionary corpus based method word sense disambiguation evolutionary algorithm automatic structure design neural networks nlp wsd;settore inf 01 informatica;evolutionary computation;neural nets evolutionary computation natural language processing;neural networks;text mining;neural nets;word sense disambiguation;neural networks evolutionary computation text mining artificial neural networks neural networks encoding evolutionary algorithms word sense disambiguation;desambiguisacion;evolutionary computation text mining artificial neural networks neural networks encoding;artificial neural networks;graphe pondere;grafo pondero;polisemia;disambiguation;polysemy;evolutionary algorithms;polysemie;algorithme evolutionniste;algoritmo evolucionista;weighted graph;evolutionary algorithm;desambiguisation;reseau neuronal;encoding;natural language processing;red neuronal;neural network	The proposed approach to word sense disambiguation uses an evolutionary algorithm to automatically design the structure and learn the connection weights of neural networks.	artificial neural network;evolutionary algorithm;word sense;word-sense disambiguation	Antonia Azzini;Célia da Costa Pereira;Mauro Dragoni;Andrea Tettamanzi	2012	IEEE Intelligent Systems	10.1109/MIS.2011.108	natural language processing;semeval;computer science;artificial intelligence;machine learning;artificial neural network;encoding	Robotics	-25.685028480626105	-79.09027938709929	104592
3dfb882bd495efde13808bb6d11263531c5ca5ed	literal and metaphorical senses in compositional distributional semantic models		Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics. The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models (CDSMs). This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework. We propose a method to learn metaphors as linear transformations in a vector space and find that, across a variety of semantic domains, explicitly modeling metaphor improves the resulting semantic representations. We then use these representations in a metaphor identification task, achieving a high performance of 0.82 in terms of F-score.	computational semantics;deep learning;distributional semantics;extensibility;f1 score;literal (mathematical logic);natural language;nonlinear system;pervasive informatics;test case;word sense;word-sense disambiguation	E. Dario Gutiérrez;Ekaterina Shutova;Tyler Marghetis;Benjamin Bergen	2016			natural language processing;computer science;linguistics	NLP	-26.180381295040245	-72.10172764996561	104621
94a610b6d44ddd07b5a6cec4b316b2cd46e3accf	active learning for dependency parsing with partial annotation		Different from traditional active learning based on sentence-wise full annotation (FA), this paper proposes active learning with dependency-wise partial annotation (PA) as a finer-grained unit for dependency parsing. At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining. Compared with sentence-wise FA, dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence. Our work makes the following contributions. First, we are the first to apply a probabilistic model to active learning for dependency parsing, which can 1) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2) directly learn parameters from PA based on a forest-based training objective. Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English. Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality.	experiment;iteration;marginal model;parsing;simulation;statistical model;uncertainty principle	Zhenghua Li;Min Zhang;Yue Zhang;Zhanyi Liu;Wenliang Chen;Hua Wu;Haifeng Wang	2016			bottom-up parsing;natural language processing;active learning;computer science;machine learning;artificial intelligence;dependency grammar;s-attributed grammar;top-down parsing;annotation	NLP	-19.151685896859945	-76.60722023215054	104839
73b3c8624b1492683cdd260411d63e122a89b9a5	what can distributional semantic models tell us about part-of relations?		The term Distributional semantic models (DSMs) refers to a family of unsupervised corpus-based approaches to semantic similarity computation. These models rely on the distributional hypothesis (Harris, 1954), which states that semantically related words tend to share many of their contexts. So, by collecting information about the contexts in which words are used in a corpus, DSMs are able to measure the distributional similarity of two words, which theoretically translates into a semantic one.	computation;distributional semantics;harris affine region detector;semantic data model;semantic similarity;text corpus	François Morlane-Hondère	2015			semantic similarity;computation;pattern recognition;mathematics;artificial intelligence	NLP	-25.237371565718263	-72.28402579698107	104840
95526a57e511830457b8f6cad9ad57c38fd37c29	sweat2012: pattern based english slot filling system for knowledge base population at tac 2012		In this paper, we describe the english slot filling system of sweat2012 team for Knowledge Base Population (KBP) task at TAC2012. Our slot filling system is based on pattern. In specific, we first construct (target, value) pairs for every attribute of our interests from previous evaluation results, and divide these entity pairs into training set and assessment set; then the extraction patterns are learned from the training set and the confidence of every learned pattern is evaluated on the assessment set. Secondly, we use the learned pattern to extract candidate values for each required slot of target entities. Finally we post-process filler candidates extracted by patterns which includes estimating probabiliy of each candidate, validating these candidates according to tag and type constraints and removing duplications. Our system achieves Fmeasure 0.099 on the final test dataset, which turns out to be the median level in all 11 teams who summit their results to the English Slot-Filling task this year.	attribute–value pair;entity;knowledge base;online and offline;star filler;test set	Fang Liu;Jian Zhao	2012			simulation;data mining	NLP	-24.383739313728515	-70.99421849940364	104950
44703c55053b19dc3096e39fb79bb499740c29f0	the ml4hmt workshop on optimising the division of labour in hybrid machine translation	machine translation;machine learning	We describe the “Shared Task on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid Machine Translation” (ML4HMT) which aims to foster research on improved system combination approaches for machine translation (MT). Participants of the challenge are requested to build hybrid translations by combining the output of several MT systems of different types. We first describe the ML4HMT corpus used in the shared task, then explain the XLIFF-based annotation format we have designed for it, and briefly summarize the participating systems. Using both automated metrics scores and extensive manual evaluation, we discuss the individual performance of the various systems. An interesting result from the shared task is the fact that we were able to observe different systems winning according to the automated metrics scores when compared to the results from the manual evaluation. We conclude by summarising the first edition of the challenge and by giving an outlook to future work.	automated reasoning;hybrid machine translation;machine learning;microsoft outlook for mac;xliff	Christian Federmann;Eleftherios Avramidis;Marta R. Costa-Jussà;Josef van Genabith;Maite Melero;Pavel Pecina	2012			division of labour;artificial intelligence;natural language processing;machine translation;hybrid machine translation;annotation;computer science	NLP	-31.04966389114933	-72.62241171445135	104977
4c678af24dfafab7e4a35a9e4c36ea5f24660cd1	comparison of vocabulary richness in two translated hongloumeng			vocabulary	Yu Fang;Haitao Liu	2015	Glottometrics		linguistics;vocabulary;psychology;species richness	HCI	-31.038836703257868	-78.41058626231568	104995
2538b5c8991a98d3887cb25200253b0e20834876	towards the derivation of verbal content relations from patent claims using deep syntactic structures	relation clustering;similarity relation;dependency relation;high priority;dependence structure;specialized discourse;cluster labeling;dependency parsing;deep dependency parsing;natural language processing;knowledge base	Research on the extraction of content relations from text corpora is a high-priority topic in natural language processing. This is not surprising since content relations form the backbone of any ontology, and ontologies are increasingly made use of in knowledge-based applications. However, so far most of the works focus on the detection of a restricted number of prominent verbal relations, including in particular IS-A, HAS-PART and CAUSE. Our application, which aims to provide comprehensive, easy-to-understand content representations of complex functional objects described in patent claims, faces the need to derive a large number of content relations that cannot be limited a priori. To cope with this problem, we take advantage of the fact that deep syntactic dependency structures of sentences capture all relevant content relations—although without any abstraction. We implement thus a three-step strategy. First, we parse the claims to retrieve the deep syntactic dependency structures from which we then derive the content relations. Second, we generalize the obtained relations by clustering them according to semantic criteria, with the goal to unite all sufficiently similar relations. Finally, we identify a suitable name for each generalized relation. To keep the scope of the article within reasonable limits and to allow for a comparison with state-of-the-art techniques, we focus on verbal relations. 2011 Elsevier B.V. All rights reserved.	cluster analysis;internet backbone;is-a;knowledge-based systems;natural language processing;ontology (information science);parsing;text corpus	Gabriela Ferraro;Leo Wanner	2011	Knowl.-Based Syst.	10.1016/j.knosys.2011.05.014	natural language processing;knowledge base;computer science;artificial intelligence;data mining;dependency grammar	AI	-31.5141882529026	-70.43527717096374	105013
5d2d797ee4053dada784639d7462abbfb2220031	guided open vocabulary image captioning with constrained beam search		Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.	approximation algorithm;australian research council centre of excellence for robotic vision;beam search;expectation–maximization algorithm;finite-state machine;ground truth;imagenet;search algorithm;the australian;vocabulary;word embedding	Peter Anderson;Basura Fernando;Mark Johnson;Stephen Gould	2017			computer vision;speech recognition;computer science;machine learning;multimedia;automatic image annotation	NLP	-19.778309432706408	-74.94282100481068	105104
12d082264f516923af538a7ded663e2cf61843a5	the ariel-cmu situation frame detection pipeline for lorehlt16: a model translation approach		The LoReHLT16 evaluation challenged participants to extract Situation Frames (SFs)—structured descriptions of humanitarian need situations—from monolingual Uyghur text. The ARIEL-CMU SF detector combines two classification paradigms, a manually curated keyword-spotting system and a machine learning classifier. These were applied by translating the models on a per-feature basis, rather than translating the input text. The resulting combined model provides the accuracy of human insight with the generality of machine learning, and is relatively tractable to human analysis and error correction. Other factors contributing to success were automatic dictionary creation, the use of phonetic transcription, detailed, hand-written morphological analysis, and naturalistic glossing for error analysis by humans. The ARIEL-CMU SF pipeline produced the top-scoring LoReHLT16 situation frame detection systems for the metrics SFType, SFType+Place+Need, SFType+Place+Relief, and SFType+Place+Urgency, at each of the three checkpoints.	cobham's thesis;dictionary;error analysis (mathematics);error detection and correction;gloss (annotation);machine learning;transcription (software)	Patrick Littell;Tian Tian;Ruochen Xu;Zaid Sheikh;David R. Mortensen;Lori S. Levin;Francis Tyers;Hiroaki Hayashi;Graham Horwood;Steve Sloto;Emily Tagtow;Alan W. Black;Yiming Yang;Teruko Mitamura;Eduard H. Hovy	2017	Machine Translation	10.1007/s10590-017-9205-3	error detection and correction;natural language processing;information extraction;linguistics;artificial intelligence;computer science;generality;learning classifier system;phonetic transcription	NLP	-23.11694245654501	-78.48309906238283	105198
0ce9c261a7f69668da2066da0ad736e6eccdcd36	inducing multilingual text analysis tools via robust projection across aligned corpora	lemmatization;induced morphological analyzer;raw text;induced stand-alone part-of-speech tagger;arbitrary foreign language;multilingual;parallel corpora;morphology;complete french verbal system;text analysis;existing text analysis tool;noun phrase brac- keting;robust projection;named entity;part-of-speech tagging;accurate system;core part-of-speech;corresponding induced noun-phrase bracketer;multilingual text analysis tool;bilingual text corpus;foreign language;part of speech;spanish language;knowledge based systems;chinese language;french language;language translation;accuracy;noun phrase;computational linguistics;natural language	This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection.	algorithm;brill tagger;lemmatisation;part-of-speech tagging;text corpus	David Yarowsky;Grace Ngai;Richard Wicentowski	2001			natural language processing;speech recognition;computer science;linguistics	NLP	-25.45358963263078	-75.50116210249998	105224
e836784f1fdffc2fdaaa441057a9a68c23cf5972	a model of context adapted to domain-independent machine translation	lexical semantics;linguistica matematica;traduccion automatica;language translation;traduction connaissance;contextual information;tratamiento lenguaje;discourse structure;traduction automatique;language processing;traitement langage;linguistique mathematique;computational linguistics;analyse contextuelle;analisis semantico;analyse semantique;machine translation;semantic analysis;automatic translation	In this paper, we explore the integration of context into domain-independent machine translation based on a pseudo-semantic approach. We choose a theory of discourse structure (SDRT) to provide the model of context. The incompatibility between the knowledge poor translation model with the knowledge rich discourse theory leads to supplement the first with some basic lexical semantics, and to replace the specific rules of the second with more general pragmatic principles. How contextual information is used to choose the preferred interpretation of globally ambiguous sentences is then described with two examples.		Cathy Berthouzoz	1999		10.1007/3-540-48315-2_5	natural language processing;lexical semantics;example-based machine translation;computer science;computational linguistics;linguistics;machine translation;rule-based machine translation	NLP	-27.214511355068996	-78.22481711045586	105251
ac484c3906c28223daf0b811484c6c3de4698466	modeling math word problems with augmented semantic networks	mathematical formula;mathematical calculation;language model;mathematical structure;math word problems algorithmically;natural language processing;natural language processing module;augmented semantic network;natural language;mathematical equation;mathematical text problem	mathematical formula;mathematical calculation;language model;mathematical structure;math word problems algorithmically;natural language processing;natural language processing module;augmented semantic network;natural language;mathematical equation;mathematical text problem	semantic network	Christian Liguda;Thies Pfeiffer	2012		10.1007/978-3-642-31178-9_29	natural language processing;computer science;theoretical computer science;algorithm	NLP	-30.865057529483934	-79.23804695471983	105419
6d5675628e8cb1093f675ea7c679e5c9762463be	challenges in the development of annotated corpora of computer-mediated communication in indian languages: a case of hindi		The present paper describes an ongoing effort to compile and annotate a large corpus of computer-mediated communication (CMC) in Hindi. It describes the process of the compilation of the corpus, the basic structure of the corpus and the annotation of the corpus and the challenges faced in the creation of such a corpus. It also gives a description of the technologies developed for the processing of the data, addition of the metadata and annotation of the corpus. Since it is a corpus of written communication, it provides quite a distinctive challenge for the annotation process. Besides POS annotation, it will also be annotated at higher levels of representation. Once completely developed it will be a very useful resource of Hindi for research in the areas of linguistics, NLP and other social sciences research related to communication, particularly computer-mediated communication..Besides this the challenges discussed here and the way they are tackled could be taken as the model for developing the corpus of computer-mediated communication in other Indian languages. Furthermore the technologies developed for the construction of this corpus will also be made available publicly.	compiler;computer-mediated communication;natural language processing;text corpus	Ritesh Kumar	2012			artificial intelligence;compiler;natural language processing;metadata;hindi;computer-mediated communication;computer science;annotation	NLP	-29.911780180388178	-74.28359592233535	105431
80d4fe74359316d42045a7d39869a0177fa9d80e	applications of language modeling in speech-to-speech translation	performance measure;n gram model;mandarin chinese;speech translation;natural language understanding;target language;natural language generation;speech recognition;statistical language model;domain specificity;language model	This paper describes various language modeling issues in a speech-to-speech translation system. These issues are addressed in the IBM speech-to-speech system we developed for the DARPA Babylon program in the context of two-way translation between English and Mandarin Chinese. First, the language models for the speech recognizer had to be adapted to the specific domain to improve the recognition performance for in-domain utterances, while keeping the domain coverage as broad as possible. This involved considerations of disfluencies and lack of punctuation, as well as domain-specific utterances. Second, we used a hybrid semantic/syntactic representation to minimize the data sparseness problem in a statistical natural language generation framework. Serious inflection and synonym issues arise when words in the target language are to be determined in the translation output. Instead of relying on tedious handcrafted grammar rules, we used N-gram models as a post-processing step to enhance the generation performance. When an interpolated language model was applied to a Chinese-to-English translation task, the translation performance, measured by an objective metric of BLEU, improved substantially to 0.514 from 0.318 when we used the correct transcription as input. Similarly, the BLEU score improved to 0.300 from 0.194 for the same task when the input was speech data.	language model	Fu-Hua Liu;Liang Gu;Yuqing Gao;Michael Picheny	2004	I. J. Speech Technology	10.1023/B:IJST.0000017019.94181.d4	direct method;natural language processing;language identification;cache language model;bleu;speech recognition;transfer-based machine translation;universal networking language;mandarin chinese;computer science;computational linguistics;evaluation of machine translation;linguistics;machine translation;rule-based machine translation;modeling language;machine translation software usability;language model	NLP	-21.87359084964733	-80.21110201584301	105483
00a4731d12a6addf0eabbb1fe486632a518d62f2	let sense bags do talking: cross lingual word semantic similarity for english and hindi	conference article;computer science and technology	Cross Lingual Word Semantic (CLWS) similarity is defined as a task to find the semantic similarity between two words across languages. Semantic similarity has been very popular in computing the similarity between two words in same language. CLWS similarity will prove to be very effective in the area of Cross Lingual Information Retrieval, Machine Translation, Cross Lingual Word Sense Disambiguation, etc. In this paper, we discuss a system that is developed to compute CLWS similarity of words between two languages, where one language is treated as resourceful and other is resource scarce. The system is developed using WordNet. The intuition behind this system is that, two words are semantically similar if their senses are similar to each other. The system is tested for English and Hindi with the accuracy 60.5% precision@1 and 72.91% precision@3.	information retrieval;machine translation;mathematical model;semantic similarity;similarity learning;text corpus;word sense;word-sense disambiguation;word2vec;wordnet	Apurva Nagvenkar;Jyoti Pawar;Pushpak Bhattacharyya	2015			natural language processing;speech recognition;computer science;linguistics	NLP	-26.452608452730768	-68.3201458911953	105511
314b673cc28baddafefa31ac2aeb5a7cf36878e6	structured lexical similarity via convolution kernels on dependency trees	settore ing inf 05 sistemi di elaborazione delle informazioni	A central topic in natural language processing is the design of lexical and syntactic features suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.	convolution;experiment;lexicon;natural language processing;smoothing	Danilo Croce;Alessandro Moschitti;Roberto Basili	2011			natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.600147829775267	-72.85110065772362	105822
3eea18aa966e605ffc71b2d476325c04c3d46b2f	combining pattern matching with word embeddings for the extraction of experimental variables from scientific literature	spark;units and measures;regular expressions;machine learning;neural networks;biomedical;pattern matching	Scientists frequently use experiments published in other articles or reports by governing entities (e.g. NIH) as templates for reporting on their own experiments. Those templates occasionally change to reflect new discoveries. For creating retrospective studies and meta-analyses, finding the template parameters associated with scientific results can be critical. To aid in the extraction of experimental parameters (e.g. animal housing temperature) in a corpus of ∼8M scientific reports, we used a combination of pattern matching, part of speech tagging, units and measures extraction, and machine learning. We describe a use case where the housing temperature used for experiments involving mice was shown to impact their response to tumor reduction drugs. We show that 1) combining deep learning and pattern matching is a good model to address the problem described and 2) that researcher's behavior and experimental template usage takes a while to change after the publication of an important discovery.	deep learning;entity;experiment;machine learning;part-of-speech tagging;pattern matching;scientific literature;text corpus;while;word embedding	Helena F. Deus;Corey A. Harper;Darin McBeath;Ron Daniel	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258456	units of measurement;machine learning;artificial intelligence;deep learning;artificial neural network;scientific literature;pattern matching;part-of-speech tagging;computer science	Robotics	-19.46100832506384	-70.12643215808207	106080
82f9186f73d649a409727939b471351fb5fb73b9	computing idioms frequency in text corpora	headwords;text corpora;frequency of idioms;czech language	The idioms are phrases which meaning is not composed from the meanings of each word in the phrase. This is one of the natural examples of violating the principle of compositionality that means that idioms are in area of natural language processing problem of meaning mining. To count the frequency of phrases such idioms in corpora has one big aim: To get to know which phrases we use often and which less. We do it to be able to start with getting the meaning of the whole phrases not just each word. This improves the understanding natural language.	natural language processing;text corpus	Jan Busta	2008			natural language processing;computer science;specifier;linguistics;communication	NLP	-26.54090299445237	-72.82371302305106	106104
8c89473cac3c6402b7b62b5aa9a6035d5828bcd7	lt3 at semeval-2018 task 1: a classifier chain to detect emotions in tweets		This paper presents an emotion classification system for English tweets, submitted for the SemEval shared task on Affect in Tweets, subtask 5: Detecting Emotions. The system combines lexicon, n-gram, style, syntactic and semantic features. For this multi-class multilabel problem, we created a classifier chain. This is an ensemble of eleven binary classifiers, one for each possible emotion category, where each model gets the predictions of the preceding models as additional features. The predicted labels are combined to get a multilabel representation of the predictions. Our system was ranked eleventh among thirty five participating teams, with a Jaccard accuracy of 52.0% and macroand micro-average F1scores of 49.3% and 64.0%, respectively.	binary classification;jaccard index;lexicon;n-gram;semeval;test set	Luna De Bruyne;Orphée De Clercq;Véronique Hoste	2018			natural language processing;semeval;computer science;artificial intelligence;classifier (linguistics)	NLP	-22.138551976766774	-70.05483746381965	106196
ebcfc3afa6e080f8f5df9167a6e8b87dc0140a91	chinese terminology extraction using window-based contextual information	terminology extraction;chinese terminology;termhood;contextual information;window based contextual word;unithood;semantic information;hybrid method;natural language;domain specificity;extraction method	Terminology extraction is an important work for automatic update of domain specific knowledge. Contextual information helps to decide whether the extracted new terms are terminology or not. As extraction based on fixed patterns has very limited use to handle natural language text, we need both syntactical and semantic information in the context of a term to determine its termhood. In this paper, we investigate two window-based context word extraction methods taking into account of syntactic and semantic information. Based on the performance of each method individually, a hybrid method which combines both syntactical and semantic information is proposed. Experiments show that the hybrid method can achieve significant improvement.	natural language;terminology extraction	Luning Ji;Mantai Sum;Qin Lu;Wenjie Li;Yi-Rong Chen	2007		10.1007/978-3-540-70939-8_6	natural language processing;relationship extraction;computer science;pattern recognition;linguistics;natural language;information retrieval	AI	-29.626771996178675	-70.10339216298722	106239
007f917d7ceee1e9a2093e4556b9dc8ef0e18377	use of deep linguistic features for the recognition and labeling of semantic arguments	full parser;deep linguistic feature;deep syntactic feature;syntactic argument;syntactic feature;semantic argument;surface-oriented feature;semantic role;computer science;information technology	We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.	parsing;propbank;semantic role labeling;text corpus	John Chen;Owen Rambow	2003			natural language processing;speech recognition;deep linguistic processing;computer science;syntactic predicate;linguistics;information technology	NLP	-22.640146626075076	-74.4996363413592	106337
c32d0589e7b9a8144bd8b3af7d430da0af7cc4c4	automatic term extraction in technical domain using part-of-speech and common-word features.		"""Extracting key terms from technical documents allows us to write effective documentation that is specific and clear, with minimum ambiguity and confusion caused by nearly synonymous but different terms. For instance, in order to avoid confusion, the same object should not be referred to by two different names (e.g. """"hydraulic oil filter""""). In the modern world of commerce, clear terminology is the hallmark of successful RFPs (Requests for Proposal) and is therefore a key to the growth of competitive organizations. While Automatic Term Extraction (ATE) is a well-developed area of study, its applications in the technical domain have been sparse and constrained to certain narrow areas such as the biomedical research domain. We present a method for Automatic Term Extraction (ATE) for the technical domain based on the use of part-of-speech features and common words information. The method is evaluated on a C programming language reference manual as well as a manual of aircraft maintenance guidelines, and has shown comparable or better results to the reported state of the art results."""	brill tagger;documentation;experiment;natural language toolkit;part-of-speech tagging;point of sale;precision and recall;programming language reference;request for proposal;requests;sparse matrix;terminology extraction;wikipedia	Julia Buthmann;Vlado Keselj	2018		10.1145/3209280.3229100	part of speech;documentation;confusion;information retrieval;aircraft maintenance;computer science;ambiguity;terminology;technical documentation;terminology extraction	NLP	-30.334028995468547	-73.20982774942969	106357
8429d4f66648855a70bc81cb1348fc316b6edf89	a hybrid method for manufacturing text mining based on document clustering and topic modeling techniques		As the volume of online manufacturing information grows steadily, the need for developing dedicated computational tools for information organization and mining becomes more pronounced. This paper proposes a novel approach for facilitating search and organization of textual documents and also extraction of thematic patterns in manufacturing corpora using document clustering and topic modeling techniques. The proposed method adopts K-means and Latent Dirichlet Allocation (LDA) algorithms for document clustering and topic modeling, respectively. Through experimental validation, it is shown that topic modeling, in conjunction with document clustering, facilitates automated annotation and classification of manufacturing webpages as well as extraction of useful patterns, thus improving the intelligence of supplier discovery and knowledge acquisition tools.	algorithm;cluster analysis;information;iteration;k-means clustering;knowledge acquisition;knowledge organization;latent dirichlet allocation;ontology (information science);text corpus;text mining;theme (computing);thesaurus;topic model;unsupervised learning	Peyman Yazdizadeh Shotorbani;Farhad Ameri;Boonserm Kulvatunyou;Nenad Ivezic	2016		10.1007/978-3-319-51133-7_91	text mining;document clustering;data science;data mining;information retrieval	ML	-33.106918206994706	-67.18540045902998	106393
1e3125693a495a705d5da5c43ae781dd1219b095	a new combination method based on adaptive genetic algorithm for medical image retrieval		Medical image retrieval could be based on the text describ- ing the image as the caption or the title. The use of text terms to retrieve images have several disadvantages such as term-disambiguation. Recent studies prove that representing text into semantic units (concepts) can improve the semantic representation of textual information. However, the use of conceptual representation has other problems as the miss or erroneous semantic relation between two concepts. Other studies show that combining textual and conceptual text representations leads to bet- ter accuracy. Popularly, a score for textual representation and a score for conceptual representation are computed and then a combination func- tion is used to have one score. Although the existing of many combina- tion methods of two scores, we propose in this paper a new combination method based on adaptive version of the genetic algorithm. Experiments are carried out on Medical Information Retrieval Task of the ImageCLEF 2009 and 2010. The results confirm that the combination of both textual and conceptual scores allows best accuracy. In addition, our approach outperforms the other combination methods.	genetic algorithm;image retrieval	Karim Gasmi;Mouna Torjmen Khemakhem;Lynda Tamine;Maher Ben Jemaa	2014		10.1007/978-3-319-12844-3_25	natural language processing;computer science;machine learning;pattern recognition;data mining;information retrieval	Vision	-26.933441764273216	-67.24845493389071	106398
371abb12dfd53850daf6c02221574ff3212e6e9d	on the effectiveness of feature set augmentation using clusters of word embeddings		Word clusters have been empirically shown to offer important performance improvements on various Natural Language Processing (NLP) tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-anderror procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to better understand the role of such features in NLP tasks we perform a systematic empirical evaluation on three tasks, that of named entity recognition, fine grained sentiment classification and fine grained sentiment quantification.	computer cluster;feature engineering;feature extraction;n-gram;named-entity recognition;natural language processing;sentiment analysis;wikipedia;word embedding	Georgios Balikas;Ioannis Partalas;Massih-Reza Amini	2017	CoRR		computer science;machine learning;pattern recognition;data mining	NLP	-19.839938981074717	-71.70364246402995	106523
5bd7f2cd44b03e9f8a392f5f5a7511376105555c	using a variety of n-grams for the detection of different kinds of plagiarism notebook for pan at clef 2013		A text can be plagiarised in different ways. The text may be copied and pasted word by word, parts of the text may be changed, or the whole text may be summarised into one or two lines. Different kinds of plagiarism require different strategies to detect them. But rarely do we know beforehand what type of plagiarism we are dealing with. In this paper we present a system that can detect verbatim plagiarism and obfuscated plagiarism with similar accuracy. Our system uses three different types of n-grams: stopword n-grams, n-grams with at least one named entity, and all words n-grams. After detecting and merging the detections to obtain passages, the system finds new detections incrementally based on the idea that the passages on the vicinity of plagiarised passages are more likely to be plagiarised. The system performs well on verbatim as well as obfuscated plagiarism cases.	grams;n-gram;named entity;obfuscation (software);sensor	Prasha Shrestha;Thamar Solorio	2013			world wide web;named entity;information retrieval;merge (version control);obfuscation;clef;engineering	NLP	-29.15743369121877	-67.46699517139444	106574
fa950513e9fd9ab46b88a28f3330feedf401cb6f	impact of stemming on arabic text summarization	text mining;information retrieval;redundancy;feature extraction;clustering algorithms;natural language processing;algorithm design and analysis	Stemming is a process of reducing inflected words to their stem or root from a generally written word form. This process is used in many text mining application as a feature selection technique. Moreover, Arabic text summarization has increasingly become an important task in natural language processing area (NLP). Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. Khoja, Larekey and Alkhalil's stemmer) on the text summarization performance for Arabic language. The evaluation of the proposed system, with the three different stemmers and without stemming, on the dataset used shows that the best performance was achieved by Khoja stemmer in term of recall, precision and F1-measure. The evaluation also shows that the performances of the proposed system are significantly improved by applying the stemming process in the pre-processing stage.	automatic summarization;benchmark (computing);coefficient;cosine similarity;euclidean distance;experiment;f1 score;feature selection;graph (abstract data type);jaccard index;latent semantic analysis;machine learning;multi-master replication;natural language processing;performance;preprocessor;similarity measure;stemming;text mining	Nabil Alami;Mohammed Meknassi;Saïd El Alaoui Ouatik;Noureddine Ennahnahi	2016	2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)	10.1109/CIST.2016.7805067	natural language processing;text graph;speech recognition;computer science;information retrieval	NLP	-25.396990588909926	-67.99587177451673	106593
39a7d65c21081393bf2edd29155151dde2cee61f	word-alignment-based segment-level machine translation evaluation using word embeddings		One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.	bleu;bitext word alignment;data structure alignment;experiment;microsoft word for mac;statistical machine translation;word embedding	Junki Matsuo;Mamoru Komachi;Katsuhito Sudoh	2017	CoRR		machine translation;artificial intelligence;word error rate;natural language processing;machine learning;computer science;rule-based machine translation;example-based machine translation	NLP	-21.81685695458883	-76.86566682656608	106660
c7db0de8b0d710388b260e5e18dc5c6b215aa561	language-independent multi-document text summarization with document-specific word associations	co occurrence analysis;text mining;text summarization;natural language processing	The goal of automatic text summarization is to generate an abstract of a document or a set of documents. In this paper we propose a word association based method for generating summaries in a variety of languages. We show that a robust statistical method for finding associations which are specific to the given document(s) is applicable to many languages. We introduce strategies that utilize the discovered associations to effectively select sentences from the document(s) to constitute the summary. Empirical results indicate that the method works reliably in a relatively large set of languages and outperforms methods reported in MultiLing 2013.	automatic summarization;language-independent specification	Oskar Gross;Antoine Doucet;Hannu Toivonen	2016		10.1145/2851613.2851647	natural language processing;text graph;text mining;multi-document summarization;computer science;automatic summarization;data mining;tf–idf;information retrieval	NLP	-27.80168649653412	-67.85791578343925	106706
844bc841f2c8380fa483aaab6f53a2ef7efdd24c	nlptea 2017 shared task - chinese spelling check.		This paper provides an overview along with our findings of the Chinese Spelling Check shared task at NLPTEA 2017. The goal of this task is to develop a computerassisted system to automatically diagnose typing errors in traditional Chinese sentences written by students. We defined six types of errors which belong to two categories. Given a sentence, the system should detect where the errors are, and for each detected error determine its type and provide correction suggestions. We designed, constructed, and released a benchmark dataset for this task.	benchmark (computing)	Gabriel Pui Cheong Fung;Maxime Debosschere;Dingmin Wang;Bo Li;Jia Zhu;Kam-Fai Wong	2017			natural language processing;spelling;computer science;artificial intelligence	NLP	-24.18900805796859	-76.30437406788792	106721
ce1c505942b3c12d5d0d504738e5240c81c7dc56	university of manitoba: description of the nuba system as used for muc-5	plan recognition;word sense disambiguation;natural language understanding	Abduction is the inference to the best explanation. Many tasks in natural language understanding such as word-sense disambiguity [1], local pragmatics [4], metaphor interpretation [3], and plan recognition [5, 8], can be viewed as abduction.		Dekang Lin	1993		10.3115/1072017.1072042	natural language processing;computer science;linguistics;communication	Robotics	-32.436501293272876	-79.54359364076672	106835
73f95cca28b390528a19da47dd9e06961399c23e	construction of an aligned monolingual treebank for studying semantic similarity	semantic similarity;comparable text;parallel text;corpus;monolingual treebank;semantic relations;paraphrase;alignment;tree alignment	Modern paraphrase research would benefit from large corpora with detailed annotations. However, currently these corpora are still thin on the ground. In this paper, we describe the development of such a corpus for Dutch, which takes the form of a parallel monolingual treebank consisting of over 2 million tokens and covering various text genres, including both parallel and comparable text. This publicly available corpus is richly annotated with alignments between syntactic nodes, which are also classified using five different semantic similarity relations. A quarter of the corpus is manually annotated, and this informs the development of an automatic tree aligner used to annotate the remainder of the corpus. We argue that this corpus is the first of this size and kind, and offers great potential for paraphrasing research.	institute for operations research and the management sciences;natural language processing;ontology components;open-source software;semantic similarity;text corpus;treebank	Erwin Marsi;Emiel Krahmer	2014	Language Resources and Evaluation	10.1007/s10579-013-9252-1	natural language processing;semantic similarity;explicit semantic analysis;computer science;treebank;linguistics;information retrieval	NLP	-30.564532760359054	-74.87899533710106	106918
ec98416719b9e795ee341dd65b55b41b0ceda251	hmsl (hierarchical music specification language): a real-time environment for formal, perceptual and compositional experimentation				David Rosenboom;Larry Polansky	1985			programming language;natural language processing;perception;specification language;computer science;artificial intelligence	PL	-30.465400767046482	-80.04312749319045	106952
52304eec4724ab10a40458b10ef07b5c4d0367f8	hubs in languages: scale free networks of synonyms	scale free network;statistical properties;scale free;statistical analysis;natural language	Natural languages are described in this paper in terms of networks of synonyms: a word is identified with a node, and synonyms are connected by undirected links. Our statistical analysis of the network of synonyms in Polish language showed it is scale-free; similar to what is known for English. The statistical properties of the networks are also similar. Thus, the statistical aspects of the networks are good candidates for culture independent elements of human language. We hypothesize that optimization for robustness and efficiency is responsible for this universality. Despite the statistical similarity, there is no one-to-one mapping between networks of these two languages. Although many hubs in Polish are translated into similarly highly connected hubs in English, there are also hubs specific to one of these languages only: a single word in one language is equivalent to many different and disconnected words in the other, in accordance with the Whorf hypothesis about language relativity. Identifying language-specific hubs is vitally important for automatic translation, and for understanding contextual, culturally related messages that are frequently missed or twisted in a naïve, literary translation.	algorithmic efficiency;graph (discrete mathematics);machine translation;mathematical optimization;naivety;one-to-one (data model);twisted;universality probability	Hanna E. Makaruk;Robert Owczarek	2008	CoRR		natural language processing;combinatorics;computer science;scale-free network;mathematics;statistics	NLP	-25.08513096149198	-74.1371200891428	107278
999dcc2c7d347dd6f7897dcdea245fabb72456ee	concept-based relevance models for medical and semantic information retrieval	term dependency;relevance model;semantic type;information retrieval;concepts;umls;concept relevance models;medical;semantic relation;semantic search;relevance;concept relevance;ontology	"""Relevance models provide an important approach for estimating probabilities of words in the relevant class. However, the associated bag-of-words assumption breaks dependencies between words, especially between those within a phrase. If such dependencies could be preserved, it would permit matching the query terms with document terms having the same dependencies. Additionally, during the estimation of relevance, relevance models are unable to distinguish relevant and non-relevant information in a feedback document, and hence take the entire document into account, which potentially hurts the accuracy of estimation. In this paper, we define the notion of """"concept"""", and design a concept-based information retrieval framework. Using this framework, we transform documents and queries from term space into concept space, and propose a concept-based relevance model for improved estimation of relevance. Our approach has three advantages. First, this approach only assumes independence between concepts, so is able to keep the strong dependencies between the words of a concept. Second, it unifies synonyms or different surface forms of a concept, leading to reduced dimensionality of the space, increased sample size of a concept, and consequently more accurate and reliable estimates of the relevance. Third, when knowledge bases are available, our approach enables the semantic analysis of query concepts, and thus identifies concepts related to the query, from which a more accurate distribution of relevance can be estimated. This work is aligned with semantic search methods.  We apply our concept-based relevance model to information retrieval in the medical domain, where concepts are abundant and their variations are numerous. We compare with relevance models, BM25 with pseudo relevance feedback, and the state of the art conceptual language models, on several data collections. The proposed model demonstrates consistent and statistically significant improvements across collections, outperforming top benchmark conceptual language models by at least 9% and up to 20% on a number of metrics."""	bag-of-words model;benchmark (computing);information retrieval;language model;matching (graph theory);okapi bm25;relevance feedback;semantic analysis (compilers);semantic search	Chunye Wang;Ram Akella	2015		10.1145/2806416.2806497	natural language processing;concepts;relevance;relevance;semantic search;computer science;concept search;ontology;data mining;unified medical language system;information retrieval	Web+IR	-29.93029596743865	-66.54855324131414	107318
0850674a0cd454978a5899c94c500322319c4a95	detection of imperative and declarative question-answer pairs in email conversations	information extraction;email;question answer pairing;nlp	Question-answer pairs extracted from email threads can help construct summaries of the thread, as well as inform semantic-based assistance with email. Previous work dedicated to email threads extracts only questions in interrogative form. We extend the scope of question and answer detection and pairing to encompass also questions in imperative and declarative forms, and to operate at sentence-level fidelity. Building on prior work, our methods are based on learned models over a set of features that include the content, context, and structure of email threads. For two large email corpora, we show that our methods balance precision and recall in extracting question-answer pairs, while maintaining a modest computation time.	algorithm;computation;conversation threading;declarative programming;email;heuristic;imperative programming;named entity;named-entity recognition;nick mckeown;precision and recall;text corpus;time complexity	Helen Kwong;Neil Yorke-Smith	2003	AI Commun.	10.3233/AIC-2012-0516	html email;computer science;data mining;database;programming language;world wide web;information extraction	AI	-25.024321931640873	-70.4980123564568	107320
df74b8ddb7f15751295794a3ba175fc337d657a9	example-based methods for natural language processing with applications to machine translation and preposition correction			machine translation;natural language processing	J. S. Smith	2012				NLP	-29.905834538589684	-77.95853281649055	107442
794b11dd7cae34600e4a260e95d6155124a4a950	unsupervised learning of arabic non-concatenative morphology	pj6001 arabic;p0098 computational linguistics natural language processing	Unsupervised approaches to learning the morphology of a language play an important role in computer processing of language from a practical and theoretical perspective, due their minimal reliance on manually produced linguistic resources and human annotation. Such approaches have been widely researched for the problem of concatenative affixation, but less attention has been paid to the intercalated (non-concatenative) morphology exhibited by Arabic and other Semitic languages.#R##N##R##N#The aim of this research is to learn the root and pattern morphology of Arabic, with accuracy comparable to manually built morphological analysis systems. The approach is kept free from human supervision or manual parameter settings, assuming only that roots and patterns intertwine to form a word.#R##N##R##N#Promising results were obtained by applying a technique adapted from previous work in concatenative morphology learning, which uses machine learning to determine relatedness between words. The output, with probabilistic relatedness values between words, was then used to rank all possible roots and patterns to form a lexicon. Analysis using trilateral roots resulted in correct root identification accuracy of approximately 86% for inflected words.#R##N##R##N#Although the machine learning-based approach is effective, it is conceptually complex. So an alternative, simpler and computationally efficient approach was then devised to obtain morpheme scores based on comparative counts of roots and patterns. In this approach, root and pattern scores are defined in terms of each other in a mutually recursive relationship, converging to an optimized morpheme ranking. This technique gives slightly better accuracy while being conceptually simpler and more efficient.#R##N##R##N#The approach, after further enhancements, was evaluated on a version of the Quranic Arabic Corpus, attaining a final accuracy of approximately 93%. A comparative evaluation shows this to be superior to two existing, well used manually built Arabic stemmers, thus demonstrating the practical feasibility of unsupervised learning of non-concatenative morphology.	galaxy morphological classification;unsupervised learning	Bilal Khaliq	2015			natural language processing;speech recognition;computer science;communication	NLP	-25.24292280127103	-73.39948983264966	107607
0136003b5c927f0e4e97c4d4e70b302f8592c36b	modeling content structures of domain-specific texts with rup-hdp-hsmm and its applications			hidden semi-markov model	Youwei Lu;Shogo Okada;Katsumi Nitta	2017	IEICE Transactions		pattern recognition;computer science;natural language processing;machine learning;artificial intelligence;text mining	HCI	-23.738588448931832	-72.82505891473635	107618
15433e331c287c6c812c54756d87f82683ed7b5b	the phonological representation of affricates	article letter to editor			Janine Berns	2016	Language and Linguistics Compass	10.1111/lnc3.12179	psychology;natural language processing;speech recognition;linguistics;sociology	NLP	-31.35696287009529	-78.92360225066805	107661
af5eb611e5ead5fa5b7234fd8bb56fe591bd8435	chunk-based verb reordering in vso sentences for arabic-english statistical machine translation	statistical machine translation;word order;word alignment;long range;machine translation	In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 ArabicEnglish benchmark.	bleu;baseline (configuration management);benchmark (computing);chunking (computing);compiler;discriminative model;distortion;human-readable medium;information;language model;microsoft visual studio;parallel text;parsing;preprocessor;statistical machine translation	Arianna Bisazza;Marcello Federico	2010			natural language processing;speech recognition;example-based machine translation;computer science;linguistics;rule-based machine translation	NLP	-22.19160602588396	-77.09115333048935	107726
4f0d8580ff2ae7249036cd16615ebcec499aaa9e	lexical story co-segmentation of chinese broadcast news	belief propagation (bp);foreground and background story modeling;lexical clustering;mrf;qpbo;story co-segmentation	We present an unsupervised technique, namely story cosegmentation, to automatically extract the common stories on the same topic within a pair of Chinese broadcast news transcripts. Unlike classical topic tracking that usually relies on previously trained topic models, our method is purely data-driven and is able to simultaneously determine the common stories of the input texts. Specifically, we propose an iterative four-step MRF solution to the problem of story co-segmentation using lexical cues only. We first construct a sentence-level graph formulation of the input news transcripts, and initialize foreground and background labeling by lexical clustering. We then update both foreground and background models based on the current labeling. We formalize story co-segmentation as a Gibbs energy minimization problem that balances the optimal objectives of foreground/background likelihood, intra-doc coherence, and inter-doc similarity. Finally, the labeling refinement is obtained by hybrid optimization with QPBO and BP. The effectiveness of our method has been validated on real-world CCTV corpus.	closed-circuit television;cluster analysis;energy minimization;iterative method;markov random field;mathematical optimization;refinement (computing);topic model;unsupervised learning	Wei Feng;Xuecheng Nie;Liang Wan;Lei Xie;Jianmin Jiang	2012			speech recognition;artificial intelligence;topic model;pattern recognition;cluster analysis;minification;computer science;segmentation;broadcasting;graph	AI	-19.688755591591157	-77.35448285792062	107877
09e6692722448141dd10c71ad46c463f4f4fea99	part-of-speech tagging and detection of social media texts	textklassifikation;elektrotechnik elektronik;text classification;naturliche sprachverarbeitung;part of speech tagging;automatische wortartenbestimmung;natural language processing	First, the task of social media text classification in Web pages is addressed, where sequences of Web text segments are classified based on a high-dimensional feature vector. New features motivated by social media text characteristics are introduced and investigated with respect to different classifiers. Two classification problems in the context of social media text classification are treated, (1) the problem of social media text detection and (2) a method for Web page cleaning for social media platforms. A new Web page corpus, particularly designed to train and test the classifiers on representative Web pages is created.		Melanie Neunerdt	2016			natural language processing;speech recognition;linguistics	Web+IR	-22.24720528435708	-68.35338247581234	107898
cb289b8e34be7dcbacb20084178a2bdf4f5720a4	a customized dependency tree kernel for effective sentiment classification			kernel (operating system)	Zhou Sun;Chao Gu;Chunping Li	2012		10.3233/978-1-61499-105-2-532	tree kernel;pattern recognition;artificial intelligence;computer science	NLP	-19.264435693097177	-66.84175857666023	107900
4d5e5556c7079e7d3709c09d1f6a9161be226e81	an effective category classification method based on a language model for question category recommendation on a cqa service	category or topic classification;category or topic recommendation;word weighting;language model;cqa service	Classiying user's question into several topics helps respondents answering the question in a cQA service. The word weighting method must estimate the appropriate weight of a word to improve the category (or topic) classification. In this paper, we propose a novel effective word weighting method based on a language model for automatic category classification in the cQA service. We first calculate the occurrence probability of a word in each category by using a language model and then the final weight of each word is estimated by ratio of the occurrence probability of the word on a category to the occurrence probability of the word on the other categories. As a result, the proposed method significantly improves the performance of the category classification.	language model;statistical classification	Kyoungman Bae;Youngjoong Ko	2012		10.1145/2396761.2398614	natural language processing;computer science;pattern recognition;information retrieval;language model	NLP	-23.150301768284372	-66.47210660072419	107909
8f160b44abc3b0e86567833f6acaab2d77cdc845	translation using japio patent corpora: japio at wat2016		Japan Patent Information Organization (JAPIO) participates in scientific paper subtask (ASPEC-EJ/CJ) and patent subtask (JPC-EJ/CJ/KJ) with phrase-based SMT systems which are trained with its own patent corpora. Using larger corpora than those prepared by the workshop organizer, we achieved higher BLEU scores than most participants in EJ and CJ translations of patent subtask, but in crowdsourcing evaluation, our EJ translation, which is best in all automatic evaluations, received a very poor score. In scientific paper subtask, our translations are given lower scores than most translations that are produced by translation engines trained with the indomain corpora. But our scores are higher than those of general-purpose RBMTs and online services. Considering the result of crowdsourcing evaluation, it shows a possibility that CJ SMT system trained with a large patent corpus translates non-patent technical documents at a practical level.	bleu;crowdsourcing;e-services;electronic organizer;general-purpose modeling;image organizer;jpc;knowledge organization;peer-to-patent;scientific literature;text corpus	Satoshi Kinoshita;Tadaaki Oshio;Tomoharu Mitsuhashi;Terumasa Ehara	2016			computer science;natural language processing;artificial intelligence	NLP	-31.01179991275184	-72.75721699728344	107913
26604510c93ec3f7e8da97ee13c12e9b52dcd611	experiments on chinese text indexing -- clarit trec-5 chinese track report				Xiang Tong;ChengXiang Zhai;Natasa Milic-Frayling;David A. Evans	1996				NLP	-31.979901535886274	-76.90644995915335	107925
81ce52d3d5fd2782a813d5d473a74365f0a262c3	sample-dependent feature selection for faster document image categorization	databases;image document classification;image classification;text analysis;error analysis;accuracy;estimation;machine learning;text analysis document image processing feature extraction image classification learning artificial intelligence;feature extraction;document image processing;feature selection;learning artificial intelligence;confidence rated multi label classification image document classification feature selection;adaboost sample dependent feature selection document image categorization document image classification pixel level features automatic text transcription;calibration;confidence rated multi label classification;feature extraction databases accuracy machine learning calibration estimation error analysis	In document image classification, some classes of documents can be easily identified using pixel-level features, whereas some distinctions can only be made using semantics, which usually involves a full automatic text transcription. To be as much efficient as possible, the classification system should be able to avoid extracting high-level and time consuming features when they are not necessary to classify with confidence. We introduce here this issue of sample-dependent feature selection, which has not been addressed before as far as we know. We propose a method to tackle this problem, that can be generalized to any classifier that provides a confidence score along with its prediction. Empirical results using AdaBoost on three mail classification problems show that our approach allows to significantly improve classification efficiency (up to 40% CPU time off) without significant loss of accuracy in comparison to the baseline.	adaboost;baseline (configuration management);categorization;central processing unit;computer vision;document classification;document-oriented database;feature selection;high- and low-level;pixel;selection algorithm;statistical classification;transcription (software)	Jérôme Louradour;Christopher Kermorvant	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.70	estimation;contextual image classification;calibration;feature extraction;computer science;machine learning;pattern recognition;data mining;accuracy and precision;feature selection	Vision	-20.561677519831733	-69.44513357709367	108196
e4cedd9fa41dc8facf6fa21ea79c18c5b460097e	cluster labeling for multilingual scatter/gather using comparable corpora	multilingual;comparable corpora;school of automation;topic models;cluster labeling;computer science automation formerly;scatter gather	Scatter/Gather systems are increasingly becoming useful in browsing document corpora. Usability of the present-day systems are restricted to monolingual corpora, and their methods for clustering and labeling do not easily extend to the multilingual setting, especially in the absence of dictionaries/machine translation. In this paper, we study the cluster labeling problem for multilingual corpora in the absence of machine translation using comparable corpora. Using a variational approach, we show that multilingual topic models can effectively handle the cluster labeling problem, which in turn allows us to design a novel Scatter/Gather system ShoBha. Experimental results on three datasets, namely the Canadian Hansards corpus, the entire overlapping Wikipedia of English, Hindi and Bengali articles, and a trilingual news corpus containing 41,000 articles, confirm the utility of the proposed system.	cluster analysis;dictionary;experiment;machine translation;text corpus;topic model;usability testing;variational principle;vectored i/o;vocabulary;wikipedia	Goutham Tholpadi;Mrinal Kanti Das;Chiranjib Bhattacharyya;Shirish K. Shevade	2012		10.1007/978-3-642-28997-2_33	natural language processing;speech recognition;computer science;machine learning;topic model;information retrieval	NLP	-23.919545979839278	-73.63982204042048	108276
139292feefb570e93b389d72bfe6ab141404800c	logicon: a system for extracting semantic structure using partial parsing		Partial parsing is an established NLP technique used to perform syntactic analysis without generating a full constituent parse tree. This paper presents LOGICON, an endto-end system using partial parsing, which assigns novel semantic structures to natural language text. Evaluating against a test set of 500 previously unseen sentences, the system has an accuracy of 62.4% as measured by exact matching against the expected semantic output. Since partial parsing is used, the system is robust and will assign partial semantic structure to sentences it may not fully understand. As stochastic methods are not used, the system is deterministic and fast. A syntactic tagging scheme is proposed which is closely aligned to the corresponding semantics. The system was developed as part of a PhD research project, and was written to evaluate partial parsing as the first step to creating a full natural language question-answering system.	deterministic algorithm;end system;natural language processing;parse tree;parsing;question answering;test set	Kais Dukes	2009			syntax;machine learning;parse tree;semantics;natural language;artificial intelligence;parsing;computer science;test set	NLP	-25.06276976933122	-75.74502029642086	108345
faa0281fc701e562a67dfe23ebeecbb16b50a92a	efficient higher-order crfs for morphological tagging		Training higher-order conditional random fields is prohibitive for huge tag sets. We present an approximated conditional random field using coarse-to-fine decoding and early updating. We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1-order models.	approximation algorithm;brill tagger;conditional random field;experiment;high-level programming language;policy and charging rules function;stochastic gradient descent;trigram	Thomas Müller;Helmut Schmid;Hinrich Schütze	2013			crfs;artificial intelligence;computer science;machine learning;conditional random field;decoding methods	NLP	-21.294937248092488	-76.14448629061388	108375
1df3a41bba6f3cdbbc31339dbc098d0cf06e2d28	learning chinese-japanese bilingual word embedding by using common characters	bilingual word embedding;distributed representation;common characters;chinese-japanese	Bilingual word embedding, which maps word embedding of two languages into one vector space, has been widely applied in the domain of machine translation, word sense disambiguation and so on. However, no model has been universally accepted for learning bilingual word embedding. In this work, we propose a novel model named CJ-BOC to learn Chinese-Japanese word embeddings. Given Chinese and Japanese share a large portion of common characters, we exploit them in our training process. We demonstrated the effectiveness of such exploitation through theoretical and also experimental study. To evaluate the performance of CJ-BOC, we conducted a comprehensive experiment, which reveals its speed advantage, and high quality of acquired word embeddings as well.	word embedding	Jilei Wang;Shiying Luo;Yanning Li;Shu-Tao Xia	2016		10.1007/978-3-319-47650-6_7	natural language processing;speech recognition	NLP	-19.624873117096634	-73.30295439631682	108383
745461e362291c619de5ff73d6d237a9b446dad9	creating pos tagging and dependency parsing experts via topic modeling		Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics.	baseline (configuration management);brill tagger;experiment;latent dirichlet allocation;parsing;part-of-speech tagging;point of sale;sparse matrix;test set;topic model;unsupervised learning	Matthias Scheutz;Sandra Kübler;Atreyee Mukherjee	2017			machine learning;artificial intelligence;topic model;natural language processing;computer science;dependency grammar	NLP	-22.46854237107781	-72.29042363314838	108491
a5d77bc95934d8b0183783edce6d85b40d7829b7	phrase table induction using monolingual data for low-resource statistical machine translation		We propose a new method for inducing a phrase-based translation model from a pair of unrelated monolingual corpora. Our method is able to deal with phrases of arbitrary length and to find phrase pairs that are useful for statistical machine translation, without requiring large parallel or comparable corpora. First, our method generates phrase pairs through coupling source and target phrases separately collected from respective monolingual data. Then, for each phrase pair, we compute features using the monolingual data and a small quantity of parallel sentences. Finally, incorrect phrase pairs are pruned, and a phrase table is made using the remaining phrase pairs. In our experiments on French--Japanese and Spanish--Japanese translation tasks under low-resource conditions, we observe that incorporating a phrase table induced by our method to the machine translation system leads to large improvements in translation quality. Furthermore, we show that a phrase table induced by our method can also be useful in a wide range of configurations, including configurations where we have already access to large parallel corpora and configurations where only small monolingual corpora are available.	domain adaptation;experiment;parallel text;pivot table;statistical machine translation;text corpus	Benjamin Marie;Atsushi Fujita	2018	ACM Trans. Asian & Low-Resource Lang. Inf. Process.	10.1145/3168054	machine translation;knowledge acquisition;semantic similarity;phrase;computer science;pattern recognition;artificial intelligence	NLP	-23.629931692961858	-74.52457375256358	108559
c14e841f1d383c6019afc8efed189e27ab26f9d5	buap: polarity classification of short texts		We report the results we obtained at the subtask B (Message Polarity Classification) of SemEval 2014 Task 9. The features used for representing the messages were basically trigrams of characters, trigrams of PoS and a number of words selected by means of a graph mining tool. Our approach performed slightly below the overall average, except when a corpus of tweets with sarcasm was evaluated, in which we performed quite well obtaining around 6% above the overall average.	brown corpus;point of sale;semeval;sensor;structure mining;text corpus;trigram;unbalanced circuit;vocabulary	David Pinto;Darnes Vilariño Ayala;Saúl León;Miguel Jasso-Hernández;Cupertino Lucero	2014			natural language processing;computer science;machine learning;communication	NLP	-23.321041348743503	-69.65399975172838	108719
85802e8366dce3dc30754ae00426d3c0874200c0	parsing turkish with the lexical functional grammar formalism		This paper describes our work on parsing Turk-ish using the lexical-functional grammar formalism. This work represents the first effort for parsing Turkish. Our implementation is based on Tomita's parser developed at Carnegie-Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer. After a discussion of key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82% of the sentences directly and almost all the remaining with very minor pre-editing.	formal grammar;lexical functional grammar;machine translation;mathematical morphology;parsing expression grammar;semantics (computer science);the turk	Zelal Güngördü;Kemal Oflazer	1994	CoRR			NLP	-29.184017611615182	-75.9889416631287	108805
0e4e093f703313654690c9dc0f46b74a407a9653	using cross-language cues for story-specific language modeling	machine translation;specification language;language model	o Motivation : 4 Multi-lingual, multi-domain, multi-source linguistic resources are available 4 Most resources are concentrated on popular languages such as English, French and German 4 Relatively small resources for Chinese, Arabic, and other languages. o Stochastic models require a large amount of data for training o How to construct stochastic models in resource deficient languages? Ô Boostrap methods by reusing models from resource rich languages, e.g. 4 Universal phone-set for ASR 4 Exploit parallel texts to project morphological analyzers, POS taggers, etc. o We present: 4 An approach to sharpen an LM in a resource deficient language using comparable text from resource rich languages 4 Story-specific language models from parallel text 4 Integration of machine translation (MT), cross-language information retrieval (CLIR), and language modeling (LM)	cross-language information retrieval;language model;machine translation;multi-source;parallel text;stochastic process	Sanjeev Khudanpur;Woosung Kim	2002			first-generation programming language;language identification;modeling language;data control language;universal networking language;cache language model;object language;natural language processing;language primitive;artificial intelligence;computer science	NLP	-22.10343653025859	-80.06505963782445	108807
0b691f6a863a2876e4c4609674f534938981ef86	french and german corpora for audience-based text type classification		This paper presents some of the results of the CLASSYN project which investigated the classification of text according to audience-related text types. We describe the design principles and the properties of the French and German linguistically annotated corpora that we have created. We report on tools used to collect the data and on the quality of the syntactic annotation. The CLASSYN corpora comprise two text collections to investigate general text types difference between scientific and popular science text on the two domains of medical and computer science.	computer science;text corpus	Amalia Todirascu-Courtier;Sebastian Padó;Jennifer Krisch;Max Kisselew;Ulrich Heid	2012			natural language processing;speech recognition;linguistics	NLP	-29.675092211159416	-74.96112773402028	108819
729628ca14d8fb14b038348c4725cb2e10a8aca3	unsupervised disambiguation of image captions	yarowsky-inspired algorithm;text-only disambiguation;sense-tagged image;unsupervised disambiguation;social media;latent dirichlet allocation;unsupervised word sense disambiguation;previous work;amazon mechanical turk;image caption;unsupervised text-only disambiguation;related caption	Given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media. We extend previous work in unsupervised text-only disambiguation with methods that integrate text and images. We construct a corpus by using Amazon Mechanical Turk to caption sensetagged images gathered from ImageNet. Using a Yarowsky-inspired algorithm, we show that gains can be made over text-only disambiguation, as well as multimodal approaches such as Latent Dirichlet Allocation.	algorithm;amazon mechanical turk;collocation;imagenet;latent dirichlet allocation;multimodal interaction;social media;text corpus;text-based user interface;the turk;unsupervised learning;web services for devices;word lists by frequency;word sense;word-sense disambiguation	Wesley May;Sanja Fidler;Afsaneh Fazly;Sven J. Dickinson;Suzanne Stevenson	2012			natural language processing;computer science;pattern recognition;information retrieval	NLP	-21.068268052949577	-66.97713284375075	108868
3b0d2ab0a7ce9fa97cb7470b904055f3b81fc73a	transition-based parsing with confidence-weighted classification	pruning feature;transition-based parsing;parsing time;confidence-weighted classification;faster training	We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.	algorithm;machine learning;parsing;statistical classification	Martin Haulrich	2010			natural language processing;computer science;machine learning;pattern recognition	NLP	-20.185125827324043	-66.20749996151417	108955
8c7661965349022b4e3a69938ad08171269c3171	a probabilistic model for the arc length in quantitative linguistics			statistical model	Peter Zörnig	2015		10.1515/9783110362879-015	arc length;natural language processing;quantitative linguistics;statistical model;artificial intelligence;computer science	NLP	-29.844999059890256	-78.08530742954169	109244
1f62242b9d063bb71cf039ea4d4c5222cdc02ccb	mining of social networks from literary texts of resource poor languages		We describe our work on automatic identification of social events and mining of social networks from literary texts in Tamil. Tamil belongs to Dravidian language family and is a morphologically rich language. This is a resource poor language; sophisticated resources for document processing such as parsers, phrase structure tree tagger are not available. In our work we have used shallow parsing for document processing. Conditional Random Fields (CRFs), a machine learning technique is used for automatic identification of social events. We have obtained an F-measure of 62% on social event identification. Social networks are mined by forming triads of the actors in the social events. The social networks are evaluated using graph comparison technique. The system generated social networks is compared with the gold network. We have obtained a very encouraging similarity score of 0.75.	social network	Pattabhi R. K. Rao;Sobha Lalitha Devi	2016		10.1007/978-3-319-75487-1_31	computer science;document processing;dravidian languages;tamil;information retrieval;artificial intelligence;natural language processing;phrase structure rules;parsing;shallow parsing;conditional random field;social network	NLP	-23.534437274483118	-67.58805763440333	109297
57b13ec6e83349852167a5131848b4b059ee6085	chinese news classification		We applied three different machine learning models to classify Chinese news into a group of classes in two schemes. The first scheme is to process the texts into TF-IDF matrices prior to running support vector machines (SVM) and maximum entropy (MAXENT) models, while the second scheme uses an embedding layer in a convolutional neural network (CNN) in order to learn features during the training process. We then compare the results obtained by all the models in terms of overall accuracy, precision, recall and F-scores. The MAXENT model showed the best performance, with an overall accuracy of 93.71%. The CNN model showed a lower performance in comparison with MAXENT and SVM models, with an overall accuracy around 73.58%. This result was not expected and we conclude with some considerations about the CNN design and possible future improvements.	artificial neural network;convolutional neural network;machine learning;multinomial logistic regression;principle of maximum entropy;support vector machine;tf–idf	David Cecchini;Li Na	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00125	principle of maximum entropy;convolutional neural network;support vector machine;matrix (mathematics);data modeling;embedding;computer science;artificial intelligence;pattern recognition	Vision	-20.334805889653595	-70.09553895573042	109324
197ea7195886fd4513bd19f59f094183af3f57c4	identificación de entidades con nombre basada en modelos de markov y árboles de decisión	markovian model;apprentissage automatique;nom propre;traitement automatique des langues naturelles;computacion informatica;decision tree;processus markovien;information retrieval;modelos de markov;filologias;reconocimiento de entidades;linguistique appliquee;info eu repo semantics article;informacion documentacion;proper noun;linguistica;automatic recognition;arbre de decision;machine learning;ciencias basicas y experimentales;automatic documentation;name entity recognition;markov models;documentation automatique;computational linguistics;espagnol;grupo a;decision trees;ciencias sociales;linguistique informatique;grupo b;natural language processing;arboles de decision;named entity;reconnaissance automatique;recherche d information;entite nommee;applied linguistics	In this paper we investigate Named Entity Recognition (NER) systems using two well-known classifiers in the machine learning literature: Markov Models and Decision Trees. We have designed several systems to check the impact of introducing different characteristics which have a weak dependence of the language used. We also report the results obtained by our systems on the Spanish corpus provided in the NER Task of ConNLL 2002 conference.	decision tree learning;machine learning;markov chain;markov model;named entity;named-entity recognition;naruto shippuden: clash of ninja revolution 3;text corpus	José Antonio Troyano Jiménez;Víctor J. Díaz;Fernando Enríquez;Javier Barroso;Vicente Carrillo	2003	Procesamiento del Lenguaje Natural		natural language processing;computer science;artificial intelligence;computational linguistics;decision tree;applied linguistics;linguistics;algorithm	Logic	-26.857274455373904	-77.52600695237098	109335
4cfe7375b40e01e421263554d606d400f66ed4a0	uniba - integrating distributional semantics features in a supervised approach for detecting irony in italian tweets				Pierpaolo Basile;Giovanni Semeraro	2018				NLP	-21.611491947463605	-69.73763946056462	109439
e65eb81da5c92c8b6ce5b72568388869faac54b1	supervised word sense disambiguation for venetan: a proof-of-concept experiment		Word Sense Disambiguation (WSD) is a classification task that consists of determining which of the senses of an ambiguous word is activated in a specific context. Research in this field has primarily concentrated on investigating English and a few other well-resourced languages. Recently, studies done on a corpus of Old English (Wunderlich 2015) showed that, even with limited resources, it is still possible to approach the problem of WSD. In this paper, a WSD system has been developed for the Low Resource Language (LRL) Venetan, which has recently received some attention from the Natural Language Processing (NLP) community. Our main contributions are twofold: first, we select and annotate a corpus for Venetan, considering two words (one abstract and one concrete term) and using two levels of annotation (fineand coarse-grained), reporting on annotator agreement. Second, we report results of proof-of-concept experiments of supervised WSD performed with Support Vector Machines on this corpus. To our knowledge, our work is the first time that WSD for a European Dialect like Venetan has been studied.	experiment;natural language processing;supervised learning;support vector machine;text corpus;web services for devices;word sense;word-sense disambiguation;wunderlich (vacuum tube)	Costanza Conforti;Alexander M. Fraser	2017			proof of concept;artificial intelligence;natural language processing;machine learning;semeval;word-sense disambiguation;computer science	NLP	-22.98966053284324	-70.01776220812629	109453
002ebc31e959f500570146d6b44eded3b51e4072	cmu system combination in wmt 2011	machine translation;cmu system combination;flexibly aligning system;combination scheme;system combination task	This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translation’s system combination task. We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks.	bleu;creole (markup);humans;language model;machine translation	Kenneth Heafield;Alon Lavie	2011			simulation;engineering;engineering drawing;cartography	NLP	-33.47412583646277	-75.77458251753109	109517
85de75a77c7bbf091f9315e88fcee7c32e5b399c	semantic information extraction for improved word embeddings		Word embeddings have recently proven useful in a number of different applications that deal with natural language. Such embeddings succinctly reflect semantic similarities between words based on their sentence-internal contexts in large corpora. In this paper, we show that information extraction techniques provide valuable additional evidence of semantic relationships that can be exploited when producing word embeddings. We propose a joint model to train word embeddings both on regular context information and on more explicit semantic extractions. The word vectors obtained from such an augmented joint training show improved results on word similarity tasks, suggesting that they can be useful in applications that involve word meanings.	coefficient;definition;enumerated type;information extraction;microsoft word for mac;natural language;semantic web;text corpus;word embedding	Jiaqiang Chen;Gerard de Melo	2015			natural language processing;speech recognition;semeval;computer science;linguistics	NLP	-20.79404475200923	-72.16197270402631	109525
b742ede9539d4e829ce0412a6f509315c20a2d5f	relieving polysemy problem for synonymy detection	noun;multiple choice	In order to automatically identify noun synonyms, we propose a new idea which opposes classical polysemous representations of words to monosemous representations based on the “one sense per discourse” hypothesis. For that purpose, we apply the attributional similarity paradigm on two levels: corpus and document. We evaluate our methodology on well-known standard multiple choice synonymy question tests and evidence that it steadily outperforms the baseline.	baseline (configuration management);categorization;coefficient;experiment;heuristic;mathematical optimization;natural language processing;programming paradigm;ruby document format;similarity learning;test case;text corpus;word sense;word-sense disambiguation	Gaël Dias;Rumen Moraliyski	2009		10.1007/978-3-642-04686-5_50	multiple choice;natural language processing;noun;computer science	NLP	-26.275931835651456	-72.416314192116	109567
aaa03114a082646062067e1aeca04e58691528d0	distributional lexical semantics: toward uniform representation paradigms for advanced acquisition and processing tasks	lexical semantics;settore ing inf 05 sistemi di elaborazione delle informazioni	The distributional hypothesis states that words with similar distributional properties have similar semantic properties (Harris 1968). This perspective on word semantics, was early discussed in linguistics (Firth 1957; Harris 1968), and then successfully applied to Information Retrieval (Salton, Wong and Yang 1975). In Information Retrieval, distributional notions (e.g. document frequency and word co-occurrence counts) have proved a key factor of success, as opposed to early logic-based approaches to relevance modeling (van Rijsbergen 1986; Chiaramella and Chevallet 1992; van Rijsbergen and Lalmas 1996).		Roberto Basili;Marco Pennacchiotti	2010	Natural Language Engineering	10.1017/S1351324910000112	natural language processing;lexical semantics;speech recognition;computer science;linguistics	AI	-29.67612329255202	-78.42543231130126	109630
a28a804ccd2fcf356a6c6bcf4461369857366c46	a comparison study of some arabic root finding algorithms	truncation;arabic language;morphological analysis;algorithms	Arabic has a complex structure, which makes it difficult to apply natural language processing (NLP). Much research on Arabic NLP (ANLP) does exist; however, it is not as mature as that of other languages. Finding Arabic roots is an important step toward conducting effective research on most of ANLP applications. The authors have studied and compared six root-finding algorithms with success rates of over 90&percnt;. All algorithms of this study did not use the same testing corpus and-or benchmarking measures. They unified the testing process by implementing their own algorithm descriptions and building a corpus out of 3823 triliteral roots, applying 73 triliteral patterns, and with 18 affixes, producing around 27.6 million words. They tested the algorithms with the generated corpus and have obtained interesting results; they offer to share the corpus freely for benchmarking and ANLP research. © 2010 Wiley Periodicals, Inc.	root-finding algorithm	Emad Al-Shawakfa;Amer Al-Badarneh;Safwan Shatnawi;Khaleel Al-Rabab'ah;Basel Bani-Ismail	2010	JASIST	10.1002/asi.21301	natural language processing;speech recognition;computer science;arabic;information extraction	ECom	-29.64037916495758	-72.91371735306389	109892
e9f8c48c6af2904f1e82983ef63303343ddf6519	measuring language development in early childhood education: a case study of grammar checking in child language transcripts	automatic detection;child language transcript;statistical system;language development;early childhood education;automatic grammar checking;grammatical complexity;grammatical mistake;language sample analysis;verb related grammatical error;case study;natural language processing	Language sample analysis is an important technique used in measuring language development. At present, measures of grammatical complexity such as the Index of Productive Syntax (Scarborough, 1990) are used to measure language development in early childhood. Although these measures depict the overall competence in the usage of language, they do not provide for an analysis of the grammatical mistakes made by the child. In this paper, we explore the use of existing Natural Language Processing (NLP) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking. We explore the automatic detection of 6 types of verb related grammatical errors. We compare rule based systems to statistical systems and investigate the use of different features. We found the statistical systems performed better than the rule based systems for most of the error categories.	grammar checker;natural language processing;rule-based system	Khairun-nisa Hassanali;Yang Liu	2011			direct method;natural language processing;language identification;cache language model;universal networking language;object language;computer science;developmental linguistics;linguistics;natural language;communication	NLP	-27.657229065574388	-76.02385008011635	109954
8e6546f37a2df3206dd62a8fcca101ca04d8a389	from character to document representation with global context awareness		Bag-of-Words with TF-IDF or other weighting schemes is commonly adopted ways for document representation. However, they fail to capture sequential or semantic information in the sentence, and would lead to high-dimensional vector due to misspelling, acronyms and so on. Distributed word embedding and even document embedding methods are proposed to encode the semantic or contextual information. Whereas, the quality of the representation is not always good. To relieve the above mentioned problems, we propose a high-quality document representation model, which takes word morphology, semantic and sequential information of global context into consideration. The proposed model could outperform state-of-the-art traditional ways, word embedding-based and character-aware models on text classification task.	bag-of-words model in computer vision;context awareness;document classification;encode;galaxy morphological classification;tf–idf;word embedding	Zhenzhou Wu;Xin Zheng;Daniel Dahlmeier	2017		10.1145/3162957.3162973	word embedding;morphology (linguistics);deep learning;context awareness;embedding;artificial intelligence;weighting;computer science;pattern recognition;sentence	AI	-22.933937837758076	-66.37654413005666	109958
e1d1098659029384d7c02b8b742c1944d1bf3059	recall is the proper evaluation metric for word segmentation		We extensively analyse the correlations and drawbacks of conventionally employed evaluation metrics for word segmentation. Unlike in standard information retrieval, precision favours under-splitting systems and therefore can be misleading in word segmentation. Overall, based on both theoretical and experimental analysis, we propose that precision should be excluded from the standard evaluation metrics and that the evaluation score obtained by using only recall is sufficient and better correlated with the performance of word segmentation systems.	information retrieval;lazy evaluation;text segmentation	Yan Shao;Christian Hardmeier;Joakim Nivre	2017			pattern recognition;artificial intelligence;computer science;recall;text segmentation	NLP	-26.99313630162276	-68.21179524362876	110023
a70ef09f3018d058bea7c3a530e8c1b4e8115d02	learning hierarchies from ambiguous natural language data	natural language	 This paper presents a method for acquiringa semantic hierarchy and updating an incompletehierarchy. The creation of a comprehensivehierarchy is one important step in constructinga system for translating Japanesetexts into English. The hierarchy is usedto bias the learning of rules that indicatethe English translation of a Japanese verb.The task is particularly challenging becausetraining examples are ambiguous in the sensethat each of the attributes forming an examplemay... 	natural language	Takefumi Yamazaki;Michael J. Pazzani;Christopher J. Merz	1995			natural language processing;computer science;machine learning;natural language	ML	-25.76610487068142	-75.84556045995873	110354
d3c1178e3cd343a6153cfe5760e885dfb787ef3e	luminoso at semeval-2018 task 10: distinguishing attributes using text corpora and relational knowledge		Luminoso participated in the SemEval 2018 task on “Capturing Discriminative Attributes” with a system based on ConceptNet, an open knowledge graph focused on general knowledge. In this paper, we describe how we trained a linear classifier on a small number of semantically-informed features to achieve an F1 score of 0.7368 on the task, close to the task’s high score of 0.75.	f1 score;knowledge graph;linear classifier;open mind common sense;open knowledge;semeval;text corpus	Robert Speer;Joanna Lowry-Duda	2018		10.18653/v1/S18-1162	discriminative model;semeval;machine learning;linear classifier;computer science;general knowledge;artificial intelligence;small number;text corpus;luminoso;graph	NLP	-21.700444394906178	-70.6902623328057	110481
0725984d1786b90512cb1aa9486bfff5c07291fe	inter-annotator agreement on a multilingual semantic annotation task	information technology;computer science	Six sites participated in the Interlingual Annotation of Multilingual Text Corpora (IAMTC) project (Dorr et al., 2004; Farwell et al., 2004; Mitamura et al., 2004). Parsed versions of English translations of news articles in Arabic, French, Hindi, Japanese, Korean and Spanish were annotated by up to ten annotators. Their task was to match open-class lexical items (nouns, verbs, adjectives, adverbs) to one or more concepts taken from the Omega ontology (Philpot et al., 2003), and to identify theta roles for verb arguments. The annotated corpus is intended to be a resource for meaning-based approaches to machine translation. Here we discuss inter-annotator agreement for the corpus. The annotation task is characterized by annotators’ freedom to select multiple concepts or roles per lexical item. As a result, the annotation categories are sets, the number of which is bounded only by the number of distinct annotator-lexical item pairs. We use a reliability metric designed to handle partial agreement between sets. The best results pertain to the part of the ontology derived from WordNet. We examine change over the course of the project, differences among annotators, and differences across parts of speech. Our results suggest a strong learning effect early in the project.	corpus linguistics;inter-rater reliability;machine translation;omega;parsing;text corpus;web ontology language;wordnet	Rebecca J. Passonneau;Nizar Habash;Owen Rambow	2006			natural language processing;computer science;database;semantic technology;information retrieval	NLP	-28.737358774916967	-72.49231181762633	110715
14189976c1b7219782c1a56721f6697ecac0f776	automatic sense disambiguation of the near-synonyms in a dictionary entry	linguistique;lexicon;dictionnaire;linguistica;lexical knowledge base;synonyme;lexical acquisition;dictionaries;desambiguisation;lexico;diccionario;lexique;linguistics	We present an automatic method to disambiguate the senses of the near-synonyms in the entries of a dictionary of synonyms. We combine different indicators that take advantage of the structure on the en tries and of lexical knowledge in WordNet. We also present the results of human ju dges doing the disambiguation for 50 randomly selected entries. This smal l amount of annotated data is used to tune and evaluate our system.	algorithm;automatic summarization;dictionary;inter-rater reliability;peripheral;randomness;web services for devices;word-sense disambiguation;wordnet	Diana Inkpen;Graeme Hirst	2003		10.1007/3-540-36456-0_25	natural language processing;speech recognition;computer science;linguistics;rule-based machine translation	NLP	-26.9944153912061	-71.59978940705842	110852
1909960e0c0bd43b2d0911b8123cecb511bcc7ef	detecting promotional content in wikipedia		This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.	experiment;language model;n-gram;sensor;statistical classification;stochastic context-free grammar;stylometry;wikipedia	Shruti Bhosale;Heath Vinicombe;Raymond J. Mooney	2013			computer science;multimedia;world wide web;information retrieval	NLP	-21.97945974487336	-67.7026520787642	110877
a508d1ba6450c3beabd52db53503181dcef48ab1	large scale corpus analysis and recent applications	opinion mining;knowledge extraction;anaphora resolution;large scale;corpus analysis;machine learning;natural language;automatic annotation;dependency parsing;natural language processing	Recent progress of corpus and machine learning-based natural language processing methodologies have made it possible to handle large scale corpus with a quite high accuracy. The speaker is now involved in a project for constructing a large scale contemporary Japanese balanced corpus, aiming at constructing automatic annotation tools on various levels of natural language analyses. I will first introduce our activities on corpus based natural language analyzers for word dependency parsing and anaphora resolution and annotated corpus management environment. Then, I will explain recent natural language applications such as sentiment/opinion mining and knowledge extraction from a large scale text data like Weblogs.	anaphora (linguistics);blog;machine learning;natural language processing;parsing;text corpus	Yuji Matsumoto	2008		10.1007/978-3-540-89197-0_4	natural language processing;language identification;speech recognition;question answering;computer science;text corpus;knowledge extraction;natural language;temporal annotation;dependency grammar	NLP	-28.10102741242257	-73.66737415500629	110914
77f1a24ee899fe982125bbb5ee9ee81ccbf47341	intertwining deep syntactic processing and named entity detection	lenguaje natural;linguistique;langage naturel;tratamiento lenguaje;analyse syntaxique;linguistica;named entity recognition;language processing;analisis sintaxico;syntactic analysis;natural language;traitement langage;natural language processing;named entity;linguistics	In this paper, we present a robust incremental architecture for natural language processing centered around syntactic analysis but allowing at the same time the description of specialized modules, like named entity recognition. We show that the flexibility of our approach allows us to intertwine general and specific processing, which has a mutual improvement effect on their respective results: for example, syntactic analysis clearly benefits from named entity recognition as a pre-processing step, but named entity recognition can also take advantage of deep syntactic information.	categorization;las vegas algorithm;linear algebra;mathematical optimization;maynard electronics;meaning–text theory;name;named entity;natural language processing;pl/i;parsing;preprocessor;systems architecture;value (ethics);word sense;word-sense disambiguation	Caroline Brun;Caroline Hagège	2004		10.1007/978-3-540-30228-5_18	natural language processing;speech recognition;computer science;parsing;entity linking;natural language	NLP	-29.150645403964955	-77.30981490795516	110922
096fd36589d4cec91d9a08d5c1da6d031bc498a2	inside-outside estimation of a lexicalized pcfg for german	free grammar;phrase markup;lexicalized probabilistic context;inside-outside estimation;german verb-final clause;lexicalized pcfg;extensive experiment;successive model;formalism feature;context free grammar	The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verbfinal clauses. Grammar and formalism features which make the experiment feasible are described. Successive models are evaluated on precision and recall of phrase markup. 1 I n t r o d u c t i o n Charniak (1995) and Carroll and Rooth (1998) present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates. The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order). We emphasize techniques which made it practical to apply inside-outside estimation of a lexicalized context free grammar to such a language. These techniques relate to the treatment of argument cancellation and scrambled phrase order; to the treatment of case features in category labels; to the category vocabulary for nouns, articles, adjectives and their projections; to lexicalization based on uninflected lemmata rather than word forms; and to exploitation of a parameter-tying feature. 2 C o r p u s and morphology The data for the experiment is a corpus of German subordinate clauses extracted by regular expression matching from a 200 million token newspaper corpus. The clause length ranges between four and 12 words. Apart from infinitival VPs as verbal arguments, there are no further clausal embeddings, and the clauses do not contain any punctuation except for a terminal period. The corpus contains 4128873 tokens and 450526 clauses which yields an average of 9.16456 tokens per clause. Tokens are automatically annotated with a list of part-of-speech (PoS) tags using a computational morphological analyser based on finite-state technology (Karttunen et al. (1994), Schiller and StSckert (1995)). A problem for practical inside-outside estimation of an inflectional language like German arises with the large number of terminal and low-level non-terminal categories in the grammar resulting from the morpho-syntactic features of words. Apart from major class (noun, adjective, and so forth) the analyser provides an ambiguous word with a list of possible combinations of inflectional features like gender, person, number (cf. the top part of Fig. 1 for an example ambiguous between nominal and adjectival PoS; the PoS is indicated following the '+' sign). In order to reduce the number of parameters to be estimated, and to reduce the size of the parse forest used in inside-outside estimation, we collapsed the inflectional readings of adjectives, adjective derived nouns, article words, and pronouns to a single morphological feature (see of Fig. 1 for an example). This reduced the number of low-level categories, as exemplified in Fig. 2: das has one reading as an article and one as a demonstrative; westdeutschen has one reading as an adjective, with its morphological feature N indicating the inflectional suffix. We use the special tag UNTAGGED indicating that the analyser fails to provide a tag for the word. The vast majority of UNTAGGED words are proper names not recognized as such. These gaps in the morphology have little effect on our experiment.	bigram;brown corpus;carroll morgan (computer scientist);experiment;galaxy morphological classification;high- and low-level;language model;markup language;mathematical morphology;parse tree;parsing;part-of-speech tagging;precision and recall;regular expression;semantics (computer science);stochastic context-free grammar;text corpus;vocabulary;xfig	Franz Beil;Glenn Carroll;Detlef Prescher;Stefan Riezler;Mats Rooth	1999	CoRR		natural language processing;id/lp grammar;generative grammar;speech recognition;computer science;stochastic grammar;phrase structure rules;extended affix grammar;emergent grammar;linguistics;context-free grammar;attribute grammar;generalized phrase structure grammar;mildly context-sensitive grammar formalism;head-driven phrase structure grammar	NLP	-26.355761818473766	-76.65762609113251	111114
866c1e86e890a52d0418292f37f759679e78c72a	a hybrid parts of speech tagger for malayalam language	training;speech;accuracy;hidden markov models;dictionaries;natural language processing;tagging	Parts of speech tagging is an important research topic in Natural Language Processing research are. Since it is one among the first steps of any natural language processing (NLP) techniques such as machine translation, if any error happens for tagging the same will repeat in the whole NLP process. So far works had been done on POS tagging based on SVM, MBLP, HMM, Ngram. All of these methods were not fixing the problem of ambiguity. So for fixing ambiguity, we put forward a new Hybrid tagger for Malayalam. The combination of traditional rules and n-gram may produce better result compared to other methodologies. And also the ambiguity will be reduced by enriching the bigram dictionary. A bigram dictionary of co-occurring words are built with their tags. About 100000 more words are there in bigram dictionary. A corpus for Malayalam must be built which may be supposed to access by the model. It contains about 100000 words which are Malayalam words as well as the words originated from English. Since it's a hybrid tagger, we can take advantage of both traditional rules as well as bigrams. Also the heart of the research is the rule set, which contains 267 manually created rules. Rules can be applied with help of a morph analyzer. Rules are also used for tagging if bigram and corpus can't be referred for tagging. The proposed method when tested on 150 words, only 11 words were not identified, and obtained 90.5% accuracy. For the unidentified words, it can be caused by either the root word may not be in corpus or bigram, or the absence of rule. So adding the word, bigram or rule, we can improve the result and enhance the work. Addition is simple task. The size of bigram dictionary, corpus, and rule set and accuracy of morph analyzer influences the performance of the system.	algorithm;alloy analyzer;bigram;brill tagger;dictionary;hidden markov model;machine translation;n-gram;natural language processing;part-of-speech tagging;point of sale;support vector machine;text corpus	T. Aziz AnishaAziz;C. Sunitha	2015	2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2015.7275825	natural language processing;speech recognition;computer science;speech;mathematics;accuracy and precision;bigram;trigram tagger;hidden markov model	NLP	-24.050528537161433	-76.90687017578801	111184
9452ae6695aa96ed5579d2eb632ffd113b2579b4	the bulgarian national corpus: theory and practice in corpus design	bulgarian national corpus;corpus design;computational linguistics	The paper discusses several key concepts related to the development of corpora and reconsiders them in light of recent developments in NLP. On the basis of an overview of present-day corpora, we conclude that the dominant practices of corpus design do not utilise adequately the technologies and, as a result, fail to meet the demands of corpus linguistics, computational lexicology and computational linguistics alike.   We proceed to lay out a data-driven approach to corpus design, which integrates the best practices of traditional corpus linguistics with the potential of the latest technologies allowing fast collection, automatic metadata description and annotation of large amounts of data. Thus, the gist of the approach we propose is that corpus design should be centred on amassing large amounts of mono- and multilingual texts and on providing them with a detailed metadata description and high-quality multi-level annotation.   We go on to illustrate this concept with a description of the compilation, structuring, documentation, and annotation of the Bulgarian National Corpus (BulNC). At present it consists of a Bulgarian part of 979.6 million words, constituting the corpus kernel, and 33 Bulgarian-X language corpora, totalling 972.3 million words, 1.95 billion words altogether. The BulNC is supplied with a comprehensive metadata description, which allows us to organise the texts according to different principles. The Bulgarian part of the BulNC is automatically processed (tokenised and sentence split) and  annotated at several levels: morphosyntactic tagging, lemmatisation, word-sense annotation, annotation of noun phrases and named entities. Some levels of annotation are also applied to the Bulgarian-English parallel corpus with the prospect of expanding multilingual annotation both in terms of linguistic levels and the number of languages for which it is available. We conclude with a brief evaluation of the quality of the corpus and an outline of its applications in NLP and linguistic research.		Svetla Koeva;Ivelina Stoyanova;Svetlozara Leseva;Rositsa Dekova;Tsvetana Dimitrova;Ekaterina Tarpomanova	2012	J. Language Modelling	10.15398/jlm.v0i1.33	natural language processing;speech recognition;computer science;corpus linguistics;text corpus;linguistics	NLP	-31.09326105024175	-74.84637522130059	111257
32d87adf6e551d5c166771a32ad1eb1c8010c5a0	limsi @ wmt12		Data Pre-processing • Better normalization tools provide better BLEU scores • Specific pre-processing for German as source language • Cleaning noisy data sets (GigaWord) – Discard sentences in other languages – Remove repeated sentences, or the ones included in the development sets – Normalize the character set – Select best half of the data set according to perplexity SOUL n-gram Model Overview wi-1	bleu;character encoding;data pre-processing;perplexity;preprocessor;signal-to-noise ratio	Hai Son Le;Thomas Lavergne;Alexandre Allauzen;Marianna Apidianaki;Li Gong;Aurélien Max;Artem Sokolov;Guillaume Wisniewski;François Yvon	2012			computer science	NLP	-23.33761351645286	-77.27465414481637	111397
ee8e3c4a8d964e38a4ce6a39c5d1b34688357c4a	computer assisted semantic annotation in the dutchsemcor project	noun	The goal of this paper is to describe the annotation protocols and the Semantic Annotation Tool (SAT) used in the DutchSemCor project. The DutchSemCor project is aiming at aligning the Cornetto lexical database with the Dutch language corpus SoNaR. 250K corpus occurrences of the 3,000 most frequent and most ambiguous Dutch nouns, adjectives and verbs are being annotated manually using the SAT. This data is then used for bootstrapping 750K extra occurrences which in turn will be checked manually. Our main focus in this paper is the methodology applied in the project to attain the envisaged Inter-annotator Agreement (IA) of 80%. We will also discuss one of the main objectives of DutchSemCor i.e. to provide semantically annotated language data with high scores for quantity, quality and diversity. Sample data with high scores for these three features can yield better results for co-training WSD systems. Finally, we will take a brief look at our annotation tool.	bootstrapping (compilers);co-training;lexical database;sonar;text corpus;web services for devices	Attila Görög;Piek T. J. M. Vossen	2010			semantic computing;artificial intelligence;natural language processing;temporal annotation;image retrieval;bootstrapping;information retrieval;lexical database;framenet;computer science;semantic web stack;annotation	NLP	-29.184048540995995	-71.89400506891016	111418
9f8f7e6cc18205b92ecf9e792bfcdb4b6f19cae3	opinion mining in online reviews about distance education programs		The popularity of distance education programs is increasing at a fast pace. En par with this development, online communication in fora, social media and reviewing platforms between students is increasing as well. Exploiting this information to support fellow students or institutions requires to extract the relevant opinions in order to automatically generate reports providing an overview of pros and cons of different distance education programs. We report on an experiment involving distance education experts with the goal to develop a dataset of reviews annotated with relevant categories and aspects in each category discussed in the specific review together with an indication of the sentiment. Based on this experiment, we present an approach to extract general categories and specific aspects under discussion in a review together with their sentiment. We frame this task as a multi-label hierarchical text classification problem and empirically investigate the performance of different classification architectures to couple the prediction of a category with the prediction of particular aspects in this category. We evaluate different architectures and show that a hierarchical approach leads to superior results in comparison to a flat model which makes decisions independently. This work has been performed while the first and last authors were at Bielefeld University.	artificial neural network;automatic summarization;bottom-up parsing;computer-mediated communication;document classification;fits;feedforward neural network;machine learning;money;multi-label classification;natural language processing;sentiment analysis;social media;software propagation;sword art online: progressive;taxonomy (general);text corpus;text mining	Janik Jaskolski;Fabian Siegberg;Thomas Tibroni;Philipp Cimiano;Roman Klinger	2016	CoRR		natural language processing;computer science;artificial intelligence;data science;machine learning;data mining;multimedia	NLP	-21.14553874213976	-68.80105190697569	111536
729d09e850b0045a13cb5279d387190b0ec5ad0b	bilingual event extraction: a case study on trigger type determination		Event extraction generally suffers from the data sparseness problem. In this paper, we address this problem by utilizing the labeled data from two different languages. As a preliminary study, we mainly focus on the subtask of trigger type determination in event extraction. To make the training data in different languages help each other, we propose a uniform text representation with bilingual features to represent the samples and handle the difficulty of locating the triggers in the translated text from both monolingual and bilingual perspectives. Empirical studies demonstrate the effectiveness of the proposed approach to bilingual classification on trigger type determination. 	database trigger;neural coding;text corpus	Zhu Zhu;Shoushan Li;Guodong Zhou;Rui Xia	2014			natural language processing;speech recognition;computer science;data mining;linguistics	AI	-25.42558629628979	-72.13445188559052	111583
6d937270157cabb23288ce6a948275f4aeeaa827	learning to recognize tables in free text	different table characteristic;real-world text;table recognition algorithm;fixed set;free text;deterministic table recognition algorithm;wall street journal news;new approach;different domain	Many real-world texts contain tables. In order to process these texts correctly and extract the information contained within the tables, it is important to identify the presence and structure of tables. In this paper, we present a new approach that learns to recognize tables in free text, including the boundary, rows and columns of tables. When tested on Wall Street Journal news documents, our learning approach outperforms a deterministic table recognition algorithm that identifies tables based on a fixed set of conditions. Our learning approach is also more flexible and easily adaptable to texts in different domains with different table characteristics.	algorithm;column (database);table (database);the wall street journal	Hwee Tou Ng;Chung Yong Lim;Jessica Li Teng Koo	1999			speech recognition;computer science;artificial intelligence;machine learning;table;data mining	NLP	-23.90962366089995	-67.14099131334392	111783
7877d8f102b86344dcab740ab4abf270d483cd26	a self-training crf method for recognizing product model mentions in web forums		Important applications in product opinion mining such as opinion summarization and aspect extraction require the recognition of product mentions as a basic task. In the case of consumer electronic products, Web forums are important and popular sources of valuable opinions. Forum users often refer to products by means of their model numbers. In a post a user would employ model numbers, e.g., “BDP-93” and “BDP-103”, to compare Blu-ray players. To properly handle opinions in such a scenario, applications need to correctly recognize products by their model numbers. Forums, however, are informal and many challenges for undertaking automatic product model recognition arise, since users mention model numbers in many different ways. In this paper we propose the use of a self-training strategy to learn a suitable CRF model for this task. Our method requires only a set of seed model numbers. Experiments in four different settings demonstrate that our method, by leveraging unlabeled sentences from the target forum, yielded an improvement of 19% in recall and 12% in F-measure over a supervised CRF model.	conditional random field	Henry S. Vieira;Altigran Soares da Silva;Marco Cristo;Edleno Silva de Moura	2015		10.1007/978-3-319-16354-3_27	speech recognition;world wide web	NLP	-21.315223845485626	-66.43131322668006	111789
5b432b73908b9b394efa6528555544cc5d7a1a14	knowledge representation for web based services in a multi-cultural environment	internet;information resources;knowledge based systems;knowledge representation;language translation;natural languages;internet;rdf based language;web based applications;web based services;abstract concepts;cultural context;domain ontology;global resource;intelligent web based application;knowledge based application;knowledge representation;knowledge representation scheme;knowledge representation technique;linguistic context;media property specifications;multi-cultural environment;natural language;nontextual symbols;ontological description;ontology network	With the Internet being a global resource, Web based applications need to break the barriers of language and culture. The core of an intelligent Web based application comprises an ontological description of the domain. A domain ontology needs a medium for expression, which usually consists of terminology borrowed from a natural language. Thus, a knowledge based application becomes susceptible to linguistic and cultural context. The authors present a novel knowledge representation technique that distinguishes between the abstract concepts in a domain and their expressions. It can associate expressions from different languages with the concepts in an ontology network. Non-textual symbols and media property specifications can also be used to express the concepts using this technique. The resulting ontology can thus be used in a multi-lingual and multi-cultural environment. An RDF based language is used as a vehicle for the knowledge representation scheme.	knowledge representation and reasoning;web service	Cornelia Boldyreff	2001			web service;knowledge representation and reasoning;web application security;web development;web modeling;data web;web mapping;knowledge integration;web standards;computer science;knowledge management;ws-policy;semantic web;web navigation;social semantic web;open knowledge base connectivity;semantic web stack;web intelligence;web 2.0	Web+IR	-32.83298914803046	-68.62800904609068	111815
a65e6a974212ed28133c2fe1e3b97a18dbc40cb6	feature-rich named entity recognition for bulgarian using conditional random fields		The paper presents a feature-rich approach to the automatic recognition and categorization of named entities (persons, organizations, locations, and miscellaneous) in news text for Bulgarian. We combine well-established features used for other languages with language-specific lexical, syntactic and morphological information. In particular, we make use of the rich tagset annotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive suitable task-specific tagsets (local and nonlocal). We further add domain-specific gazetteers and additional unlabeled data, achieving F1=89.4%, which is comparable to the state-of-the-art results for English.	brill tagger;categorization;conditional random field;domain-specific language;experiment;lexicon;named entity;quantum nonlocality;software feature	Georgi Georgiev;Preslav Nakov;Kuzman Ganchev;Petya Osenova;Kiril Ivanov Simov	2009			natural language processing;computer science;pattern recognition;data mining	NLP	-24.271435930005442	-73.58870576609705	111836
8ba95947c5c2e09f19da301257712606b52977f3	using semantic networks to identify temporal expressions from semantic roles		Nowadays, the temporal aspects of natural language are receiving a great research interest. TimeML has been adopted as a standard for temporal information annotation by a large number of researchers. Available TimeML resources are very limited in size and in diversity of languages. This paper analyzes a combination of semantic roles and semantic networks information for improving this situation. An automatic approach using semantic networks to convert temporal semantic roles into TimeML TIMEX3 elements is presented. This approach has been quantitatively evaluated for English and Spanish. The results point out that the presented approach can help in a semi-automatic creation of TimeML resources for the evaluated languages and could be also valid for other European languages.	baseline (configuration management);natural language;semantic network;semiconductor industry;temporal expressions;text corpus;timeml	Hector Llorens;Borja Navarro-Colorado;Estela Saquete Boró	2009			natural language processing;semantic computing;computer science;data mining;linguistics;temporal annotation;information retrieval	NLP	-29.124234839874827	-70.49661005120593	111923
d83de1a281beb843ecab0575cc155b7c3b6b0565	supervised topic classification for modeling a hierarchical conference structure	labeled classification;probabilistic latent semantic analysis;hierarchical topic model;em approach	The paper presents a method of constructing a supervised topic model of a major conference. The supervised part is the expert information about document-topic correspondence. To exploit the expert information we use a regularization term which penalizes difference between a constructed and an expert-given model. To find optimal parameters we add the regularization term to the log-likelihood function and use EM algorithm with its stochastic modification. The proposed method is used to construct a topic model for the European Conference on Operational Research to automatize the abstract submission system.	expectation–maximization algorithm;experiment;matrix regularization;operations research;perplexity;supervised learning;synthetic intelligence;topic model	Mikhail P. Kuznetsov;Marianne Clausel;Massih-Reza Amini;Éric Gaussier;Vadim V. Strijov	2015		10.1007/978-3-319-26532-2_11	computer science;machine learning;pattern recognition;data mining;probabilistic latent semantic analysis	Vision	-19.884348775911505	-76.86972663766439	111937
788a8482e4924114516e4513d77426c99c85d4b5	using maximum entropy model to extract protein-protein interaction information from biomedical literature	information extraction;text mining;machine learning;protein protein interaction;maximum entropy model;maximum entropy	Protein-Protein interaction (PPI) information play a vital role in biological research. This work proposes a two-step machine learning based method to extract PPI information from biomedical literature. Both steps use Maximum Entropy (ME) model. The first step is designed to estimate whether a sentence in a literature contains PPI information. The second step is to judge whether each protein pair in a sentence has interaction. Two steps are combined through adding the outputs of the first step to the model of the second step as features. Experiments show the method achieves a total accuracy of 81.9% in BC-PPI corpus and the outputs of the first step can effectively prompt the performance of the PPI information extraction.	interaction information;principle of maximum entropy	Chengjie Sun;Lei Lin;Xiaolong Wang;Yi Guan	2007		10.1007/978-3-540-74171-8_72	text mining;computer science;principle of maximum entropy;machine learning;pattern recognition;data mining;information extraction;statistics	NLP	-24.340020916013405	-67.8161584958068	111941
b4e68896d40b11457651dfdcbe04bbc0553ce04a	integrating semantic frames from multiple sources	argument structure;language understanding;representacion conocimientos;ontologie;base donnee;verbe;database;base dato;semantics;multiplicite;intelligence artificielle;semantica;semantique;multiplicidad;recouvrement ensemble;verbo;representation connaissance;artificial intelligence;ontologia;difference set;inteligencia artificial;set covering;cubierta conjunto;knowledge representation;ontology;multiplicity;verb	Making senses : bootstrapping sense-tagged lists of semantically-related words p. 13 Enriching wordnets with new relations and with event and argument structures p. 28 Experiments in cross-language morphological annotation transfer p. 41 Sentence segmentation model to improve tree annotation tool p. 51 Markov cluster shortest path founded upon the alibi-breaking algorithm p. 55 Unsupervised learning of verb argument structures p. 59 A methodology for extracting ontological knowledge from Spanish documents p. 71 Automatically determining allowable combinations of a class of flexible multiword expressions p. 81 Web-based measurements of intra-collocational cohesion in Oxford collocations dictionary p. 93 Probabilistic neural network based English-Arabic sentence alignment p. 97 Towards the automatic lemmatization of 16th century Mexican Spanish : a stemming scheme for the CHEM p. 101 Word frequency approximation for Chinese without using manually-annotated corpus p. 105 Abbreviation recognition with MaxEnt model p. 117 An efficient multi-agent system combining POS-taggers for Arabic texts p. 121 A comparative evaluation of a new unsupervised sentence boundary detection approach on documents in English and Portuguese p. 132 A general and multilingual phrase chunking model based on masking method p. 144 UCSG shallow parser p. 156 Evaluating the performance of the survey parser with the NIST scheme p. 168 Sequences of part of speech tags vs. sequences of phrase labels : how do they help in parsing? p. 180 Verb sense disambiguation using support vector machines : impact of WordNet-extracted features p. 192 Preposition senses : generalized disambiguation model p. 196 An unsupervised language independent method of name discrimination using second order co-occurrence features p. 208 Extracting key phrases to disambiguate personal names on the Web p. 223 Chinese noun phrase metaphor recognition with maximum entropy approach p. 235 Zero anaphora resolution in Chinese discourse p. 245 Random walks on text structures p. 249 Shallow case role annotation using two-stage feature-enhanced string matching p. 263 SPARTE, a test suite for recognising textual entailment in Spanish p. 275 Analysis of a textual entailer p. 287 Referring via document parts p. 299 Generation of natural language explanations of rules in an expert system p. 311 Generating a set of rules to determine honorific expression using decision tree learning p. 315 NLP (natural language processing) for NLP (natural language programming) p. 319 Balancing transactions in practical dialogues p. 331 Finite state grammar transduction from distributed collected knowledge p. 343	anaphora (linguistics);approximation;artificial neural network;boolean expression;bootstrapping (compilers);cohesion (computer science);collocation;decision tree learning;dictionary;earley parser;expert system;finite-state machine;frame language;lemmatisation;markov chain;multi-agent system;multinomial logistic regression;natural language processing;natural language programming;phrase chunking;point of sale;principle of maximum entropy;probabilistic neural network;shallow parsing;shortest path problem;stemming;string searching algorithm;support vector machine;test suite;textual entailment;transduction (machine learning);unsupervised learning;word lists by frequency;word-sense disambiguation;wordnet;world wide web	Namhee Kwon;Eduard H. Hovy	2006		10.1007/11671299_1	natural language processing;framenet;computer science;artificial intelligence;ontology;data mining;semantics;linguistics;multiplicity;algorithm;difference set	NLP	-26.898771076789153	-77.42200838552117	112046
4350f643102a06d093055b22b85eee353f41ef5c	a novel dependency-to-string model for statistical machine translation	statistical machine translation;translation quality;translation rule;state-of-the-art constituency-to-string model;source dependency structure;dependency structure;previous work;head-dependents rule;head-dependents relation;state-of-the-art translation model;hierarchical phrase-based model	Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.	bleu;dependency grammar;experiment;heuristic (computer science);java;parsing;statistical machine translation;string (computer science)	Jun Xie;Haitao Mi;Qun Liu	2011			natural language processing;speech recognition;computer science;artificial intelligence;machine learning;algorithm	NLP	-21.00366985994637	-76.45620498239884	112139
1e959a79d5620d0f4e2aacdb52208377ceaf0d15	joint chinese word segmentation and pos tagging on heterogeneous annotated corpora with multiple task learning		Chinese word segmentation and part-ofspeech tagging (S&T) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). Then we regard the Chinese S&T with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods.	chinese room;chinese wall;coding tree unit;experiment;heterogeneous system architecture;natural language processing;part-of-speech tagging;performance;text corpus;text segmentation;treebank;unified model	Xipeng Qiu;Jiayi Zhao;Xuanjing Huang	2013			natural language processing;speech recognition;computer science	NLP	-20.45893393233574	-72.70695229184676	112312
d397e412c056ddc13934bfdcb609f96ec7f3ebf2	fourth-order dependency parsing		We present and implement a fourth-order projective dependency parsing algorithm that effectively utilizes both “grand-sibling” style and “tri-sibling” style interactions of third-order and “grand-tri-sibling” style interactions of forth-order factored parts for performance enhancement. This algorithm requires O(n5) time and O(n4) space. We implement and evaluate the parser on two languages—English and Chinese, both achieving state-of-the-art accuracy. This results show that a higher-order (≥4) dependency parser gives performance improvement over all previous lower-order parsers.	algorithm;coding tree unit;interaction;natural language processing;parser combinator;parsing;triangular function	Xuezhe Ma;Hai Zhao	2012			parser combinator;bottom-up parsing;parsing;s-attributed grammar;top-down parsing	NLP	-21.87102864327069	-77.1465779711834	112380
2dbc4a746fe48dbdc1abe32dd02c78db82e33ac0	augmenting smt with semantically-generated virtual-parallel corpora from monolingual texts		Several natural languages have undergone a great deal of processing, but the problem of limited textual linguistic resources remains. The manual creation of parallel corpora by humans is rather expensive and time consuming, while the language data required for statistical machine translation (SMT) do not exist in adequate quantities for their statistical information to be used to initiate the research process. On the other hand, applying known approaches to build parallel resources from multiple sources, such as comparable or quasi-comparable corpora, is very complicated and provides rather noisy output, which later needs to be further processed and requires in-domain adaptation. To optimize the performance of comparable corpora mining algorithms, it is essential to use a quality parallel corpus for training of a good data classifier. In this research, we have developed a methodology for generating an accurate parallel corpus (Czech-English) from monolingual resources by calculating the compatibility between the results of three machine translation systems. We have created translations of large, single-language resources by applying multiple translation systems and strictly measuring translation compatibility using rules based on the Levenshtein distance. The results produced by this approach were very favorable. The generated corpora successfully improved the quality of SMT systems and seem to be useful for many other natural language processing tasks.	parallel text;text corpus	Krzysztof Wolk;Agnieszka Wolk	2018		10.1007/978-3-319-77703-0_37	machine translation;natural language;natural language processing;levenshtein distance;classifier (linguistics);artificial intelligence;computer science	NLP	-25.23311503369653	-75.34532689690677	112383
c4a22ddf6b14ecac0c29ee93993612f191e72a12	pos multi-tagging based on combined models		In the POS tagging task, there are two kinds of statistical models: one is generative model, such as the HMM, the others are discriminative models, such as the Maximum Entropy Model (MEM). POS multi-tagging decoding method includes the N-best paths method and forward-backward method. In this paper, we use the forward-backward decoding method based on a combined model of HMM and MEM. If P(t) is the forward-backward probability of each possible tag t, we first calculate P(t) according HMM and MEM separately. For all tags options in a certain position in a sentence, we normalize P(t) in HMM and MEM separately. Probability of the combined model is the sum of normalized forward-backward probabilities P norm(t) in HMM and MEM. For each word w, we select the best tag in which the probability of combined model is the highest. In the experiments, we use combined model and get higher accuracy than any single model on POS tagging tasks of three languages, which are Chinese, English and Dutch. The result indicates that our combined model is effective. 1. Motivation Being different from POS single-tagging, POS multitagging can assign more than one single best POS tag to a word in a sentence, according to the rank of probability of each tag calculated by a certain statistical model. A common usage of POS multi-tagging is a pre-processing part for a parser to increase the accuracy in comparison with single-tagging. Is single-tagging or multi-tagging suitable for parser? It depends on the kind of parser. In the experiments of PCFG parsing (Charniak and Carroll, 1996) and RASP parser(Watson, 2006), single-tagging is preferable to a multi-tagging, because multi-tagging provides only a minor improvement in accuracy, but with a significant loss in efficiency. On the contrary, for a parser based on highly lexicalized grammars, such as CCG parser and Alpino parser(Prins and van Noord, 2001), the accuracy of the single-tagging is only about 92% to 94% due to the large number of tags (hundreds of or thousands of tags), far below the current 97% accuracy in English POS tagging. Multi-tagger has been shown to be quite necessary in such two parsers. For other language, such as Chinese, the POS tagging is still not good enough due to the relatively small size training corpus and different annotation guidelines, so the multi-tagging is also promising for some further NLP applications. POS tagging is one of the best-studied applications in the statistical NLP domain. There are two kinds of statistical models: one is generative model, such as HMM(Brants, 2000), and the other is discriminative model, such as Maximum Entropy(ME) model(Ratnaparkhi, 1996). In multitagging task, (Prins and van Noord, 2001) used forwardbackward method based on HMM in Dutch corpus, and (Curran et al., 2006) used the same forward-backward method based on ME model. In this paper, for POS multitagging task, we test N-best paths and forward-backward method on three languages separately, and combined HMM and ME model based on forward-backward method. In methodology, we firstly introduce HMM and MEM briefly; Then, describe the two decoding methods: N-best paths and forward-backward method; lastly, we give the detail about how to combine HMM and ME model based on forward-backward frame. In the experiment section, I compare four kinds of multi-tagging methods based on HMM and MEM. The last section is conclusion. 2. Methodology 2.1. HMM and MEM POS tagging may be described as a decoding process of a noisy-channel. A sequence of POS tags , which is generated by a source with probability, is transmitted through a noisy channel. The output of the channel is a sequence of words with conditional probability . POS tagging need to covert output word sequence into the original input tag sequence . This task can be accomplished by finding that maximizes the probability . T̂ = argmax T P (T |W ) (1) Usually, There is not enough corpus in which we can estimate the probability directly, So Bayes theorem is applied to swap the order of dependence between the tag sequence T and the word sequence W . P (T |W ) = P (T,W ) P (W ) = P (W |T )P (T ) P (W ) (2) Eliminating the normalizing constantP (W ) , the decoding is equivalent to T̂ = argmax T P (W |T )P (T ) (3) P (W |T ) can be calculated by the state-specific observation probability. P (T ) can be estimated as the product of transition probability, as defined in formula (4): P (T ) = P (t1, · · · , ti−1) N ∏ i=n P (ti|ti−n+1, · · · , ti−1) (4)	brill tagger;brown corpus;carroll morgan (computer scientist);combinatory categorial grammar;database normalization;decoding methods;discriminative model;experiment;generative model;hidden markov model;markov chain;natural language processing;noisy-channel coding theorem;parsing;part-of-speech tagging;point of sale;preprocessor;principle of good enough;principle of maximum entropy;quantum gate;random-access stored-program machine;statistical model;stochastic context-free grammar;tag cloud	Yan Zhao;Gertjan van Noord	2010			principle of maximum entropy;discriminative model;speech recognition;generative model;bayes' theorem;parsing;conditional probability;artificial intelligence;statistical model;hidden markov model;pattern recognition;computer science	NLP	-24.234884478676065	-78.49727009600797	112387
5415aaf4edaca740d1e38f9f3f55193c3cee441c	stacking or supertagging for dependency parsing - what's the difference?		Supertagging was recently proposed to provide syntactic features for statistical dependency parsing, contrary to its traditional use as a disambiguation step. We conduct a broad range of controlled experiments to compare this specific application of supertagging with another method for providing syntactic features, namely stacking. We find that in this context supertagging is a form of stacking. We furthermore show that (i) a fast parser and a sequence labeler are equally beneficial in supertagging, (ii) supertagging/stacking improve parsing also in a cross-domain setting, and (iii) there are small gains when combining supertagging and stacking, but only if both methods use different tools. The important consideration is therefore not the method but rather the diversity of the tools involved.	experiment;label printer applicator;parsing;stacking;word-sense disambiguation	Agnieszka Falenska;Anders Björkelund;Özlem Çetinoglu;Wolfgang Seeker	2015		10.18653/v1/W15-2215	natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.084341114644683	-74.17128412578356	112523
2d5423bda3a690290c9cad0e97b6131f17393246	preference judgement in comprehending conversational sentences using mult-paradigm world knowledge			commonsense knowledge (artificial intelligence);programming paradigm	Teruhiko Ukita;Kazuo Sumita;Satoshi Kinoshita;Hiroshi Sano;Shin'ya Amano	1988			judgement;natural language processing;computer science;artificial intelligence	NLP	-32.58088513465726	-79.63323064894375	112541
9683f116d6098a04dc78b541539b6aa635cd286c	natural language generation for text-to-text applications using an information-slim representation	probability distribution;natural language generation;language model	I propose a representation formalism and algorithms to be used in a new language generation mechanism for text-to-text applications. The generation process is driven by both text-specific information encoded via probability distributions over words and phrases derived from the input text, and general language knowledge captured by n-gram and syntactic language models. A Text-to-Text Perspective on Natural	algorithm;language model;n-gram;natural language generation;semantics (computer science)	Radu Soricut	2005			n-gram;natural language processing;language identification;probability distribution;cache language model;natural language programming;speech recognition;universal networking language;language primitive;object language;specification language;computer science;low-level programming language;modeling language;lexical choice;context-sensitive language;language model	NLP	-29.18355315379757	-80.12781847638755	112550
1e938ad3043703e5a43d8996a79fc8d74d352d17	demographic-aware word associations		Variations of word associations across different groups of people can provide insights into people’s psychologies and their world views. To capture these variations, we introduce the task of demographicaware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male/female) and from different locations (India/United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neural net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outperform generic methods that are agnostic to such information.	artificial neural network;compiler;computation;data science;microsoft word for mac;n-gram;natural language processing;spamming	Aparna Garimella;Carmen Banea;Rada Mihalcea	2017		10.18653/v1/d17-1242	computer science;artificial intelligence;natural language processing	NLP	-20.129110804856165	-68.2043138681924	112603
23c5e2762d535bc4f5c53942089b4e3a6a2682e3	dialogue act classification, instance-based learning, and higher order dialogue structure		This paper briefly describes the Turkish Discourse Bank, the first publicly available annotated discourse resource for Turkish. It focuses on the challenges posed by annotating Turkish, a free word order language with rich inflectional and derivational morphology. It shows the usefulness of the PDTB style annotation but points out the need to expand this annotation style with the needs of the target language.		Barbara Di Eugenio;Zhuli Xie;Riccardo Serafin	2010	D&D	10.5087/d	natural language processing;computer science;communication;information retrieval	NLP	-29.82726778337006	-75.84552574316137	112640
f95b026cb5747a695e14c72d43a04d7904ea7be6	semi-automatic domain ontology construction from spoken corpus in tunisian dialect: railway request information		In this paper, we present a hybrid method for semi-automatic building of domain ontology from spoken dialogue corpus in Tunisian Dialect for the railway request information domain. The proposed method is based on a statistical method for term and concept extraction and a linguistic method for semantic relation extraction. This method consists of three fundamental phases, namely the corpus construction and treatment, the ontology construction and the ontology evaluation. The proposed method is implemented through the ABDO system to generate the RIO ontology that contains 14 concepts, 25 semantic relations and 387 concepts instances. The generated domain ontology is used to semantically label Tunisian dialect utterances in spoken dialogue. Keywords—concept, ontology, semantic relation, spoken dialogue, term, Tunisian dialect.	dialog system;ontology (information science);ontology components;relationship extraction;semantic interpretation;semiconductor industry	Jihen Karoui;Marwa Graja;Mohamed Mahdi Boudabous;Lamia Hadrich Belguith	2013	iJES		natural language processing;upper ontology;ontology alignment;bibliographic ontology;ontology inference layer;computer science;ontology;ontology-based data integration;process ontology;suggested upper merged ontology	NLP	-30.359588961896794	-70.70966786181769	112869
59a07b54fcb6ea868d6b718a6bc6c10895e6c876	large-scale taxonomy induction using entity and word embeddings		Taxonomies are an important ingredient of knowledge organization, and serve as a backbone for more sophisticated knowledge representations in intelligent systems, such as formal ontologies. However, building taxonomies manually is a costly endeavor, and hence, automatic methods for taxonomy induction are a good alternative to build large-scale taxonomies. In this paper, we propose TIEmb, an approach for automatic unsupervised class subsumption axiom extraction from knowledge bases using entity and text embeddings. We apply the approach on the WebIsA database, a database of subsumption relations extracted from the large portion of the World Wide Web, to extract class hierarchies in the Person and Place domain.	backward induction;class hierarchy;complementarity theory;coupled cluster;dbpedia;entity;extract class;internet backbone;knowledge graph;knowledge base;knowledge organization;mathematical induction;microsoft outlook for mac;mined;ontology (information science);subsumption architecture;taxonomy (general);word embedding;world wide web	Petar Ristoski;Stefano Faralli;Simone Paolo Ponzetto;Heiko Paulheim	2017		10.1145/3106426.3106465	extract class;hierarchy;natural language processing;axiom;intelligent decision support system;ontology (information science);knowledge organization;class subsumption;artificial intelligence;mathematics	AI	-31.884005320872017	-68.16247563065865	112983
ba654b0a7784b8acb0c79b9743505c288def2ff4	using a corpus for teaching turkish morphology	user interface;intelligent tutoring;ambiguity resolution;morphological analysis	This paper reports on the preliminary phase of our ongoing research towards developing an intelligent tutoring environment for Turkish grammar. One of the components of this environment is a corpus search tool which, among other aspects of the language, will be used to present the learner sample sentences along with their morphological analyses. Following a brief introduction to the Turkish language and its morphology, the paper describes the morphological analysis and ambiguity resolution used to construct the corpus used in the search tool. Finally, implementation issues and details involving the user interface of the tool are discussed.	computer-aided test tool;galaxy morphological classification;mathematical morphology;programming paradigm;text corpus;user interface	H. Altay Güvenir;Kemal Oflazer	1995	CoRR		natural language processing;morphological analysis;computer science;linguistics;user interface	NLP	-32.707311167173195	-78.03568962495507	113003
68e82881b47f99a8b501ba94bcb54676cba56295	improving statistical word alignment with a rule-based machine translation system	ibm statistical translation model;source word;statistical word alignment;improved alignment;statistical word alignment lie;target language;target word;improving statistical word alignment;rule-based machine translation system;inappropriate target word;word alignment	The main problems of statistical word alignment lie in the facts that source words can only be aligned to one target word, and that the inappropriate target word is selected because of data sparseness problem. This paper proposes an approach to improve statistical word alignment with a rule-based translation system. This approach first uses IBM statistical translation model to perform alignment in both directions (source to target and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment.	bilingual dictionary;bitext word alignment;compiler;data structure alignment;hyperlink;logic programming;named entity;named-entity recognition;natural language processing;neural coding;precision and recall;rule-based machine translation;statistical machine translation	Hua Wu;Haifeng Wang	2004			natural language processing;speech recognition;transfer-based machine translation;word error rate;computer science;pattern recognition;rule-based machine translation	NLP	-22.223213511499043	-77.05361631902329	113081
149d612877cb9cb7b4c9b8340086746ae6227c24	a machine learning approach to acronym generation	markov model;machine learning	This paper presents a machine learning approach to acronym generation. We formalize the generation process as a sequence labeling problem on the letters in the definition (expanded form) so that a variety of Markov modeling approaches can be applied to this task. To construct the data for training and testing, we extracted acronym-definition pairs from MEDLINE abstracts and manually annotated each pair with positional information about the letters in the acronym. We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation. Experimental results show that our machine learning method gives significantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values.	brill tagger;heuristic;medline;machine learning;markov chain;maximum-entropy markov model;sequence labeling;test set	Yoshimasa Tsuruoka;Sophia Ananiadou;Jun'ichi Tsujii	2005		10.3115/1641484.1641488	biology;computer science;bioinformatics;artificial intelligence;machine learning;data mining;markov model	NLP	-23.865768796552725	-78.11072516967049	113167
6a9397c013cba1d088b471e1a982a842c64e104f	advances in artificial intelligence: from theory to practice		This paper aims at the annotation of movement phrases in Vietnamese folk dance videos that were mainly gathered, stored and used in teaching at art schools and in preserving cultural intangible heritages (performed by different famous folk dance masters). We propose a framework of automatic movement phrase annotation, in which the motion vectors are used as movement phrase features. Movement phrase classification can be carried out, based on dancer’s trajectories. A deep investigation of Vietnamese folk dance gives an idea of using optical flow as movement phrase features in movement phrase detection and classification. For the richness and usefulness in annotation of Vietnamese folk dance, a lookup table of movement phrase descriptions is defined. In initial experiments, a sample movement phrase dataset is built up to train k-NN classification model. Experiments have shown the effectiveness of the proposed framework of automatic movement phrase annotation with classification accuracy at least 88%.	artificial intelligence;experiment;k-nearest neighbors algorithm;lookup table;optical flow;statistical classification	Víctor Codocedo;Céline Robardet	2017		10.1007/978-3-319-60045-1	artificial psychology	AI	-33.284846060389434	-77.91050034067405	113208
592752776f4ae8c80b69238b953e9dd026cfefe0	natural language generation for sponsored-search advertisements	model selection;sponsored search;pay per click;word order;web search engine;machine learning;natural language generation;web search;query logs;natural language processing;query log;automation	In sponsored search, advertisers bid on phrases representative of offered products or services. For large advertisers, these phrases often come from quasi-algorithmically generated lists of thousands of terms prone to poor linguistic construction. A bidded term by itself is usually unsuitable for direct insertion into an ad copy template; it must be rephrased and capitalized properly to fit the template, possibly with additional language to avoid semantic ambiguity. We develop a natural language generation system to automate these steps, preparing a list of terms for insertion into an ad template. For each input term, our system first finds a proper word ordering by mining a corpus of Web search query logs. Next it determines whether the term is ambiguous and--if semantics dictate--attaches a clarifying modifier culled from query logs. Finally, it applies proper capitalization by analyzing pages from Web search engine results. Each step yields a plausible set of displayable forms from which a machine-learned model selects the best. The models are trained and tested on a large set of human-labeled data. The overall system significantly outperforms baseline systems that use simple heuristics.	algorithm;baseline (configuration management);heuristic (computer science);human-readable medium;insertion sort;learning to rank;microsoft word for mac;modifier key;natural language generation;orthographic projection;search advertising;search engine marketing;spamming;text corpus;web search engine;web search query;world wide web	Kevin Bartz;Cory Barr;Adil Aijaz	2008		10.1145/1386790.1386792	word order;natural language processing;query expansion;web query classification;web search engine;computer science;artificial intelligence;automation;machine learning;data mining;web search query;world wide web;algorithm;search engine;model selection;statistics	NLP	-30.240027114294342	-73.162333474966	113264
b3339722992a24c22394245ece36f8564fdd59de	improving chunk-based semantic role labeling with lexical features		We present an approach for Semantic Role Labeling (SRL) using Conditional Random Fields in a joint identification/classification step. The approach is based on shallow syntactic information (chunks) and a number of lexicalized features such as selectional preferences and automatically inferred similar words, extracted using lexical databases and distributional similarity metrics. We use semantic annotations from the Proposition Bank for training and evaluate the system using CoNLL-2005 test sets. The additional lexical information led to improvements of 15% (in-domain evaluation) and 12% (out-of-domain evaluation) on overall semantic role classification in terms of F-measure. The gains come mostly from a better recall, which suggests that the addition of richer lexical information can improve the coverage of existing SRL models even when very little syntactic knowledge is available.	chunk (information);chunking (computing);conditional random field;database;f1 score;semantic role labeling	Wilker Aziz;Miguel Rios;Lucia Specia	2011			natural language processing;computer science;pattern recognition;information retrieval	NLP	-22.770331371545403	-72.29919593491839	113319
3891cf3eaeddc964673d48380732210d17cbfb2d	segmenting dna sequence into 'words' based on statistical language model		[Abstract] This paper presents a novel method to segment/decode DNA sequences based on n-gram statistical language model. Firstly, we find the length of most DNA “words” is 12 to 15 bps by analyzing the genomes of 12 model species. The bound of language entropy of DNA sequence is about 1.5674 bits. After building an n-gram biology languages model, we design an unsupervised ‘probability approach to word segmentation’ method to segment the DNA sequences. The benchmark of segmenting method is also proposed. In cross segmenting test, we find different genomes may use the similar language, but belong to different branches, just like the English and French/Latin. We present some possible applications of this method at last.	benchmark (computing);language model;n-gram;text segmentation	Wang Liang	2012	CoRR			Comp.	-24.30039667357362	-78.33239627049457	113326
aad32266549bf55ebccd9dde1a59bb386247ab63	predicting student learning from conversational cues		In the work here presented, we apply textual and sequential methods to assess the outcomes of an unconstrained multiparty dialogue. In the context of chat transcripts from a collaborative learning scenario, we demonstrate that while low-level textual features can indeed predict student success, models derived from sequential discourse act labels are also predictive, both on their own and as a supplement to textual feature sets. Further, we find that evidence from the initial stages of a collaborative activity is just as effective as using the whole.	agile software development;experiment;high- and low-level;ibm notes;real-time transcription	David Adamson;Akash Bharadwaj;Ashudeep Singh;Colin Ashe;David J. Yaron;Carolyn Penstein Rosé	2014		10.1007/978-3-319-07221-0_26	natural language processing;computer science;machine learning;multimedia	ML	-26.818863204983565	-70.02761998568134	113527
e746d8e6de20aca24236c5a6f3e182ee7ac693f3	annotating illocutionary force types and phonological features into a spontaneous dialogue corpus: an experimental study.				Kazuyo Tanaka;Kanae Kinebuchi;Naoko Houra;Kazuyuki Takagi;Shuichi Itahashi;Katsunobu Itou;Satoru Hayamizu	1994			natural language processing;computer science;linguistics	NLP	-29.942547359002965	-79.61846613839907	113554
16397a48bf23edd852158e91a8aa87105ec219fb	reinforcement learning based argument component detection		Argument component detection (ACD) is an important sub-task in argumentation mining. ACD aims at detecting and classifying different argument components in natural language texts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAsaugmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%.	algorithm;effective method;natural language processing;reinforcement learning;sl (complexity);sensor;supervised learning;text corpus	Yang Gao;Hao Wang;Chen Zhang;Wei Wang	2017	CoRR		error-driven learning;artificial intelligence;machine learning;pattern recognition	NLP	-21.697255827088885	-70.25705576904275	113558
eeebddac33fafa0fe5aec2cd7b133ff71e24bbf9	predicting the position of attributive adjectives in the french np	french;logistic regression;probabilistic syntax;attributive adjective alternation;corpus linguistics	French displays the possibility of both pre-nominal and post-nominal ordering of adjectives within the noun phrase (NP).	experiment;logistic regression;performance;relevance;text corpus	Gwendoline Fox;Juliette Thuilier	2011		10.1007/978-3-642-31467-4_1	noun	NLP	-27.4470701229314	-76.79991171265861	113681
a3b36baea047449e47e79e00b1af5c37b8fa4a0a	matching handwritten document images		Given a pair of handwritten documents written by different individuals, compute a document similarity score irrespective of (i) handwritten styles, (ii) word forms, word ordering and word overflow. • IIIT-HWS: Introducing a large scale synthetic corpus of handwritten word images for enabling deep architectures. • HWNet: A deep CNN architecture for state of the art handwritten word spotting in multi-writer scenarios. • MODS: Measure of document similarity score irrespective of word forms, ordering and paraphrasing of the content. • Applications in Educational Scenario: Comparing handwritten assignments, searching through instructional videos. 2. Contributions 3. Challenges	microsoft word for mac;synthetic intelligence	Praveen Krishnan;C. V. Jawahar	2016		10.1007/978-3-319-46448-0_46	natural language processing;speech recognition;computer science;pattern recognition	Vision	-22.77213388883163	-73.67146453556875	113758
77602ca886d0d33a4da65901085166bf371ef31c	system description of citlab's recognition & retrieval engine for icdar2017 competition on information extraction in historical handwritten records		We present a recognition and retrieval system for the ICDAR2017 Competition on Information Extraction in Historical Handwritten Records which successfully infers person names and other data from marriage records. The system extracts information from the line images with a high accuracy and outperforms the baseline. The optical model is based on Neural Networks. To infer the desired information, regular expressions are used to describe the set of feasible words sequences.	algorithm;baseline (configuration management);information extraction;microsoft outlook for mac;neural networks;recurrent neural network;regular expression	Tobias Strauß;Max Weidemann;Johannes Michael;Gundram Leifert;Tobias Grüning;Roger Labahn	2018	CoRR		information extraction;artificial neural network;information retrieval;computer science;regular expression	ML	-23.467827807292608	-69.79107943539555	113890
d731ad4cdc6ec1047725686231d1a420b5f303fc	a framework for figurative language detection based on sense differentiation	feature selection;sense differentiation;figurative language use detection;figurative language detection;automatic extraction;algorithms work;high-level semantically rich feature;big challenge;style detection;russian language data;figurative language	Various text mining algorithms require the process of feature selection. High-level semantically rich features, such as figurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for figurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data.	algorithm;cluster analysis;feature selection;language identification;literal (mathematical logic);text mining	Daria Bogdanova	2010			natural language processing;computer science;linguistics	NLP	-25.466676790290716	-70.98690447604335	113918
8d7c4e495992cf46427a2b3ca76b8bea4530edf4	caters: causal and temporal relation scheme for semantic annotation of event structures		Learning commonsense casual and temporal relation between events is one of the major steps towards deeper language understanding. This is crucial for understanding stories and narrative structure learning. In this paper we introduce a novel semantic annotation framework, called Causal and Temporal Relation Scheme (CaTeRS), which is a prerequisite for learning narrative structures of stories. This scheme is unique in capturing both temporal and causal aspects of inter-event relations. By annotating a total of 1,600 sentences in 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation.	causal filter;causality;entity;inter-rater reliability;link relation;natural language understanding;relation (database);temporal annotation	Nasrin Mostafazadeh;Alyson Grealish;Nathanael Chambers;James F. Allen;Lucy Vanderwende	2016		10.18653/v1/W16-1007	pattern recognition;data mining;information retrieval	NLP	-26.4618721685585	-71.21915620228364	113922
7099fd783370d14cdd6b5bc399feca970b658563	not an interlingua, but close: comparison of english amrs to chinese and czech		Abstract Meaning Representations (AMRs) are rooted, directional and labeled graphs that abstract away from morpho-syntactic idiosyncrasies such as word category (verbs and nouns), word order, and function words (determiners, some prepositions). Because these syntactic idiosyncrasies account for many of the cross-lingual differences, it would be interesting to see if this representation can serve, e.g., as a useful, minimally divergent transfer layer in machine translation. To answer this question, we have translated 100 English sentences that have existing AMRs into Chinese and Czech to create AMRs for them. A cross-linguistic comparison of English to Chinese and Czech AMRs reveals both cases where the AMRs for the language pairs align well structurally and cases of linguistic divergence. We found that the level of compatibility of AMR between English and Chinese is higher than between English and Czech. We believe this kind of comparison is beneficial to further refining the annotation standards for each of the three languages and will lead to more compatible annotation guidelines between the languages.	adaptive multi-rate audio codec;align (company);machine translation	Nianwen Xue;Ondrej Bojar;Jan Hajic;Martha Palmer;Zdenka Uresová;Xiuhong Zhang	2014			noun;word order;machine translation;syntax;artificial intelligence;natural language processing;czech;interlingua;computer science;divergence (linguistics);annotation	NLP	-27.11278652126769	-75.44415057910035	114067
85779f8dc12816787f6a468eeb3340496e2b8cb8	a rule-based approach to the identification of spanish zero pronouns		This paper presents a new rule-based method to identify Spanish zero pronouns. The paper describes the comparative evaluation of a baseline method for the identification of zero pronouns with an approach that supplements the baseline by adding a set of restrictions treating impersonal sentences and other zero subject expressions. The identification rules have been tested on a new corpus in which zero pronouns have been manually annotated (the Z-Corpus). The comparative evaluation shows that this rulebased method outperforms the baseline.	algorithm;anaphora (linguistics);baseline (configuration management);inter-rater reliability;logic programming;machine learning;speech corpus;text corpus	Luz Rello;Iustina Ilisei	2009			natural language processing;linguistics	NLP	-26.054729353800578	-75.5193328086686	114171
404df4d253835aa86423f8197fe9843877569da5	a semantic grammar for beginning communicators	grammatical framework;simple upper ontology;semantic grammar;controlled language;ontology;framenet;beginning communicators	http://dx.doi.org/10.1016/j.knosys.2015.06.002 0950-7051/ 2015 Elsevier B.V. All rights reserved. ⇑ Corresponding author. E-mail addresses: dofer@ujaen.es (F. Martínez-Santiago), mcdiaz@ujaen.es (M.C. Díaz-Galiano), laurena@ujaen.es (L.A. Ureña-López), R.Mitkov@wlv.ac.uk (R. Mitkov). 1 Non-symbolic communication, or pre-linguistic communication, refers to both intentional and unintentional behaviours that may be either conventional or unconventional in form and that do not involve the use of symbolic modes such as pictures, manual signs, or printed words [4, p. 226]. F. Martínez-Santiago a,⇑, M.C. Díaz-Galiano , L.A. Ureña-López , R. Mitkov b	advanced audio coding;controlled natural language;framenet;grammatical framework;knowledge acquisition;language-independent specification;partial template specialization;pictogram;printing;protologism;semantic web;synonym ring;text corpus;upper ontology;verbnet;vocabulary;wordnet	Fernando Martínez Santiago;Manuel Carlos Díaz-Galiano;Luis Alfonso Ureña López;Ruslan Mitkov	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.06.002	natural language processing;upper ontology;semantic similarity;framenet;computer science;ontology	AI	-31.473612516670993	-79.93313701783009	114173
9342b54f38f85574a112d447a9cc5d8409b6fece	bag of experts architectures for model reuse in conversational language understanding		Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants. Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow. To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models. Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.	baseline (configuration management);conditional random field;entity;expert system;long short-term memory;natural language understanding;personal digital assistant;sensor	Rahul Jha;Alex Marin;Suvamsh Shivaprasad;Imed Zitouni	2018			natural language processing;reuse;artificial intelligence;computer science	NLP	-22.939239101835017	-72.04187216412988	114178
634a1db729f005052dff40016fabcc60b7c2fc84	tac2011 multiling pilot overview		The Text Analysis Conference MultiLing Pilot of 2011 posed a multi-lingual summarization task to the summarization community, aiming to quantify and measure the performance of multi-lingual, multi-document summarization systems. The task was to create a 240–250 word summary from 10 news texts, describing a given topic. The texts of each topic were provided in seven languages (Arabic, Czech, English, French, Greek, Hebrew, Hindi) and each participant generated summaries for at least 2 languages. The evaluation of the summaries was performed using automatic (AutoSummENG, Rouge) and manual processes (Overall Responsiveness score). The participating systems were 8, some of which providing summaries across all languages. This paper provides a brief description for the collection of the data, the evaluation methodology, the problems and challenges faced, and an overview of participation and corresponding results.	automatic summarization;multi-document summarization;rouge (metric);responsiveness	George Giannakopoulos;Mahmoud El-Haj;Benoît Favre;Marina Litvak;Josef Steinberger;Vasudeva Varma	2011				NLP	-30.87916473902802	-72.54708935304583	114210
6836894d835e8595d441703fd9418f0d53466cad	syntactic category disambiguation through relaxation processes			linear programming relaxation;word-sense disambiguation	Marcello Pelillo;Mario Refice	1991			artificial intelligence;syntactic category;pattern recognition;computer science	NLP	-29.9334831633074	-77.30763437948256	114288
23345c4ccf71131bbd35b80d3056f07d6472c955	discovering continuous multi-word expressions in czech		Multi-word expressions frequently cause incorrect annotations in corpora, since they often contain foreign words or syntactic anomalies. In case of foreign material, the annotation quality depends on whether the correct language of the sequence is detected. In case of inter-lingual homographs, this problem becomes difficult. In the previous work, we created a dataset of Czech continuous multi-word expressions (MWEs). The candidates were discovered automatically from Czech web corpus considering their orthographic variability. The candidates were classified and annotated manually. Afterwards, the dataset was extended automatically by generating all word forms of those MWEs that were annotated as nouns. In this work, we used the dataset as positive examples, we filtered out negative examples from the MWE candidates. We trained a classifier with mean accuracy 92.7%. We have shown that the combined approach slightly outperforms approaches concerning only association measures mainly on MWEs containing inter-lingual homographs and out-of-vocabulary words. The discovery methods can be applied to other languages which encounter orthographic variability in web corpora.		Zuzana Neverilová	2018	Computación y Sistemas		artificial intelligence;natural language processing;noun;czech;multiword expression;expression (mathematics);classifier (linguistics);syntax;annotation;computer science;orthographic projection	DB	-25.94883527166577	-73.78103129728106	114320
070bd2284a7a54daf5d51a6fa2bfd621f3d042ef	bilingual lexicon extraction from comparable corpora enhanced with parallel corpora	comparable corpora;bilingual lexicon;parallel corpora	In this article, we present a simple and effective approach for extracting bilingual lexicon from comparable corpora enhanced with parallel corpora. We make use of structural characteristics of the documents comprising the comparable corpus to extract parallel sentences with a high degree of quality. We then use state-of-the-art techniques to build a specialized bilingual lexicon from these sentences and evaluate the contribution of this lexicon when added to the comparable corpus-based alignment technique. Finally, the value of this approach is demonstrated by the improvement of translation accuracy for medical words.	lexicon;parallel text;text corpus	Emmanuel Morin;Emmanuel Prochasson	2011			natural language processing;speech recognition;computer science;linguistics	NLP	-25.837143493135542	-74.49654660984413	114353
f5039907a0c2efa8a9472ce6faf8ee1d1799e153	a case-based reasoning approach to convert natural language into first order logic		Text is the backbone of the web and most of the information and human knowledge is represented in natural language. Every day, a vast amount of textual information is posted in web portals, wikis and news sites and necessitates automated approaches to analyze and understand their content. In this paper, we present a case-based reasoning approach to transform natural language sentences into first order logic formulas. The formalization approach relies on the principle that natural language sentences with similar grammatical structures and dependency trees would have similar representation in first order logic. The approach consists of two main stages. First, a deep analysis of the natural language sentence is performed, where proper characteristics are extracted and dependencies are specified. After that, in the second stage a cased based reasoning approach is used to utilize existing knowledge (formalized sentences) and drive the formalization of a new sentence. The similarity between natural language sentences is conducted on their dependency trees and is calculated based on the tree edit distance. Then, if needed, the adaptation of a solution is made based on rules. Example studies have shown the applicability of the method and the results on a small number of sentences are very promising.	case-based reasoning;first-order logic;graph edit distance;internet backbone;natural language;portals;wiki	Isidoros Perikos;Ioannis Hatzilygeroudis	2016	2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)	10.1109/CSE-EUC-DCABES.2016.228	knowledge representation and reasoning;natural language programming;question answering;knowledge-based systems;natural language;language identification;natural language processing;machine learning;case-based reasoning;computer science;sentence;artificial intelligence	AI	-31.587202842885542	-69.04529691186136	114410
61921ac777b647484bcc6606db997c36d8652975	apply word vectors for sentiment analysis of app reviews	polarity of sentiment words detection;sentiment words extraction;vector representations;sentiment analysis	Vector representations for language have been shown to be useful in a number of Natural Language Processing tasks. In this paper, we aim to investigate the effectiveness of word vector representations for the problem of Sentiment Analysis. In particular, we target three sub-tasks namely sentiment words extraction, polarity of sentiment words detection, and text sentiment prediction. We investigate the effectiveness of vector representations over different text data and evaluate the quality of domain-dependent vectors. Vector representations has been used to compute various vector-based features and conduct systematically experiments to demonstrate their effectiveness. Using simple vector based features, we achieve F1 85.77%, the recall 85.20%, and the accuracy 86.35% for text sentiment analysis of APP reviews.	bootstrapping (compilers);experiment;lexicon;linear discriminant analysis;naive bayes classifier;natural language processing;precision and recall;sensitivity and specificity;sentiment analysis;text corpus;usability;user interface;word embedding;word2vec	Xian Fan;Xiaoge Li;Kornelija Maciuliene;Xin Li;Mian Wei	2016	2016 3rd International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2016.7811108	natural language processing;speech recognition;computer science;pattern recognition;sentiment analysis	NLP	-21.4381564621806	-68.04982047515945	114426
a725a884666904cf09031ddfe6d1a7f7b75832e0	a semantic exploration method based on an ontology of 17 ^th century texts on theatre: la haine du théâtre		This paper proposes a method to explore a collection of texts with an ontology depending on a particular point of view. In the first part, the paper points out the characteristics of the corpus, composed of 17th century French texts. In the second part, it explains the methodology to isolate the discriminant terms for the ontology creation. Furthermore, not only the projection of the ontology on the texts is pointed out, but also how to explore the corpus thanks to the defined perspective based on semantic fields.	linear algebra	Chiara Mainardi;Zied Sellami;Vincent Jolivet	2015		10.1007/978-3-319-23201-0_47	natural language processing;upper ontology;computer science;ontology;artificial intelligence;database	ML	-32.66460077852812	-70.67018790476442	114586
aea4c98d09e98f60f493fe5e8d9fbe69defb7a58	improving unsupervised dependency parsing with knowledge from query logs	additional knowledge;natural annotations;dependency parsing;query logs	Unsupervised dependency parsing becomes more and more popular in recent years because it does not need expensive annotations, such as treebanks, which are required for supervised and semi-supervised dependency parsing. However, its accuracy is still far below that of supervised dependency parsers, partly due to the fact that their parsing model is insufficient to capture linguistic phenomena underlying texts. The performance for unsupervised dependency parsing can be improved by mining knowledge from the texts and by incorporating it into the model. In this article, syntactic knowledge is acquired from query logs to help estimate better probabilities in dependency models with valence. The proposed method is language independent and obtains an improvement of 4.1% unlabeled accuracy on the Penn Chinese Treebank by utilizing additional dependency relations from the Sogou query logs and Baidu query logs. Morever, experiments show that the proposed model achieves improvements of 8.07% on CoNLL 2007 English using the AOL query logs. We believe query logs are useful sources of syntactic knowledge for many natural language processing (NLP) tasks.	experiment;information retrieval;natural language processing;parsing;parsing expression grammar;semi-supervised learning;semiconductor industry;treebank;unsupervised learning	Xiuming Qiao;Hailong Cao;Tiejun Zhao	2016	ACM Trans. Asian & Low-Resource Lang. Inf. Process.	10.1145/2903720	natural language processing;computer science;database;information retrieval	NLP	-22.075735175615954	-72.99508892492418	114650
220a0efb8aa7dab1cfd31b4ff1c5277043b283a1	automatic induction of language model data for a spoken dialogue system	word error rate;information extraction;automatic generation;spoken dialogue system;modele de langage;generation automatique;evaluation;computational linguistics;synthetic data;sampling methods;user interaction;language model spoken dialogue systems user simulation;linguistique informatique;example based generation;assessment;domain specificity;language model;user model	When building a new spoken dialogue application, large amounts of domain specific data are required. This paper addresses the issue of generating in-domain training data when little or no real user data are available. The twostage approach taken begins with a data induction phase whereby linguistic constructs from out-of-domain sentences are harvested and integrated with artificially constructed in-domain phrases. After some syntactic and semantic filtering, a large corpus of synthetically assembled user utterances is induced. The second stage involves sampling the synthetic corpus towards the goal of obtaining data that would be representative of the statistics of applicationspecific real user interactions. The sampling methods proposed employ an example-based generation framework, a simulated user model and information extracted from development data. Evaluation is conducted on recognition performance in a restaurant information domain. We show that word error rate can be reduced when limited amounts of real user training data are augmented with synthetic data derived by our methods.	corporation for national research initiatives;dialog system;experiment;formal methods;generative model;interaction;language model;mit computer science and artificial intelligence laboratory;np (complexity);pp (complexity);parsing;sampling (signal processing);simulation;spoken dialog systems;synthetic data;text corpus;word error rate	Chao Wang;Grace Chung;Stephanie Seneff	2006	Language Resources and Evaluation	10.1007/s10579-006-9007-3	natural language processing;sampling;user modeling;speech recognition;word error rate;computer science;evaluation;computational linguistics;linguistics;information extraction;educational assessment;language model;synthetic data	NLP	-24.217115927907734	-79.05596228479865	114704
4e9aaafc373e0e3b324355abf2425f4813803472	a state-transition grammar for data-oriented parsing	data-oriented parsing;data-oriented approach;state-transition grammar;fresh text;probabilistic language model;language processing;grammar formalism	"""There is not space here to present full justification 1. for adopting such an approach or to detail the advantages that it offers. The main claim it makes is that effective language processing requires a consideration of both the structural and statistical as2. pects of language, whereas traditional competence grammars rely only on the former, and standard statistical techniques such as n-gram models only on the latter. DOP at tempts to combine these two traditions and produce """"performance grammars"""", which:"""	data-oriented parsing;formal grammar;n-gram	David Tugwell	1995			natural language processing;parser combinator;parsing expression grammar;top-down parsing language;computer science;bottom-up parsing;parsing;s-attributed grammar;stochastic grammar;extended affix grammar;emergent grammar;linguistics;programming language;top-down parsing;mildly context-sensitive grammar formalism;head-driven phrase structure grammar	AI	-22.21928707318125	-78.4890267971159	114718
7f9641b62d9ea62014e0c070cefa7c941bdb1379	stance detection on tweets: an svm-based approach		Stance detection is a subproblem of sentiment analysis where the stance of the author of a piece of natural language text for a particular target (either explicitly stated in the text or not) is explored. The stance output is usually given as Favor, Against, or Neither. In this paper, we target at stance detection on sports-related tweets and present the performance results of our SVM-based stance classifiers on such tweets. First, we describe three versions of our proprietary tweet data set annotated with stance information, all of which are made publicly available for research purposes. Next, we evaluate SVM classifiers using different feature sets for stance detection on this data set. The employed features are based on unigrams, bigrams, hashtags, external links, emoticons, and lastly, named entities. The results indicate that joint use of the features based on unigrams, hashtags, and named entities by SVM classifiers is a plausible approach for stance detection problem on sports-related tweets.	bigram;emoticon;experiment;hashtag;mediawiki;named entity;named-entity recognition;natural language;natural language processing;performance evaluation;sentiment analysis;support vector machine;whole earth 'lectronic link	Dilek Küçük;Fazli Can	2018	CoRR		artificial intelligence;machine learning;support vector machine;sentiment analysis;natural language;bigram;computer science	NLP	-22.38283444888919	-70.75859082316227	114732
3a8109872359383895fcd9b13af74408443af451	classification analysis of authorship fiction texts in the space of semantic fields		The use of naive Bayesian classifier (NB) and the classifier by the k nearest neighbors (kNN) in classification semantic analysis of authors’ texts of English fiction has been analysed. The authors’ works are considered in the vector space the basis of which is formed by the frequency characteristics of semantic fields of nouns and verbs. Highly precise classification of authors’ texts in the vector space of semantic fields indicates about the presence of particular spheres of author’s idiolect in this space which characterizes the individual author’s style.	bayesian network;k-nearest neighbors algorithm;naive bayes classifier	Bohdan Pavlyshenko	2013	Journal of Quantitative Linguistics	10.1080/09296174.2013.799914	natural language processing;speech recognition;computer science;pattern recognition;linguistics	Web+IR	-27.505567317044857	-75.8007650767139	114785
1736c46b96d1fb5ee502b80c9e4c659023bc1f63	initial explorations on using crfs for turkish named entity recognition	conditional random fields	This paper reports the highest results (95% in MUC and 92% in CoNLL metric) in the literature for Turkish named entity recognition; more specifically for the task of detecting person, location and organization entities in general news texts. We give an in depth analysis of the previous reported results and make comparisons with them whenever possible. We use conditional random fields (CRFs) as our statistical model. The paper presents initial explorations on the usage of rich morphological structure of the Turkish language as features to CRFs together with the use of some basic and generative gazetteers.	baseline (configuration management);compiler;conditional random field;entity;evaluation function;message understanding conference;ner model;named entity;named-entity recognition;sensor;statistical model;timex sinclair	Gökhan Akin Seker;Gülsen Eryigit	2012			natural language processing	NLP	-22.575665153513693	-73.82817940425471	114823
692bab005b2e11bc70764da388d68d445284b3d6	semantic chunk annotation for questions using maximum entropy	feature selection maximum entropy model semantic chunk annotation chinese question and answer system internet mutual information;maximum entropy methods;mutual information semantic chunk annotation q a maximum entropy;query processing;search engines;probability density function;semantic chunk annotation;training;data mining;chinese question and answer system;q a;training data;internet;semantic web;mutual information;maximum entropy model;feature selection;entropy;data consistency;semantic web maximum entropy methods query processing search engines;maximum entropy;entropy testing information retrieval search engines databases mutual information natural languages computer architecture computer science internet;conferences	We present a ME (Maximum Entropy) model for Semantic Chunk Annotation in a Chinese Question and Answer (Q&A) system. The model was derived from a corpus of real world questions, which are collected from some discussion groups on the Internet. The questions are supposed to be answered by other people, so the questions are very complex. The semantic chunks were introduced. Feature for the model was described and MI (mutual information) was adopted for feature selection. The training data consists of 14000 sentences and the test data consists of 4000 sentences. The result: F-score is 90.68%.	algorithm;conditional random field;experiment;f1 score;feature selection;focus group;internet;mutual information;principle of maximum entropy;test data;text corpus;weatherstar	Shixi Fan;Yaoyun Zhang;Daniel S. Yeung;Xuan Wang;Xiaolong Wang	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811317	natural language processing;computer science;principle of maximum entropy;pattern recognition;feature selection;information retrieval;statistics	Robotics	-24.149463073416413	-66.80429277525595	114867
e2f250992f091f1d2cef2d64cecad352142e7a0b	a language-independent approach to extracting derivational relations from an inflectional lexicon	formal analogy;morphological analysis;derivational relation	In this paper, we describe and evaluate an unsupervised method for acquiring pairs of lexical entries belonging to the same morphological family, i.e., derivationally related words, starting from a purely inflectional lexicon. Our approach relies on transformation rules that relate lexical entries with the one another, and which are automatically extracted from the inflected lexicon based on surface form analogies and on part-of-speech information. It is generic enough to be applied to any language with a mainly concatenative derivational morphology. Results were obtained and evaluated on English, French, German and Spanish. Precision results are satisfying, and our French results favorably compare with another resource, although its construction relied on manually developed lexicographic information whereas our approach only requires an inflectional lexicon.	language-independent specification;lexicographical order;lexicon;mathematical morphology;unsupervised learning	Marion Baranes;Benoît Sagot	2014			natural language processing;morphological analysis;computer science;linguistics	NLP	-26.754963100001092	-76.10039609577612	115118
e1e8118eff4aa07d4564d277590a8c0f4109db51	bayesian induction of bracketing inversion transduction grammars		We present a novel approach to learning phrasal inversion transduction grammars via Bayesian MAP (maximum a posteriori) or information-theoretic MDL (minimum description length) model optimization so as to incorporate simultaneously the choices of model structure as well as parameters. In comparison to most current SMT approaches, the model learns phrase translation lexicons that (a) do not require enormous amounts of run-timememory, (b) contain significantly less redundancy, and (c) provide an obvious basis for generalization to abstract translation schemas. Model structure choice is biased by a description length prior, while parameter choice is driven by data likelihood biased by a parameter prior. The search over possible model structures is made feasible by a novel top-down rule segmenting heuristic which efficiently incorporates estimates of the posterior probabilities. Since the priors reward model parsimony, the learned grammar is very concise and still performs significantly better than the maximum likelihood driven bottom-up rule chunking baseline.	attribute grammar;baseline (configuration management);bayesian network;emoticon;heuristic;information theory;lexicon;map;mathematical optimization;maximum parsimony (phylogenetics);minimum description length;nos;occam's razor;phrasal template;shallow parsing;top-down and bottom-up design;transduction (machine learning)	Markus Saers;Dekai Wu	2013			speech recognition;computer science;machine learning;pattern recognition	NLP	-20.565074306583146	-76.9242841509076	115121
9dbee7b0211e2e1023c01f9e1dd159fd7e1cd30c	a large margin approach to anaphora resolution for neuroscience knowledge discovery	anaphora resolution;knowledge discovery	A discriminative large margin classifier based approach to anaphora resolution for neuroscience abstracts is presented. The system employs both syntactic and semantic features. A support vector machine based word sense disambiguation method combining evidence from three methods, that use WordNet and Wikipedia, is also introduced and used for semantic features. The support vector machine anaphora resolution classifier with probabilistic outputs achieved almost four-fold improvement in accuracy over the baseline method.	anaphora (linguistics);baseline (configuration management);dictionary;margin classifier;ontology (information science);support vector machine;syntax (logic);thesaurus;web services for devices;wikipedia;word sense;word-sense disambiguation;wordnet	Ibrahim Burak Özyurt	2009			natural language processing;computer science;artificial intelligence;pattern recognition;information retrieval	ML	-25.385602678276918	-69.86548282920145	115133
79976be76417bbdc967bc5c68d62476008672729	a diverse dirichlet process ensemble for unsupervised induction of syntactic categories		We address the problem of unsupervised tagging of phrase structure trees with phrase categories (parse tree nonterminals). Motivated by the inability of a range of direct clustering approaches to improve over the current leading algorithm, we propose a mixture of experts approach. In particular, we tackle the difficult challenge of producing a diverse collection of useful tagging experts, which can then be aggregated into a final high-quality tagging. To do so, we use the particular properties of the Dirichlet Process mixture model. We evaluate on English, German and Chinese corpora and demonstrate both a substantial and consistent improvement in overall performance over previous work, as well as empirical justification of our algorithmic choices.	algorithm;apache axis;chinese wall;cluster analysis;ensemble learning;mixture model;parse tree;parsing;phrase structure rules;supervised learning;tag (metadata);text corpus;unsupervised learning	Roi Reichart;Gal Elidan;Ari Rappoport	2012			natural language processing;computer science;machine learning;pattern recognition	NLP	-21.71688556164502	-74.33945358982278	115204
352b161ef8a605705259f615fcaea1322768dcf8	discrimination between similar languages, varieties and dialects using cnn- and lstm-based deep neural networks		In this paper, we describe a system (CGLI) for discriminating similar languages, varieties and dialects using convolutional neural networks (CNNs) and long short-term memory (LSTM) neural networks. We have participated in the Arabic dialect identification sub-task of DSL 2016 shared task for distinguishing different Arabic language texts under closed submission track. Our proposed approach is language independent and works for discriminating any given set of languages, varieties and dialects. We have obtained 43.29% weighted-F1 accuracy in this subtask using CNN approach using default network parameters.	artificial neural network;convolutional neural network;deep learning;digital subscriber line;long short-term memory;neural networks	Chinnappa Guggilla	2016			natural language processing;artificial intelligence;artificial neural network;computer science	NLP	-20.054958160645537	-70.99667955393902	115243
4d2417602861aedcaba96564f2d4cda5a91a7d66	regularized training of compositional distributional semantic models	standards;compounds;training;semantics;gold;mathematical model;numerical models	The compositional distributional semantic models (cDSMs) aim to use numerical vectors to represent the meaning of complex language expressions. cDSMs are usually trained using single training target, either from the basic DSM or a pseudo gold standard. In this paper, a new regularized training approach that integrates multiple training targets is proposed to improve semantic composition models. The experiment results show that the proposed training algorithm can effectively enhance compositional distributional semantic models.	algorithm;numerical analysis;word embedding	Xuefeng Yang;Kezhi Mao;Rui Zhao	2015	2015 10th International Conference on Information, Communications and Signal Processing (ICICS)	10.1109/ICICS.2015.7459847	gold;natural language processing;computer science;machine learning;mathematical model;data mining;semantics	NLP	-23.59329790592577	-73.14543813866354	115302
63208d334161ad897c3dccee5c61aba0d1464e28	romanization of thai proper names based on popularity of usages	statistical model;language processing;statistical language processing;thai romanization;proper names;machine translation	The lack of standards for Romanization of Thai proper names makes searching activity a challenging task. This is particularly important when searching for people-related documents based on orthographic representation of their names using either solely Thai or English alphabets. Romanization based directly on the names' pronunciations often fails to deliver exact English spellings due to the non-1-to-1 mapping from Thai to English spelling and personal preferences. This paper proposes a Romanization approach where popularity of usages is taken into consideration. Thai names are parsed into sequences of grams, units of syllable-sized or larger governed by pronunciation and spelling constraints in both Thai and English writing systems. A Gram lexicon is constructed from a corpus of more than 130,000 names. Statistical models are trained accordingly based on the Gram lexicon. The proposed method significantly outperformed the current Romanization approach. Approximately 46% to 75% of the correct English spellings are covered when the number of proposed hypotheses increases from 1 to 15.	name	Akegapon Tangverapong;Atiwong Suchato;Proadpran Punyabukkana	2009		10.1007/978-3-642-01307-2_56	natural language processing;statistical model;romanization;speech recognition;computer science;proper noun;machine translation;statistics	AI	-22.602408499669206	-79.56120698910424	115306
3bb0a4630a7055858d669b9be194c643a95af3e2	relations such as hypernymy: identifying and exploiting hearst patterns in distributional vectors for lexical entailment		We consider the task of predicting lexical entailment using distributional vectors. We focus experiments on one previous classifier which was shown to only learn to detect prototypicality of a word pair. Analysis shows that the model single-mindedly learns to detect Hearst Patterns, which are well known to be predictive of lexical relations. We present a new model which exploits this Hearst Detector functionality, matching or outperforming prior work on multiple data sets.	concatenation;distributional semantics;experiment;kadir–brady saliency detector;lexicon;naive bayes classifier;sensor	Stephen Roller;Katrin Erk	2016			natural language processing;machine learning;pattern recognition;data mining	NLP	-19.950349151900898	-69.53186059365623	115350
8ced67ccb6ba4c77aa83cecd9d2dc3e0cd661499	a survey of sequential combination of word recognizers in handwritten phrase recognition at cedar	spacing;espacement;classifier combination;combinatorics;hand writing;espaciamiento;caracter manuscrito;combinatoria;lexicon;manuscript character;metodo combinatorio;combinatoire;phase;clasificador;methode combinatoire;metodo secuencial;sequential method;logistic regression;fase;regresion logistica;classifier;reconnaissance caractere;escritura manual;regression logistique;palabra;pattern recognition;classificateur;methode sequentielle;combinatorial method;word;reconnaissance forme;lexico;reconocimiento patron;caractere manuscrit;character recognition;reconocimiento caracter;mot;ecriture;lexique	Several methods for classifier combination have been explored at CEDAR. A sequential method for combining word recognizers in handwritten phrase recognition is revisited. The approach is to take phrase images as concatenations of the constituent words and submit them to multiple recognizers. An improvement of this method takes advantage of the spacing between words in a phrase. We describe the improvements to the overall system as a consequence of the second approach.	finite-state machine	Sargur N. Srihari	2000		10.1007/3-540-45014-9_4	natural language processing;speech recognition;classifier;computer science;artificial intelligence;word;phase;logistic regression;algorithm	NLP	-26.471711245877113	-78.98729001306394	115434
00cf902b27676cdc376e26567e70298b96c672a1	joint parsing and named entity recognition	language technology;named entity recognition;named entity recognizer;named entity;question answering	For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser). This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system. We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser. Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree. The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition.	aggregate data;central processing unit;discriminative model;experiment;finite-state machine;language technology;named entity;parse tree;parser combinator;parsing;question answering;semantic role labeling;word sense;word-sense disambiguation	Jenny Rose Finkel;Christopher D. Manning	2009		10.3115/1620754.1620802	natural language processing;speech recognition;question answering;computer science;pattern recognition;entity linking;language technology	NLP	-23.55579903060498	-74.71221834268124	115469
cec08fd5298404556bb8d41fffe272273558e25c	exploring self-training and co-training for hindi dependency parsing using partial parses	pragmatics;syntax;syntax self training co training parsing partial parsing;training;training data;parsing;accuracy;grammars;gold;syntactics;self training;co training;accuracy training training data data models syntactics pragmatics gold;program compilers;partial parsing;program compilers grammars natural language processing;bootstrapping self training co training hindi dependency parsing partial parses malt parser mst parser;natural language processing;data models	In this paper, we explore the effect of self-training and co-training on Hindi dependency parsing using partial parses. We use Partial parser and apply self-training using a large unannotated corpus. For co-training, we use Malt and MST parser along with Partial Parser. We explore different criteria for choosing partial parses to be used for bootstrapping. Through these experiments, we compare the impact of self-training and co-training on Hindi dependency parsing.	bootstrapping (compilers);co-training;experiment;parsing	Rahul Goutam	2012	2012 International Conference on Asian Language Processing	10.1109/IALP.2012.38	gold;natural language processing;data modeling;parser combinator;training set;speech recognition;syntax;computer science;parsing;accuracy and precision;linguistics;programming language;top-down parsing;pragmatics	NLP	-22.498009051534954	-74.74976858496979	115564
01c9f5a68cfd13e3a9bea9b8839d739fdff50e79	factors influencing effectiveness in automated essay scoring with lsa	singular value;latent semantic analysis;similarity measure	This paper addresses the ongoing discussion on influencing factors of automatic essay scoring with latent semantic analysis (LSA). Throughout this paper, we contribute to this discussion by presenting evidence for the effects of the parameters text pre-processing, weighting, singular value dimensionality and type of similarity measure on the scoring results. We benchmark this effectiveness by comparing the machine assigned with human assigned scores in a real world case. The paper shows, that each of the identified factors significantly influences the quality of automated essay scoring, but the factors are not to be independent of each other.	automated essay scoring;benchmark (computing);knowledge acquisition;latent semantic analysis;preprocessor;similarity measure;singular value decomposition;text corpus	Fridolin Wild;Christina Stahl;Gerald Stermsek;Yoseba K. Penya;Gustaf Neumann	2005			latent semantic analysis;computer science;data science;data mining;singular value;information retrieval	NLP	-27.785017277571153	-66.9540620288377	115597
04d90a89958adc7a568224b77faf1aa393af9695	effective bio-event extraction using trigger words and syntactic dependencies	negation detection;heuristic system;speculation recognition;dependency parsing;bionlp;biological event extraction	The scientific literature is the main source for comprehensive, up-to-date biological knowledge. Automatic extraction of this knowledge facilitates core biological tasks, such as database curation and knowledge discovery. We present here a linguistically inspired, rule-based and syntax-driven methodology for biological event extraction. We rely on a dictionary of trigger words to detect and characterize event expressions and syntactic dependency based heuristics to extract their event arguments. We refine and extend our prior work to recognize speculated and negated events. We show that heuristics based on syntactic dependencies, used to identify event arguments, extend naturally to also identify speculation and negation scope. In the BioNLP’09 Shared Task on Event Extraction, our system placed third in the Core Event Extraction Task (F-score of 0.4462), and first in the Speculation and Negation Task (F-score of 0.4252). Of particular interest is the extraction of complex regulatory events, where it scored second place. Our system significantly outperformed other participating systems in detecting speculation and negation. These results demonstrate the utility of a syntax-driven approach. In this article, we also report on our more recent work on supervised learning of event trigger expressions and discuss event annotation issues, based on our corpus analysis.	automatic control;dictionary;digital curation;heuristic (computer science);logic programming;scientific literature;sensor;speculative execution;supervised learning	Halil Kilicoglu;Sabine Bergler	2011	Computational Intelligence	10.1111/j.1467-8640.2011.00401.x	natural language processing;biomedical text mining;computer science;data mining;linguistics;algorithm;dependency grammar	NLP	-24.894700579682837	-70.49913280556635	115795
9e92e18053c4fbab851a38ee61a4a5c100de1108	a pos tagger for social media texts trained on web comments	natural language processing;opinion mining;german	Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-ofSpeech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/T ̈ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.	blog;brill tagger;elegant degradation;erdős–rényi model;estimation theory;general instrument ay-3-8910;lexicon;markov chain;markov model;natural language processing;part-of-speech tagging;point of sale;preprocessor;regular expression;semi-supervised learning;semiconductor industry;social media;supervised learning;text corpus;text normalization;traffic collision avoidance system;undefined behavior	Melanie Neunerdt;Michael Reyer;Rudolf Mathar	2013	Polibits		natural language processing;speech recognition;computer science;multimedia	NLP	-25.107045698223352	-77.04337316700293	115999
3d425026b054afc0ac1bf2b4d21fc3b153be7290	semantic model of possessive pronouns machine translation for english to bulgarian language		The paper presents technique to interpret possessive pronouns for English to Bulgarian language syntax-based machine translation. It uses Universal Networking Language (UNL) as a formal framework to present possessive pronouns grammar features by employing semantic networks for both English and Bulgarian language. The technique includes also use of statistically-based estimation of accuracy of translation by measuring precision and recall of both training and controlled electronic text corpora and by improving related grammar rules.	machine translation	Velislava Stoykova	2015		10.1007/978-3-319-26138-6_38	natural language processing;personal pronoun;speech recognition;linguistics;english grammar;rule-based machine translation;possessive;oblique case	NLP	-26.741450311259754	-76.4175424256734	116047
0f5eb7e2a211c6c105cd6c1692bb88dff02ae7bf	importance of retrieving noun phrases and named entities from digital library content	filtering;rule based and j48 algorithm;digital repository;noun phrase;digital library;rule based;harmonic mean;system performance;word sense disambiguation;feature vector;ratna sanyal kushal keshri vidya nand importance of retrieving noun phrases and named entities from digital library content;hybrid approach;coreference resolution;named entity	We present a novel approach for extracting noun phrases in general and named entities in particular from a digital repository of text documents. The problem of coreference resolution has been divided into two subproblems: pronoun resolution and non-pronominal resolution. A rule based-technique was used for pronoun resolution while a learning approach for nonpronominal resolution. For named entity resolution, disambiguation arises mainly due to polysemy and synonymy. The proposed approach fixes both problems with the help of WordNet and the Word Sense Disambiguation tool. The proposed approach, to our knowledge, outperforms several baseline techniques with a higher balanced F-measure, which is harmonic mean of recall and precision. The improvements in the system performance are due to the filtering of antecedents for the anaphor based on several linguistic disagreements, use of a hybrid approach, and increment in the feature vector to include more linguistic details in the learning technique.	anaphora (linguistics);baseline (configuration management);digital data;digital library;f1 score;feature vector;named entity;precision and recall;word sense;word-sense disambiguation;wordnet	Ratna Sanyal;Kushal Keshri;Vidya Nand	2010	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1001003	filter;natural language processing;noun phrase;digital library;feature vector;computer science;information retrieval;harmonic mean	NLP	-27.191337710027813	-67.04354100693637	116180
5e918feb53283b48fcd83fdb6febc482b8ac577f	anncorra: building tree-banks in indian languages	tree bank;linear notation;typing effort;tagging scheme;paninian grammatical model;indian language	"""This paper describes a dependency based tagging scheme for creating tree banks for Indian languages. The scheme has been so designed that it is comprehensive, easy to use with linear notation and economical in typing effort. It is based on Paninian grammatical model. 1.BACKGROUND The name AnnCorra, shortened for """"Annotated Corpora"""", is for an electronic lexical resource of annotated corpora. The purpose behind this effort is to fill t he lacuna in such resources for Indian languages. It will be an important resource for the development of Indian language parsers, machine learning of grammars, lakshancharts (discrimination nets for sense disambiguation) and a host of other such tools. 2. AIMS AND OBJECTIVE The aim of the project is to : develop a generalised linear syntactosemantic tag scheme for all Indian languages annotate training corpus for all Indian languages develop parallel tree-banks for all Indian languages To fulfill t he above aim a marathon task a collaborative model has been concieved. Any collaborative model implies involvement of several people with varying levels of expertise. This case, becomes further complicated as the tag scheme to be designed has to be equally eff icient for all the Indian languages. These languages, though quite similar, are not identical in their syntactic structures. Thus the tag scheme demands the following properties :comprehensive enough to capture various sysntactic relations across languages. simple enough for anyone, with some background in linguistics, to use. economical in typing effort (the corpus has to be manually annotated). 3. AN ILL USTRATION The task can be better understood with the help of an ill ustration. Look at the following sentence from Hindi 0:: rAma ne moHana ko 'Rama' 'ErgPostP' 'Mohan' 'PostP' nI lI kitAba dI 'blue' 'book' 'gave' 'Rama gave the blue book to Mohan.' Tree-1 is a representation of the above verb, argument relationship within the various constituents of sentence 0 dI ------------------------| | | k1 | k4| k2| | | | rAma_ne moHana_ko kitAba | |nmod | nIlI"""	computation;emoticon;list of tools for static code analysis;machine learning;marathon;objective-c;parsing;tag cloud;text corpus;word-sense disambiguation	Akshar Bharati;Rajeev Sangal;Vineet Chaitanya;Amba P. Kulkarni;Dipti Misra Sharma;K. V. Ramakrishnamacharyulu	2002			natural language processing;computer science;theoretical computer science;algorithm	NLP	-28.616769523535833	-73.41059249982995	116249
4737617587f02831f2df08bd3fb3614d9d577701	maltilex: a computational lexicon for maltese	preliminary phase;different language type;computational lexicon;main characteristic;latter part;general approach;current use	The project described in this paper, which is still in the preliminary phase, concerns the design and implementation of a computational lexicon for Maltese, a language very much in current use but so far lacking most of the infrastructure required for NLP. One of the main characteristics of Maltese, a source of many difficulties, is that it is an amalgam of different language types (chiefly Semitic and Romance), as illustrated in the first part of the paper. The latter part of the paper describes our general approach to the problem of constructing the lexicon. 1 I n t r o d u c t i o n With few exceptions (e.g. Galea (1996)) Maltese is pretty much virgin territory as far as language processing is concerned, and therefore one question worth asking is: where to begin? There are basically two extreme positions that one can adopt in answering this question. One is to attack a variety of applications first, e.g. translation, speech, dialogue etc., and hope that in so doing, enough general expertise can be acquired to build the basis of an NLP culture that is taken for granted with more computationally established languages. The other extreme is to attack the linguistic issues first, since, for whatever reason, there is currently rather little in the way of an accepted linguistic framework from which to design computational materials. We have decided to adopt the middle ground by embarking upon the construction of a substantial machine-tractable lexicon of the language, since whether we think in terms of applications or linguistic theory, the lexicon is clearly a resource of fundamental importance. The construction of the lexicon involves two rather separate subtasks which may in practice 97 become interleaved. The first is the identification of a set of lexical entries, i.e. entries that will serve as the carriers of information. The second is the population of the entries with information of various kinds e.g. syntactic, semantic, phonological etc. Our initial task, trivial as it may sound, is to concentrate on the first of these subtasks, creating what amounts to a word list, in a machinereadable and consistent format, for all the basic lexical entries of the language. The idea is that this will subsequently be used not only as a basis for applications (initially we will concentrate on spell-checking), but also as a tool for linguistic research on the language itself. 2 T h e M a l t e s e L a n g u a g e Maltese is the national language of Malta and, together with English, one of the two official languages of the Republic of Malta. Its use beyond the shores of the Maltese islands is limited to small emigrant communities in Canada and Australia, but within the geographical confines of Malta, the language is used for the widest possible range of types of interaction and communication, including education, journalism, broadcasting, administration, business and literary discourse. Unsurprisingly in view of the disparate political and cultural influences the islands have been exposed to over the centuries, Maltese is a so-called 'mixed' language, with a substrate of Arabic, a considerable superstrate of Romance origin (especially Sicilian) and, to a much more limited extent, English. The Semitic (Western/Maghrebi Arabic) element is evident enough to justify considering the language a peripheral dialect of Arabic. Its script, codified as recently as the 1920s, utilises a modified Latin alphabet. This is just one of the peculiarities of Maltese as compared to other dialectal varieties of Arabic, more important ones being its status as a 'high' variety and its use in literary, formal and official discourse, its lack of reference to any Qur'anic Arabic ideal, as well as its handling of extensive borrowings from non-Semitic sources. These features make Maltese a very interesting area for those working in the fields of language contact and Arabic dialectology. 2.1 T h e M a l t e s e A l p h a b e t As noted above, Maltese is the only dialect of Arabic with a Latin script. Maltese orthography was standardised in the 1920s, utilising an alphabet largely identical with the Latin one, with the following additions/modifications:	cobham's thesis;computation;computational linguistics;greek diacritics;head-driven phrase structure grammar;lexicon;mathematical morphology;natural language processing;norm (social);parsing;peripheral;speech recognition;spell checker;text corpus;theme (computing);transducer	M. Rosner;J. Caruana;R. Fabri	1998			natural language processing;computer science;linguistics;communication	NLP	-31.029137345882223	-75.3090900118929	116285
3cf7e9d46c30a4071dc8b15e791567eb6f02622e	embedding learning of figurative phrases for emotion classification in micro-blog texts		Figurative phrases such as idioms are a type of Multi-Word Expressions (MWE) that possess a specialized meaning, which is independent and different from the literal meaning of the constituent words. Figurative language is widely used to express emotions and are very predominant in micro-blog data.Therefore, an efficient model of emotion categorization for micro-blogs should be able to correctly represent the instances of figurative phrases in the data. However, due to their non-compositional nature, the phrasal representation of figurative language cannot be directly obtained from the constituent words and hence this requires novel approaches for addressing the problem of modeling figurative phrases in micro-blogs. Most of the existing methods of modeling figurative idiomatic phrases in traditional text data use the broader textual context available for better results. However, in case of micro-blog data, such large context is not available due to very short length of text, which poses an additional challenge. Given the need to model figurative language for emotion classification, this paper develops the novel idea of Emotion Sensitive Figurative Phrase Embedding (ESFPE) to model idiomatic phrases in micro-blog data and show upto 14% improvement in emotion classification performance over baseline. To the best of our knowledge, this is the first work towards figurative phrase modeling for emotion classification in micro-blog text.	baseline (configuration management);blog;categorization;literal (mathematical logic);microsoft word for mac;minimal working example;text corpus	Shreshtha Mundra;Sandya Mannarswamy;Manjira Sinha;Anirban Sen	2017		10.1145/3041823.3041828	microblogging;supervised learning;categorization;literal and figurative language;emotion classification;natural language processing;embedding;phrase;expression (mathematics);social media;artificial intelligence;computer science	NLP	-21.733819874220806	-70.94297983172167	116365
aa74dc0c3101276567f2aa7e6785b9a14e6ff4f0	a possibilistic approach for the automatic morphological disambiguation of arabic texts	pragmatics;training possibility theory pragmatics uncertainty educational institutions testing;uncertainty;training;natural language processing computational linguistics;testing;pos possibilistic classifier automatic morphological disambiguation arabic nonvocalized text disambiguation morphological analyzer morphological feature possibilistic network arabic stories part of speech;morphological disambiguation;morphological analysis;arabic natural language processing morphological analysis morphological disambiguation naive possibilistic classifier;possibility theory;computational linguistics;arabic natural language processing;naive possibilistic classifier;natural language processing	This paper presents a new approach for Arabic non-vocalized texts disambiguation based on a possibilistic classifier. A morphological analyzer provides all the possible solutions and the values of the morphological features of words. When texts are vocalized, the number of solutions is reduced and in many cases, we can identify the correct analysis of the input word. The main idea of this paper is to exploit this type of texts in order to learn contextual dependencies between the different values of morphological features modeled as a possibilistic network. This knowledge is used later to disambiguate non-vocalized texts. In order to evaluate our approach, we perform experiments on a corpus of arabic stories. In this paper, we present results concerning the Part-Of-Speech (POS) which is the main morphological feature. Our results are compared to the SVM-based system called MADA.	document;experiment;information retrieval;morphological pattern;part-of-speech tagging;statistical classification;support vector machine;test set;text corpus;word-sense disambiguation	Raja Ayed;Ibrahim Bounhas;Bilel Elayeb;Fabrice Evrard;Narjès Bellamine Ben Saoud	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.21	natural language processing;possibility theory;speech recognition;uncertainty;morphological analysis;computer science;artificial intelligence;computational linguistics;software testing;pragmatics	NLP	-24.29994221081004	-67.1468929087027	116418
13ad54b3c2e783bb761ecca6d9a90ea9984bd1a8	recurrent neural network language model adaptation for multi-genre broadcast speech recognition		Recurrent neural network language models (RNNLMs) have recently become increasingly popular for many applications i ncluding speech recognition. In previous research RNNLMs have normally been trained on well-matched in-domain data. The adaptation of RNNLMs remains an open research area to be explored. In this paper, genre and topic based RNNLM adaptation techniques are investigated for a multi-genre broad cast transcription task. A number of techniques including Proba bilistic Latent Semantic Analysis, Latent Dirichlet Alloc ation and Hierarchical Dirichlet Processes are used to extract sh ow level topic information. These were then used as additional input to the RNNLM during training, which can facilitate unsupervised test time adaptation. Experiments using a state-o f-theart LVCSR system trained on 1000 hours of speech and more than 1 billion words of text showed adaptation could yield pe rplexity reductions of 8% relatively over the baseline RNNLM and small but consistent word error rate reductions.	artificial neural network;baseline (configuration management);language model;latent semantic analysis;open research;recurrent neural network;speech analytics;speech recognition;transcription (software);word error rate	Xi Chen;Ting Xu Tan;Xunying Liu;Pierre Lanchantin;M. Wan;Mark J. F. Gales;Philip C. Woodland	2015			natural language processing;speech recognition;computer science;machine learning	NLP	-20.14466290354274	-78.74382760665674	116570
36a2b3d6918f8ebfdbfd3c457d4e4ea6d0ee0a12	非監督式學習於中文電視新聞自動轉寫之初步應用 (unsupervised learning for chinese broadcast news transcription) [in chinese]			medical transcription;unsupervised learning	Jen-wei Kuo;Wen-Hung Tsai;Berlin Chen	2004			unsupervised learning;speech recognition;artificial intelligence;pattern recognition;computer science;transcription (biology);broadcasting	NLP	-24.974852991516247	-79.17737455843384	116578
a985c55bd06368d2eeaea9577cb6125e25553ffc	context based classification for automatic collaborative learning process analysis	cmu;text processing;collaborative learning;text classification;automatic process analysis;collabortive learning;process analysis	We present a publicly available tool called TagHelper that can be used to support the analysis of conversational data using automatic text classification technology. The contribution of this paper is to explore the limitations of the current simple approach to text processing employed by TagHelper tools with respect to identifying context-sensitive categories of conversational behavior. TagHelper can be downloaded from http://www.cs.cmu.edu/~cprose/TagHelper.html.	approximation algorithm;context-sensitive grammar;document classification;ibm notes;machine learning;perceptron	Yi-Chia Wang;Mahesh Joshi;Carolyn Penstein Rosé;Frank Fischer;Armin Weinberger;Karsten Stegmann	2007			natural language processing;collaborative learning;computer science;machine learning;data mining;world wide web;information retrieval;pedagogy	NLP	-26.70869217756155	-69.9931984598852	116621
c61a1a0694d9c4ac61ca172ee93ed5bfc658fb34	semantic classification of tweets: a contextual knowledge based approach for tweet classification		In this paper we propose a novel approach and technique for tweet classification based on the Contextual Knowledge Structures (CKS). We first discover the popular, trending topics, then tap the web for the related, relevant content for the topics and harness the same to build CKS. CKS are built using text mining techniques and Computational Linguistics; they are relevant Subject-Predicate-Object triples that depict a specific topic or event. Since the tweets are sparse and most of them do not contain a hashtag, it is difficult to map them to a specific topic. We leverage the CKS to train the Naïve Bayes (NB) classifier and achieve a semantic classification of user tweets. We evaluate the performance of our CKS based classifier by comparing it with the baseline Bag-of-Words (BOW) learning model. The CKS based NB classifier exhibits a consistent performance with an accuracy of approximately 94%. This approach has a two-fold advantage: a) A small training set of knowledge structures is effectively used for machine learning, b) The model adapts to the topic. Trending topics are automatically discovered, the associated CKS are built and the classifier is trained using these dynamic CKS. This model is dynamic, topic-adaptive and efficient.	bag-of-words model in computer vision;baseline (configuration management);computation;computational linguistics;emoticon;hashtag;machine learning;naive bayes classifier;sparse matrix;test set;text mining	Javed Nazura;B. L. Muralidhara	2017	2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)	10.1109/IISA.2017.8316358	naive bayes classifier;computational linguistics;classifier (linguistics);computer science;artificial intelligence;training set;pattern recognition;text mining	AI	-19.58534813256559	-67.03458188158857	116902
1b1eed518d7f0c67874a5e6e018486feb3923732	early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons	greedy sequence modeling;conditionally-trained model;inter-dependent feature;natural language task;feature induction;early result;generative probabilistic model;conditional random field;non-independent feature;character n-grams;capitalization pattern;conditional maximum entropy model;web-enhanced lexicon;entity recognition;domain knowledge;mean field	Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models’ great flexibility to include a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only those feature conjunctions that significantly increase loglikelihood, the approach builds on that of Della Pietra et al (1997), but is altered to work with conditional rather than joint probabilities, and with a mean-field approximation and other additional modifications that improve efficiency specifically for a sequence model. In comparison with traditional approaches, automated feature induction offers both improved accuracy and significant reduction in feature count; it enables the use of richer, higherorder Markov models, and offers more freedom to liberally guess about which atomic features may be relevant to a task.	approximation;computation;conditional random field;dictionary attack;finite-state machine;grams;graph (discrete mathematics);graphical model;greedy algorithm;lexicon;markov chain;markov model;n-gram;named entity;named-entity recognition;natural language processing;part-of-speech tagging;principle of maximum entropy;statistical model;text segmentation;time complexity;triangular function	Andrew McCallum;Wei Li	2003			natural language processing;speech recognition;computer science;mean field theory;machine learning;pattern recognition;conditional random field	NLP	-20.180374924723626	-75.23418239288195	116905
bf09957039851754117d37650d358214813207e3	a semi-automated approach to building text summarisation classifiers	liverpool;jtacs;text summarisation;journal;questionnaire data mining;repository;text classification;university;computer science	An investigation into the extraction of useful information from the free text element of questionnaires, using a semi-automated summarisation extraction technique, is described. The summarisation technique utilises the concept of classification but with the support of domain/human experts during classifier construction. A realisation of the proposed technique, SARSET (Semi-Automated Rule Summarisation Extraction Tool), is presented and evaluated using real questionnaire data. The results of this evaluation are compared against the results obtained using two alternative techniques to build text summarisation classifiers. The first of these uses standard rule-based classifier generators, and the second is founded on the concept of building classifiers using secondary data. The results demonstrate that the proposed semi-automated approach outperforms the other two approaches considered.	archive;c4.5 algorithm;experiment;information extraction;logic programming;naive bayes classifier;rule-based system;semiconductor industry;sequential minimal optimization;subject-matter expert	Matias Garcia-Constantino;Frans Coenen;P.-J. Noble;Alan Radford;Christian Setzkorn	2012		10.1007/978-3-642-31537-4_39	natural language processing;computer science;data mining;information retrieval	AI	-31.10521379459814	-69.23695574778263	117206
25a40a88e29734470711981e4db65cf42b4e1c2f	udrst: a novel system for unlabeled discourse parsing in the rst framework		This paper presents UDRST, an unlabeled discourse parsing system in the RST framework. UDRST consists of a segmentation model and a parsing model. The segmentation model exploits subtree features to rerank N-best outputs of a base segmenter, which uses syntactic and lexical features in a CRF framework. In the parsing model, we present two algorithms for building a discourse tree from a segmented text: an incremental algorithm and a dual decomposition algorithm. Our system achieves 77.3% in the unlabeled score on the standard test set of the RST Discourse Treebank corpus, which improves 5.0% compared to HILDA [6], a state-of-the-art discourse parsing system.	intel matrix raid;parsing	Ngo Xuan Bach;Minh Le Nguyen;Akira Shimazu	2012		10.1007/978-3-642-33983-7_25	natural language processing;speech recognition;linguistics	NLP	-22.386367047334033	-75.74056581459871	117295
db0ae846a9a20d472c9df8546921ea0270b860dc	more is better: large scale partially-supervised sentiment classication		We describe a bootstrapping algorithm to learn from partially labeled data, and the results of an empirical study for using it to improve performance of sentiment classication using up to 15 million unlabeled Amazon product reviews. Our experiments cover semi-supervised learning, domain adaptation and weakly supervised learning. In some cases our methods were able to reduce test error by more than half using such large amount of data.	sentiment analysis	Yoav Haimovitch;Koby Crammer;Shie Mannor	2012			semi-supervised learning;computer science;machine learning;pattern recognition;data mining	AI	-19.740579085721787	-66.29091416630712	117337
e0bc402f5e9e0512ec29af7538bc5955f09bf3ee	kul: data-driven approach to temporal parsing of newswire articles		This paper describes a system for temporal processing of text, which participated in the Temporal Evaluations 2013 campaign. The system employs a number of machine learning classifiers to perform the core tasks of: identification of time expressions and events, recognition of their attributes, and estimation of temporal links between recognized events and times. The central feature of the proposed system is temporal parsing – an approach which identifies temporal relation arguments (eventevent and event-timex pairs) and the semantic label of the relation as a single decision.	machine learning;parsing;timex sinclair	Oleksandr Kolomiyets;Marie-Francine Moens	2013			parsing;artificial intelligence;pattern recognition;computer science	AI	-20.271782010503593	-69.64539443332046	117381
ee16022468bd4b847bf33a400ca55dab3b08ae51	semantic similarity of short texts in languages with a deficient natural language processing support	text dbs;semantic similarity of words;corpus based measures;paraphrase corpora construction;similarity of short texts;linguistic tools for is modeling	Measuring the semantic similarity of short texts is a noteworthy problem since short texts are widely used on the Internet, in the form of product descriptions or captions, image and webpage tags, news headlines, etc. This paper describes a methodology which can be used to create a software system capable of determining the semantic similarity of two given short texts. The proposed LInSTSS approach is particularly suitable for application in situations when no large, publicly available, electronic linguistic resources can be found for the desired language. We describe the basic working principles of the system architecture we propose, as well as the stages of its construction and use. Also, we explain the procedure used to generate a paraphrase corpus which is then utilized in the evaluation process. Finally, we analyze the evaluation results obtained from a system created for the Serbian language, and we discuss possible improvements which would increase system accuracy.	co-occurrence matrix;document-term matrix;natural language processing;preprocessor;semantic similarity;software system;text corpus	Bojan Furlan;Vuk Batanovic;Bosko Nikolic	2013	Decision Support Systems	10.1016/j.dss.2013.02.002	natural language processing;semantic similarity;computer science;database;information retrieval	NLP	-28.837211188452553	-71.80324626248253	117413
2294e75a75b18808d39070d5c602d7cc997ca8f8	attention-based end-to-end speech recognition in mandarin		Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. In this paper, we explore the use of attention-based encoder-decoder model for Mandarin speech recognition and to the best of our knowledge, achieve the first promising result. We reduce the source sequence length by skipping frames and regularize the weights for better generalization and convergence. Moreover, we investigate the impact of varying attention mechanism (convolutional attention and attention smoothing) and the correlation between the performance of the model and the width of beam search. On the MiTV dataset, we achieve a character error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% without using any lexicon or language model. While together with a trigram language model, we reach 2.81% CER and 5.77% SER.	beam search;encoder;end-to-end principle;language model;lexicon;smoothing;speech recognition;super robot monkey team hyperforce go!;trigram	Changhao Shan;Junbo Zhang;Yujun Wang;Lei Xie	2017	CoRR		mandarin chinese;speech recognition;end-to-end principle;computer science	NLP	-19.512685620002415	-76.83614508347362	117502
4956b345cfdf30586dfb3c20ea6a3a96bb4115ee	effect of part-of-speech and lemmatization filtering in email classification for automatic reply.		We study the automatic reply of email business messages in Brazilian Portuguese. We present a novel corpus containing messages from a real application, and baseline categorization experiments using Naive Bayes and Support Vector Machines. We then discuss the effect of lemmatization and the role of part-of-speech tagging filtering on precision and recall. Support Vector Machines classification coupled with non-lemmatized selection of verbs and nouns, adjectives and adverbs was the best approach, with 87.3% maximum accuracy. Straightforward lemmatization in Portuguese led to the lowest classification results in the group, with 85.3% and 81.7% precision in SVM and Naive Bayes respectively. Thus, while lemmatization reduced precision and recall, part-ofspeech filtering improved overall results.	baseline (configuration management);categorization;email;experiment;lemmatisation;naive bayes classifier;part-of-speech tagging;precision and recall;support vector machine;text corpus	Rogerio Bonatti;Arthur G. de Paula;Victor S. Lamarca;Fábio Gagliardi Cozman	2016			data mining;world wide web;information retrieval	NLP	-23.87190728904694	-69.04247299873941	117533
fb11bbeaa37724108060490c81f4f1900f962e10	a minimal spanning tree based method for text document classification	minimal spanning tree		document classification;file spanning;minimum spanning tree	Hrishikesh Bhaumik;Nirmalya Chowdhury	2005			artificial intelligence;pattern recognition;computer science;machine learning;minimum spanning tree;document classification	NLP	-24.99625646836449	-67.29980802832966	117540
660a046263bd12e80cb14987be117653b9d6d11c	learning the optimal use of dependency-parsing information for finding translations with comparable corpora	optimal use;optimal transformation matrix;similar word;linear transformation;new word translation;different context position;dependency-parsing information;successor position;comparable corpus;context vector;similar context;certain dependency position;context position	Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy.	approximation algorithm;bag-of-words model in computer vision;baseline (configuration management);bilingual dictionary;cosine similarity;dependency grammar;experiment;markov chain monte carlo;modified discrete cosine transform;monte carlo method;parse tree;parsing;protologism;statistical model;text corpus;the matrix;transformation matrix	Daniel Andrade;Takuya Matsuzaki;Jun'ichi Tsujii	2011			natural language processing;speech recognition;computer science;algorithm	NLP	-20.015384256145826	-78.948567498618	117683
614cf133f365aa93d7337188b516781d64b2d8e9	generation of bangla text from universal networking language expression	selected works;bepress	This paper presents a work on generating Bangla sentences from an interlingua representation called Universal Networking Language (UNL). UNL represents knowledge in the form of semantic network like hyper-graphs which contains disambiguated words, binary semantic relations, and speech act like attributes associated with the words, assisted by the semantically rich lexicon and a set of analysis and generation rules. We have developed a set of generation rules for converting UNL expression to Bangla sentences. Our experiment shows that these rules successfully generate correct Bangla sentences from UNL expressions.	dictionary;experiment;lexicon;universal networking language	Md. Nawab Yousuf Ali;Shaikh Muhammad Allayear;Mohammad Ameer Ali;Golam Sorwar	2011		10.1007/978-3-642-32573-1_25	natural language processing;speech recognition;computer science;linguistics	NLP	-30.833453017135085	-80.1301153759885	117784
1d4e581237bc2c7cbc7b9963cc539c92bd11f595	an efficient dictionary mechanism based on double-byte	information retrieval;real time processing;information retrieve;large scale;multi language;double byte;dictionary;natural language processing	Dictionary is an efficient management of large sets of distinct strings in memory. It has significant influence on Natural Language Process, Information Retrieval and other areas. In this paper, we propose an efficient dictionary mechanism, which is suitable for Double-Byte coding languages. Compared with other five popular dictionary mechanisms, this mechanism performs the best of all. It improves the search performance greatly and reduces the complexity of the construction and maintenance of the dictionary. It can be well applied in large-scale and real-time processing systems. Since Unicode is a typical double-byte code which can represents all kinds of characters in the world, this dictionary will be applicable for multi-language dictionaries.	byte;dictionary	Lei Yang;Jian-Yun Shang;Yan-Ping Zhao	2007		10.1007/978-3-540-77094-7_53	natural language processing;speech recognition;k-svd;computer science;information retrieval;associative array;data dictionary	NLP	-30.257063004788403	-68.91937109476302	117888
69db575e6546a294aa5946f4beb25559cf7843fd	long-distance dependencies in a logic grammar: scp				Akira Ishikawa;Seiki Akama	1989	J. Inf. Sci. Eng.		computer science;machine learning;artificial intelligence;grammar	DB	-29.888732765932858	-78.63991916454613	118109
b2b5295d9699a78c60e4afc26c45ad40ddcb716c	lattice-based minimum error rate training for statistical machine translation	translation task;n-best mert;error surface;lattice-based minimum error rate;resulting error count;candidate translation;n-best list;exact error surface;feature function weight;probable translation;phrase-based statistical machine translation;measurement system;linear model	Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared toN -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N -best MERT.	algorithm;bleu;computation;experiment;linear model;log-linear model;multi-environment real-time;smoothing;statistical machine translation;symbian	Wolfgang Macherey;Franz Josef Och;Ignacio Thayer;Jakob Uszkoreit	2008			speech recognition;computer science;machine learning;linear model;pattern recognition;system of measurement;statistics	NLP	-20.721085906986026	-78.05470529041506	118114
2259e26e77ef91cbe428b148982accdd27e3983d	extracting word lists for domain-specific implicit opinions from corpora		Sentiment analysis relies to a large extent on lexical resources. While lists of words bearing a contextindependent evaluative polarity (‘great’, ‘bad’) are available for many languages now, the automatic extraction of domain-specific evaluative vocabulary still needs attention. This holds especially for implicit opinions or so-called polar facts. In our work, we focus on German and on a genre that has not received much attention yet: customer emails. As the prime downstream application is identifying customers’ complaints, we concentrate here on finding negative words, but our method applies to positive ones as well. Using a seed list approach, we provide a comparative analysis along three dimensions: effect of different seed lists, different linguistic analysis units, and different statistical correlation tests.	dictionary attack;downstream (software development);email;qualitative comparative analysis;sentiment analysis;text corpus;vocabulary	Nuria Bertomeu;Manfred Stede	2017				NLP	-27.498936611372862	-74.35173236926069	118117
717fbab57c817ab6afc8132257179b3ac24c21c1	exploring fine-grained emotion detection in tweets		We examine if common machine learning techniques known to perform well in coarsegrained emotion and sentiment classification can also be applied successfully on a set of fine-grained emotion categories. We first describe the grounded theory approach used to develop a corpus of 5,553 tweets manually annotated with 28 emotion categories. From our preliminary experiments, we have identified two machine learning algorithms that perform well in this emotion classification task and demonstrated that it is feasible to train classifiers to detect 28 emotion categories without a huge drop in performance compared to coarser-grained classification schemes.	algorithm;experiment;machine learning;statistical classification;text corpus	Jasy Suet Yan Liew;Howard R. Turtle	2016			artificial intelligence;natural language processing;computer science	NLP	-21.282095304013545	-68.43342862355608	118206
7dbd66edfd7c426a9ffa516d9e779aeca0cab257	question classification using multiple kernel learning and semantic information	tree kernel;multiple kernel learning;question classification;support vector machine;semantic features	Question Classification is an important stage in Question Answering, and it has been a hot topic in the field of Information Retrieval in recent years. In this paper we explore the role of semantic features and propose two separate tree kernel functions incorporating the semantic features into the Support Vector Machine model. Then Multiple Kernel Learning approach is proposed to combine the two kernels and gather their advantages together. Experimental results show that using the method proposed in this paper is very effective and the accuracy reaches 95.8% which significantly outperforms the state-of-the-art approaches.	effective method;entity;experiment;information retrieval;kernel (operating system);math kernel library;multiple kernel learning;parse tree;parsing;question answering;statistical classification;support vector machine;surface-mount technology;vii;wordnet	Guohua Chen;Yong Tang;Yan Pan;Qiang Deng	2011	JCP	10.4304/jcp.6.11.2325-2334	support vector machine;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;data mining;graph kernel;tree kernel;polynomial kernel	NLP	-19.760808610057246	-66.73806399645517	118207
6c5d2e4bc54beb4260cd56f9a45bf90e98d1187d	a corpus and model integrating multiword expressions and supersenses		This paper introduces a task of identifying and semantically classifying lexical expressions in running text. We investigate the online reviews genre, adding semantic supersense annotations to a 55,000 word English corpus that was previously annotated for multiword expressions. The noun and verb supersenses apply to full lexical expressions, whether singleor multiword. We then present a sequence tagging model that jointly infers lexical expressions and their supersenses. Results show that even with our relatively small training corpus in a noisy domain, the joint task can be performed to attain 70% class labeling F1.	baseline (configuration management);downstream (software development);expression (computer science);gunnar johannsen;machine translation;open-source software;parsing;regular expression;semantic analysis (machine learning);word lists by frequency	Nathan Schneider;Noah A. Smith	2015		10.3115/v1/N15-1177	natural language processing;speech recognition;computer science;linguistics	NLP	-23.155700407274026	-74.00063403776194	118467
7c6aeea3bc6f99faf2d97e997af351a8399501bb	populating biomedical ontologies from natural language texts		Ontology population is a knowledge acquisition activity that relies on (semi-) automatic methods to transform unstructured, semi-structured and structured data sources into instance data. In this work, a semantic-role based process for ontology population is presented that provides a suitable framework for textual knowledge acquisition in the biological domain. In particular, with our approach, a given ontology can be enriched by adding instances gathered from biological natural language texts. Our system’s modular architecture provides a greater versatility than current approaches in the mentioned domain, as the process of ontology population is not directly dependent on the linguistic rules developed from the corpus.	ccir system a;controlled vocabulary;entity;existential quantification;field (computer science);framenet;knowledge acquisition;machine learning;modular programming;natural language;ontology (information science);ontology components;open biomedical ontologies;population;precision and recall;relationship extraction;semi-structured data;semiconductor industry;verbnet	Juana María Ruiz-Martínez;Rafael Valencia-García;Rodrigo Martínez-Béjar;Achim G. Hoffmann	2010			linguistics	AI	-32.21055652777051	-68.6913114735658	118483
52d01806f510d9c2fa9b1db38fd2b9cd812df9df	whuir at the ntcir-12 temporal intent disambiguation task		WHUIR participated in the Temporal Intend Disambiguation (TID) Task of the Temporalia track at NTCIR-12. This paper describes our work of this specific subtask. Given a query, the task is to assign the probability value to four temporal classes i.e. Past, Recency, Future or Atemporal. Our overall strategy has been to rely on established off-the-shelf components (e.g., standard classifiers from LIBSVM and natural language processing methods from Stanford CoreNLP) and focus on feature discovering. We considered nineteen features in total from query itself. We used all the features for SVR in different parameter sets and chose the best three sets on the dry run data for the formal run. Results are presented and discussed in this paper.	dry run (testing);natural language processing;word-sense disambiguation	Sisi Gui;Wei Lu	2016			computer science	NLP	-23.044192974888414	-69.49877031987099	118649
61d7d3ed3e4435caa3d5fb8927117ce98a186333	screening twitter users for depression and ptsd with lexical decision lists		This paper describes various systems from the University of Minnesota, Duluth that participated in the CLPsych 2015 shared task. These systems learned decision lists based on lexical features found in training data. These systems typically had average precision in the range of .70 – .76, whereas a random baseline attained .47 – .49.	baseline (configuration management);decision list;information retrieval	Ted Pedersen	2015			natural language processing;speech recognition;data mining	NLP	-22.332813745896225	-70.20125642071987	118678
8558d0ac648c15411bd45e2e831b9f205c65749b	a markovian kernel-based approach for italian speech act labeling		English. This paper describes the UNITOR system that participated to the itaLIan Speech acT labEliNg task within the context of EvalIta 2018. A Structured Kernel-based Support Vector Machine has been here applied to make the classification of the dialogue turns sensitive to the syntactic and semantic information of each utterance, without relying on any task-specific manual feature engineering. Moreover, a specific Markovian formulation of the SVM is adopted, so that the labeling of each utterance depends on speech acts assigned to the previous turns. The UNITOR system ranked first in the competition, suggesting that the combination of the adopted structured kernel and the Markovian modeling is beneficial. Italian. Questo lavoro descrive il sistema UNITOR che ha partecipato all’itaLIan Speech acT labEliNg task organizzato nell’ambito di EvalIta 2018. Il sistema è basato su una Structured Kernelbased Support Vector Machine (SVM) che rende la classificazione dei turni di dialogo dipendente dalle informazioni sintattiche e semantiche della frase, evitando la progettazione di alcuna feature specifica per il task. Una specifica formulazione Markoviana dell’algoritmo di apprendimento SVM permette inoltre di etichettare ciascun turno in funzione delle classificazioni dei turni precedenti. Il sistema UNITOR si é classificato al primo posto nella competizione, e questo conferma come la combinazione della funzione kernel e del modello Markoviano adottati sia molto utile allo sviluppo di sistemi di dialoghi robusti.		Danilo Croce;Roberto Basili	2018				ML	-22.897690480310786	-72.02657522796532	118806
2db668b0bd7b92a398298de060e5d8d3209db8ff	simplifying words in context. experiments with two lexical resources in spanish	text simplification;lexical simplification;spanish;evaluation	HighlightsWe developed the first lexical simplification for Spanish.Human-informed evaluation of the system.Comparison of two WSD strategies.Comparison of two lexical resources.Software and dataset made available for testing and verification. In this paper we study the effect of different lexical resources for selecting synonyms and strategies for word sense disambiguation in a lexical simplification system for the Spanish language. The resources used for the experiments are the Spanish EuroWordNet, the Spanish Open Thesaurus and a combination of both. As for the synonym selection strategies, we have used both local and global contexts for word sense disambiguation. We present a novel evaluation framework in lexical simplification that takes into account the level of ambiguity of the word to be simplified. The evaluation compares various instances of the lexical simplification system, a gold standard, and a baseline. The paper presents an in-depth qualitative error analysis of the results.		Horacio Saggion;Stefan Bott;Luz Rello	2016	Computer Speech & Language	10.1016/j.csl.2015.02.001	natural language processing;text simplification;speech recognition;computer science;evaluation;linguistics;lexical choice;spanish	NLP	-27.73614724465293	-73.51283890977615	118985
8433290eeecf512a48a49268141da620781d79c7	automatic arabic text summarisation system (aatss) based on morphological analysis				Laiali Almazaydeh	2018	IJISTA	10.1504/IJISTA.2018.10015230	computer vision;artificial intelligence;engineering;arabic;morphological analysis	NLP	-31.046268826304647	-77.19479560349177	119023
f483484cd6857cecbcab6c7eec0939a09d07729a	treebank annotation schemes and parser evaluation for german	korpus;annotation;parser evaluation;machine translating;data structure;german language;syntaktische analyse	Recent studies focussed on the question whether less-configurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Kübler et al. (2006). This claim is based on the assumption that PARSEVAL metrics fully reflect parse quality across treebank encoding schemes. In this paper we present new experiments to test this claim. We use the PARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output. We also provide extensive past-parsing crosstreebank conversion. The results of the experiments show that, contrary to Kübler et al. (2006), the question whether or not German is harder to parse than English remains undecided.	data structure;experiment;parser;treebank	Ines Rehbein;Josef van Genabith	2007			natural language processing;speech recognition;data structure;german;computer science;treebank;linguistics	NLP	-22.23580612416156	-77.8444226546577	119140
39d21c550727ca7f0c4feb61eb0090cdda4fbf20	integrating pattern-based and distributional similarity methods for lexical entailment acquisition	lexical semantics	This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation. Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations – the patternbased and the distributional similarity approaches. The integrated method exploits mutual complementary information of the two approaches to obtain candidate relations and informative characterizing features. Then, a small size training set is used to construct a more accurate supervised classifier, showing significant increase in both recall and precision over the original approaches.	information;lexical substitution;machine learning;precision and recall;similarity learning;supervised learning;test set;web search query	Shachar Mirkin;Ido Dagan;Maayan Zhitomirsky-Geffet	2006		10.3115/1273073.1273148	natural language processing;lexical semantics;computer science;lexical chain;pattern recognition;linguistics;lexical choice	NLP	-25.78765478253547	-70.4963138300329	119281
f83c9d1ab36753c275135336246ba0f7f7d32211	automatically deriving event ontologies for a commonsense knowledge base		We describe work aimed at building commonsense knowledge by reading word definitions using deep understanding techniques. The end result is a knowledge base allowing complex concepts to be reasoned about using OWL-DL reasoners. We show that we can use this system to automatically create a mid-level ontology for WordNet verbs that has good agreement with human intuition with respect to both the hypernym and causality relations. We present a detailed error analysis that reveals areas of future work needed to enable high-performance learning of conceptual knowledge by reading.	causality;commonsense knowledge (artificial intelligence);error analysis (mathematics);knowledge base;ontology (information science);wordnet	James F. Allen;William de Beaumont;Lucian Galescu;Jansen Orfan;Mary D. Swift;Choh Man Teng	2013			computer vision;machine learning;pattern recognition;commonsense knowledge	AI	-27.4419354190322	-71.72870865098533	119287
9f499d66099ca3dc829829d46098313eab3613bf	weakly supervised spoken term discovery using cross-lingual side information		Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.	crowdsourcing;data pre-processing;information retrieval;noisy text;preprocessor;uniform theory of diffraction	Sameer Bansal;Herman Kamper;Sharon Goldwater;Adam Lopez	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953260	natural language processing;speech recognition;computer science;artificial intelligence;machine learning	NLP	-24.042438502787864	-74.39012326425748	119328
15c7eb7ee68d8ad6c34dcd7b0bf16ccdeffff831	using morphological, syntactical, and statistical information for automatic term acquisition	morphologie;linguistique;linguistica matematica;analisis estadistico;morphology;linguistica;statistical analysis;analyse statistique;linguistique mathematique;computational linguistics;morfologia;traitement automatique langage naturel;natural language processing;linguistics	Terminologies are useful in all areas that use specialized languages. The development of terminologies is a hard work, when manually done. It can be assisted with tools to ease and improve the achievement of such a work. In this article, we present ATA, an automatic terms extractor using both linguistic and statistical information.		Joana Paulo Pardal;Margarita Correia;Nuno J. Mamede;Caroline Hagège	2002		10.1007/3-540-45433-0_31	natural language processing;morphology;computer science;artificial intelligence;computational linguistics	NLP	-27.551113756110603	-77.40012295645496	119355
dcef53c81b6e8270e74d2e4f0115f4c97ad3deb0	learning task specific distributed paragraph representations using a 2-tier convolutional neural network	会议论文	We introduce a type of 2-tier convolutional neural network model for learning distributed paragraph representations for a special task (e.g. paragraph or short document level sentiment analysis and text topic categorization). We decompose the paragraph semantics into 3 cascaded constitutes: word representation, sentence composition and document composition. Specifically, we learn distributed word representations by a continuous bag-of-words model from a large unstructured text corpus. Then, using these word representations as pre-trained vectors, distributed task specific sentence representations are learned from a sentence level corpus with task-specific labels by the first tier of our model. Using these sentence representations as distributed paragraph representation vectors, distributed paragraph representations are learned from a paragraph-level corpus by the second tier of our model. It is evaluated on DBpedia ontology classification dataset and Amazon review dataset. Empirical results show the effectiveness of our proposed learning model for generating distributed paragraph representations.	convolutional neural network	Tao Chen;Ruifeng Xu;Yulan He;Xuan Wang	2015		10.1007/978-3-319-26532-2_51	natural language processing;speech recognition;computer science;machine learning	NLP	-19.279132661790182	-70.98327095529578	119365
b82e735f5b4fcb1b91b95514fd2af4a9115fc59d	interlingual annotation of parallel text corpora: a new framework for annotation and evaluation	semantic annotation;information retrieval;text summarization;research and development;foreign language;parallel corpora;machine translation;question answering	This paper focuses on an important step in the creation of a system of meaning representation and the development of semantically-annotated parallel corpora, for use in applications such as machine translation, question answering, text summarization, and information retrieval. The work described below constitutes the first effort of any kind to annotate multiple translations of foreign-language texts with interlingual content. Three levels of representation are introduced: deep syntactic dependencies (IL0), intermediate semantic representations (IL1), and a normalized representation that unifies conversives, non-literal language, and paraphrase (IL2). The resulting annotated, multilingually-induced, parallel corpora will be useful as an empirical basis for a wide range of research, including the development and evaluation of interlingual NLP systems and paraphrase-extraction systems as well as a host of other research and development efforts in theoretical and applied linguistics, foreign language pedagogy, translation studies, and other related disciplines.	automatic summarization;information retrieval;literal (mathematical logic);machine translation;natural language processing;parallel text;question answering;text corpus;translation studies	Bonnie J. Dorr;Rebecca J. Passonneau;David Farwell;Rebecca J Green;Nizar Habash;Stephen Helmreich;Eduard H. Hovy;Lori S. Levin;Keith J. Miller;Teruko Mitamura;Owen Rambow;Advaith Siddharthan	2010	Natural Language Engineering	10.1017/S1351324910000070	foreign language;natural language processing;universal networking language;question answering;computer science;linguistics;machine translation;information retrieval	NLP	-31.520898771903884	-71.61201582986016	119407
92349b09337460277d384bf077e06b25408058d4	word and phrase translation with word2vec		Word and phrase tables are key inputs to machine translations, but costly to produce. New unsupervised learning methods represent words and phrases in a high-dimensional vector space, and these monolingual embeddings have been shown to encode syntactic and semantic relationships between language elements. The information captured by these embeddings can be exploited for bilingual translation by learning a transformation matrix that allows matching relative positions across two monolingual vector spaces. This method aims to identify high-quality candidates for word and phrase translation more costeffectively from unlabeled data. This paper expands the scope of previous attempts of bilingual translation to four languages (English, German, Spanish, and French). It shows how to process the source data, train a neural network to learn the high-dimensional embeddings for individual languages and expands the framework for testing their quality beyond the English language. Furthermore, it shows how to learn bilingual transformation matrices and obtain candidates for word and phrase translation, and assess their quality.	artificial neural network;benchmark (computing);encode;edit distance;google news;machine translation;named entity;parsing;preprocessor;software testing;source data;supervised learning;transformation matrix;unsupervised learning;wikipedia;word2vec	Stefan Jansen	2017	CoRR		noun phrase;word error rate;determiner phrase;machine translation	NLP	-20.10229146198434	-77.79540464232194	119430
6aa47412804ec815fc5f53f9bfc5cacc18351624	journal of language modelling	journal of language modelling		language model	Adam Przepiórkowski	2012	J. Language Modelling	10.15398/jlm.v0i1.64	natural language processing;linguistics	AI	-30.96659810522788	-77.81733328391336	119449
901c49759802742c4b34b2b34744f9a8de87f25d	character-aware convolutional neural networks for paraphrase identification		Convolutional Neural Network CNN have been successfully used for many natural language processing applications. In this paper, we propose a novel CNN model for sentence-level paraphrase identification. We learn the sentence representations using character-aware convolutional neural network that relies on character-level input and gives sentence-level representation. Our model adopts both random and one-hot initialized methods for character representation and trained with two paraphrase identification corpora including news and social media sentences. A comparison between the results of our approach and the typical systems participating in challenge on the news sentence, suggest that our model obtains a comparative performance with these baselines. The experimental result with tweets corpus shows that the proposed model has a significant performance than baselines. The results also suggest that character inputs are effective for modeling sentences.		Jiangping Huang;Dong-Hong Ji;Shuxin Yao;Wenzhi Huang	2016		10.1007/978-3-319-46672-9_21	deep learning	NLP	-19.446169821113326	-71.11083672421958	119617
88a88bbb7c42b6770b4248ad79c82b5da099c1e1	medieval manuscript layout model	manuscript;automatic segmentation;annotation;segmentation;layout;medieval manuscript;medieval;layout model	Medieval manuscript layouts are quite complex. Additionally to their main text flow, which can spread over one or several columns, such manuscripts contain also other textual elements such as insertions, annotations, and corrections. They are often richly decorated with ornaments, illustrations, and drop capitals making their layout even more complex. In this paper we propose a generic layout model to represent their physical structure.  To achieve this goal we propose to use four layers in order to distinguish between the different graphical elements. In this paper we show how this model is used to represent automatic segmentation results and how it allows a quantitative measure of their accuracy.	column (database);java annotation	Micheal Baechler;Rolf Ingold	2010		10.1145/1860559.1860622	layout;computer science;programming language;segmentation	NLP	-26.897561710903602	-80.189746958639	119667
1500064d3d715005433970f54ad40537dc714468	cornell belief and sentiment system at tac 2017		In this paper we describe the 2017 system of the CornMich team for the TAC Belief and Sentiment (BeSt) task for Chinese.		Kai Sun;Claire Cardie	2017				NLP	-22.446022123640397	-69.12732374870403	119734
b5aed4d5cc45332b19161de866797707528ceaa3	automatic audio sentiment extraction using keyword spotting	audio sentiment detection;asr;youtube;ut dallas opinion audio archive;reviews;maximum entropy;nlp;kaldi;kws	Most existing methods for audio sentiment analysis use automatic speech recognition to convert speech to text, and feed the textual input to text-based sentiment classifiers. This study shows that such methods may not be optimal, and proposes an alternate architecture where a single keyword spotting system (KWS) is developed for sentiment detection. In the new architecture, the text-based sentiment classifier is utilized to automatically determine the most powerful sentiment-bearing terms, which is then used as the term list for KWS. In order to obtain a compact yet powerful term list, a new method is proposed to reduce text-based sentiment classifier model complexity while maintaining good classification accuracy. Finally, the term list information is utilized to build a more focused language model for the speech recognition system. The result is a single integrated solution which is focused on vocabulary that directly impacts classification. The proposed solution is evaluated on videos from YouTube.com and UT-Opinion corpus (which contains naturalistic opinionated audio collected in real-world conditions). Our experimental results show that the KWS based system significantly outperforms the traditional architecture in difficult practical tasks.	information extraction;language model;naive bayes classifier;sentiment analysis;speech recognition;text-based (computing);vocabulary	Lakshmish Kaushik;Abhijeet Sangwan;John H. L. Hansen	2015			speech recognition;computer science;principle of maximum entropy;internet privacy;world wide web;statistics	NLP	-21.443833694021198	-67.45800863171982	119783
6393dea56fb8382c0d70e6913741dffe35aa6099	exploring high-level features for detecting cyberpedophilia	articulo;cyberpedophilia;sentiment analysis;emotion detection	In this paper, we suggest a list of high-level features and study their applicability in detection of cyberpedophiles. We used a corpus of chats downloaded from www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online and the NPS chat corpus. The SVM classification results show that the NPS data and the pedophiles’ conversations can be accurately discriminated with character n-grams, while in the more complicated case of cybersex logs high-level features significantly outperform the low-level ones and achieve a 97% accuracy.	bigram;binary prefix;campus;cybersex;enterprise 2.0;experiment;feature extraction;grams;high- and low-level;instability;microsoft research;multimodal interaction;n-gram;nec shun-ei;null character;sensor;trigram;vlc media player;word sense;word-sense disambiguation	Dasha Bogdanova;Paolo Rosso;Thamar Solorio	2014	Computer Speech & Language	10.1016/j.csl.2013.04.007	natural language processing;speech recognition;computer science;world wide web;sentiment analysis	NLP	-21.65827466392232	-67.4229346180623	119922
6afbba82086b9869a29c02d20c3e6948cbd04927	overview of nlpcc shared task 4: stance detection in chinese microblogs	target;interest;task a;task b.	This paper presents the overview of the shared task, stance detection in Chinese microblogs, in NLPCC-ICCPOL 2016. The submitted systems are expected to automatically determine whether the author of a Chinese microblog is in favor of the given target, against the given target, or whether neither inference is likely. Different from regular evaluation tasks on sentiment analysis, the microblog text may or may not contain the target of interest, and the opinion expressed may or may not be towards to the target of interest. We designed two tasks. Task A is a mandatory supervised task which detects stance towards five targets of interest with given labeled data. Task B is an optional unsupervised task which gives only unlabeled data. Our shared task has had sixteen team participants for Task A and five results of Task B. The highest F-score obtained was 0.7106 for Task A and 0.4687 for Task B, respectively.	lazy evaluation;sentiment analysis;supervised learning;unsupervised learning	Ruifeng Xu;Yu Zhou;Dongyin Wu;Lin Gui;Jiachen Du;Yun Xue	2016		10.1007/978-3-319-50496-4_85	simulation;multimedia;communication	NLP	-22.27440047988942	-69.42984619268037	119925
0614f5431ec0347b3e38614c79a26d0c0ecc5f6d	using naïve text queries for robust audio information retrieval	databases;analytical models;audio signal processing;audio descriptions;information retrieval;out of vocabulary;out of vocabulary problem;vocabulary;intermediate audio description layer;robustness information retrieval audio databases humans data mining natural languages signal processing laboratories usability vocabulary;semantics;text analysis;natural languages;data mining;text analysis audio signal processing information retrieval open systems;onomatopoeic descriptions;accuracy;interoperability issues;topic model naïve text queries robust audio information retrieval interoperability issues audio descriptions intermediate audio description layer semantic descriptions onomatopoeic descriptions human to human communication experiments latent semantic analysis;signal processing;semantic descriptions;robust audio information retrieval;robustness;approximation methods;humans;audio databases;psychoacoustic models;naive text query;open systems;out of vocabulary problem audio descriptions naive text query audio information retrieval;usability;naïve text queries;latent semantic analysis;topic model;human to human communication experiments;audio information retrieval	The goal of this work is to build an audio information retrieval system which provides users with flexibility in formulating their queries: from audio examples to naïve text. Specifically, the focus of this paper is on using naïve text to create input queries describing the desired information of the users. Using naïve text queries, however, raises interoperability issues between annotation and retrieval processes due to the wide variety of available audio descriptions. In this paper, we propose an intermediate audio description layer (iADL) to solve the interoperability issues between the annotation and retrieval processes. The iADL comprises two axes corresponding to semantic and onomatopoeic descriptions based on human-to-human communication experiments on how humans express sounds verbally. Various text modeling schemes, such as latent semantic analysis (LSA) and latent topic model, are utilized to transform the naïve text onto the proposd iADL.	audio description;experiment;information retrieval;interoperability;latent semantic analysis;naivety;topic model	Samuel Kim;Panayiotis G. Georgiou;Shrikanth (Shri) Narayanan;Shiva Sundaram	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5496235	natural language processing;speech recognition;usability;latent semantic analysis;audio signal processing;computer science;signal processing;semantics;accuracy and precision;topic model;open system;natural language;information retrieval;robustness	Web+IR	-32.07077197385784	-68.95941102817416	120033
99bb13508a528acd2402c7476a4fbef8bcb3b2e0	noun phrase chunking and categorization for authoring aids	noun phrase;syntactic analysis	Effective authoring aids, whether for novice, secondlanguage, or experienced writers, require linguistic knowledge. With respect to depth of analysis, authoring aids that aim to support revising and editing go beyond POS-tagging but cannot work on complete, mostly well-formed sentences to perform deep syntactic analysis, since a text undergoing revision is in a constant state of flux. In order to cope with incomplete and changing text, authoring aids for revising and editing thus have to use shallow analyses, which are fast and robust. In this paper, we discuss noun phrase chunking for German as resource for language-aware editing functions as developed in the LingURed project. We will identify requirements for resources with respect to availability, interactivity, performance and quality of results. From our experiments we also provide some information concerning ambiguity of German noun phrases.	brill tagger;capability maturity model;categorization;computer;correctness (computer science);experiment;interactivity;natural language processing;open-source software;part-of-speech tagging;phrase chunking;quality of results;requirement;robustness (computer science);shallow parsing;text corpus;well-formed formula	Cerstin Mahlow;Michael Piotrowski	2010			head-marking language;noun;noun phrase;nominalization;endocentric and exocentric;grammatical category;chunking (psychology);determiner phrase;natural language processing;computer science;artificial intelligence	NLP	-28.05308979314493	-75.89929227462194	120110
3d3dff813a571c5dad0a6bc57d68727e2512226f	large multi-lingual, multi-level and multi-genre annotation corpus		High accuracy for automated translation and information retrieval calls for linguistic annotations at various language levels. The plethora of informal internet content sparked the demand for porting state-of-art natural language processing (NLP) applications to new social media as well as diverse language adaptation. Effort launched by the BOLT (Broad Operational Language Translation) program at DARPA (Defense Advanced Research Projects Agency) successfully addressed the internet information with enhanced NLP systems. BOLT aims for automated translation and linguistic analysis for informal genres of text and speech in online and in-person communication. As a part of this program, the Linguistic Data Consortium (LDC) developed valuable linguistic resources in support of the training and evaluation of such new technologies. This paper focuses on methodologies, infrastructure, and procedure for developing linguistic annotation at various language levels, including Treebank (TB), word alignment (WA), PropBank (PB), and co-reference (CoRef). Inspired by the OntoNotes approach with adaptations to the tasks to reflect the goals and scope of the BOLT project, this effort has introduced more annotation types of informal and free-style genres in English, Chinese and Egyptian Arabic. The corpus produced is by far the largest multi-lingual, multi-level and multi-genre annotation corpus of informal text and speech.	bitext word alignment;information retrieval;linguistic data consortium;machine translation;natural language processing;propbank;social media;terabyte;treebank	Xuansong Li;Martha Palmer;Nianwen Xue;Lance A. Ramshaw;Mohamed Maamouri;Ann Bies;Kathryn Conger;Stephen Grimes;Stephanie Strassel	2016			natural language processing;artificial intelligence;speech recognition;computer science;annotation	NLP	-29.746721262398406	-74.7584684400366	120128
6655db1fd9309b3f0a6f1d6628a09d18afdaf672	lexical acquisition and information extraction	extraction information;lenguaje natural;analyse lexicale;architecture systeme;information extraction;langage naturel;tratamiento lenguaje;language processing;ariosto;lexical acquisition;natural language;traitement langage;arquitectura sistema;system architecture;analisis lexical;lexical analysis;extraction informacion	"""The estimation of probabilistic preference for prepositional disambiguation as well as other acquisition modules (e.g. the Lexicon Acquisition module) crucially depend on naive semantic types . However, semantic ambiguity still exists even if high level tags (e.g. In s t rumen t or Human E n t i t y ) are used. An average of 2.8 classes per noun has been evaluated in a 1.300.000 words corpus of financial news ([5]). In the architecture of Fig. (2) a specific module (i.e. """"Semantic Tuning and Classification"""") is foreseen for acquisition of semantic classification rules in a domain. An unsupervised method for (1) tuning an existing taxonomy (i.e. Wordnet) by means of an application corpus and (2) local word sense disambiguation has been described in [13, 12]. Collective contexts as hints for sense assignment (in the narrow meaning of semantic classification) are used in a way similar to [79]. However, as opposed to this approach, a method to bias the probabilistic model of semantic classes (i.e. conditional probabilities of typical contexts) to the corpus is proposed: representative nouns/verbs in each semantic class are first selected according to the corpus and Wordnet. Only representative candidates in each class are thus used during the learning phase (see [12] for details). Performance evaluation on samples of verbs and nouns in"""	high-level programming language;information extraction;lexicon;performance evaluation;statistical model;word sense;word-sense disambiguation;wordnet	Roberto Basili;Maria Teresa Pazienza	1997		10.1007/3-540-63438-X_4	natural language processing;speech recognition;lexical analysis;computer science;natural language;programming language;information extraction	NLP	-26.170822643197567	-70.9403827676747	120155
bf6a51b6384aa0fcd31826570e00ecb9bc4c943f	extracting concepts from the software requirements specification using natural language processing		Extracting concepts from the software requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. The methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object-oriented analysis of the problem. This method uses natural language processing (NLP) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them.	dictionary;natural language processing;regular expression;requirement;software development process;software requirements specification;text mining;wordnet	Jaroslaw Kuchta;Priti Padhiyar	2018	2018 11th International Conference on Human System Interaction (HSI)	10.1109/HSI.2018.8431221	software development process;natural language;ambiguity;natural language processing;software;requirements engineering;wordnet;software requirements specification;software requirements;artificial intelligence;computer science	SE	-31.045571494540287	-69.79340271417949	120249
b24c46ff934405de301e01ff7575b1506a95de0f	exploiting a chinese-english bilingual wordlist for english-chinese cross language information retrieval	bilingual dictionaries;cross language information retrieval	We investigated using the LDC English/Chinese bilingual wordlists for English-Chinese cross language retrieval. It is shown that the Chinese-to-English wordlist can be considered as both a phrase and word dictionary, and is preferable to the English-to-Chinese version in terms of phrase translations and word translation selection. Additional techniques such as target corpus frequency-based term selection and weighting were employed. Experiments show that over 70% of monolingual effectiveness is achievable for the TREC Chinese corpus and retrieval environment with short queries of a few English words.	cross-language information retrieval;dictionary;experiment;linguistic data consortium;text corpus	Kui-Lam Kwok	2000		10.1145/355214.355239	natural language processing;speech recognition;computer science;linguistics	NLP	-28.01489168205804	-68.46872307810479	120272
86b4462541e45100977540eaa67422bd9b9f0223	simulating letter and word recognition: a fuzzy logical model of integrating visual information and orthographic structure in reading	word recognition		logical data model;orthographic projection	Dominic W. Massaro	1982			machine learning;fuzzy logic;computer science;word error rate;artificial intelligence;logical data model;natural language processing;word recognition;orthographic projection	AI	-29.61385261127764	-79.28200218344128	120284
12790e656c462c1869edd2241a262e9ae9a95593	research methodology for machine translation		The general approach used at The RAND Corporation is that of convergence by successive refinements. The philosophy that underlies this approach is empirical. Statistical data are collected from careful translation of actual Russian text, analyzed, and used to improve the program. Text preparation, glossary development , translation, and analysis are described.	emoticon;glossary;machine translation;punched card;semiconductor industry	H. P. Edmundson;David G. Hays	1958	Mechanical Translation		computer-assisted translation;example-based machine translation;machine translation software usability	NLP	-32.30260867014215	-77.6943700257006	120518
52ed5006109139fc03fb50e92ec36abe6cbd4847	discriminative word alignment via alignment matrix modeling	translation quality;approach model;alignment matrix modeling;alignment quality;new discriminative word alignment;available information;different language pair;different method;new phrase table;alignment matrix;word alignment	In this paper a new discriminative word alignment method is presented. This approach models directly the alignment matrix by a conditional random field (CRF) and so no restrictions to the alignments have to be made. Furthermore, it is easy to add features and so all available information can be used. Since the structure of the CRFs can get complex, the inference can only be done approximately and the standard algorithms had to be adapted. In addition, different methods to train the model have been developed. Using this approach the alignment quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4alignments. Furthermore the word alignment was used to generate new phrase tables. These could improve the translation quality significantly.	algorithm;bitext word alignment;conditional random field;data structure alignment;experiment;formal language;machine learning;microsoft word for mac;optimizing compiler;pivot table	Jan Niehues;Stephan Vogel	2008			natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.968222447489488	-76.81307080908739	120562
d6273d8f9e4d9e25167d14380b63b36e642878e7	dcu and uta at imageclefphoto 2007	information retrieval;dcu;data fusion;fuzzy matching;text retrieval;query translation;content based image retrieval;image retrieval	Dublin City University (DCU) and University of Tampere (UTA) participated in ImageCLEF 2007 photographic retrieval task with several monolingual and bilingual runs. The approach was language independent with text retrieval utilizing fuzzy s-gram query translation and combined with visual retrieval. Data fusion was achieved through unsupervised query-time weight generation approaches. The baseline was a combination of dictionary-based query translation and visual retrieval, which achieved the best result. The best mixed modality runs using fuzzy s-gram translation reached on average around 83% of the baselines’ performance. This approach was much closer at the early precision levels of P@10 and P@20. This suggests that our language independent approach could be a cheap alternative for cross-lingual image retrieval. Both sets of results further emphasize the merit in our query-time weight generation schemes for data fusion, with the fused runs exhibiting marked performance increases over single modalities without the use of prior training data.	baseline (configuration management);dictionary;document retrieval;image retrieval;modality (human–computer interaction);result set;unsupervised learning	Anni Järvelin;Peter Wilkins;Tomasz Adamek;Eija Airio;Gareth J. F. Jones;Alan F. Smeaton;Eero Sormunen	2007		10.1007/978-3-540-85760-0_66	query expansion;visual word;speech recognition;approximate string matching;image retrieval;computer science;pattern recognition;sensor fusion;information retrieval	Web+IR	-24.163715434010157	-71.49589031387895	120577
c9f8fee82be261c75240acd3535668d56e533477	ltv: labeled topic vector		In this paper, we present LTV , a website and an API that generate labeled topic classifications based on the Dewey Decimal Classification (DDC), an international standard for topic classification in libraries. We introduce nnDDC, a largely language-independent neural network-based classifier for DDC-related topic classification, which we optimized using a wide range of linguistic features to achieve an F-score of 87,4%. To show that our approach is language-independent, we evaluate nnDDC using up to 40 different languages. We derive a topic model based on nnDDC, which generates probability distributions over semantic units for any input on sense-, wordand text-level. Unlike related approaches, however, these probabilities are estimated by means of nnDDC so that each dimension of the resulting vector representation is uniquely labeled by a DDC class. In this way, we introduce a neural network-based Classifier-Induced Semantic Space (nnCISS).	artificial neural network;dewey decimal classification;f1 score;language-independent specification;library (computing);topic model	Daniel Baumartz;Tolga Uslu;Alexander Mehler	2018			natural language processing;artificial intelligence;pattern recognition;computer science	NLP	-20.445696620819046	-70.69229753114529	120584
f9902d037cea1d1231420bf8cc5a7a48dd85b155	on the role of discourse markers in interactive spoken question answering systems	discourse marker;question answering system	This paper presents a preliminary analysis of the role of some discourse markers and the vocalic hesitation euh in a corpus of spoken human utterances collected with the RITEL system, an open domain and spoken dialog system. The frequency and contextual combination patterns of classical discourse markers and of the vocalic hesitation has been studied. This analysis highlights some specificities in terms of combination patterns of the analyzed items. The classical discourse markers seem to help initiating larger discursive blocks both at initial and medial positions of the ongoing turns. The vocalic hesitation also stand for marking the user’s embarrassments and wish to close the dialog.	dialog system;item unique identification;medial graph;question answering;spoken dialog systems	Ioana Vasilescu;Sophie Rosset;Martine Adda-Decker	2010			natural language processing;artificial intelligence;discourse marker;computer science;dialog box;question answering;dialog system	NLP	-29.921178562949905	-80.15196800673992	120605
1336146e7f95b295bb73c7659c6af4befd86cbdd	text understanding from scratch		This article demonstrates that we can apply deep learning to text understanding from characterlevel inputs all the way up to abstract text concepts, using temporal convolutional networks(LeCun et al., 1998) (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.	categorization;convolutional neural network;deep learning;document classification;sentiment analysis;web ontology language	Xiang Zhang;Yann LeCun	2015	CoRR		natural language processing;speech recognition;computer science	NLP	-19.623393569675052	-71.55414466639185	120637
f888489d3432bd7a5fe4b8720a4e4adc6936215d	bdlex lexical data and knowledge base of spoken and written french	knowledge base		knowledge base	Guy Perennou;Martine de Calmès	1987			linguistics;knowledge base;computer science	NLP	-30.6155396571559	-77.71878421131626	120643
9759c425008506dac507ed26057febd9cab822b8	active sentiment domain adaptation		Domain adaptation is an important technology to handle domain dependence problem in sentiment analysis field. Existing methods usually rely on sentiment classifiers trained in source domains. However, their performance may heavily decline if the distributions of sentiment features in source and target domains have significant difference. In this paper, we propose an active sentiment domain adaptation approach to handle this problem. Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples.	benchmark (computing);domain adaptation;experiment;general-purpose markup language;information;lexicon;linear classifier;mined;naive bayes classifier;sentiment analysis;unified model	Fangzhao Wu;Yongfeng Huang;Jun Yan	2017		10.18653/v1/P17-1156	artificial intelligence;machine learning;computer science;natural language processing;domain adaptation	AI	-19.359353285942593	-66.59352901052168	120822
0e479e745374189934b57eafe637c53b490ad1c3	forest rescoring: faster decoding with integrated language models	search method;language model;machine translation	Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy.	algorithm;beam search;decade (log scale);language model;machine translation;nonlocal lagrangian;parsing expression grammar;random forest;semantic role labeling;simulated annealing;word lists by frequency	Liang Huang;David Chiang	2007			natural language processing;speech recognition;transfer-based machine translation;computer science;machine learning;machine translation;programming language;language model	NLP	-21.325938394474242	-76.58100562428419	120885
51c4884e2eeb87be5a350705de1ecc4f72304c11	sentiment-bearing new words mining: exploiting emoticons and latent polarities		New words and new senses are produced quickly and are used wide- ly in micro blogs, so to automatically extract new words and predict their se- mantic orientations is vital to sentiment analysis in micro blogs. This paper proposes Extractor and PolarityAssigner to tackle this task in an unsupervised manner. Extractor is a pattern-based method which extracts sentiment-bearing words from large-scale raw micro blog corpus, where the main task is to elimi- nate the huge ambiguities in the un-segmented raw texts. PolarityAssigner predicts the semantic orientations of words by exploiting emoticons and la- tent polarities, using a LDA model which treats each sentiment-bearing word as a document and each co-occurring emoticon as a word in that document. The experimental results are promising: many new sentiment-bearing words are extracted and are given proper semantic orientations with a relatively high precision, and the automatically extracted sentiment lexicon improves the performance of sentiment analysis on an open opinion mining task in micro blog corpus.	emoticon	Fei Wang;Yunfang Wu	2015		10.1007/978-3-319-18117-2_13	natural language processing;speech recognition;computer science;world wide web	ML	-23.73987031130053	-66.43542500523365	120973
9b05cfe80a1a1c36e9b797ab6b9f83f32088f4d9	investigating entity linking in early english legal documents		In this paper we investigate the accuracy and overall suitability of a variety of Entity Linking systems for the task of disambiguating entities in 17th century depositions obtained during the 1641 Irish Rebellion. The depositions are extremely difficult for modern NLP tools to work with due to inconsistent spelling, use of language and archaic references. In order to assess the severity of difficulty faced by Entity Linking systems when working with the depositions we use them to create an evaluation corpus. This corpus is used as an input to the General Entity Annotator Benchmarking Framework a standard benchmarking platform for entity annotation systems. Based on this corpus and the results obtained from General Entity Annotator Benchmarking Framework we observe that the accuracy of existing Entity Linking systems is lacking when applied to content like these depositions. This is due to a number of issues ranging from problems with existing state-of-the-art systems to poor representation of historic entities in modern knowledge bases. We discuss some interesting questions raised by this evaluation and put forward a plan for future work in order to learn more.	archive;entity linking;knowledge base;natural language processing;niche blogging;word-sense disambiguation	Gary Munnelly;Séamus Lawless	2018		10.1145/3197026.3197055	information retrieval;benchmarking;natural language processing;entity linking;cultural heritage;digital humanities;computer science;ranging;spelling;artificial intelligence;annotation	NLP	-30.051497651837792	-72.11223746043896	121020
1537c6c60aeeb9dbc44986244b92e11cf449e4be	strategies for short text representation in the word vector space		Short texts are present in many computer systems. Examples include social media messages, advertisement, Q&A websites, and an increasing number of other applications. They are characterized by little context words and a large vocabulary. As a consequence, traditional short text representations, such as TF and TF-IDF, have high dimensionality and are very sparse. The research field of word vectors has produced interesting word representations that are discriminative regarding semantics, which can be algebraically composed to create vector representations for paragraphs and documents. Literature reports limitations of this approach, producing the alternative Paragraph Vector method. Firstly, we investigate whether these limitations involving word vector operations are true for short text. Then, we propose a novel representation method based on the PSO meta-heuristic. Results in a document classification task are competitive with TF-IDF and show significant improvement over Paragraph Vector, with the advantage of dense and compact document vector representation.		Marcelo Pita;Gisele L. Pappa	2018	2018 7th Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2018.00053	paragraph;task analysis;semantics;artificial neural network;vector space;curse of dimensionality;artificial intelligence;vocabulary;pattern recognition;document classification;computer science	NLP	-22.5310589531209	-66.39981490030488	121042
ab87be5433e1fbc0e953f28ef373d1f68f9687f2	алгоритмы комплексного анализа русских поэтических текстов с целью автоматизации процесса создания метрических справочников и конкордансов (the algorithms of complex analysis of russian poetic texts for the purpose of automation of the process of creation of metric reference books and concordances)			automation;bible concordance	Vladimir Barakhnin;Olga Kozhemyakina;Alexey Zabaykin	2015			automation;poetry;natural language processing;artificial intelligence;computer science	Robotics	-31.473432438427505	-77.69134097147557	121065
288ef622fe53317f92ddb66677a7a0b691719c38	creating the open wordnet bahasa	conference paper	This paper outlines the creation of the Wordnet Bahasa as a re source for the study of lexical semantics in the Malay language. It is created by c ombining information from several lexical resources: the French-English-Malay dict ionary FEM , the KAmus MelayuInggerisKAMI , and wordnets for English, French and Chinese. Constructio n went through three steps: (i) automatic building of word candidates; (ii ) evaluation and selection of acceptable candidates from merging of lexicons; (iii) final hand ch e k of the 5,000 core synsets. Our Wordnet Bahasa is only in the first phase of building a full fledged wordNet and needs to be further expanded, however it is already large enough to be useful for sense tagging both Malay and Indonesian.	finite element method;item unique identification;lexicon;synonym ring;wordnet	Nurril Hirfana Bte Mohamed Noor;Suerya Sapuan;Francis Bond	2011			natural language processing;wordnet;extended wordnet;speech recognition;computer science;linguistics	NLP	-28.975753205022766	-72.75892496051081	121168
5f0fe9ee48ff2d3fc48ea29d254e048fde43f2df	almost flat functional semantics for speech translation	open source speech translation;semantic element;flat feature;engineering solution;aff representation;argument semantics;central idea;novel semantic representation formalism;flat functional semantics;medium-vocabulary speech translation application;almost flat functional semantics;generic algorithm	We introduce a novel semantic representation formalism, Almost Flat Functional semantics (AFF), which is designed as an intelligent compromise between linguistically motivated predicate/argument semantics andad hocengineering solutions based on flat feature/value lists; the central idea is to tag each semantic element with the functional marking which most closely surrounds it. We argue that AFF is well-suited for medium-vocabulary speech translation applications, and describe simple and general algorithms for parsing, generating and performing transfer using AFF representations. The formalism has been fully implemented within a mediumvocabulary interlingua-based Open Source speech translation system which translates between English, French, Japanese and Arabic.	algorithm;automated flight following;backup;cobham's thesis;gulf of evaluation;item unique identification;machine translation;parsing;semantics (computer science);speech processing;vocabulary	Manny Rayner;Pierrette Bouillon;Beth Ann Hockey;Yukie Nakao	2008			natural language processing;speech recognition;genetic algorithm;computer science;artificial intelligence;linguistics	NLP	-28.49688579105119	-79.45206017952773	121322
003686417ab3b3c249369784a47b20fa0d7f18ff	english-spanish large statistical dictionary of inflectional forms		The paper presents an approach for constructing a weighted bilingual dictionary of inflectional forms using as input data a traditional bilingual dictionary, and not parallel corpora. An algorithm is developed that generates all possible morphological (inflectional) forms and weights them using information on distribution of corresponding grammar sets (grammar information) in large corpora for each language. The algorithm also takes into account the compatibility of grammar sets in a language pair; for example, verb in past tense in languageL normally is expected to be translated by verb in past tense in Language L. We consider that the developed method is universal, i.e. can be applied to any pair of languages. The obtained dictionary is freely available. It can be used in several NLP tasks, for example, statistical machine translation.	algorithm;bilingual dictionary;natural language processing;parallel text;statistical machine translation;text corpus	Grigori Sidorov;Alberto Barrón-Cedeño;Paolo Rosso	2010			direct method;natural language processing;speech recognition;machine-readable dictionary;computer science;linguistics	NLP	-25.969699355012654	-76.31357204907023	121425
f1dc3c6fe251b273722793255855b2049ebbf9e9	verb sense disambiguation based on thesaurus of predicate-argument structure - an evaluation of thesaurus of predicate-argument structure for japanese verbs		This paper presents a system for word sense disambiguation based on a manually constructed thesaurus of predicate-argument structure, which is an ontology on the linguistic side providing essential information for mapping form texts to verb concepts. This system can be effective for word sense disambiguation even though the target word sense system is different from the thesaurus. We applied the proposed word sense disambiguation system to the test corpus of SemEval-2010 Japanese tasks. Experimental results showed that the thesaurus-based disambiguation system outperformed a CRFs-based system in recall rates of verb sense disambiguation. From the results of verb sense disambiguation, we clarified that the abstracted verb classes (709 types) in our proposed system were effective sets for verb sense disambiguation.	approximation;conditional random field;knowledge engineering;lambda calculus;machine learning;semeval;sensitivity and specificity;thesaurus;type conversion;web services for devices;word sense;word-sense disambiguation	Koichi Takeuchi;Suguru Tsuchiyama;Masato Moriya;Yuuki Moriyasu;Koichi Satoh	2011				NLP	-26.1021579023711	-70.83959627280382	121601
a2c5b0d53df875acc7646f7eb7580e820695fdc8	semantic features and selection restrictions	expert system;russian word;bibliographic information;lexical database;existing dictionary;diverse information;semantic feature;selection restriction;essential aspect;semantic information;individual lexical entry	One of the essential aspects is described of an expert system (called LEXICOGRAPItER), designed to supply the user with diverse brformation about Russian words, h~cluding bibliographic hi formation concemhrg hrdividual lexical entries. Tire lexical database of tire system contahrs semantic btfonnation that catmot be elicited from the existhrg dictionaries. Tire priority is given to semantic features influenchtg lexical or grammatical co-occurrence restrictions. Possibilities are discussed of predicting selectional restrictions on the basis of semantic features of a word bz the lexicon. 1 . L E X I C A L D A T A B A S E O F T H E	dictionary;expert system;lexical database;lexicon	Elena V. Paducheva	1991			natural language processing;semantic computing;lexical item;computer science;linguistics;expert system;information retrieval	AI	-32.26425371246476	-71.42663100472627	121667
5356dd91cc6960f6cd0c303616723995c61f4cbf	gimme' the context: context-driven automatic semantic annotation with c-pankow	semantic annotation;web pages;metadata;information extraction;pattern generation;annotation;formal semantics;pattern matching;automatic annotation;semantic web;named entity	Without the proliferation of formal semantic annotations, the Semantic Web is certainly doomed to failure. In earlier work we presented a new paradigm to avoid this: the 'Self Annotating Web', in which globally available knowledge is used to annotate resources such as web pages. In particular, we presented a concrete method instantiating this paradigm, called PANKOW (Pattern-based ANnotation through Knowledge On the Web). In PANKOW, a named entity to be annotated is put into several linguistic patterns that convey competing semantic meanings. The patterns that are matched most often on the Web indicate the meaning of the named entity --- leading to automatic or semi-automatic annotation.In this paper we present C-PANKOW (Context-driven PANKOW), which alleviates several shortcomings of PANKOW. First, by downloading abstracts and processing them off-line, we avoid the generation of large number of linguistic patterns and correspondingly large number of Google queries.Second, by linguistically analyzing and normalizing the downloaded abstracts, we increase the coverage of our pattern matching mechanism and overcome several limitations of the earlier pattern generation process. Third, we use the annotation context in order to distinguish the significance of a pattern match for the given annotation task. Our experiments show that C-PANKOW inherits all the advantages of PANKOW (no training required etc.), but in addition it is far more efficient and effective.	download;experiment;named entity;online and offline;pattern matching;programming paradigm;semantic web;semiconductor industry;software testing controversies;web page;world wide web	Philipp Cimiano;Günter Ladwig;Steffen Staab	2005		10.1145/1060745.1060796	image retrieval;computer science;pattern matching;semantic web;social semantic web;formal semantics;web page;data mining;semantic web stack;database;metadata;world wide web;information extraction;information retrieval	Web+IR	-29.232139023539453	-67.41684682506289	121716
659bc19ce8a4ab44571d4ca809412c828edd18d4	selecting text spans for document summaries: heuristics and metrics	sentence extraction;selected works;text summarization;bepress	Human-quality text summarization systems are difficult to design, and even more difficult to evaluate, in part because documents can differ along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents an analysis of newsarticle summaries generated by sentence extraction. Sentences are ranked for potential inclusion in the summary using a weighted combination of linguistic features – derived from an analysis of news-wire summaries. This paper evaluates the relative effectiveness of these features. In order to do so, we discuss the construction of a large corpus of extractionbased summaries, and characterize the underlying degree of difficulty of summarization at different compression levels on articles in this corpus. Results on our feature set are presented after normalization by this degree of difficulty.	automatic summarization;document layout analysis;experiment;heuristic (computer science);mike lesser;redundancy (information theory);sentence extraction;text corpus	Vibhu O. Mittal;Mark Kantrowitz;Jade Goldstein-Stewart;Jaime G. Carbonell	1999			natural language processing;multi-document summarization;computer science;automatic summarization;data mining;information retrieval	AI	-27.88962861805927	-67.78458763247619	121719
4b327de343085c416486ddf57e20fa1fff315154	word sense induction using cluster ensemble		In this paper, we describe the implementation of an unsupervised learning method for Chinese word sense induction in CIPS-SIGHAN-2010 bakeoff. We present three individual clustering algorithms and the ensemble of them, and discuss in particular different approaches to represent text and select features. Our main system based on cluster ensemble achieves 79.33% in F-score, the best result of this WSI task. Our experiments also demonstrate the versatility and effectiveness of the proposed model on data sparseness problems.	algorithm;cluster analysis;experiment;f1 score;information;k-means clustering;neural coding;unsupervised learning;wafer-scale integration;word sense;word-sense induction	Bichuan Zhang;Jiashen Sun	2010			word-sense induction;artificial intelligence;pattern recognition;mathematics	NLP	-24.414939516611255	-68.40287257457037	121785
8f4552ebb2be75f408678bbf402e1c9dc06fcf5d	an entailment-based question answering system over semantic web data	semantic web;textual entailment;ontology;question answering	This paper reports a novel knowledge-based Question Answering (QA) method with the use of Semantic Web technologies and textual entailment recognition. Different from most of ontology-driven QA methods, this method does not perform deep question analysis to transform a natural language question into an ontology-compliant query for answer retrieval. Instead, it performs textual entailment recognition to discover the question template entailed by a user question from the whole machine-generated set and then takes the associated SPARQL query template to produce the complete query for retrieving the answers from the Semantic Web data that subscribe to the same ontology. An evaluation was carried out to assess the accuracy of the QA method, and the results revealed that the generated question templates can cover almost all the user questions and 65.6% of the user questions can be correctly answered with the support of a semantic entailment engine.		Shiyan Ou;Zhenyuan Zhu	2011		10.1007/978-3-642-24826-9_39	natural language processing;textual entailment;question answering;computer science;semantic web;ontology;data mining;information retrieval	Web+IR	-30.13149339084638	-66.48590713772293	121975
c513412236b19a372959e81b9704ce4bf36535e9	poster paper: hunpos - an open source trigram tagger		In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000). We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT’s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages. We present HunPos 1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.	brill tagger;hidden markov model;mathematical morphology;natural language processing;open-source software;part-of-speech tagging;trigram tagger	Péter Halácsy;András Kornai;Csaba Oravecz	2007				NLP	-29.951050692471515	-74.81903864714046	121990
143d335c20da5d65cce3177e35c1e22e7794f7cd	a system to adapt techniques of text summarizing to polish.	text summarization	This paper describes a system, in which various methods of text summarizing can be adapted to Polish. A structure of the system is presented. A modular construction of the system and access to the system via the Internet are signaled. Keywords—Automatic summary generation, linguistic analysis, text generation.	anaphora (linguistics);automatic summarization;heuristic (computer science);internet;modular design;natural language generation;prototype	Marcin Ciura;Damian Grund;Slawomir Kulików;Nina Suszczanska	2004			text graph;multi-document summarization;computer science;automatic summarization	NLP	-30.769685161858195	-76.83430776892723	122043
4d5f9a0aba65ba6294c543ba5e6108e6d690f133	intra-document structural frequency features for semi-supervised domain adaptation	document structure;information extraction;text mining;user preferences;km_information_extraction;km_statistical_techniques;semi structured;meta data;ir_content structured;social tagging;km_text_mining	In this work we try to bridge the gap often encountered by researchers who find themselves with few or no labeled examples from their desired target domain, yet still have access to large amounts of labeled data from other related, but distinct source domains, and seemingly no way to transfer knowledge from one to the other. Experimentally, we focus on the problem of extracting protein mentions from academic publications in the field of biology, where the source domain data are abstracts labeled with protein mentions, and the target domain data are wholly unlabeled captions. We mine the large number of such full text articles freely available on the Internet in order to supplement the limited amount of annotated data available. By exploiting the explicit and implicit common structure of the different subsections of these documents, including the unlabeled full text, we are able to generate robust features that are insensitive to changes in marginal and conditional distributions of classes and data across domains. We supplement these domain-insensitive features with automatically obtained high-confidence positive and negative predictions on the target domain to learn extractors that generalize well from one section of a document to another. Finally, lacking labeled target testing data, we employ comparative user preference studies to evaluate the relative performance of the proposed methods with respect to existing baselines.	domain adaptation;domain-specific language;experiment;internet;marginal model;semi-supervised learning;semiconductor industry	Andrew Arnold;William W. Cohen	2008		10.1145/1458082.1458253	natural language processing;text mining;computer science;document structure description;machine learning;data mining;database;metadata;world wide web;information extraction;information retrieval	Comp.	-30.220218085268662	-66.50391380934659	122086
7275c0749524e8463d4671a2f038cee181e95d1c	the prediction of ellipses using topic model for japanese colloquial inquiry text	gibbs sampling	Generally inquiries through Web forms and e-mails are increasing. These inquiry texts usually include many informal expressions use of the colloquial style and many omitted words. An omitted word causes the meaning of a sentence to become ambiguous and makes the reader misread and misunderstand a context. In this paper we propose a method to predict omitted words from context and knowledge using topic information. From the results of evaluation experiment, we have confirmed that some of our methods can predict omitted words at the accuracy rate more than 40% for the expression that we used in the experiment. c © 2013 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of KES International.	ambiguous grammar;email;experiment;form (html);informatics;lazy evaluation;topic model	Tomohiko Harada;Yoshikatsu Fujita;Kazuhiko Tsuda	2013		10.1016/j.procs.2013.09.219	natural language processing;computer science	NLP	-23.60843073794798	-66.57030874596538	122188
88e0b8ecaa35de0ec57e152fd7dbfa11cbc958b3	towards automatic semantic annotation of thai official correspondence: leave of absence case study		The realization of semantic web depended on the availability of web of data associated with knowledge and information in the real world. The first stage for web of data preparation is semantic annotation. However framing such manual semantic annotation is inappropriate for inexperienced users because they require specialist knowledge of ontology and syntax. To address this problem, this paper proposes an approach of automatic semantic annotation based on the integration between natural language processing techniques and semantic web technology. A case study on leave of absence correspondence in Thai language is chosen as the domain of interest. Our study shows that the proposed approach verified the effectiveness of semantic annotation.		Siraya Sitthisarn;Bukhoree Bahoh	2014		10.1007/978-3-319-06538-0_27	psychology;natural language processing;data mining;information retrieval	NLP	-30.493795423490788	-70.76886387958223	122208
3038a6de236dbc29f7698292af6f6157808c4769	comprehensive stemmer for morphologically rich urdu language				Mubashir Ali;Shehzad Khalid;Muhammad Saleemi	2019	Int. Arab J. Inf. Technol.			NLP	-30.36510985996034	-76.37060540287929	122237
56b1e7454776c33f21b4ca935b37c9cd094f36af	pivot-based topic models for low-resource lexicon extraction		This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-ofthe-art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an IcelandicKorean lexicon from Wikipedia.	baseline (configuration management);bilingual dictionary;lexicon;machine translation;microsoft word for mac;wikipedia	John Richardson;Toshiaki Nakazawa;Sadao Kurohashi	2015			topic model;natural language processing;lexicon;training set;artificial intelligence;computer science	NLP	-20.325137123950444	-74.7667915048838	122389
35c1668dc64d24a28c6041978e5fcca754eb2f4b	sequence level training with recurrent neural networks		Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.	bleu;baseline (configuration management);beam search;discrepancy function;greedy algorithm;language model;natural language processing;rouge (metric);recurrent neural network	Marc'Aurelio Ranzato;Sumit Chopra;Michael Auli;Wojciech Zaremba	2015	CoRR		computer science;artificial intelligence;machine learning;mathematics;algorithm	NLP	-19.135367526846874	-75.97541188307154	122462
5d5d6e72af4bdf8a6321083e20f7cd9bda07edd2	monotone string-to-string translation for nlu and asr tasks		Monotone string-to-string translation problems have to be tackled as part of almost all stateof-the-art natural language understanding and large vocabulary continuous speech recognition systems. In this work, two such tasks will be investigated in detail and improved using conditional random fields, namely concept tagging and grapheme-to-phoneme conversion. Concept tagging is usually one of the first modules within a dialogue or natural language understanding system. Here, the recognition result of a speech recognition system is augmented with task and domain dependent semantic information. Within this work, six different approaches are compared and evaluated on three different tasks in various languages on several levels. Considered are manual transcriptions versus speech recognition hypotheses as input as well as attribute name and attribute value level tags as output. By using an improved approach based on conditional random fields, the best results on all tasks and languages could be achieved. On the well-known French MEDIA task, conditional random fields lead to a concept error rate of 12.6% for attribute name and value extraction, which is a 35% relative improvement over the best published result within the MEDIA evaluation campaign in 2005 in the relaxed-simplified condition with 19.6%. The improvements over the classical conditional random fields-based approach as for example the introduction of a modified training criterion are discussed in detail. Additionally, recognizer output voting error reduction is applied as a system combination technique which could further reduce the concept error rate. A combination of rule-based and statistical attribute value extraction based on conditional random fields could be developed to improve over the standard rule-based baseline. The second monotone string-to-string translation task covers grapheme-to-phoneme conversion. Here, the pronunciation of a given word is derived automatically. With such a conversion module, it is possible to augment pronunciation dictionaries for speech recognition with e.g. named entities or other domain specific words, which might change over time. From a conceptual point, the main difference between this task and concept tagging is that an alignment between source and target side has to be modelled or given. In a first series of experiments, various state-of-the-art generative grapheme-to-phoneme conversion approaches are compared and evaluated on large pronunciation dictionaries in various languages. For the application of conditional random fields, a number of features and techniques to reduce computational complexity had to be implemented and derived. The alignment problem has been tackled by either using an external model or integrating a hidden variable within the conditional random fields training process. Using these modifications, state-of-theart accuracy results could be achieved on a couple of English pronunciation dictionaries. Additionally, state-of-the-art speech recognition systems have been trained using a graphemeto-phoneme conversion module based on hidden conditional random fields and compared with	attribute–value pair;baseline (configuration management);computational complexity theory;conditional random field;dictionary;experiment;finite-state machine;hidden variable theory;lambda calculus;logic programming;media evaluation;named entity;natural language understanding;speech recognition;vocabulary;whole earth 'lectronic link;monotone	Stefan Hahn	2015			discrete mathematics;monotone polygon;conditional random field;computer science	NLP	-22.21686771343355	-76.78068616658814	122598
2b9b71a4cfb5f369522b0ec547c7653ea4fb9f03	errors in inflection in czech as a second language and their automatic classification		When analyzing language acquisition of inflective languages like Czech, it is necessary to distinguish between errors in word stems and errors in inflection. We use the data of the learner corpus CzeSL, but we propose a simpler error classification based on levels of language description (orthography, morphonology, morphology, syntax, lexicon), which takes into account the uncertainty about the causes of the error. We present a rule-based automatic annotation tool, which can assist both the task of manual error classification and stochastic automatic error annotation with preliminary results of types of errors related to the language proficiency of the text authors.		Tomás Jelínek	2017		10.1007/978-3-319-64206-2_30	computer science;natural language processing;language acquisition;artificial intelligence;speech recognition;orthography;morphology (linguistics);syntax;czech;language proficiency;lexicon;inflection	NLP	-27.338904324343083	-76.29341714590215	122620
c0257fc14c3f0fb5268ba3f4d7c1dc51a709ccf6	classifying the wikipedia articles into the opencyc taxonomy		This article presents a method of classification of the Wikipedia articles into the taxonomy of OpenCyc. This method utilises several sources of the classification information, namely the Wikipedia category system, the infoboxes attached to the articles, the first sentences of the articles, treated as their definitions and the direct mapping between the articles and the Cyc symbols. The classification decision made using these methods are accommodated using the Cyc built-in inconsistency detection mechanism. The combination of the best classification methods yields 1.47 millions of classified articles and has a manually verified precision above 97%, while the combination of all of them yields 2.2 millions of articles with estimated precision of 93%.	algorithm;crostata;cyc;information extraction;sensor;wikipedia	Aleksander Pohl	2012				Web+IR	-28.883955905102123	-66.42915968800888	122737
2d87eaae51c66ab61b9497179462674277aec421	detection of inconsistencies in concept classifications in a large dictionary  toward an improvement of the edr electronic dictionary 		The EDR electronic dictionary is a machine-tractable dictionary developed for advanced computer-based processing of natural language. This dictionary comprises eleven sub-dictionaries, including a concept dictionary, word dictionaries, bilingual dictionaries, co-occurrence dictionaries, and a technical terminology dictionary. In this study, we focus on the concept dictionary and aim to revise the arrangement of concepts for improving the EDR electronic dictionary. We believe that unsuitable concepts in a class differ from other concepts in the same class from an abstract perspective. From this notion, we first try to automatically extract those concepts unsuited to the class. We then try semi-automatically to amend the concept explications used to explain the meanings to human users and rearrange them in suitable classes. In the experiment, we try to revise those concepts that are the lower-concepts of the concept “human” in the concept hierarchy and that are directly arranged under concepts with concept explications such as “person as defined by –” and “person viewed from –.” We analyze the result and evaluate our approach.	bilingual dictionary;bluetooth;cobham's thesis;jargon;natural language processing;semiconductor industry	Eiko Yamamoto;Kyoko Kanzaki;Hitoshi Isahara	2006				DB	-32.69958669913741	-71.02755755413867	122777
8687d957ffcbf881ee32cc73b68462105e38e85b	a supervised method of feature weighting for measuring semantic relatedness	word clustering;known technique;supervised method;related word;similar context;evaluation data;semantic relatedness;unsupervised feature weighting;supervised feature weighting;pointwise mutual information;similar meaning;natural language processing application	The clustering of related words is crucial for a variety of Natural Language Processing applications. Many known techniques of word clustering use the context of a word to determine its meaning. Words which frequently appear in similar contexts are assumed to have similar meanings. Word clustering usually applies the weighting of contexts, based on some measure of their importance. One of the most popular measures is Pointwise Mutual Information. It increases the weight of contexts where a word appears regularly but other words do not, and decreases the weight of contexts where many words may appear. Essentially, it is unsupervised feature weighting. We present a method of supervised feature weighting. It identifies contexts shared by pairs of words known to be semantically related or unrelated, and then uses Pointwise Mutual Information to weight these contexts on how well they indicate closely related words. We use Roget’s Thesaurus as a source of training and evaluation data. This work is as a step towards adding new terms to Roget’s Thesaurus automatically, and doing so with high confidence.	cluster analysis;natural language processing;pointwise mutual information;roget's thesaurus;semantic similarity;supervised learning;unsupervised learning	Alistair Kennedy;Stan Szpakowicz	2011		10.1007/978-3-642-21043-3_27	natural language processing;machine learning;pattern recognition	NLP	-24.56429682171994	-68.05605036968718	123020
c00f0d87d4e03bcdaffff4bc9e20fa80f4ee88f2	a psycholinguistic application of synergetic linguistics			synergetics (haken);synergy	Reinhard Köhler;Reinhard Rapp	2007	Glottometrics		natural language processing;linguistics;artificial intelligence;computer science	NLP	-31.107078554345858	-78.45355429441496	123029
329f8aa2b6868bdd3f61a263742f1e03163d4787	bilingual active learning for relation classification via pseudo parallel corpora		Active learning (AL) has been proven effective to reduce human annotation efforts in NLP. However, previous studies on AL are limited to applications in a single language. This paper proposes a bilingual active learning paradigm for relation classification, where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle. Instead of using a parallel corpus, labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora. Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning.	ace;active learning (machine learning);ibm basic assembly language and successors;natural language processing;parallel text;programming paradigm;sampling (signal processing);semi-supervised learning;semiconductor industry;statistical classification;supervised learning;text corpus	Longhua Qian;Haotian Hui;Yanan Hu;Guodong Zhou;Qiaoming Zhu	2014			semi-supervised learning;natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-22.360765765029	-72.57096459797377	123175
2c629ed8d032f14e1b73b3109e0735c5f8b80013	uncovering discourse relations to insert connectives between the sentences of an automatic summary		This paper presents a machine learning approach to find and classify discourse relations between two unseen sentences. It describes the process of training a classifier that aims to determine (i) if there is any discourse relation among two sentences, and, if a relation is found, (ii) which is that relation. The final goal of this task is to insert discourse connectives between sentences seeking to enhance text cohesion of a summary produced by an extractive summarization system for the Portuguese language.	discourse relation;logical connective;machine learning;statistical classification	Sara Silveira;António Branco	2014		10.1007/978-3-319-10888-9_26	natural language processing;computer science;linguistics;communication	NLP	-25.26985368542438	-72.59611465499665	123214
cce809d7b6aaa88ff3081e8c267c729949bb3977	an ers model for tense and aspect information in chinese sentences				Hsi-Jian Lee;Ren-Rong Hsu	1990				NLP	-30.143817861336736	-78.5223265652437	123467
0c60d47c18fd4a3e0fb9f7d9fabec5e84e262528	cwig3g2 - complex word identification task across three text genres and two user groups		Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI dataset (CWIG3G2) covering three text genres (NEWS, WIKINEWS, and WIKIPEDIA) annotated by both native and non-native English speakers. Unlike previous datasets, we cover single words, as well as complex phrases, and present them for judgment in a paragraph context. We present the first study on cross-genre and cross-group CWI, showing measurable influences in native language and genre types.	accessibility;blue (queue management algorithm);crowdsourcing;inter-rater reliability;performance;wikipedia	Seid Muhie Yimam;Sanja Stajner;Martin Riedl;Christian Biemann	2017			computer science;natural language processing;artificial intelligence	NLP	-26.859217240470446	-73.96983627964451	123813
8b6f78a0fb57a1861d9828c209d85bc65d2a59f6	a hybrid two-stage approach for discipline-independent canonical representation extraction from references	style free reference metadata extraction;surface representation;metadata extraction;empirical evidence;multi class classification;knowledge acquisition;canonical representation extraction;reverse engineering;canonical representation	In education and research, references play a key role. However, extracting and parsing references are difficult problems. One concern is that there are many styles of references; hence, given a surface form, identifying what style was employed is problematic, especially in heterogeneous collections of theses and dissertations, which cover many fields and disciplines, and where different styles may be used even in the same publication. We address these problems by drawing upon suitable knowledge found in the WWW. In particular, we research a two-stage classifier approach, involving multi-class classification with respect to reference styles, and partially solve the problem of parsing surface representations of references. We describe empirical evidence for the effectiveness of our approach and plans for improvement of our methods.	multiclass classification;parsing;www	Sung Hee Park;Roger W. Ehrich;Edward A. Fox	2012		10.1145/2232817.2232871	natural language processing;canonical form;empirical evidence;computer science;theoretical computer science;multiclass classification;data mining;world wide web;reverse engineering	NLP	-31.50735938222071	-71.8703903277878	123908
5d4b42a0ca3a0a52bda138fe95b6081843cfe061	automatic identification of biblical quotations in hebrew-aramaic documents		Quotations in a text document contain important information about the content, the context, the sources that the author uses, their importance and impact. Therefore, automatic identification of quotations from documents is an important task. Quotations included in rabbinic literature are difficult to identify and to extract for various reasons. The aim of this research is to automatically identify Biblical quotations included in rabbinic documents written in Hebrew-Aramaic. We deal with various kinds of quotations: partial, missing and incorrect. We formulate nineteen features to identify these quotations. These features were divided into seven different feature sets: matches, best matches, sums of weights, weighted averages, weighted medians, common words, and quotation indicators. Several features are novel. Experiments on various combinations of these features were performed using four common machine learning methods. A combination of 17 features using J48 (an improved version of C4.5) achieves an accuracy of 91.2%, which is an improvement of about 8% compared to a baseline result.	automatic identification and data capture;baseline (configuration management);c4.5 algorithm;experiment;machine learning;mathematical morphology	Yaakov HaCohen-Kerner;Nadav Schweitzer;Yaakov Shoham	2010			computer science;data mining;linguistics;hebrew	NLP	-25.384926298138573	-68.3939318084897	123919
391335ec19c054ae2dca8647fc34a8465e4d09cf	the dcu-ictcas mt system at wmt 2014 on german-english translation task		This paper describes the DCU submission to WMT 2014 on German-English translation task. Our system uses phrasebased translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation. Our final submission is the result of system combination on several systems which have different pre-processing and alignments.	interpolation;language model;moses;preprocessor;sequence alignment	Liangyou Li;Xiaofeng Wu;Santiago Cortes Vaíllo;Jun Xie;Andy Way;Qun Liu	2014			natural language processing;speech recognition;computer science;theoretical computer science	NLP	-23.144558111899293	-77.11878363666752	123960
21af6984c72c0a774b9a52412259c3244c04d09f	annotating educational questions for student response analysis		Questions play an important role in the educational domain, representing the main form of interaction between instructors and students. In this paper, we introduce the first taxonomy and annotated educational corpus of questions that aims to help with the analysis of student responses. The dataset can be employed in approaches that classify questions based on the expected answer types. This can be an important component in applications that require prior knowledge about the desired answer to a given question, such as educational and question answering systems. To demonstrate the applicability and the effectiveness of the data within approaches to classify questions based on expected answer types, we performed extensive experiments on our dataset using a neural network with word embeddings as features. The approach achieved a weighted F1-score of 0.511, overcoming the baseline by 12%. This demonstrates that our corpus can be effectively integrated in simple approaches that classify questions based on the response type.	artificial neural network;baseline (configuration management);experiment;f1 score;question answering;text corpus;word embedding	Andreea Godea;Rodney Nielsen	2018			natural language processing;artificial intelligence;response analysis;computer science	NLP	-26.722250499161678	-69.9914060108678	124195
c82474a064fce7e5f9e768053e94b7fd8c6b018d	towards tailored semantic annotation systems from wikipedia	semantic annotation;wikipedia;information sources;tailored;semantics;text analysis;annotation;internet encyclopedias electronic publishing ash semantics context;external source;internet;knowledge acquisition;natural language;fragment annotation external source wikipedia context tailored;web sites;ash;electronic publishing;encyclopedias;natural language semantic annotation system wikipedia text annotation;natural language processing;context;web sites knowledge acquisition natural language processing text analysis;fragment	The annotation of texts in natural language links some terms of the text to an external information source that gives us more detailed information about them. Most of the approaches made in this field get any text and annotate it by trying to find out the context of each term, as there are terms that have different meanings depending on the topic treated. In this article, we propose a variant of this process that annotates a text knowing in advance its context. The external source of information used is Wikipedia and we extract and use a fragment of it that embraces all the terms related to the context known beforehand.	information source;natural language;wikipedia	Shahad Kudama;Rafael Berlanga Llavori;Lisette García-Moya;Victoria Nebot;María José Aramburu Cabo	2011	2011 22nd International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2011.82	natural language processing;the internet;computer science;brand;semantics;electronic publishing;natural language;world wide web;information retrieval;encyclopedia	NLP	-30.574330406286748	-67.33157035560923	124488
20a74b68e112d16b5d323047263775899d83aa55	constraint grammar-based swedish-danish machine translation		This paper describes and evaluates a grammar-based machine translation system for the Swedish-Danish language pair. Source-language structural analysis, polysemy resolution, syntactic movement rules and target- language agreement are based on Constraint Grammar morphosyntactic tags and dependency trees. Lexical transfer rules exploit dependency links to access contextual information, such as syntactic argument function, semantic type and quantifiers, or to integrate verbal features, e.g. diathesis and auxiliaries. Out-of- vocabulary words are handled by derivational and compound analysis with a combined coverage of 99.3%, as well as systematic morpho-phonemic transliterations for the remaining cases. The system achieved BLEU scores of 0.65-0.8 depending on references and outperformed both STMT and RBMT competitors by a large margin.	constraint grammar;machine translation	Eckhard Bick	2014		10.1007/978-3-319-10888-9_23	natural language processing;speech recognition;computer science;linguistics;rule-based machine translation;constraint grammar	NLP	-24.163582168709898	-75.29355664948409	124711
51e1b3b5ca89048e4a442926f2dfdd7e0f65ed87	psycholinguistic models of sentence processing improve sentence readability ranking		While previous research on readability has typically focused on document-level measures, recent work in areas such as natural language generation has pointed out the need of sentence-level readability measures. Much of psycholinguistics has focused for many years on processing measures that provide difficulty estimates on a word-by-word basis. However, these psycholinguistic measures have not yet been tested on sentence readability ranking tasks. In this paper, we use four psycholinguistic measures: idea density, surprisal, integration cost, and embedding depth to test whether these features are predictive of readability levels. We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document-level readability metric baseline.	baseline (configuration management);linear model;natural language generation;self-information;text corpus	Vera Demberg;David M. Howcroft	2017			natural language processing;readability;artificial intelligence;computer science;speech recognition;sentence processing;sentence;ranking	NLP	-21.90270225210866	-73.51414657493243	124821
e1fb455b81ee1bd4dd37d8b23de32d5fecc730a2	towards a treebank of spoken french (vers un treebank du français parlé) [in french]		Towards a treebank of spoken French We present the first results of an attempt to build a spoken treebank for French. It has been conducted as part of the ANR project Etape (resp. G. Gravier). Contrary to other languages such as English (see the Switchboard treebank (Meteer, 1995)), there is no sizable spoken corpus for French annotated for syntactic constituents and grammatical functions. Our project is to build such a resource which will be a natural extension of the Paris 7 treebank (FTB : (Abeillé et al., 2003))) for written French, in order to be able to compare with similar annotations written and spoken French. We have reused and adapted the parser (Petrov et al., 2006) which has been trained on the written treebank, with manual correction and validation. The first results are promising. MOTS-CLÉS : Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique.	automatic number plate recognition;telephone switchboard;treebank	Anne Abeillé;Benoît Crabbé	2013				NLP	-28.12491831326497	-77.99363749793095	124953
64c3c92044f67f58af2e9bfc55b75e5c3ca715c6	transliteration extraction from classical chinese buddhist literature using conditional random fields	conference paper	Extracting plausible transliterations from historical literature is a key issues in historical linguistics and other resaech fields. In Chinese historical literature, the characters used to transliterate the same loanword may vary because of different translation eras or different Chinese language preferences among translators. To assist historical linguiatics and digial humanity researchers, this paper propose a transliteration extraction method based on the conditional random field method with the features based on the characteristics of the Chinese characters used in transliterations which are suitable to identify transliteration characters. To evaluate our method, we compiled an evaluation set from the two Buddhist texts, the Samyuktagama and the Lotus Sutra. We also construct a baseline approach with suffix array based extraction method and phonetic similarity measurement. Our method outperforms the baseline approach a lot and the recall of our method achieves 0.9561 and the precision is 0.9444. The results show our method is very effective to extract transliterations in classical Chinese texts.	baseline (configuration management);compiler;conditional random field;lotus 1-2-3;suffix array	Yu-Chun Wang;Richard Tzong-Han Tsai	2013	IJCLCLP		natural language processing;speech recognition;computer science;traditional medicine	NLP	-23.76391704832713	-77.5898411577747	125036
76da2ea9f32c31a2685bddbf5188566ce726d068	feasibility of enriching a chinese synonym dictionary with a synchronous chinese corpus	lenguaje natural;news;linguistique;langage naturel;semantics;chino;semantica;semantique;classification;feasibility;dictionnaire;linguistica;natural language;dictionaries;noticias;chinois;chinese;actualites;diccionario;clasificacion;practicabilidad;faisabilite;linguistics	This paper reports on a first step toward the construction of a PanChinese lexical resource. We investigated the plausibility of extending and enhancing an existing Chinese synonym dictionary, the Tongyici Cilin, with lexical items from the financial news domain obtained from a synchronous Chinese corpus, LIVAC. Results showed that 23-40% of the words from various subcorpora are unique to the individual communities, and as much as 70% of such unique items are not yet covered in Cilin. Our next step would be to explore automatic means for extracting related lexical items from the corpus, and to incorporate them into existing semantic classifications.	dictionary;livac;lexicon;plausibility structure;text corpus	Oi Yee Kwong;Benjamin Ka-Yin T'sou	2006		10.1007/11816508_33	natural language processing;speech recognition;news;computer science;semantics;natural language;chinese	NLP	-27.817650197065245	-77.03167577941697	125116
ff2ef289e6e30c0fff85d96cd96c79b61aeacd14	a quality-based active sample selection strategy for statistical machine translation		This paper presents a new active learning technique for machine translation based on quality estimation of automatically translated sentences. It uses an error-driven strategy, i.e., it assumes that the more errors an automatically translated sentence contains, the more informative it is for the translation system. Our approach is based on a quality estimation technique which involves a wider range of features of the source text, automatic translation, and machine translation system compared to previous work. In addition, we enhance the machine translation system training data with post-edited machine translations of the sentences selected, instead of simulating this using previously created reference translations. We found that re-training systems with additional post-edited data yields higher quality translations regardless of the selection strategy used. We relate this to the fact that post-editions tend to be closer to source sentences as compared to references, making the rule extraction process more reliable.	information;rule induction;simulation;statistical machine translation	Varvara Logacheva;Lucia Specia	2014			data science;machine learning;pattern recognition	NLP	-19.71627052109842	-77.14635095185768	125175
64d02eacda1d74f0c929dea1666081fc7f2b0787	detecting non-compositional mwe components using wiktionary		We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary. The approach makes use of the definitions, synonyms and translations in Wiktionary, and is applicable to any type of MWE in any language, assuming the MWE is contained in Wiktionary. Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods.	experiment;f1 score;minimal working example;self-replicating machine;sensor;supervised learning;the australian;unsupervised learning	Bahar Salehi;Paul Cook;Timothy Baldwin	2014			natural language processing;speech recognition;computer science;data mining	NLP	-23.728099447120577	-73.76456199275803	125198
795a568299328465e1b7024b6df1574179d8eebb	反向異文字音譯相似度評量方法與跨語言資訊檢索 (similarity measure in backward transliteration between different character sets and its application to clir) [in chinese]		This paper classifies the problem of machine transliteration into four types, i.e., forward/backward transliteration between same/different character sets, based on transliteration direction and character sets. A phoneme-based similarity measure is proposed to deal with backward transliteration between different character sets. Chinese-English information retrieval is taken as an example. The experiments show that phoneme-based approach is better than grapheme-based approach. In a mate matching of 1,261 candidates, the average rank is 7.80 and 57.65% of candidates are ranked as number one.	character encoding;cross-language information retrieval;experiment;similarity measure	Wei-Hao Lin;Hsin-Hsi Chen	2000			character encoding;natural language processing;transliteration;similarity measure;computer science;artificial intelligence	NLP	-26.68266927998987	-67.39582189579504	125284
30b3efe82d97b6974c0a11d0750d994723826954	using wikipedia to validate the terminology found in a corpus of basic textbooks		A scientific vocabulary is a set of terms that designate scientific concepts. This set of lexical units can be used in several applications ranging from the development of terminological dictionaries and machine translation systems to the development of lexical databases and beyond. Even though automatic term recognition systems exist since the 80s, this process is still mainly done by hand, since it generally yields more accurate results, although not in less time and at a higher cost. Some of the reasons for this are the fairly low precision and recall results obtained, the domain dependence of existing tools and the lack of available semantic knowledge needed to validate these results. In this paper we present a method that uses Wikipedia as a semantic knowledge resource, to validate term candidates from a set of scientific text books used in the last three years of high school for mathematics, health education and ecology. The proposed method may be applied to any domain or language (assuming there is a minimal coverage by Wikipedia).	book;database;dictionary;ecology;machine translation;precision and recall;terminology extraction;vocabulary;wikipedia	Jorge Vivaldi;Luis Adrián Cabrera-Diego;Gerardo Sierra;María Pozzi	2012			natural language processing;computer science;data science;linguistics	NLP	-29.686174088218934	-72.27038261932498	125326
239f2ea3fb70a3dc827602a0dfc42ea853584e49	sentence compression for arbitrary languages via multilingual pivoting		In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 MOSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.	domain adaptation;experiment;map overlay and statistical system;neural machine translation;reinforcement learning;text corpus	Jonathan Mallinson;Rico Sennrich;Mirella Lapata	2018			natural language processing;artificial intelligence;computer science;compression (physics);sentence	NLP	-19.47620927422348	-76.16538897697832	125410
678da3c8e75d7934cbb19dd81f96bb799f687188	webmt: developing and validating an example-based machine translation system using the world wide web	memoire;parallel corpus;computacion informatica;rule based;performance;filologias;grupo de excelencia;linguistique appliquee;linguistique de corpus;linguistic resources;linguistica;traduction automatique;internet;anglais;evaluative study;francais;ciencias basicas y experimentales;ressources linguistiques;machine translating;world wide web;corpus linguistics;english;computational linguistics;grupo a;example based machine translation;example;linguistique informatique;extraction;machine translation;memory;etude evaluative;applied linguistics;corpus parallele;exemple	We have developed an example-based machine translation (EBMT) system that uses the World Wide Web for two different purposes: First, we populate the system's memory with translations gathered from rule-based MT systems located on the Web. The source strings input to these systems were extracted automatically from an extremely small subset of the rule types in the Penn-II Treebank. In subsequent stages, the source, target translation pairs obtained are automatically transformed into a series of resources that render the translation process more successful. Despite the fact that the output from on-line MT systems is often faulty, we demonstrate in a number of experiments that when used to seed the memories of an EBMT system, they can in fact prove useful in generating translations of high quality in a robust fashion. In addition, we demonstrate the relative gain of EBMT in comparison to on-line systems. Second, despite the perception that the documents available on the Web are of questionable quality, we demonstrate in contrast that such resources are extremely useful in automatically postediting translation candidates proposed by our system.	batch processing;bell test experiments;compiler;computation;computational linguistics;database;display resolution;dummy variable (statistics);evaluation of machine translation;example-based machine translation;experiment;hoc (programming language);interaction;language model;lazy evaluation;lexicon;linear algebra;logic programming;n-gram;no symbol;online and offline;population;postediting;string (computer science);sublanguage;test set;translation memory;treebank;verification and validation;word error rate;world wide web	Andy Way;Nano Gough	2003	Computational Linguistics	10.1162/089120103322711596	natural language processing;extraction;speech recognition;transfer-based machine translation;performance;computer science;computational linguistics;english;applied linguistics;corpus linguistics;linguistics;machine translation;rule-based machine translation;memory	NLP	-27.49641143594745	-78.28343959946936	125562
f81dbf3dedd50b7680727f1656578a7318e9ec6e	lexical simplification with neural ranking		We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest Accuracy, Precision and F1 scores to date in standard datasets for the task.	error analysis (mathematics);least squares;lexical simplification;natural language processing;neural networks;semeval;text simplification;word embedding	Lucia Specia;Gustavo Paetzold	2017			artificial intelligence;natural language processing;computer science;machine learning;lexical simplification;ranking	NLP	-21.402240732612196	-72.94906014799041	125594
2c19f077711a841e56eef47f4efc12bd35d5bacb	incorporating pronunciation variation into different strategies of term transliteration	noun;out of vocabulary;spoken document retrieval;indexing and retrieval;conference paper	Term transliteration addresses the problem of converting terms in one language into their phonetic equivalents in the other language via spoken form. It is especially concerned with proper nouns, such as personal names, place names and organization names. Pronunciation variation refers to pronunciation ambiguity frequently encountered in spoken language, which has a serious impact on term transliteration. More than one transliteration variants can be generated by an out-of-vocabulary term due to different kinds of pronunciation variations. It is important to take this issue into account when dealing with term transliteration. Several models, which take pronunciation variation into consideration, are proposed for term transliteration in this paper. They describe transliteration from various viewpoints and utilize the relationships trained from extracted transliterated-term pairs. An experiment in applying the proposed models to term transliteration was conducted and evaluated. The experimental results show promise. These proposed models are not only applicable to term transliteration, but also are helpful in indexing and retrieving spoken document retrieval.	algorithm;document retrieval;experiment;naivety;performance;vocabulary	Jin-Shea Kuo;Ying-Kuei Yang	2004			natural language processing;speech recognition;linguistics	NLP	-22.28959722413596	-79.96316215784697	125674
05d10fbd14e6d82deab2612ed803b989e8732c08	a linkset quality metric measuring multilingual gain in skos thesauri		Linked Data is largely adopted to share and make data more accessible on the web. A quite impressive number of datasets has been exposed and interlinked according to the Linked Data paradigm but the quality of these datasets is still a big challenge in the consuming process. Measures for quality of Linked Data datasets have been proposed, mainly by adapting concepts defined in the research field of information systems. However, very limited attention has been dedicated to the quality of linksets, the connections of information belonging to distinct datasets, that might be as important as dataset’s quality when consuming Linked Data. In this paper, we present a first linkset quality measure proposing a function able to estimate the new information gained through linksets among SKOS thesauri. A scoring function, the linkset importing is provided focusing on the multilingual gain, in terms of the new translated labels, obtained by complementing a SKOS thesaurus through skos:exactMatch links. We finally discuss how the linkset importing can be significantly used in the context of the EU project eENVplus.	information system;linked data;programming paradigm;scoring functions for docking;simple knowledge organization system;thesaurus	Riccardo Albertoni;Monica De Martino;Paola Podestà	2015			data mining;simple knowledge organization system;linked data;information system;computer vision;artificial intelligence;computer science	AI	-32.36624337530057	-66.76881278054638	125708
23f284aba0dfb725fcaeb88ae146866770137d51	opinion mining by transformation-based domain adaptation	opinion mining;model transformation;natural language processing	Here we propose a novel approach for the task of domain adaptation for Natural Language Processing. Our approach captures relations between the source and target domains by applying a model transformation mechanism which can be learnt by using labeled data of limited size taken from the target domain. Experimental results on several Opinion Mining datasets show that our approach significantly outperforms baselines and published systems when the amount of labeled data is extremely small.	algorithm;baseline (configuration management);domain adaptation;experiment;model transformation;natural language processing;open-source software;privacy;source data	Róbert Ormándi;István Hegedüs;Richárd Farkas	2010		10.1007/978-3-642-15760-8_21	natural language processing;computer science;data science;data mining;sentiment analysis	NLP	-19.393634459892258	-66.60684046809145	125850
a67f083830790586ed41823d45a7b330d0a2fd95	roget's thesaurus and semantic similarity	semantic similarity;noun;similarity measure	Roget’s Thesaurus has not been sufficiently appreciated in Natural Language Processing. We show that Roget's and WordNet are birds of a feather. In a few typical tests, we compare how the two resources help measure semantic similarity. One of the benchmarks is Miller and Charles’ list of 30 noun pairs to which human judges had assigned similarity measures. We correlate these measures with those computed by several NLP systems. The 30 pairs can be traced back to Rubenstein and Goodenough’s 65 pairs, which we have also studied. Our Roget’sbased system gets correlations of .878 for the smaller and .818 for the larger list of noun pairs; this is quite close to the .885 that Resnik obtained when he employed humans to replicate the Miller and Charles experiment. We further evaluate our measure by using Roget’s and WordNet to answer 80 TOEFL, 50 ESL and 300 Reader’s Digest questions: the correct synonym must be selected amongst a group of four words. Our system gets 78.75%, 82.00% and 74.33% of the questions respectively, better than any published results.	digest access authentication;humans;natural language processing;roget's thesaurus;self-replicating machine;semantic similarity;wordnet	Mario Jarmasz;Stan Szpakowicz	2003			noun;semantic similarity;computer science;linguistics	NLP	-26.747467310761596	-68.64454307315887	125919
3f5778fa82a00cb0405b57b35372f0ec5f3d39cf	atlas – the multilingual language processing platform	online services;servicios en linea;computacion informatica;sistema de gestion de contenidos;filologias;uima;herramientas linguisticas;language resources;info eu repo semantics article;recursos linguisticos;informacion documentacion;linguistica;ciencias basicas y experimentales;servicios web;web services;content management system;linguistic tools;grupo a;ciencias sociales;grupo b	This paper presents the ATLAS platform – multilingual language processing framework integrating the common set of linguistic tools for a group of European languages (less-resourced: Bulgarian, Croatian, Greek, Polish and Romanian together with English and German as reference languages). State-of-the-art NLP functionality offered by the platform allows for multilingual annotation of texts on lower levels (segmentation, morphosyntax) which in turn supports higher-level processing such as automated categorization, information extraction, machine translation or summarization. More elaborate annotation methods such as named entity extraction or multiword unit lemmatization are also available. Multilevel annotation of texts is governed by language processing chains constructed with UIMA (Unstructured Information Management Application) industry standard. To demonstrate capabilities of the framework, three linguistically-aware online services have been built on top of it: i-Publisher (Web-based content management platform), i-Librarian (a digital library of scientific works) and EUDocLib (site for browsing and searching through EUR-LEX documents).	atlas;automatic summarization;categorization;digital library;e-services;in the beginning... was the command line;information extraction;information management;lemmatisation;librarian;machine translation;named entity;named-entity recognition;natural language processing;scientific literature;technical standard;web content;world wide web	Maciej Ogrodniczuk;Diman Karagiozov	2011	Procesamiento del Lenguaje Natural		web service;world wide web	NLP	-32.910209628689685	-72.44218651006288	125972
77eb955b0023ea688d5ad9e9fe0f4164a58ff063	ordbøger i danmark: datamatstøttet leksikografi i praksis (dictionaries in denmark: computer-assisted lexicography in practice) [in danish]			dictionary;lexicography	Hanne Ruus;Dorthe Duncker	1988			linguistics;danish;lexicography;history	Crypto	-31.82627175113072	-78.37756427024459	125974
9bf0f490c8c6d3864e3773b45f49db086695475a	revisiting again document length hypotheses trec 2004 genomics track experiments at patolis	locuslink;medline;reference database feedback;support vector machines.;pseudo-relevance feedback;language modeling for information retrieval;mesh;document length;information retrieval;support vector machine;leave one out cross validation;language model	The TREC-2004 Genomics track evaluation experiments at Patolis Corporation are described with a focus on the document length issues in different retrieval models such as TF*IDF or probabilistic language modeling approaches. In the genomics ad hoc retrieval task, combination of pseudo-relevance feedback and reference database feedback is applied. For the triage sub-task, we trained a SVM classifier using leave-one-out-cross-validation, and calibrated parameters to be optimal against the training set.	bibliographic database;binary classification;cross-validation (statistics);dirichlet kernel;experiment;hoc (programming language);kl-one;kullback–leibler divergence;language model;relevance feedback;smoothing;software bug;test set;tf–idf	Sumio Fujita	2004				NLP	-23.267613112398585	-70.76614989015516	126127
b9eb2ae66dbb555bb9a8d1263f140924c67438b0	temporal text classification for romanian novels set in the past		In this paper we look at a task in historical linguistics and the study of language development, namely that of identifying the time when a text was written. The novelty is that we evaluate our classifier and our selected features on literary texts having their action placed in the past and written so as to give off the impression of the respective epoch. We investigate several types of features and ultimately go with a very simple set of 10 features which very accurately classifies the texts based on the century they were actually written in. We use random forests to obtain high performance.	document classification;epoch (reference date);random forest;simple set	Alina Maria Ciobanu;Liviu P. Dinu;Octavia-Maria Sulea;Anca Dinu;Vlad Niculae	2013			natural language processing;computer science;linguistics;literature	NLP	-31.015299936954136	-74.1664445346711	126207
6b3d701d24325235153b11bd96211a214b5ae7e5	lexical similarity of information type hypernyms, meronyms and synonyms in privacy policies		Privacy policies are used to communicate company data practices to consumers and must be accurate and comprehensive. Each policy author is free to use their own nomenclature when describing data practices, which leads to different ways in which similar information types are described across policies. A formal ontology can help policy authors, users and regulators consistently check how data practice descriptions relate to other interpretations of information types. In this paper, we describe an empirical method for manually constructing an information type ontology from privacy policies. The method consists of seven heuristics that explain how to infer hypernym, meronym and synonym relationships from information type phrases, which we discovered using grounded analysis of five privacy policies. The method was evaluated on 50 mobile privacy policies which produced an ontology consisting of 355 unique information type names. Based on the manual results, we describe an automated technique consisting of 14 reusable semantic rules to extract hypernymy, meronymy, and synonymy relations from information type phrases. The technique was evaluated on the manually constructed ontology to yield .95 precision and .51 recall.	formal ontology;heuristic (computer science);information theory;privacy policy	Mitra Bokaei Hosseini;Sudarshan Wadkar;Travis D. Breaux;Jianwei Niu	2016			lexical similarity;information retrieval;natural language processing;computer science;privacy policy;synonym;artificial intelligence	SE	-28.454948445492388	-70.6130869182101	126215
2cf125ac67385d8ef20d4674701e8482874dd5f1	something borrowed, something blue: rule-based combination of pos taggers	sprakteknologi sprakvetenskaplig databehandling;rule based;part of speech;language technology computational linguistics;off the shelf	Linguistically annotated text resources are still scarce for many languages and for many text types, mainly because their creation represents a major investment of work and time. For this reason, it is worthwhile to investigate ways of reusing existing resources in novel ways. In this paper, we investigate how off-the-shelf part of speech (POS) taggers can be combined to better cope with text material of a type on which they were not trained, and for which there are no readily available training corpora. We indicate—using freely available taggers for German (although the method we describe is not language-dependent)—how such taggers can be combined by using linguistically motivated rules so that the tagging accuracy of the combination exceeds that of the best of the individual taggers.	linear algebra;text corpus	Lars Borin	2000			rule-based system;natural language processing;speech recognition;part of speech;computer science;artificial intelligence;linguistics	NLP	-29.559790069191745	-73.66977815527184	126247
c80df4270863d522a12539bd4667c40df3059ef4	question answering using statistical language modelling	language modelling;question answering	In this paper we present a statistical approach to question answering (QA). Our motivation is to build robust systems for many languages without the need for highly tuned linguistic modules. Consequently, word tokens and web data are used extensively but neither explicit linguistic knowledge nor annotated data is incorporated. A mathematical model for answer retrieval and answer classification is derived. Experiments are conducted by searching for answers in the AQUAINT corpus, as well as in web data. The redundancy inherent in web data outperforms retrieval from a fixed corpus, where there are typically relatively few answer occurrences for any given question. We participated with an implementation of this framework in the TREC 2006 QA evaluations, where we ranked 9th among 27 participants on the factoid task.		Matthias H. Heie;Edward W. D. Whittaker;Sadaoki Furui	2012	Computer Speech & Language	10.1016/j.csl.2011.11.001	natural language processing;question answering;computer science;data mining;information retrieval	NLP	-27.53046621108172	-69.19306405868647	126325
f7e69faaac792660b519a7de66e6a587b5cfac8a	graph based decoding for event sequencing and coreference resolution		Events in text documents are interrelated in complex ways. In this paper, we study two types of relation: Event Coreference and Event Sequencing. We show that the popular tree-like decoding structure for automated Event Coreference is not suitable for Event Sequencing. To this end, we propose a graph-based decoding algorithm that is applicable to both tasks. The new decoding algorithm supports flexible feature sets for both tasks. Empirically, our event coreference system has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference task and our event sequencing system beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations.	algorithm;baseline (configuration management);computation;natural language processing	Zhengzhong Liu;Teruko Mitamura;Eduard H. Hovy	2018			artificial intelligence;machine learning;decoding methods;coreference;computer science;pattern recognition;graph	NLP	-21.406993161327563	-73.7630495025037	126390
b2eb3491729cc19a8c92a98f6d9c83c6720d5899	fine-grained semantic textual similarity for serbian		Although the task of semantic textual similarity (STS) has gained in prominence in the last few years, annotated STS datasets for model training and evaluation, particularly those with fine-grained similarity scores, remain scarce for languages other than English, and practically non-existent for minor ones. In this paper, we present the Serbian Semantic Textual Similarity News Corpus (STS.news.sr) – an STS dataset for Serbian that contains 1192 sentence pairs annotated with fine-grained semantic similarity scores. We describe the process of its creation and annotation, and we analyze and compare our corpus with the existing news-based STS datasets in English and other major languages. Several existing STS models are evaluated on the Serbian STS News Corpus, and a new supervised bag-of-words model that combines part-of-speech weighting with term frequency weighting is proposed and shown to outperform similar methods. Since Serbian is a morphologically rich language, the effect of various morphological normalization tools on STS model performances is considered as well. The Serbian STS News Corpus, the annotation tool and guidelines used in its creation, and the STS model framework used in the evaluation are all made publicly available.	bag-of-words model;performance;semantic similarity;text corpus;tf–idf	Vuk Batanovic;Milos Cvetanovic;Bosko Nikolic	2018			speech recognition;natural language processing;artificial intelligence;computer science;serbian	NLP	-26.811216835372058	-73.8287103539157	126681
410aeec0b03e210d418e5a80f8d2e8d8ebffcbbd	automatic extraction of morphological lexicons from morphologically annotated corpora		We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.	algorithm;baseline (configuration management);galaxy morphological classification;lexicon;text corpus;unsupervised learning	Ramy Eskander;Nizar Habash;Owen Rambow	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-23.183601836572407	-75.55984200143145	126695
d816130616695ee96116ad5634004f9d426a7e66	factrunner: fact extraction over wikipedia		The increasing role of Wikipedia as a source of human-readable knowledge is evident as it contains an enormous amount of high quality information written in natural language by human authors. However, querying this information using traditional keyword based approaches requires often a time-consuming, iterative process to explore the document collection to find the information of interest. Therefore, a structured representation of information and queries would be helpful to be able to directly query for the relevant information. An important challenge in this context is the extraction of structured information from unstructured knowledge bases which is addressed by Information Extraction (IE) systems. However, these systems struggle with the complexity of natural language and produce frequently unsatisfying results. In addition to the plain natural language text, Wikipedia contains links between documents which directly link a term of one document to another document. In our approach for fact extraction from Wikipedia, we consider these links as an important indicator for the relevance of the linked information. Thus, our proposed system FactRunner focusses on extracting structured information from sentences containing such links. We show that a natural language parser combined with Wikipedia markup can be exploited for extracting facts in form of triple statements with a high accuracy.	ambiguous grammar;archive;display resolution;entity;human-readable medium;information extraction;iteration;lemmatisation;level of detail;markup language;microsoft outlook for mac;natural language;parsing;preprocessor;relevance;semiconductor consolidation;wikipedia	Rhio Sutoyo;Christoph Quix;Fisnik Kastrati	2013			information retrieval	AI	-30.80549435031631	-67.45954887747575	126717
27c9f8bf8c1079c3e89afebdf4466d34eb044b55	comparing semantic relatedness between word pairs in portuguese using wikipedia		The growth of available data in digital format has been facilitating the development of new models to automatically infer the semantic similarity between word pairs. However, there are still many natural languages without sufficient resources to evaluate measures of semantic relatedness. In this paper we translated word pairs from a well-known baseline for evaluating semantic relatedness measures into Portuguese and performed a manual evaluation of each pair. We compared the correlation with similar datasets in other languages and generated LSA models from Wikipedia articles in order to verify the pertinence of each dataset and how semantic similarity conveys across languages.	semantic similarity;wikipedia	Roger Granada;Cássia Trojahn dos Santos;Renata Vieira	2014		10.1007/978-3-319-09761-9_17	natural language processing;linguistics;communication	NLP	-27.556675724816838	-71.57505016656678	126780
29981cd478a2726f80e6fc94bd7816e4b6b778d1	"""experimenting a """"general purpose"""" textual entailment learner in ave"""	settore inf 01 informatica;feature space;settore ing inf 05 sistemi di elaborazione delle informazioni;answer validation exercise;question answering	In this paper we present the use of a ”general purpose” textua l ent iment recognizer in the Answer Validation Exercise (AVE) task. Our system has been develop ed to learn entailment rules from annotated examples. The main idea of the system is the cross-pair simil irity measure we defined. This similarity allows us to define an implicit feature space using kernel fun ctio s in SVM learners. We experimented with our system using different training and testing sets: R TE data sets and AVE data sets. The comparative results show that entailment rules can be learned from data sets, e.g. RTE, that are different from AVE. Moreover, it seems that better results are obtained usi ng more controlled training data (the RTE set) that less controlled ones (the AVE development set). Al though, the high variability of the outcome prevents us to derive definitive conclusions, the results of our system show that our approach is quite promising and improvable in the future.	algorithm;cross-validation (statistics);experiment;feature vector;finite-state machine;machine learning;neural coding;spatial variability;speech recognition;test engineer;textual entailment	Fabio Massimo Zanzotto;Alessandro Moschitti	2006		10.1007/978-3-540-74999-8_61	natural language processing;question answering;feature vector;computer science;artificial intelligence;machine learning;data mining;database;programming language;algorithm	ML	-21.520623630962366	-71.4838235492654	126862
4e5a12ef339bd51fc187a83c6c23572ce4f5d3d1	integration of document detection and information extraction	document detection;extraction system;integrated detection;detection step;extraction step;high recall;advanced information extraction method;muc-6 extraction capability;low recall;phrase extraction	"""We have conducted a number of experiments to evaluate various modes of building an integrated detection/extraction system. The experiments were performed using SMART system as baseline. The goal was to determine if advanced information extraction methods can improve recall and precision of document detection. We identified the following two modes of integration: I. Extraction to Detection:broad-coverage extraction 1. Extraction step: identify concepts for indexing 2. Detection step 1: low recall, high initial precision 3. Detection step 2: automatic relevance feedback using top N retrieved documents to regain recall. I1. Detection to Extraction: query-specific extraction 1.Detection step 1: high recall, low precision run 2.Extraction step: learn concept(s) from query and retrieved subcollection 3.Detection step 2: re-rank the subcollection to increase precision Our integration effort concentrated on mode I, and the following issues: 1.use of shallow but fast NLP for phrase extractions and disambiguation in place of a full syntactic parser 2.use existing MUC-6 extraction capabilities to index a retrieval collection 3.mixed Boolean/soft match retrieval model 4.create a Universal Spotter algorithm for learning arbitrary concepts L E X l C O S E M A N T I C P A T T E R N M A T C H I N G FOR S H A L L O W NLP The lexico-semantic pattern matching method allows for capturing of word sequences in text using a simple pattern language that can be compiled into a set of non-deterministic finite automata. Each automoton represents a single rule within the language, with several related rules forming a package. As a result of matching a rule against the input, a series of variables within the rule are bound to lexical elements in text. These bindings are subsequently used to generate single-word and/or multiple-word terms for indexing. Long phrasal terms are decomposed into pairs in two phases as follows. In the first phase, only unambiguous pairs are collected, while all longer and potentially structurally ambiguous noun phrases are passed to the second phase. In the second phase, the distributional statistics gathered in the first phase are used to predict the strength of alternative two-word sub-components within long phrases. For example, we may have multiple unambiguous occurrences of """"insider trading"""", while very few of """"trading case"""". At the same time, there are numerous phrases such as =insider trading case"""", =insider trading legislation"""", etc., where the pair =insider trading"""" remains stable while the other elements get changed, and significantly fewer cases where, say, """"trading case"""" is constant and the other words change. The experiments performed on a subset of U.S. PTO's patent database show healthy 10%+ increase in average precision over baseline SMART system. The average precision (11-point) has increased from 49% SMART baseline on the test sample to 56%. Precision at 5 top retrieved documents jumped from 48% to 52%. We also noticed that phrase disambiguation step was critical for improved precision."""	algorithm;automata theory;baseline (configuration management);compiler;deterministic finite automaton;experiment;finite-state machine;information extraction;information retrieval;know-how trading;lexico;natural language processing;nondeterministic finite automaton;parsing;pattern language;pattern matching;precision and recall;relevance feedback;smart system;word-sense disambiguation	Louise Guthrie;Tomek Strzalkowski;Zhongjing Wang;Fang Lin	1996			relationship extraction;computer science;pattern recognition;data mining;information retrieval	NLP	-27.10625246716934	-70.2986330125101	127012
14d289678407d3a37bd5453fd89c0292f5552a13	the role of lexical resources in matching classification schemas		In this paper, we describe the role and the use of WORDNET as an external lexical resource in a methodology for matching hierarchical classification schemas. The main difference between our methodology and others which were presented is that we pay a lot of effort in eliciting the meaning of the structures we match, and we do this by using extensively lexical knowledge about the words occurring in labels. The result of this elicitation process is encoded in a formal language, called WDL (WORDNET Description Logic), which is our proposal for injecting lexical semantics into more standard knowledge representation languages.	description logic;formal language;knowledge representation and reasoning;wordnet	Paolo Bouquet;Luciano Serafini;Stefano Zanobini	2006			natural language processing;computer science;schema (psychology);artificial intelligence;pattern recognition	AI	-31.923922915080368	-69.97357255212434	127180
d1d2dcc2e39980ae8ec902a9a74ab9df29e72b09	detection and evaluation of cheating on college exams using supervised classification		Text mining has been used for various purposes, such as document classification and extraction of domain-specific information from text. In this paper we present a study in which text mining methodology and algorithms were properly employed for academic dishonesty (cheating) detection and evaluation on open-ended college exams, based on document classification techniques. Firstly, we propose two classification models for cheating detection by using a decision tree supervised algorithm. Then, both classifiers are compared against the result produced by a domain expert. The results point out that one of the classifiers achieved an excellent quality in detecting and evaluating cheating in exams, making possible its use in real school and college environments.	algorithm;data mining;decision tree;document classification;experiment;fits;machine learning;nonlinear gameplay;open-source software;sensor;subject-matter expert;text mining	Elmano Ramalho Cavalcanti;Carlos Eduardo S. Pires;Elmano Pontes Cavalcanti;Vládia Freire Pires	2012	Informatics in Education		cheating;subject-matter expert;data mining;decision tree;content analysis;academic dishonesty;document classification;computer science;text mining	ML	-31.113889395385968	-69.148587093334	127268
7299838bce51611babf08faca4e0a235370bf7ee	paradigm: paraphrase diagnostics through grammar matching		Paraphrase evaluation is typically done either manually or through indirect, taskbased evaluation. We introduce an intrinsic evaluation PARADIGM which measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality.	authorization;bleu;formal grammar;ibm notes;indexed grammar;natural language processing;paradigm;parallel text;parsing;stochastic context-free grammar;terminal and nonterminal symbols;text corpus	Jonathan Weese;Juri Ganitkevitch;Chris Callison-Burch	2014			natural language processing;speech recognition;computer science;linguistics;programming language	NLP	-26.040513358252255	-76.40488449662031	127269
6724c0ac36058ee16cdca362fae18f271d771251	a unified syntactic model for parsing fluent and disfluent speech	training data;speech repair;unified syntactic model;syntactic representation;better use;syntax tree;tree representation;standard switchboard parsing task;disfluent speech;right corner;high accuracy;special syntax rule	This paper describes a syntactic representation for modeling speech repairs. This representation makes use of a right corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data. PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task.	parsing;sensor;telephone switchboard	Timothy A. Miller;William Schuler	2008			natural language processing;abstract syntax;speech recognition;computer science;s-attributed grammar;linguistics;homoiconicity;abstract syntax tree	NLP	-28.010086852245042	-80.11616722451197	127521
92803e8364900f78d73acdcf95738c857947b2f9	lsi uned at m-wepnad: embeddings for person name disambiguation		In this paper we describe the participation of the LSI UNED team in the multilingual web person name disambiguation (M-WePNaD) task of the IberEval 2017 competition. Our proposal is based on the use of word embeddings for representing the documents related to individuals sharing the same person name. This may lead to an improvement of the clustering process that aims to eventually separate those documents depending on the individual they refer to. This is one of the first approximations to the use of techniques based on embeddings for this kind of tasks. Our preliminary experiments show that a system using a representation setting based on word embeddings is able to obtain promising results on the addressed task, overcoming the proposed baselines in all the tested configurations.	approximation;baseline (configuration management);cluster analysis;document;experiment;hoc (programming language);named entity;norm (social);preprocessor;web page;word embedding;word-sense disambiguation	Andres Duque Fernandez;Lourdes Araujo;Juan Martinez-Romo	2017			natural language processing;speech recognition;mathematics;artificial intelligence	NLP	-24.627798967122253	-68.03567800381771	127641
86f361c713d880273eb1f357fefefcfc48d44fc0	detecting body location modifiers of disorders in clinical texts via sequence labeling			sequence labeling	Jun Xu;Yonghui Wu;Yaoyun Zhang;Hua Xu	2017			sequence labeling;artificial intelligence;pattern recognition;biology	HCI	-25.334103424623653	-79.22240026059804	127709
d19ab63b89d9b2346506ce1dde06e7601a2a914b	language-independent sentiment analysis using subjectivity and positional information		We describe a novel language-independent approach to the task of determining the polarity, positive or negative, of the author’s opinion on a specific topic in natural language text. In particular, weights are assigned to attributes, individual words or word bi-grams, based on their position and on their likelihood of being subjective. The subjectivity of each attribute is estimated in a two-step process, where first the probability of being subjective is calculated for each sentence containing the attribute, and then these probabilities are used to alter the attribute’s weights for polarity classification. The evaluation results on a standard dataset of movie reviews shows 89.85% classification accuracy, which rivals the best previously published results for this dataset for systems that use no additional linguistic information nor external resources.	dependency grammar;grams;language-independent specification;natural language;nonlinear gameplay;nonlinear system;parser combinator;sentiment analysis;tree (data structure)	Veselin Raychev;Preslav Nakov	2009			natural language processing;information retrieval	NLP	-25.0543241780676	-69.99105423088014	127758
a0c914e7195e8e135a09dfb44cc6323eca061fcb	camelparser: a system for arabic syntactic analysis and morphological disambiguation		In this paper, we present CamelParser, a state-of-the-art system for Arabic syntactic dependency analysis aligned with contextually disambiguated morphological features. CamelParser uses a state-of-the-art morphological disambiguator and improves its results using syntactically driven features. The system offers a number of output formats that include basic dependency with morphological features, two tree visualization modes, and traditional Arabic grammatical analysis.	dependence analysis;morphological pattern;parsing;word-sense disambiguation	Anas Shahrour;Salam Khalifa;Dima Taji;Nizar Habash	2016			natural language processing;artificial intelligence;arabic;computer science;parsing	NLP	-24.93665358246902	-75.90217145721168	127811
7fead81d3655f52a221ef8738216ab8826019897	a simple word embedding model for lexical substitution		The lexical substitution task requires identifying meaning-preserving substitutes for a target word instance in a given sentential context. Since its introduction in SemEval-2007, various models addressed this challenge, mostly in an unsupervised setting. In this work we propose a simple model for lexical substitution, which is based on the popular skip-gram word embedding model. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting.	lexical substitution;n-gram;semeval;skip list;unsupervised learning;word embedding	Oren Melamud;Omer Levy;Ido Dagan	2015			natural language processing;speech recognition;computer science	NLP	-19.242504403009995	-73.668492047148	127905
5679cd4dd9ef71c8c940cbf1ee5f34183dcc5dcd	constraining the phrase-based, joint probability statistical translation model	statistical machine translation;scaling up;expectation maximization;word alignment;computational complexity;probability model	The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model’s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable.	bleu;bitext word alignment;computational complexity theory;estimation theory;expectation–maximization algorithm;heuristic (computer science);neural coding;search algorithm;smoothing;statistical machine translation;text corpus;viterbi algorithm	Alexandra Birch;Chris Callison-Burch;Miles Osborne;Philipp Koehn	2006			computer science;machine learning;pattern recognition;statistics	NLP	-20.563707339632558	-77.15881532444016	127913
7ea80b4262733197df249e4c298a895fa068c988	feature stacking for sentence classification in evidence-based medicine		We describe the feature sets and methodology that produced the winning entry to the ALTA 2012 Shared Task (sentence classification in evidence-based medicine). Our approach is based on a variety of feature sets, drawn from lexical and structural information at the sentence level, as well as sequential information at the abstract level. We introduce feature stacking, a metalearner to combine multiple feature sets, based on an approach similar to the wellknown stacking metalearner. Our system attains a ROC area-under-curve of 0.972 and 0.963 on two subsets of test data.	algorithm;f1 score;logistic regression;stacking;test data	Marco Lui	2012			natural language processing;speech recognition;pattern recognition	NLP	-22.503209364933255	-71.85262716992995	128028
5b6f10a24e5bf4e9f242a28726766a89c3f26d47	error mining on syntactic parser output		We introduce an error mining technique for automatically de tecting errors in resources that are used in parsing systems. We applied this tec hnique to parsing results produced on several million words by two distinct parsing systems, wh ich share the syntactic lexicon and the pre-parsing processing chain. We are thus able to identi fy incorrectness and incompleteness sources in the resources. In particular, by comparing b oth systems’ results, we are able to isolate problems coming from shared resources from those co ming from grammars. MOTS-CLÉS :analyse syntaxique, lexique syntaxique, fouille d’erreur s.	i/o controller hub;lexicon;parser;parsing	Benoît Sagot;Éric Villemonte de la Clergerie	2008	TAL		parser combinator;recursive descent parser;programming language;syntactic predicate;canonical lr parser;glr parser;ll parser;simple lr parser;lalr parser;computer science	NLP	-28.65134757818979	-77.66380366141664	128047
168574e863375c9b10ffa2cea10b6261934163d9	extracting and selecting relevant corpora for domain adaptation in mt		The paper presents scheme for doing Domain Adaptation for multiple domains simultaneously. The proposed method segments a large corpus into various parts using self-organizing maps (SOMs). After a SOM is drawn over the documents, an agglomerative clustering algorithm determines how many clusters the text collection comprised. This means that the clustering process is unsupervised, although choices are made about cut-offs for the document representations used in the SOM. Language models aren then built over these clusters, and used as features while decoding a Statistical Machine Translation system. For each input document the appropriate auxiliary Language Model most fitting for the domain is chosen according to a perplexity criterion, providing an additional feature in the log-linear model used by Moses. In this way, a corpus induced by an unsupervised method is implemented in a machine translation pipeline, boosting overall performance in an end-to-end experiment.	algorithm;cluster analysis;determining the number of clusters in a data set;domain adaptation;end-to-end principle;language model;linear model;log-linear model;moses;organizing (structure);perplexity;self-organization;self-organizing map;statistical machine translation;text corpus;unsupervised learning	Lars Bungum	2014			domain adaptation;distributed computing;machine learning;computer science;artificial intelligence	NLP	-20.337491807547686	-77.48329678553158	128075
ba4edf2137dd4eae16df14252a8b904baf2c67c2	creating word-level language models for handwriting recognition	data structures;document image processing;handwriting recognition;natural languages;data structures;handwriting recognition;text corpus;tokenization;word segmentation;word-level language modeling;word-unigram	Abstract: For large-vocabulary handwriting-recognition applications, such as note-taking, word-level language modeling is of key importance, to constrain the recognizer's search and to contribute to the scoring of hypothesized texts. We discuss the creation of a word-unigram language model, which associates probabilities with individual words. Typically, such models are derived from a large, diverse text corpus. We describe a three-stage algorithm for determining a word unigram from such a corpus. First is tokenization, the segmenting of a corpus into words. Second, we select for the model a subset of the set of distinct words found during tokenization. Complexities of these stages are discussed. Finally, we create recognizer-specific data structures for the word set and unigram. Applying our method to a 600-million-word corpus, we generate a 50,000-word model which eliminates 45% of word-recognition errors made by a baseline system employing only a character-level language model.	handwriting recognition;language model	John F. Pitrelli;Amit Roy	2001		10.1109/ICDAR.2001.10006	natural language processing;cache language model;speech recognition;data structure;word recognition;intelligent character recognition;computer science;word lists by frequency;natural language;language model	Vision	-21.59778843934566	-78.72707667222804	128214
2842ce3e68b359a983534c030bb2a313363ed06a	pattern-based statistical machine translation for ntcir-10 patentmt		Pattern-based machine translation is a very traditional machine translation method that uses translation patterns and translation word (phrase) dictionaries. The characteristic of this translation method is that high-quality translation results can be obtained if the input sentence matches the translation pattern and this translation pattern is correct. However, translation patterns and translation word dictionaries are usually made manually. Therefore, there are many costs in making a pattern-based machine translation system. We propose making translation patterns and translation word dictionaries automatically by using statistical machine translation methods. Using these methods, we decreased the costs in making a pattern-based machine translation system. We demonstrate the effectiveness of the proposed method in a Japanese-English machine translation patent task (NTCIR-10). We obtained good results.	dictionary;statistical machine translation	Jin'ichi Murakami;Isamu Fujiwara;Masato Tokuhisa	2013			machine translation;pattern recognition;artificial intelligence;phrase;computer science;sentence	NLP	-23.38309240793552	-76.34478084090338	128292
b0eff71adf5c1da77e968eea35d558352db7a058	an information theoretic approach for using word cluster information in natural language call routing	natural language;latent semantic indexing;information gain	In this paper, an information theoretic approach for using word clusters in natural language call routing (NLCR) is proposed. This approach utilizes an automatic word class clustering algorithm to generate word classes from the word based training corpus. In our approach, the information gain (IG) based term selection is used to combine both word term and word class information in NLCR. A joint latent semantic indexing natural language understanding algorithm is derived and studied in NLCR tasks. Comparing with word term based approach, an average performance gain of 10.7% to 14.5% is observed averaged over various training and testing conditions.	information theory;natural language;routing	Li Li;Feng Liu;Wu Chou	2003			universal networking language;information extraction;temporal annotation;language identification;natural language understanding;pattern recognition;natural language processing;natural language;computer science;natural language user interface;question answering;artificial intelligence	ML	-23.672946516118014	-72.60668918125597	128319
af7f8c2dfc17638b856ee17fc6a8bdf146531d90	some quantitative characteristics of polysemy of verbs, nouns and adjectives in the german language	noun	The purpose of this work is to analyze the correlation between the number of meanings of a polysemantic word and its other characteristics, namely, the belonging of a word to (1) a certain grammatical class (verb, noun, adjective); (2) a certain grammatical subclass (masculine, feminine, neuter gender of the noun, strong and weak verbs); (3) a certain semantic class (22 classes of the verb, 23 classes of the noun, 17 classes of the adjective); (4) a word-building subclass (simple, derivative, and compound words). More accurate data on the distribution of polysemantic words in the dictionary and their use in text were obtained which made it possible to correct some conclusions made earlier by other researchers.	level of measurement	Viktor V. Levickij;V. V. Drebet;S. V. Kiiko	1999	Journal of Quantitative Linguistics	10.1076/jqul.6.2.172.4131	natural language processing;noun;proper noun;mathematics;linguistics	NLP	-27.418913170758504	-75.67531693995565	128485
2c978f74d05aeace2dcb52ec1572cae738d58bb8	identification of spoken questions using similarity-based tf.aoi		Similarity is utilized in the retrieval and extraction of information, but it can also be used in dialog processing. Spoken dialog processing must deal with speech recognition error, interjections and noise, and it is rare that the same expressions are used consistently. It is required to find a sentence which is similar to the input sentence while taking account of these phenomena. This paper proposes an identification method for the question sentence based on TF⋅AoI (term frequency × amount of information) weighting. In this method, the words contained in the input sentence are weighted by (word similarity) × (amount of information). Then, based on the calculated Euclidean distance, the response corresponding to the question with the highest similarity is output. Comparison experiments verify an improvement of 13 points over the method of comparison by matching ratio to the input sentence, and by 6.5 points over the method of “similarity by TF⋅AoI weighting.” © 2007 Wiley Periodicals, Inc. Syst Comp Jpn, 38(10): 81–94, 2007; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.20363	dialog system;digi-comp i;euclidean distance;experiment;information;john d. wiley;semantic similarity;speech recognition;tf–idf	Yasutomo Kimura;Kenji Araki;Koji Tochinai	2007	Systems and Computers in Japan	10.1002/scj.20363		NLP	-26.498311195129308	-68.07751818780584	128574
0849f99d0a9f8963449f1fd8cf9c8b27638f173f	tools for upgrading printed dictionaries by means of corpus-based lexical acquisition		We present the architecture and tools developed in the project TFB-32 for updating existing dictionaries by comparing their content with corpus data. We focus on an interactive graphical user interface for manual selection of the results of this comparison. The tools have been developed and used within a cooperation with lexicographers from two German publishing houses.	dictionary;graphical user interface;lexicography	Ulrich Heid;Bettina Säuberlich;Esther Debus-Gregor;Werner Scholze-Stubenrecht	2004			artificial intelligence;natural language processing;speech recognition;computer science	HCI	-33.30299389174874	-75.61413954531672	128679
6b763095efe72aae6a2bbf409fdd4532f5f14243	semi-supervised event extraction with paraphrase clusters		Supervised event extraction systems are limited in their accuracy due to the lack of available training data. We present a method for self-training event extraction systems by bootstrapping additional training data. This is done by taking advantage of the occurrence of multiple mentions of the same event instances across newswire articles from multiple sources. If our system can make a highconfidence extraction of some mentions in such a cluster, it can then acquire diverse training examples by adding the other mentions as well. Our experiments show significant performance improvements on multiple event extractors over ACE 2005 and TAC-KBP 2015 datasets.	ace;baseline (configuration management);experiment;ibm notes;information extraction;semiconductor industry;weather research and forecasting model;word embedding	James Ferguson;Colin Lockard;Daniel S. Weld;Hannaneh Hajishirzi	2018			machine learning;computer science;training set;bootstrapping;pattern recognition;artificial intelligence;paraphrase	NLP	-22.29624964824667	-71.46872973437142	128696
109fbf28186ec7f648d041d25b40308683a52cc2	global discriminative model for dependency parsing in nlp pipeline	dependency parsing discriminative re ranking model nlp pipeline;nlp pipeline;n best enhanced nlp pipeline global discriminative reranking model dependency parsing natural language processing word segmentation part of speech tagging pos tagging chinese nlp application cascade system error propagation parse tree reranking;pipelines tagging natural language processing joints vectors computational linguistics training;trees mathematics cascade systems grammars natural language processing;dependency parsing;discriminative re ranking model	Dependency parsing, which is a fundamental task in Natural Language Processing (NLP), has attracted a lot of interest in recent years. In general, it is a module in an NLP pipeline together with word segmentation and Part-Of-Speech (POS) tagging in real Chinese NLP application. The NLP pipeline, which is a cascade system, will lead to error propagation for the parsing. This paper proposes a global discriminative re-ranking model using non-local features from word segmentation, POS tagging and dependency parsing to re-rank the parse trees produced by an N-best enhanced NLP pipeline. Experimental results indicate that the proposed model can improve the performance of dependency parsing as well as word segmentation and POS tagging in an NLP pipeline.	discriminative model;natural language processing;parse tree;parsing expression grammar;part-of-speech tagging;propagation of uncertainty;software propagation;text segmentation	Miao Li;Hongyi Ding;Ji Wu	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936624	natural language processing;speech recognition;computer science;pattern recognition;linguistics;dependency grammar	NLP	-21.558816286828787	-74.60375978621353	128802
93c1f320e2ce799e5c0474b3824277d2173b7a6a	word ordering with phrase-based grammars		We describe an approach to word ordering using modelling techniques from statistical machine translation. The system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent, grammatical sentences. We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding.	bleu;computational complexity theory;language model;n-gram;parsing;statistical machine translation;string generation	Adrià de Gispert;Marcus Tomalin;Bill Byrne	2014			natural language processing;speech recognition;computer science;linguistics;programming language	NLP	-21.933631013305607	-77.9248766434013	128830
67a57351de66db162fa6bcd298a836eca0a4f6ab	on the morphological entropy of arabic				M. A. El-Affendi	2002	Egyptian Computer Science Journal		natural language processing;control engineering;engineering;arabic;artificial intelligence	Theory	-30.94069192802999	-79.14455020790572	128846
b2cfb621b3d65792c0a7effdd9364b359214a04a	compiling a text re-use detection corpus from scientific papers with semi-real cases of plagiarism		Automatic plagiarism detection deals with retrieval of reused fragment of texts in a document and finding source documents. Due to development of various methods for plagiarism detection, large scale plagiarism corpora are needed to evaluate these methods. Despite of their importance, few plagiarism detection corpora developed in recent years, especially in low resource languages. Because of legal issues, releasing a collection of real cases of plagiarism for evaluation purposes is not ethical. Due to these limitations, simulation and artificial based methods are the two main approaches to compile a plagiarism corpus. These approaches try to simulate real cases of plagiarism, from different point of views. However, there are still fundamental differences between simulated corpora and real cases of plagiarism. In this paper a semi-real approach is proposed to create a collection of plagiarism cases as a corpus. This approach is based on eliminating correct references from scientific papers to make them as plagiarized passages. Unlike methods based on simulated and artificial approaches, the proposed corpus can correctly simulate real cases of text re-use. The evaluation result shows high accuracy of proposed corpus with respect to n-gram similarity for different ranges of N.	compiler;n-gram;scientific literature;semiconductor industry;simulation;text corpus	Salar Mohtaj;Habibollah Asghari;Vahid Zarrabi	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300585	natural language processing;information retrieval;compiler;artificial intelligence;computer science;source document;plagiarism detection	AI	-29.72080540547676	-73.02914776063179	128963
0896b05c46dcf2b0fa2a301496100290f4084ddf	sentiment analysis using a novel human computation game	single human computation game;annotating sentiment;lexicon construction;novel human computation game;human computation game;well-known sentiment detection approach;human knowledge;discriminative lexicon;sentiment detection;sentiment analysis	In this paper, we propose a novel human computation game for sentiment analysis. Our game aims at annotating sentiments of a collection of text documents and simultaneously constructing a highly discriminative lexicon of positive and negative phrases. Human computation games have been widely used in recent years to acquire human knowledge and use it to solve problems which are infeasible to solve by machine intelligence. We package the problems of lexicon construction and sentiment detection as a single human computation game. We compare the results obtained by the game with that of other well-known sentiment detection approaches. Obtained results are promising and show improvements over traditional approaches.	artificial intelligence;crowdsourcing;display resolution;experiment;feature extraction;human-based computation;information;lexicon;machine learning;scoring functions for docking;sentiment analysis	Claudiu Cristian Musat;Alireza Ghasemi;Boi Faltings	2012			natural language processing;computer science;machine learning;data mining	AI	-22.337967013706862	-67.6289085001173	129015
3eea46adcea8b2eff6f6de492136b7d900915a97	a compositional approach toward dynamic phrasal thesaurus	phrasal thesaurus;semantic equivalence;dynamic phrasal thesaurus;lexical function;compositional approach;atomic knowledge;proposed system;natural extension;generation function;small number;compositionally explainable	To enhance the technology for computing semantic equivalence, we introduce the notion of phrasal thesaurus which is a natural extension of conventional word-based thesaurus. Among a variety of phrases that conveys the same meaning, i.e., paraphrases, we focus on syntactic variants that are compositionally explainable using a small number of atomic knowledge, and develop a system which dynamically generates such variants. This paper describes the proposed system and three sorts of knowledge developed for dynamic phrasal thesaurus in Japanese: (i) transformation pattern, (ii) generation function, and (iii) lexical function.	lexical function;thesaurus;turing completeness	Atsushi Fujita;Shuhei Kato;Naoki Kato;Satoshi Sato	2007			natural language processing;computer science;linguistics	NLP	-25.278777016852437	-75.1683860583421	129030
fce7f94690650c663bd86f04cff4ebc7ef3c032c	aspects of lexical development in artificial neural networks			artificial neural network	Nicholas John Sales	1996				NLP	-31.13492780420707	-78.58781614122098	129108
5adda101bff29d8b2e44a58a7fa14d8dcd144bfb	these are your rights - a natural language processing approach to automated rdf licenses generation	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	In the latest years, the Web has seen an increasing interest in legal issues, concerning the use and re-use of online published material. In particular, several open issues affect the terms and conditions under which the data published on the Web is released to the users, and the users rights over such data. Though the number of licensed material on the Web is considerably increasing, the problem of generating machine readable licenses information is still unsolved. In this paper, we propose to adopt Natural Language Processing techniques to extract in an automated way the rights and conditions granted by a license, and we return the license in a machine readable format using RDF and adopting two well known vocabularies to model licenses. Experiments over a set of widely adopted licenses show the feasibility of the proposed approach.	human-readable medium;natural language processing;resource description framework;vocabulary;world wide web	Elena Cabrio;Alessio Palmero Aprosio;Serena Villata	2014		10.1007/978-3-319-07443-6_18	computer science;artificial intelligence;database;world wide web	AI	-31.443666099792598	-67.81665836411982	129258
1b6c0cf7b143297484ba63dee6a0ea6f9b4432a7	the role of distributed memory in natural language parsing	distributed memory		distributed memory;natural language;parsing	Jon M. Slack	1984			bottom-up parsing;computer science;distributed memory;programming language;parsing;natural language;natural language processing;top-down parsing;s-attributed grammar;artificial intelligence	NLP	-29.684905003148128	-79.39556625670735	129347
d2ba6b0f15954316bd64d1c063d859c23069ac57	modelling highly inflected languages	approximate string matching;mixture based model;vector space;vector space retrieval model;statistical language modelling;semantic information;mixture model;levenshtein distance;retrieval model;statistical language model;similarity function;similarity measure;perplexity;topic similarity	Statistical language models encapsulate varied information, both grammatical and semantic, present in a language. This paper investigates various techniques for overcoming the difficulties in modelling highly inflected languages. The main problem is a large set of different words. We propose to model the grammatical and semantic information of words separately by splitting them into stems and endings. All the information is handled within a data-driven formalism. Grammatical information is well modelled by using short-term dependencies. This article is primarily concerned with the modelling of semantic information diffused through the entire text. It is presumed that the language being modelled is homogeneous in topic. The training corpus, which is very topically heterogeneous, is divided into three semantic levels based on topic similarity with the target environment text. Text on each semantic level is used as training text for one component of a mixture model. A document is defined as a basic unit of a training corpus, which is semantically homogeneous. The similarity of topic between a document and a collection of target environment texts is determined by the cosine vector similarity function and TFIDF weighting heuristic. The crucial question in the case of highly inflected languages is how to define terms. Terms are defined as clusters of words. Clustering is based on approximate string matching. We experimented with Levenshtein distance and Ratcliff/Obershelp similarity measure, both in combination with ending-stripping. Experiments on the Slovenian language were performed on a corpus of VECER newswire text. The results show a significant reduction in OOV rate and perplexity.		Mirjam Sepesy Maucec;Zdravko Kacic;Bogomir Horvat	2004	Inf. Sci.	10.1016/j.ins.2003.12.004	natural language processing;semantic similarity;explicit semantic analysis;approximate string matching;vector space;computer science;machine learning;perplexity;pattern recognition;mixture model;programming language	AI	-22.005624716182577	-79.20713773109112	129357
00926f20deea086083904a0f6ac179cebf658eaf	pruning-based unsupervised segmentation for korean	unsupervised learning;coreano;tecnologia electronica telecomunicaciones;classification non supervisee;unsupervised method;lexicon;pruning technique;heuristic method;compound noun segmentation;base connaissance;metodo heuristico;tratamiento lenguaje;unsupervised segmentation;apprentissage non supervise;segmentation;segmentation evaluation;korean;algorithme;algorithm;accuracy;dictionnaire;precision;language processing;coreen;clasificacion no supervisada;dictionaries;signal classification;traitement langage;classification signal;unsupervised classification;base conocimiento;methode heuristique;lexico;tecnologias;grupo a;diccionario;segmentacion;algoritmo;knowledge base;lexique	Compound noun segmentation is a key component for Korean language processing. Supervised approaches require some types of human intervention such as maintaining lexicons, manually segmenting the corpora, or devising heuristic rules. Thus, they suffer from the unknown word problem, and cannot distinguish domain-oriented or corpus-directed segmentation results from the others. These problems can be overcome by unsupervised approaches that employ segmentation clues obtained purely from a raw corpus. However, most unsupervised approaches require tuning of empirical parameters or learning of the statistical dictionary. To develop a tuning-less, learning-free unsupervised segmentation algorithm, this study proposes a pruning-based unsupervised technique that eliminates unhelpful segmentation candidates. In addition, unlike previous unsupervised methods that have relied on purely character-based segmentation clues, this study utilizes word-based segmentation clues. Experimental evaluations show that the pruning scheme is very effective to unsupervised segmentation of Korean compound nouns, and the use of word-based prior knowledge enables better segmentation accuracy. This study also shows that the proposed algorithm performs competitively with or better than other unsupervised methods.		In-Su Kang;Seung-Hoon Na;Jong-Hyeok Lee	2006	IEICE Transactions	10.1093/ietisy/e89-d.10.2670	unsupervised learning;knowledge base;speech recognition;computer science;machine learning;segmentation-based object categorization;pattern recognition;accuracy and precision;scale-space segmentation;statistics	Vision	-26.24826394718793	-77.92464256993722	129487
dca1d6db3c92ac7438cb9f4ddb038a7f36955948	tbl-improved non-deterministic segmentation and pos tagging for a chinese parser	segmentation f-score;pos tagging analysis;tbl-improved non-deterministic segmentation;pku tokenizer-tagger;chinese parser;pos tagging;word segmentation;beijing university;output multiple segmentation;sentence accuracy;word f-score;segmented sentence accuracy;syntactic analysis	Although a lot of progress has been made recently in word segmentation and POS tagging for Chinese, the output of current state-of-the-art systems is too inaccurate to allow for syntactic analysis based on it. We present an experiment in improving the output of an off-the-shelf module that performs segmentation and tagging, the tokenizer-tagger from Beijing University (PKU). Our approach is based on transformation-based learning (TBL). Unlike in other TBL-based approaches to the problem, however, both obligatory and optional transformation rules are learned, so that the final system can output multiple segmentation and POS tagging analyses for a given input. By allowing for a small amount of ambiguity in the output of the tokenizer-tagger, we achieve a very considerable improvement in accuracy. Compared to the PKU tokenizertagger, we improve segmentation F-score from 94.18% to 96.74%, tagged word F-score from 84.63% to 92.44%, segmented sentence accuracy from 47.15% to 65.06% and tagged sentence accuracy from 14.07% to 31.47%.	brill tagger;combinatory categorial grammar;head-driven phrase structure grammar;lexical analysis;lexical functional grammar;non-deterministic turing machine;parser;part-of-speech tagging;prune and search;semiconductor industry;text segmentation;text-based (computing);transformational grammar;treebank;way to go	Martin Forst;Ji Fang	2009			natural language processing;text segmentation;speech recognition;computer science;parsing;pattern recognition	NLP	-23.01003085692356	-76.43119520241163	129594
c37909ab29f250e1fb316e886d715e1a76ea9693	polyglot: multilingual semantic role labeling with unified labels		Semantic role labeling (SRL) identifies the predicate-argument structure in text with semantic labels. It plays a key role in understanding natural language. In this paper, we present POLYGLOT, a multilingual semantic role labeling system capable of semantically parsing sentences in 9 different languages from 4 different language groups. The core of POLYGLOT are SRL models for individual languages trained with automatically generated Proposition Banks (Akbik et al., 2015). The key feature of the system is that it treats the semantic labels of the English Proposition Bank as “universal semantic labels”: Given a sentence in any of the supported languages, POLYGLOT applies the corresponding SRL and predicts English PropBank frame and role annotation. The results are then visualized to facilitate the understanding of multilingual SRL with this unified semantic representation.	natural language understanding;parsing;polyglot (computing);propbank;semantic role labeling	Alan Akbik;Yunyao Li	2016		10.18653/v1/P16-4001	natural language processing;semantic computing;speech recognition;computer science;linguistics	NLP	-26.5883259724068	-75.69841445207325	129638
a9e21d9adf0a39440fc9f087c66ce512b908e0ea	acquiring lexical paraphrases from a single corpus	information retrieval;artificial intelligent;science learning	This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of “comparable” corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vectorbased approach in order to assess better the paraphrase likelihood for many verb pairs.	algorithm;comparison of raster-to-vector conversion software;text corpus	Oren Glickman;Ido Dagan	2003			natural language processing;computer science;pattern recognition	NLP	-24.628457920083214	-73.13157898839584	129705
2d99d5218460f9a12a1cb5534311c2cc54cef024	automatic alignment of japanese-chinese bilingual texts	statistical approach;lexical approach;heuristic function;example based machine translation;automatic alignment			Chew Lim Tan;Makoto Nagao	1995	IEICE Transactions		natural language processing;speech recognition;example-based machine translation;computer science;pattern recognition;heuristic function	NLP	-23.511843486179146	-78.04322044100952	129742
ebd4c1aa6430477be0883360ae2d28d6dc904bd4	modeling machine transliteration as a phrase based statistical machine translation problem	model weight;popular phrase-based smt technique;test set;machine transliteration;statistical machine translation problem;minimum error rate training;english-hindi language pair	In this paper we use the popular phrasebased SMT techniques for the task of machine transliteration, for English-Hindi language pair. Minimum error rate training has been used to learn the model weights. We have achieved an accuracy of 46.3% on the test set. Our results show these techniques can be successfully used for the task of machine transliteration.	align (company);beam search;decision table;foreign key;statistical machine translation;test set;text corpus	Taraka Rama;Karthik Gali	2009			natural language processing;speech recognition;example-based machine translation;computer science;pattern recognition;rule-based machine translation	NLP	-21.529200744029584	-77.55193419863546	129763
837f706662284b66c6a759ea2b5741f9355f0e03	experiments in trec 2004 novelty track at cas-ict		The main task in Novelty Track is to retrieve relevant sentences and remove duplicates from a document set given a TREC topic. This track took place for the first time in TREC 2002 and it is refined to four tasks in TREC 2003. Besides 25 relevant documents, irrelevant ones are given in this year of Novelty track. In other words, a given document is either relevant or irrelevant to the topic. There are 1808 documents in 50 TREC topics. Average 11.18 documents are noise for each topic. In topic N75, the number of noise is 45. Once we mistook an irrelevant document as relevance, all results in the document are wrong. Except the document retrieval, more limited information could be applied in the last three tasks than ever. Among the first 5 given documents, average 3.14 documents are relevant and average 2.76 are new. Especially, 9 topics have no relevant sentence in the first 5 ones. In TREC2004, ICT divided Novelty track into four sequential stages. It includes: customized language parsing on original dataset, document retrieval, sentence relevance and novelty detection. The architecture in novelty is given in Figure 1. In the first preprocessing stage, we applied sentence segmenter, tokenization, part-of-speech tagging, morphological analysis, stop word remover and query analyzer on topics and documents. As for query analysis, we categorized words in topics into description words and query words. Title, description and narrative parts are all merged into query with different weights. In the stage of document and sentence retrieval, we introduced vector space model (VSM) and its variance, probability model OKAPI and statistical language model. Based on VSM, we tried various query expansion strategies: pseu-feedback, term expansion with synset or synonym in WordNet[1] and expansion with highly local co-occurrence terms. With regard to the novelty stage, we defined three types of new degree: word overlapping and its extension, similarity comparison and information gain. In the last three tasks, we used the known results to adjust threshold, estimate the number of results, and turned to classifier, such as inductive and transductive SVM. Topic/Doc Parsing	categorization;description logic;document retrieval;information gain in decision trees;kullback–leibler divergence;language model;novelty detection;parsing;part-of-speech tagging;preprocessor;query expansion;relevance;statistical classification;synonym ring;text retrieval conference;tokenization (data security);viable system model;wordnet	Huaping Zhang;Hongbo Xu;Shuo Bai;Bin Wang;Xueqi Cheng	2004			novelty;information retrieval;query expansion;language model;computer science;document retrieval;vector space model;novelty detection;stop words;sentence	Web+IR	-24.708173291815715	-66.83620299482217	129826
0cd9fca74f03a5a65e547d10ffbe9817b2b5a36b	classifying patent applications with ensemble methods		We present methods for the automatic classification of patent applications using an annotated dataset provided by the organizers of the ALTA 2018 shared task Classifying Patent Applications. The goal of the task is to use computational methods to categorize patent applications according to a coarse-grained taxonomy of eight classes based on the International Patent Classification (IPC). We tested a variety of approaches for this task and the best results, 0.778 micro-averaged F1-Score, were achieved by SVM ensembles using a combination of words and characters as features. Our team, BMZ, was ranked first among 14 teams in the competition.	categorization;computation;f1 score;smart battery;taxonomy (general)	Fernando Benites;Shervin Malmasi;Marcos Zampieri	2018	CoRR			NLP	-22.182571980150616	-69.7058709996016	129903
2f811472a365c32fd90de8b92c9ce6ed60293538	equations for part-of-speech tagging	hidden markov model;first principle;part of speech tagging	We derive from rst principles the basic equations for a few of the basic hidden Markov model word taggers as well as equations for other models which may be novel the descriptions in previous papers being too spare to be sure We give performance results for all of the mod els The results from our best model on an unused test sample from the Brown corpus with dis tinct tags is on the upper edge of reported results We also hope these results clear up some confusion in the literature about the best equations to use However the major purpose of this paper is to show how the equa tions for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof	brown corpus;hidden markov model;limbo;markov chain;part-of-speech tagging	Eugene Charniak;Curtis Hendrickson;Neil Jacobson;Mike Perkowitz	1993			speech recognition;first principle;computer science;artificial intelligence;machine learning;hidden markov model;statistics	AI	-27.100826446564177	-79.71086911078442	129921
8f5171e27f4628b503e1a47b5d8a9d1f552b392d	reranking a wide-coverage ccg parser		n-best parse reranking is an important technique for improving the accuracy of statistical parsers. Reranking is not constrained by the dynamic programming required for tractable parsing, so arbitrary features of each parse may be considered. We adapt the reranking features and methodology used by Charniak and Johnson (2005) for the C&C Combinatory Categorial Grammar parser, and develop new features based on the richer formalism. The reranker achieves a labeled dependency F-score of 87.59%, which is a significant improvement over prior results.	baseline (configuration management);cobham's thesis;combinatory categorial grammar;downstream (software development);dynamic programming;formal system;natural language processing;parse tree;parsing;stochastic context-free grammar	Dominick Ng;Matthew Honnibal;James R. Curran	2010			parsing;artificial intelligence;pattern recognition;computer science	NLP	-20.91558247459137	-76.35305949564649	130132
c2a282bdad2c4cdaffb98c3bb95d21fbb8384b43	including category information as supplements in latent semantic analysis of hindi documents		Latent semantic analysis (LSA) is a mathematical model that is used to capture the semantic structure of documents by using the correlations between the textual elements in them. LSA captures the semantic structure very well being independent of external sources of semantics. However, the modelu0027s performance increases when it is supplemented with extra information. The work presented in this paper is to modify the model to analyse word correlations in documents by considering the document category information as supplements in the process. This enhancement is called supplemented latent semantic analysis (SLSA). SLSAu0027s performance is empirically evaluated in a document classification application by comparing the accuracies of classification against plain LSA for various term weighting schemes. An increment of 1.14%, 1.30% and 1.63% is observed in the classification accuracies when SLSA is compared with plain LSA for tf, idf and tfidf respectively in the initial term-bydocument matrix.	latent semantic analysis	Karthik Krishnamurthi;Vijayapal Reddy Panuganti;Vishnu Vardhan Bulusu	2017	IJCSE	10.1504/IJCSE.2017.10006984	machine learning;semantics;natural language processing;computer science;dimensionality reduction;hindi;tf–idf;singular value decomposition;artificial intelligence;latent semantic analysis;weighting;document classification;pattern recognition	Web+IR	-22.189074702336672	-66.42268448204058	130140
275de977ee6b5abfccc6875de6fb0198451e9d8e	amazigh verb in the universal networking language	semantics;dictionaries	In the perspective of achieving an UNL (Universal Networking Language) based machine translation system for Amazigh language, we are undertaking the first steps of dictionaries' development according to the UNL specifications. So far, no attempt has been made to integrate Amazigh language in the UNL project. This latter revolves around the development of an EnConverter (Analyzer) and a DeConverter (Generator) for a natural language in order to overcome language barriers and promote multilingualism. This paper focuses on morphological analysis and semantic information of Amazigh verbs, with the aim to incorporate them into the Amazigh UNL dictionary.	dictionary;machine translation;natural language;tokenization (data security);universal networking language;vii;word-sense disambiguation	Imane Taghablout;Fadoua Ataa-Allah;Mohamed El Marraki	2015	2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2015.7507205	natural language processing;computer science;semantics;rule-based machine translation;programming language	NLP	-30.913443988837145	-78.70539145375895	130167
b4fd524944b2e8f243ba6a140012954d16af71d2	neural greedy constituent parsing with dynamic oracles		Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers.	greedy algorithm;oracle database;parsing	Maximin Coavoux;Benoît Crabbé	2016			artificial intelligence;natural language processing;bottom-up parsing;parser combinator;computer science;machine learning;parsing;dependency grammar;oracle;s-attributed grammar;top-down parsing;top-down parsing language	NLP	-21.270700307896806	-76.17793469964849	130234
dccdd9d6429b9ac44231f3f0462e869b3e7b3a65	a relatedness benchmark to test the role of determiners in compositional distributional semantics		Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.	benchmark (computing);distributional semantics	Raffaella Bernardi;Georgiana Dinu;Marco Marelli;Marco Baroni	2013			natural language processing;linguistics	NLP	-21.064908290593735	-72.75541922681211	130334
b7fa8eaabf21426f731e1c939a8f53e2ce8e36db	extension automatique d'annotation et classification de documents en utilisant un modèle graphique probabiliste		With the fast growth of document images, document annotation has become a research area of great interest. Annotation allows to describe the semantic content of documents and facilitates their use and research. However, for a huge number of documents, the manual annotation of each document becomes a tedious task. A solution is to annotate a small part of the documents and to extend it automatically to the whole dataset. In this paper, we propose a model for annotation extension and document classification using a probabilistic graphical model. In this latter, we combine visual and textual characteristics and we show that the integration of the user feedback improves the annotation step. MOTS-CLÉS : Annotation, classification, modèle graphique probabiliste, retour utilisateur.	document classification;graphical model	Abdessalem Bouzaieni;Sabine Barrat;Salvatore Tabbone	2016		10.24348/sdnri.2016.CIFED-8	information retrieval;annotation;computer science	Web+IR	-32.73264740066106	-67.94618678325156	130419
328bd9254d97f30d82471b45dd18c53529862333	corpus-based identification of non-anaphoric noun phrases	training corpus;corpus-based identification;non-anaphoric noun phrase;corpus-based algorithm;definite noun phrase;coreference resolution system;anaphoric discourse entity;coreference resolution;muc-4 terrorism news article;noun phrase;noun phrase pattern;new media	"""Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases. But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., """"the White House"""" or """"the news media""""). We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems. Our algorithm generates lists of nonanaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts. Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents. 1 I n t r o d u c t i o n Most automated approaches to coreference resolution attempt to locate an antecedent for every potentially coreferent discourse entity (DE) in a text. The problem with this approach is that a large number of DE's may not have antecedents. While some discourse entities such as pronouns are almost always referential, definite descriptions I may not be. Earlier work found that nearly 50% of definite descriptions had no prior referents (Vieira and Poesio, 1997), and we found that number to be even higher, 63%, in our corpus. Some non-anaphoric definite descriptions can be identified by looking for syntactic clues like attached prepositional phrases or restrictive relative clauses. But other definite descriptions are non-anaphoric because readers understand their meaning due to common knowledge. For example, readers of this 1In this work, we define a definite description to be a noun phrase beginning with the. paper will probably understand the real world referents of """"the F.B.I.,"""" """"the White House,"""" and """"the Golden Gate Bridge."""" These are instances of definite descriptions that a coreference resolver does not need to resolve because they each fully specify a cognitive representation of the entity in the reader's mind. One way to address this problem is to create a list of all non-anaphoric NPs that could be used as a filter prior to coreference resolution, but hand coding such a list is a daunting and intractable task. We propose a corpusbased mechanism to identify non-anaphoric NPs automatically. We will refer to non-anaphoric definite noun phrases as exis tent ial NPs (Allen, 1995). Our algorithm uses statistical methods to generate lists of existential noun phrases and noun phrase patterns from a training corpus. These lists are then used to recognize existential NPs in new texts. 2 P r i o r R e s e a r c h Computational coreference resolvers fall into two categories: systems that make no attempt to identify non-anaphoric discourse entities prior to coreference resolution, and those that apply a filter to discourse entities, identifying a subset of them that are anaphoric. Those that do not practice filtering include decision tree models (Aone and Bennett, 1996), (McCarthy and Lehnert, 1995) that consider all possible combinations of potential anaphora and referents. Exhaustively examining all possible combinations is expensive and, we believe, unnecessary. Of those systems that apply filtering prior to coreference resolution, the nature of the filtering varies. Some systems recognize when an anaphor and a candidate antecedent are incompatible. In SRI's probabilistic model (Kehler,"""	algorithm;anaphora (linguistics);commonsense knowledge (artificial intelligence);computation;decision tree model;entity;hand coding;statistical model;text corpus	David L. Bean;Ellen Riloff	1999			natural language processing;noun;nominalization;noun phrase;speech recognition;new media;computer science;specifier;proper noun;determiner phrase;linguistics	NLP	-27.69060050427916	-72.21192032540435	130423
87f334e530edff74470d75b7916fe6e34be20781	borrow a little from your rich cousin: using embeddings and polarities of english words for multilingual sentiment classification		In this paper, we provide a solution to multilingual sentiment classification using deep learning. Given input text in a language, we use word translation into English and then the embeddings of these English words to train a classifier. This projection into the English space plus word embeddings gives a simple and uniform framework for multilingual sentiment analysis. A novel idea is augmentation of the training data with polar words, appearing in these sentences, along with their polarities. This approach leads to a performance gain of 7-10% over traditional classifiers on many languages, irrespective of text genre, despite the scarcity of resources in most languages.	artificial neural network;deep learning;sentiment analysis;statistical classification;word embedding	Prerana Singhal;Pushpak Bhattacharyya	2016			artificial intelligence;natural language processing;cousin;computer science	NLP	-19.908755150133985	-73.13923322858706	130515
112c1d6eb512ea986cdc4db951fbbfdb7f65b905	a named entity recognition system for dutch	named entity recognition;machine learning;knowledge acquisition;languages and literatures	We describe a Named Entity Recognition system for Dutch that combines gazetteers, handcrafted rules, and machine learning on the basis of seed material. We used gazetteers and a corpus to construct training material for Ripper, a rule learner. Instead of using Ripper to train a complete system, we used many different runs of Ripper in order to derive rules which we then interpreted and implemented in our own, hand-crafted system. This speeded up the building of a hand-crafted system, and allowed us to use many different rule sets in order to improve performance. We discuss the advantages of using machine learning software as a tool in knowledge acquisition, and evaluate the resulting system for Dutch.	knowledge acquisition;knowledge engineering;machine learning;named-entity recognition;precision and recall;ripper;rule 184;rule induction;text corpus	Fien De Meulder;Walter Daelemans;Véronique Hoste	2001			natural language processing;computer science;machine learning;pattern recognition;data mining;entity linking	ML	-23.910651222259354	-71.69915846679989	130526
