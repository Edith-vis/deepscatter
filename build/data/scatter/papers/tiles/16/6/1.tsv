id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
2312ad3a2ad69fbafe4384f2084c87cac9e3cc63	reducing time-synchronous beam search effort using stage based look-ahead and language model rank based pruning	search algorithm;word error rate;look ahead;language model;speech recognition	In this paper, we present an efficient look-ahead technique based on both the Language Model (LM) Look-Ahead and the Acoustic Model (AM) Look-Ahead, for the time-synchronous beam search in the large vocabulary speech recognition. In this so-call stage based look-ahead (SLA) technique, two predicting processes with different hypothesis evaluating criteria are organized by stages according to the different requirements for pruning the unlikely surviving hypotheses. Furthermore, in order to reduce the efforts for distributing the LM over the lexical tree more effectively, the LM Rank based Pruning (LMRP) is integrated with the extension of each new phoneme node. The recognition experiments performed on the 50k-word Mandarin Dictation task (Easytalk2000) show that a reduction by 10 percents in the search effort in comparison with the standard word-conditioned search using LM look-ahead only, and a reduction of 25 percents in the word error rates in comparison with the search algorithm without any look-ahead can be achieved.	acoustic cryptanalysis;acoustic model;beam search;experiment;language model;microsoft word for mac;programmable logic array;requirement;search algorithm;service-level agreement;speech recognition;super robot monkey team hyperforce go!;vocabulary	Jian Wu;Fang Zheng	2000			word error rate;artificial intelligence;pattern recognition;dictation;speech recognition;computer science;language model;acoustic model;look-ahead;machine learning;beam search;vocabulary;search algorithm	NLP	-19.75647578631599	-85.6460561373663	84860
fe27cc233eacefb29f1f530a57fc2994f54b4b1a	probabilistic harmonization with fixed intermediate chord constraints		During the last decades, several methodologies have been proposed for the harmonization of a given melody with algorithmic means. Among the most successful are methodologies that incorporate probabilistic mechanisms and statistical learning, since they have the ability to generate harmonies that statistically adhere to the harmonic characteristics of the idiom that the training pieces belong to. The current paper discusses the utilization of a well–studied probabilistic methodology, the hidden Markov model (HMM), in combination with additional constraints that incorporate intermediate fixed–chord constraints. This work is motivated by the fact that some parts of a phrase (like the cadence) or a piece (e.g. points of modulation, peaks of tension, intermediate cadences etc.) are characteristic about the phrase’s or piece’s idiomatic identity. The presented methodology allows to define and isolate such important parts/functions and include them as constraints in a probabilistic harmonization methodology. To this end, the constrained HMM (CHMM) is developed, harnessed with the novel general chord type (GCT) representation, while the study focuses on examples that highlight the diversity that constraints introduce in harmonizations.	algorithm;geometric complexity theory;hidden markov model;machine learning;markov chain;modulation	Maximos A. Kaliakatsos-Papakostas;Emilios Cambouropoulos	2014			machine learning;probabilistic logic;hidden markov model;harmony (music);cadence;harmonization;harmonic;artificial intelligence;chord (music);computer science;engineering drawing;phrase	ML	-19.19687727975544	-93.66655904130964	85156
60c53ff738907608b677e4ad275d3e186cd4de79	phonemicization for the generation of vi-syllable	phonological transcriptionsyllabication;virtual characters;visemes;labial synchronization;phonological transcription syllabication;vi syllable	This work presents the application of linguistic rules for the creation of vi-syllables of phrases written in the Spanish language in order to increase the believability of the labial synchronization in virtual characters. Especially, we describe the implementation of the phonological transcription and Syllabication as part of Phonemicization's process.	medical transcription;syllable;transcription (software)	María Lucila Morales-Rodríguez;Jose Antonio Radilla-Avila;Arturo Hernández-Ramírez;Apolinar Ramírez-Saldivar;Juan Javier González Barbosa	2012	IJCOPI		speech recognition;computer science;linguistics;communication	NLP	-26.078377768307003	-83.49376665156859	85356
fb16fd52ff47eec57ec6b2317b6241553ee75616	multi-scale retrieval in mei: an english-chinese translingual speech retrieval system		"""This paper presents a multi-scale retrieval approach in MEI (Mandarin-English Information), an English-Chinese crosslingual spoken document retrieval (CL-SDR) system. It accepts an entire English news story (from newspaper text) as the input query, and automatically retrieves """"relevant"""" Mandarin news stories (from broadcast audio). This allows the user to search for personally relevant content across the language and media barriers – a cross-lingual and cross-media retrieval task. MEI advocates a multi-scale paradigm for the retrieval task. Multiscale refers to the use of both words and subwords (Chinese characters and syllables) for retrieval. Words offer lexical knowledge to enhance precision, and subwords can potentially alleviate some prevailing problems in CL-SDR, e.g. open vocabularies in translation and recognition, out-of-vocabulary words in audio indexing, and ambiguities in Chinese homophones and word tokenizaiton. We present techniques for word-subword fusion, which improved retrieval performance in our experiments with the Topic Detection and Tracking collection."""	document retrieval;etsi satellite digital radio;elegant degradation;experiment;loose coupling;music encoding initiative;programming paradigm;substring;super robot monkey team hyperforce go!;vocabulary	Wai Kit Lo;Patrick Schone;Helen M. Meng	2001			speech recognition;mandarin chinese;search engine indexing;chinese characters;document retrieval;fusion;newspaper;computer science;homophone	Web+IR	-21.927338632674825	-84.14815033255171	85427
227de7c12337d5a2e1799666a2c9534430470eb0	operational validation of syntactic-semantic models in a spoken man-machine dialogue system	semantic model		dialog system;dialog tree	Guy Deville;Pierre Mousel	1991			semantic data model;speech recognition;syntax;computer science	NLP	-29.558421598266104	-80.31165249813783	85451
9c69f9190d40e3aad1ef7f30d67d77f488217c2b	real-time linguistic analysis for continuous speech understanding	electronic mailbox access;speaker-independent configuration;continuous speech understanding;real-time linguistic analysis;sun sparcstation;c language;continuous speech dialog system;corrupted speech;real time;application scenario;telephone-quality speech;information retrieval	This paper describes the approach followed in the development of the linguistic processor of the continuous speech dialog system implemented at our labs. The application scenario (voice-based information retrieval service over the telephone) poses severe specifications to the system: it has to be speakerindependent, to deal with noisy and corrupted speech, and to work in real time. To cope with these types of applications requires to improve both efficiency and accuracy. At present, the system accepts telephone-quality speech (utterances referring to an electronic mailbox access, recorded through a PABX) and, in the speaker-independent configuration, it correctly understands 72% of the utterances in about twice real time. Experimental results are discussed, as obtained from an implementation of the system on a Sun SparcStation 1 using the C language. 1 I n t r o d u c t i o n We call continuous speech, as opposed to isolatedword speech, any utterance emitted without interposing pauses between words. This is the way humans naturally speak, but to enable a machine to deal with this form of communication constitutes a difficult task because, in addition to the usual speech processing and natural language issues, there is no hint on where the single words of the utterance begin and end. Given the current state-of-the-art, speech understanding prototypes concern the comprehension of utterances referring to a well defined semantic domain on a dictionary of almost one thousand words. Our research group is interested in developing automated voice-based information retrieval services over the telephone network. This has some precise implications. First, we are committed to accept continuousspeech sentences expressed in relatively free syntax, otherwise the interaction with the service would be too unnatural. Second, the service should be public and accessible from every telephone; this means that the un*This research has been partially supported by EEC ESPRIT project no. 2218 SUNDIAL. derstanding system has to be speaker independent and able to process noisy and distorted speech. Third, the response time must be confined within a few seconds; that is, the system has to work in real time. The evolution of the research is gradual. In the present status of work the developed system is speaker independent and it is based on a telephone-line quality speech (a telephone connected through a PABX). It has a vocabulary of 787 words and the processing time is less than 5 seconds (about twice real time). The sentence correct understanding rate is nearly 72%. The application task refers to voice access to an electronic mailbox. In its first version the system has been applied to access a geographical data base, and is now adapted (with a slightly bigger vocabulary) to information retrieval from a train timetable data base. In the following we present a thorough description of the approach that led to these results. Also, the latest developments of the system are discussed. We will focus here on the understanding subsystem. An account of the recognition stage is given in [Fissore et ai. 1989] and in the references there reported, while the whole system is described in [Baggia et al. 1991c]. We will examine the role of the recognition and understanding modules, the technique used for language representation, the parsing control strategy, and finally experimental results will be discussed. 2 Recognition and understanding activities Speech understanding requires the use of different pieces of knowledge. Consequently, it is not obvious a priori what type of architecture will give the best results. Homogeneous, knowledge-based architectures date back to the late 1970s [Erman et ai. 1980] and spurred interesting research work in the subsequent years. However, unified approaches contain a weakness: they have difficulty in coping with problems of different nature through specific, focused techniques. A division may be traced between lower-level processing of speech, mostly based on acoustical knowledge, and upper-level processing, mostly based on natural language knowledge. Therefore, a two-level architecture has been developed based on this idea [Fissore et ai. 1988]. The former stage, called recognition stage (Fig. la), hypothesizes a set of words all over the utterance and feeds the lat-	algorithm;artificial intelligence;control theory;database;dialog system;dictionary;information retrieval;knowledge-based systems;linear algebra;naruto shippuden: clash of ninja revolution 3;natural language;parsing;real-time transcription;response time (technology);schedule;speech processing;speech recognition;telephone line;the sentence;vocabulary	Paolo Baggia;Elisabetta Gerbino;Egidio P. Giachin;Claudio Rullent	1992			voice activity detection;natural language processing;speech recognition;computer science;speech processing;communication	NLP	-25.65446585762994	-82.95300934389152	85744
70e789be2b1ffb0ec884511c13c94f7549c0dba6	automatic modeling of user specific words for a speaker independent recognition system	training utterances;psi_speech;speaker independent models;automatic modeling;speech processing;vocabulary;large vocabulary recognisers;modeling methods;model performance;contracts;phonetic transcriptions;state estimation;noise robustness;user specific words;training data;hidden markov models;speaker independent;speaker independent recognition system;hidden markov models vocabulary speech recognition context modeling state estimation noise reduction noise robustness terminology contracts training data;hidden markov models speech recognition speech processing;csr algorithm;noise reduction;small vocabulary recognisers;speech recognition;terminology;hmm models;recognition error reduction;context modeling;subword models;phonetic transcriptions user specific words speaker independent recognition system automatic modeling training utterances modeling methods small vocabulary recognisers hmm models recognition error reduction noise robustness speaker independent models large vocabulary recognisers csr algorithm subword models	The problem addressed in this paper, is the incorporation of user specific words in a speaker independent speech recognition system. No transcription is used to model the new words, modeling is based on a very small number of training utterances only. We investigated two different modeling methods. The first is intended for small vocabulary recognisers. The HMM models for the new words are enhanced by averaging their states with the nearest speaker independent state. This way, the recognition error was reduced by a factor two, and even the noise robustness of the speaker independent models seems to be transferred to the new models. The second method can be used in large vocabulary recognisers. Using a CSR algorithm, a transcription for the new words is found in terms of the subword models in the recogniser. The resulting models perform equally well as the models based on phonetic transcriptions.	hidden markov model;k-nearest neighbors algorithm;protologism;speech recognition;substring;transcription (software);vocabulary	Jacques Duchateau;Dirk Van Compernolle	1996		10.1109/ICASSP.1996.543259	natural language processing;speaker recognition;training set;speaker diarisation;speech recognition;computer science;pattern recognition;noise reduction;speech processing;context model;terminology	ML	-19.974268735734334	-87.37747890677008	85786
5ecfe7c612affc96d610f6841bc31d510dc48fdb	keep talking - performance effectiveness with continuous voice recognition for spreadsheet users	voice recognition;word recognition;speaker dependent	The performance of spreadsheet users was compared for two modes of input to the computer—keyboard and continuous voice recognition (CVR)—for subjects classified by their decision style. In addition, the data for this experiment was compared to results of a similar experiment that used a discrete word recognition system. Dependent measures were task completion time, accuracy, keystroke count, correction count, and user confidence for spreadsheet tasks. Results, using a speaker-dependent continuous voice recognizer, showed that for both simple data input and more complex analytical problems, subjects did not perform more effectively using CVR compared to keyboard. In addition, a subject's decision style was found not to interact with CVR for an effect on performance. Compared to earlier discrete word recognition results, CVR tended to shorten the time to complete a spreadsheet analysis task.	speech recognition;spreadsheet	Michael J. DeHaemer;George Wright;Margaret H. Richards;Thomas W. Dillon	1997	I. J. Speech Technology	10.1007/BF02215803	natural language processing;speech recognition;word recognition;computer science;linguistics	HCI	-25.68565457180237	-87.38719627777525	85892
5d7f30250e7082ac50f479e3ca8aa3dcd66b1ebc	a comparative study on system combination schemes for lvcsr	broadcast news;rover system combination feature concatenation multi stream model combination lattice rescoring;long span class posterior probability;model combination;probability;lattices;multi stream;large vocabulary continuous speech recognition;system modeling;cepstral features;lattice rescoring system combination schemes lvcsr large vocabulary continuous speech recognition long span class posterior probability cepstral features error patterns multiple knowledge sources encode independent tree rover feature concatenation;vocabulary;lattice rescoring;speech;posterior probability;multiple knowledge sources encode;cepstral analysis speech recognition lattices vocabulary mel frequency cepstral coefficient hidden markov models automatic speech recognition linear discriminant analysis vectors broadcasting;independent tree;mel frequency cepstral coefficient;automatic speech recognition;system combination schemes;cepstral analysis;hidden markov models;vectors;system combination;feature concatenation;feature extraction;comparative study;speech recognition;vocabulary feature extraction probability speech recognition;lvcsr;rover;broadcasting;decision trees;linear discriminant analysis;error patterns	We present a comparative study on combination schemes for large vocabulary continuous speech recognition by incorporating long-span class posterior probability features into conventional short-time cepstral features. System combination can improve the overall speech recognition performance when multiple systems exhibit different error patterns and multiple knowledge sources encode complementary information. A variety of combination approaches are investigated in this paper, e.g., feature concatenation single stream system, model combination multi-stream system, lattice rescoring and ROVER. These techniques work at different levels of a LVCSR system and have different computational cost. We compared their performance and analyzed their advantages and disadvantages on large vocabulary English broadcast news transcription tasks. Experimental results showed that model combination with independent tree consistently outperforms ROVER, feature concatenation and lattice rescoring. In addition, the phoneme posterior probability features do provide complementary information to short-time cepstral features.	algorithmic efficiency;cepstrum;computation;concatenation;encode;rover (the prisoner);speech analytics;speech recognition;transcription (software);vocabulary	Chengyuan Ma;Hong-Kwang Jeff Kuo;Hagen Soltau;Xiaodong Cui;Upendra V. Chaudhari;Lidia Mangu;Chin-Hui Lee	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495627	speech recognition;systems modeling;feature extraction;computer science;speech;machine learning;decision tree;comparative research;pattern recognition;lattice;probability;posterior probability;linear discriminant analysis;broadcasting;statistics	Robotics	-19.80374674818693	-88.47500682763999	86015
34a9617d6fe34d0d71947f3f2c878e7d24fba97d	thai broadcast news corpus construction and evaluation	word error rate;speech recognition;system performance	Large speech and text corpora are crucial to the development of a sta te-of-the-art speech recognition system. This paper reports on the construction and evaluation of the first Thai broadcast news speec h and text corpora. Specifications and conventions used in the transcription process are described in the paper. The speech corpus ntains about 17 hours of speech data while the text corpus was transcribed from around 35 hours of television broadcast news. The characteristics of the corpus were analyzed and shown in the paper. The speech corpus was split according to the evaluation focus condition u sed in the DARPA Hub-4 evaluation. An 18k-word Thai speech recognition system was setup to test with this speech corpus as a preliminary experiment. Acoustic model adaptations were performed to improve the system performance. The best system yielded a word err r rate of about 20% for clean and planned speech, and below 30% for the overall condition.	acoustic cryptanalysis;acoustic model;sed;speech corpus;speech recognition;text corpus;transcription (software)	Markpong Jongtaveesataporn;Chai Wutiwiwatchai;Koji Iwano;Sadaoki Furui	2008			speech recognition;word error rate;magnification;natural language processing;artificial intelligence;small number;computer science;broadcasting	NLP	-21.03697726392823	-84.2540231003604	86240
45e36e7d6d05e2c39c14e73b415d52d239e5990b	a processing model for free word order languages	word order;long distance;process model	Like many verb-final languages, German displays considerable word-order freedom: there is no syntactic constraint on the ordering of the nominal arguments of a verb, as long as the verb remains in final position. This effect is referred to as “scrambling”, and is interpreted in transformational frameworks as leftward movement of the arguments. Furthermore, arguments from an embedded clause may move out of their clause; this effect is referred to as “long-distance scrambling”. While scrambling has recently received considerable attention in the syntactic literature, the status of long-distance scrambling has only rarely been addressed. The reason for this is the problematic status of the data: not only is long-distance scrambling highly dependent on pragmatic context, it also is strongly subject to degradation due to processing constraints. As in the case of center-embedding, it is not immediately clear whether to assume that observed unacceptability of highly complex sentences is due to grammatical restrictions, or whether we should assume that the competence grammar does not place any restrictions on scrambling (and that, therefore, all such sentences are in fact grammatical), and the unacceptability of some (or most) of the grammatically possible word orders is due to processing limitations. In this paper, we will argue for the second view by presenting a processing model for German. Comments University of Pennsylvania Institute for Research in Cognitive Science Technical Report No. IRCS-95-13. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/ircs_reports/127	center embedding;cognitive science;elegant degradation;embedded system	Owen Rambow;Aravind K. Joshi	1994	CoRR		word order;natural language processing;computer science;scrambling;process modeling;linguistics	NLP	-31.090496224921612	-83.2092860710792	86462
ec027d71f7bfd88cb6ce98997307a8780c602bf8	dictionary refinements based on phonetic consensus and non-uniform pronunciation reduction	multiple sequence alignment;search space	In this paper we present a procedure to refine the recognition dictionary based on a composite approach to prune the unneeded pronunciations. First, pruning is applied in a non-uniform manner according to the characteristics of each word. Even though this straightforward operation may produce high-quality dictionaries, it makes the refined dictionary heavily dependent on the data used in this process. For the words not observed in the data, we propose, in second place, to use multiple sequence alignment techniques in order to find phonetic consensus among the pronunciation variants and select the worthy pronunciations that will represent the unobserved words. Experimental results show that our dictionary refining method helps to improve the recognition performance in two relevant aspects: it increases the recognition accuracy by reducing the cross-word confusibility and it improves the recognition speed by reducing the complexity of the search space.	data dictionary;multiple sequence alignment;non-functional requirement	Gustavo Hernández Ábrego;Lex Olorenshaw;Raquel Tato;Thomas Schaaf	2004			speech recognition;artificial intelligence;electrical connection;backplane;pattern recognition;row;pronunciation;cable gland;computer science	Vision	-20.61298490261689	-85.62498102721537	87005
97a81b48802755bcbb7efa9b39615e24417dfb9a	a real-life, french-accented corpus of air traffic control communications		This paper describes the creation of the AIRBUS-ATC corpus, which is a real-life, French-accented speech corpus of Air Traffic Control (ATC) communications (message exchanged between pilots and controllers) intended to build a robust ATC speech recognition engine. The corpus is currently composed of 59 hours of transcribed English audio, along with linguistic and meta-data annotations. It is intended to reach 100 hours by the end of the project. We describe ATC speech specificities, how the audio is collected, transcribed and what techniques were used to ensure transcription quality while limiting transcription costs. A detailed description of the corpus content (speaker gender, accent, role, type of control, speech turn duration) is given. Finally, preliminary results obtained with state-of-the-art speech recognition techniques support the idea that accent-specific corpora will play a pivotal role in building robust ATC speech recognition applications.	advanced tactical center;advanced transportation controller;real life;speech corpus;speech recognition;text corpus;transcription (software)	Estelle Delpech;Marion Laignelet;Christophe Pimm;Céline Raynal;Michal Trzos;Alexandre Arnold;Dominique Pronto	2018			natural language processing;artificial intelligence;speech recognition;air traffic control;spoken language;speech corpus;limiting;computer science	ML	-22.9814810669064	-84.65847621486328	87091
a3e388bab881e68eaa544c8b4f6ea0838ff59386	discourse chunking: a tool in dialogue act tagging	dialogue act tagger;discourse chunk;dialogue participant;dialogue act tagging;case-based reasoning approach;case base reasoning	Discourse chunking is a simple way to segment dialogues according to how dialogue participants raise topics and negotiate them. This paper explains a method for arranging dialogues into chunks, and also shows how discourse chunking can be used to improve performance for a dialogue act tagger that uses a case-based reasoning approach. 1 Dialogue act tagging A dialogue act (hereafter DA) is an encapsulation of the speakerÕs intentions in dialogueÑwhat the speaker is trying to accomplish by saying something. In DA tagging (similar to part-of-speech tagging), utterances in a dialogue are tagged with the most appropriate speech act from a tagset. DA tagging has application in NLP work, including speech recognition and language understanding. The Verbmobil-2 corpus was used for this study, with its accompanying tagset, shown in Table 1.1. Much of the work in DA tagging (Reithinger, 1997; Samuel, 2000; Stolcke et al. 2000; Wright, 1998) uses lexical information (the words or ngrams in an utterance), and to a lesser extent syntactic and phonological information (as with prosody). However, there has traditionally been a lack of true discourse-level information in tasks involving dialogue acts. Discourse information is typically limited to looking at surrounding DA tags (Reithinger, 1997; Samuel, 2000). Unfortunately, knowledge of prior DA tags does not always translate to an accurate guess of whatÕs coming next, especially when this information is imperfect. Theories about the structure of dialogue (for example, centering [Grosz, Joshi, & Weinstein 1995], and more recently Dialogue Macrogame Theory [Mann 2002]) have not generally been applied to the DA tagging task. Their use amounts to a separate tagging task of its own, with the concomitant time-consuming corpus annotation. In this work, I present the results from a DA tagging project that uses a case-based reasoning system (after Kolodner 1993). I show how the results from this DA tagger are improved by the use of a concept I call Òdiscourse chunking.Ó Discourse chunking gives information about the patterns of topic raising and negotiation in diaTag Example ACCEPT sounds good to me BACKCHANNEL mhm BYE see you CLARIFY I said the third CLOSE okay <uhm> so I guess that is it COMMIT I will get that arranged then CONFIRM well I will see you <uhm> at the airport on the third DEFER and I will get back to you on that DELIBERATE so let us see DEVIATE_SCENARIO oh I have tickets for the opera on Friday EXCLUDE January is basically shot for me EXPLAINED_REJECT I am on vacation then FEEDBACK gosh FEEDBACK_NEGATIVE not really FEEDBACK_POSITIVE okay GIVE_REASON because that is when the express flights are GREET hello Miriam INFORM <uhm> I I have a list of hotels here INIT so we need to schedule a trip to Hanover INTRODUCE Natalie this is Scott NOT_CLASSIFIABLE and <uh> OFFER <uhm> would you like me to call POLITENESS_FORMULA good of you to stop by REFER_TO_SETTING want to step into your office since we are standing right outside of it REJECT no that is bad for me unfortunately REQUEST you think so? REQUEST_CLARIFY I thought we had said twelve noon REQUEST_COMMENT is that alright with you REQUEST_COMMIT can you take care of <uhm> arranging those reservations REQUEST_SUGGEST do you have any preference SUGGEST we could travel on a Monday THANK okay thanks John Table 1.1. The tagset for the Verbmobil-2 corpus. (Verbmobil 2003) Proceedings of the ACL-2003 Student Research Workshop, pp. 58-63.	backchannel;brill tagger;commit (sql);care-of address;case-based reasoning;domain of discourse;encapsulation (networking);init;mike lesser;n-gram;natural language processing;natural language understanding;part-of-speech tagging;reasoning system;semantic prosody;shallow parsing;speech recognition;tag (metadata);verbmobil;wright (adl)	T. Daniel Midgley	2003			natural language processing;case-based reasoning;computer science;linguistics	NLP	-30.861072706412347	-83.2575489200132	87284
7a74d86af7f58fd00b574c3a8c5dfd173cd400c4	context-dependent phonetic hidden markov models for speaker-independent continuous speech recognition	interpolation;speech recognition markov processes;architecture systeme;chaine markov;cadena markov;reconocimiento palabra;taux erreur;helium;etude experimentale;hidden markov model;context dependent phone models;subword clustering procedure;triphones;training data;error analysis;continuous speech recognition;speaker independent continuous speech recognition;hidden markov models;marine vehicles;speaker independent;hidden markov models speech recognition context modeling interpolation training data error analysis robustness marine vehicles computer science;context dependent phonetic units;error rate;speech recognition;arquitectura sistema;robustness;context dependent;information theoretic measure;markov processes;reconnaissance parole;computer science;system architecture;indice error;information theoretic;context modeling;estudio experimental;error rate context dependent phone models hidden markov models speaker independent continuous speech recognition context dependent phonetic units information theoretic measure subword clustering procedure triphones training data;markov chain	Context-dependent phone models are applied to speaker-independent continuous speech recognition and shown to be effective in this domain. Several previously proposed context-dependent models are evaluated, and two new context-dependent phonetic units are introduced: function-word-dependent phone models, which focus on the most difficult subvocabulary; and generalized triphones, which combine similar triphones on the basis of an information-theoretic measure. The subword clustering procedure used for generalized triphones can find the optimal number of models, given a fixed amount of training data. It is shown that context-dependent modeling reduces the error rate by as much as 60%. >	hidden markov model;markov chain;speech recognition	Kay-Fu Lee	1990	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/29.52701	training set;markov chain;speech recognition;interpolation;word error rate;computer science;machine learning;context-dependent memory;pattern recognition;mathematics;context model;markov process;helium;hidden markov model;statistics;robustness	ML	-19.923172300550164	-91.11475648365803	87557
42674da89e09d74ad637779946bc6ae717c49980	knowledge integration in text recognition	bottom up;transition probability;top down;classification error;knowledge integration	The paper describes an algorithm based on AI techniques for recognizing words of printed or hand-written text--with the technique developed also applicable to correcting substitution spelling errors. The algorithm effectively integrates bottom-up information in the form of letter shapes, letter transitional probabilities and letter classification-error probabilities together with top-down knowledge in the form of a lexicon of legal words represented as a letter trie. Experimental results with the algorithm are reported for the combined top-down and bottom-up approach and for each of the two approaches individually.	algorithm;bottom-up parsing;bottom-up proteomics;knowledge integration;lexicon;optical character recognition;printing;top-down and bottom-up design;trie	Sargur N. Srihari;Jonathan J. Hull	1982			speech recognition;knowledge integration;computer science;artificial intelligence;machine learning;pattern recognition;top-down and bottom-up design;data mining;statistics	AI	-22.91782548351138	-80.83607311843285	87836
b827f9a11cbfe4444737bb0f29a9ec6c3be6fced	scalable architecture for word hmm-based speech recognition and vlsi implementation in complete system	field programmable gate array;cmos digital integrated circuits speech recognition hidden markov models vlsi field programmable gate arrays;hidden markov model;real time;speech analysis;noise robustness;0 18 micron speech recognition vlsi implementation speech recognizers word hidden markov models word recognition recognition vocabulary parallel computations word vocabulary speech analysis cmos cell library field programmable gate array 80 mhz;hidden markov models;cmos digital integrated circuits;parallel computer;word recognition;vlsi;speech recognition;field programmable gate arrays;speech recognition very large scale integration hidden markov models computer architecture vocabulary computational efficiency concurrent computing speech analysis noise robustness libraries;high speed;vlsi implementation hidden markov model hmm scalable architecture speech recognition	This paper describes a scalable architecture for real-time speech recognizers based on word hidden Markov models (HMMs) that provide high recognition accuracy for word recognition tasks. However, the size of their recognition vocabulary is small because its extremely high computational costs cause long processing times. To achieve high-speed operations, we developed a VLSI system that has a scalable architecture. The architecture effectively uses parallel computations on the word HMM structure. It can reduce processing time and/or extend the word vocabulary. To explore the practicality of our architecture, we designed and evaluated a complete system recognizer, including speech analysis and noise robustness parts, on a 0.18-/spl mu/m CMOS standard cell library and field-programmable gate array. In the CMOS standard-cell implementation, the total processing time is 56.9 /spl mu/s/word at an operating frequency of 80 MHz in a single system. The recognizer gives a real-time response using an 800-word vocabulary.	cmos;clock rate;computation;field-programmability;field-programmable gate array;finite-state machine;hidden markov model;markov chain;real-time clock;scalability;speech recognition;standard cell;standard library;very-large-scale integration;vocabulary;voice analysis	Shingo Yoshizawa;Naoya Wada;Noboru Hayasaka;Yoshikazu Miyanaga	2006	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2005.854408	natural language processing;speech recognition;computer science;machine learning;hidden markov model;field-programmable gate array	HPC	-24.059342181764503	-91.27955205501449	87900
37ff90bb77bf78f3e3d899085777a296fbb4653b	a parser that doesn't	logical form;grammatical theory;sentence comprehension;general principle;abstract formal model;certain basic assumption;incremental semantic interpretation;phrase structure grammars;familiar type;familiar observation;semantic interpretation	This paper describes an implemented parser-interpreter which is intended as an abstract formal model of part of the process of sentence comprehension. It is illustrated here for Phrase Structure Grammars with a translation into a familiar type of logical form, although the general principles are intended to apply to any grammatical theory sharing certain basic assumptions, which are discussed in the paper. The procedure allows for incremental semantic interpretation as a sentence is parsed, and provides a principled explanation for some familiar observations concerning properties of deeply recursive constructions. Background The starting point for the present work is a set of familiar and, for the most part, uncontroversial c|~Lm~ s about the nature of grammatical description and of human parsing of natural language. These claims and assumptions can be briefly summarised as follows: A Hierarchical S t r u c t u r e Linguists assign constituent structures to sentences on the basis ~f distributional tests of various kinds. On the basis of these tests, the 'correct' structures are always hierarchical and often deeply nested. The tree representing a sentence may impose a great deal of structure on it, with string-adjacent items often appearing at very different levels in the tree. In general, shallow, 'flat' structures are not generated by grammars, nor warranted on distributional grounds. However, as we shall see, it is likely that these deeply nested structures may be somewhat remote from any that are actually computed during parsing. B Semantics is (1) composi t ional and (ll) syntax-drlven. Both of these claims can be made in a variety of versions of different strengths, from the trivially true to the fairly clearly false. What is intended here is the assumption sometimes called the 'rule to rule' hypothesis, shared by almost all current grammatical frameworks, that to each syntactic rule of a grammar (or for each subrree induced by such a rule) there is an associated semantic rule, either producing an interpretation directly, or translating into some formal language. Interpretations for whole sentences are built up from the constituent parts in ways specified by these rules, in a fashion which mimics and uses the syntactic structure of the sentence. C Incremental interpretation As a sentence is parsed, its interpretation is built up word by word: there is little or no delay in interpreting it. In particular, we do not wait until all syntactic constituents have been completed before beginning to integrate then into some non-syntactic representation. Ample intuitive and experimental evidence supports this uncontroversial observation. D Limi ted recurs lon . One of the most firmly established facts about human syntactic processing is that constructions which are ineliminably deeply recursive (such as central self-embeddings) are difficult or impossible to parse. A sentence like: I The boy who the girl that the dog bit liked ran away is clumsy at best, and one like: 2 The boy the girl the dog the cat scratched bit saw left is utterly unmanageable under normal circumstances. Under the further assumption, recently more controversial (Katz 1981), that grammars have some kind of mental reality as representations of linguistic knowledge, it is clear that A to D, although simple and generally agreed upon observations, by no means obviously consistent with each other. Consider, for example, the natural way in which one might set about implementing a system which observed B, a principle which, in itself, is a computationally natural principle. Such a system might first parse a sentence, annotating nodes in the resulting tree with an indication of the syntactic rules used. This annotated tree would then be passed to an interpretation routine which applied the appropriate semantic operation to the topmost node (guided by the syntactic information found there, in particular a pointer to the semantic information necessary), calling itself recursively on each subtree to build up the complete interpretation. (Systems operating in more or less this manner are described in Rosenschein and Shieber 1982, Gawron et al. 1982 and Schubert and Pelletier 1982. They are not intended as psychological models in any but the most abstract sense, of course.) Such a system would, in observing B, also naturally be con* sistent with A. Obviously, though, this type of system requires a complete syntactic analysis to be available before it can even begin the process of interpretation, thus conflicting straightforwardly with C. Consider next A and D. The structures which linguists postulate in accordance with A are often recursive, and it is in the nature of hierarchical structures that this should be a possibility. This is rather puzzling in the light of D, for if D is correct, it seems to show that a class of structures which are natural from one point of view (i.e. centre embeddings) are extremely unnatural from another. It is not necessarily to be expected that human linguistic abilities have evoh'ed in a harmonious and homogeneous manner, but other things being equal, we would not expect to find two apparently co-operating modules so ill-suited to each other. Why should grammars be able to generate things that parsers can't parse? When we consider left and right recursions, some further tension between Ao B and D emerges. Multiple left recursions in English are most clearly illustrated by possessive determiner phrases, which are generally assumed to have a structure something like 3:	cognitive model;formal language;mathematical model;naruto shippuden: clash of ninja revolution 3;natural language;parser;phrase structure grammar;phrase structure rules;pointer (computer programming);recursion;semantic interpretation;tree (data structure)	Stephen G. Pulman	1985			natural language processing;semantic interpretation;logical form;computer science;linguistics;programming language;atomic sentence	NLP	-30.905969395134566	-82.870376826043	88036
3ffd87464ad461cd48659698ad3759ae01a63af1	implementation of dictation system for malayalam office document	cmu;large vocabulary continuous speech recognition;hidden markov model;out of vocabulary;rule based;openoffice;hmm;statistical method;hybrid model;cmu sphinx;dictation system;speech recognition	This paper describes the implementation of a dictation system for Malayalam office documents in OpenOffice Writer. Dictation system is built using state-of-the-art large vocabulary continuous speech recognition system for the Malayalam language. This system supports a vocabulary of 5000 most commonly used office domain words and is employed with a vocabulary updating facility to handle out-of-vocabulary words. The system is based on Hidden Markov Model (HMM), trained with huge (25 hours) amount of data. The training data is collected in room environment, ensuring the speaker variance and the phonetic richness. A hybrid model which integrates the rule based method with statistical method is used to handle the pronunciation variations for the creation of the pronunciation dictionary. The system is first of its kind which simplifies the tedious task of typing in Malayalam. Apart from dictating office documents with 75 ±5 % accuracy, the system is equipped with a facility of suggestion generation by which the user will be provided with alternate words for mis-recognized words. The system also supports some basic voice command operations for file operations like open, save, close etc. This system has an option to adapt to the user's voice which will improve the recognition accuracy by 2-5%. The system is successfully implemented in OpenOffice Writer and tested.	dictionary;hidden markov model;markov chain;openoffice basic;speech recognition;vocabulary	P. Shobana Devi;Jose Stephen;G. Sulochana Kurambath;R. Ravindra Kumar	2012		10.1145/2345396.2345521	natural language processing;speech recognition;computer science;hidden markov model	NLP	-21.992983131936935	-85.56302187341491	88212
b40a27125cb54ae4a69a52d8b5d3199c900729f0	phonetic normalization of microtext	microtext;training;data mining;phonetic;media;training data;normalization;twitter;rendering computer graphics;social media;wiktionary	Microtext normalization is the challenge of discovering the English words corresponding to the unusually-spelled words used in social-media messages and posts. In this paper, we propose a novel method for doing this by rendering both English and microtext words phonetically based on their spelling, and matching similar ones together. We present our algorithm to learn spelling-to-phonetic probabilities and to efficiently search the English language and match words together. Our results demonstrate that our system correctly handles many types of normalization problems.	algorithm;database normalization;microprinting	Richard Khoury	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2809352	natural language processing;training set;speech recognition;media;social media;computer science;machine learning;normalization;data mining;multimedia	SE	-23.6020085403602	-80.90086506962457	88337
5a7f3f0fdbdc29fddc7a41098ee8bbc3f7cfd1a1	toward human parity in conversational speech recognition		Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure a human error rate on the widely used NIST 2000 test set for commercial bulk transcription. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion, where friends and family members have open-ended conversations. In both cases, our automated system edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and long-short-term memory acoustic model architectures, combined with a novel spatial smoothing method and lattice-free discriminative acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination. Comparing frequent errors in our human and machine transcripts, we find them to be remarkably similar, and highly correlated as a function of the speaker. Human subjects find it very difficult to tell which errorful transcriptions come from humans. Overall, this suggests that, given sufficient matched training data, conversational speech transcription engines are approximating human parity in both quantitative and qualitative terms.		Wayne Xiong;Jasha Droppo;Xuedong Huang;Frank Seide;Michael L. Seltzer;Andreas Stolcke;Dong Yu;Geoffrey Zweig	2017	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2017.2756440	word error rate;pattern recognition;speech recognition;computer science;artificial intelligence;parity (mathematics);transcription (linguistics);acoustic model;test set;language model;recurrent neural network;smoothing	ML	-19.903033558580933	-83.12478474133091	88851
5bce8a6b14811ab692cd24286249b1348b26ee75	minimum text corpus selection for limited domain speech synthesis		This paper concerns limited domain TTS system based on the concatenative method, and presents an algorithm capable to extract the minimal domainoriented text corpus from the real data of the given domain, while still reaching the maximum coverage of the domain. The proposed approach ensures that the least amount of texts are extracted, containing the most common phrases and (possibly) all the words from the domain. At the same time, it ensures that appropriate phrase overlapping is kept, allowing to find smooth concatenation in the overlapped regions to reach high quality synthesized speech. In addition, several recommendations allowing a speaker to record the corpus more fluently and comfortably are presented and discussed. The corpus building is tested and evaluated on several domains differing in size and nature, and the authors present the results of the algorithm and demonstrate the advantages of using the domain oriented corpus for speech synthesis.	algorithm;command-line interface;concatenation;display resolution;speech synthesis;text corpus	Markéta Juzová;Daniel Tihelka	2014		10.1007/978-3-319-10816-2_48	natural language processing;speech recognition;pattern recognition	NLP	-20.994222486490187	-83.34266866898669	89389
f23d7076ba7d830de283b3fa1e60aebafef13dd0	boeing's nlp system and the challenges of semantic representation	semantic phenomenon;usable representation;additional transformation;initial logic generator;semantic representation;original text;nlp system;subsequent processing;general question;processing module	"""We describe Boeing's NLP system, BLUE, comprising a pipeline of a parser, a logical form (LF) generator, an initial logic generator, and further processing modules. The initial logic generator produces logic whose structure closely mirrors the structure of the original text. The subsequent processing modules then perform, with somewhat limited scope, additional transformations to convert this into a more usable representation with respect to a specific target ontology, better able to support inference. Generating a semantic representation is challenging, due to the wide variety of semantic phenomena which can occur in text. We identify seventeen such phenomena which occurred in the STEP 2008 """"shared task"""" texts, comment on BLUE's ability to handle them or otherwise, and discuss the more general question of what exactly constitutes a """"semantic representation"""", arguing that a spectrum of interpretations exist. 1 System Description 1.1 Overview and Scope As our contribution to the 2008 STEP Symposium's """"shared task"""" of comparing semantic representations (Bos, 2008), we describe Boeing's NLP system, BLUE (Boeing Language Understanding Engine), and subsequently analyze its performance on the task’s shared texts. BLUE consists of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules. The parser has broad coverage and is domain general. The logical form generator currently deals with a (reasonably large) subset of linguistic phenomena, including simple sentences, prepositional phrases, compound nouns, ordinal modifiers, proper nouns, some simple types of coordination, adverbs, negation, comparatives, and modals. The initial logic generator performs a straightforward transformation of the LF to firstorder logic syntax. Subsequent processing modules then perform word sense disambiguation, semantic role labeling, coreference resolution, and some limited metonymic and other transformations. The overall system currently produces output expressed in one of two target ontologies, namely WordNet and the University of Texas at Austin's Component Library (CLib) (Barker et al, 2001). In this paper we illustrate the system's use with WordNet's ontology. The overall system was originally developed for interpreting a controlled language called CPL (Clark et al., 2007), but also often makes reasonable interpretations of more complex, open text sentences, as we illustrate here. 1.2 Parsing and the Logical Form Generator Parsing is performed using SAPIR, a mature, bottom-up, broad coverage chart parser (Harrison & Maxwell 1986). The parser's cost function is biased by a database of manually and corpus-derived """"tuples"""" (good parse fragments), as well as handcoded preference rules. During parsing, the system also generates a logical form (LF), a semi-formal structure between a parse and full logic. The LF is a simplified and normalized tree structure with logic-type elements, generated by rules parallel to the grammar rules, that contains variables (prefixed by underscores “_”) and additional expressions for other sentence constituents. Variables can represent noun phrases, propositions, and even verb phrases (e.g., “To solve this problem is difficult”). Some disambiguation decisions are performed at this stage (e.g., structural, part of speech), while others are deferred (e.g., word senses, semantic roles), and there is no explicit quantifier scoping. Various syntactic properties and relationships are captured in the LF, including: S (sentence), PP (prepositional phrase), NN (noun compound), PN (proper name), PLUR (plural), PLURN (numbered plural). Tense, aspect, and polarity are also recorded in the LF. For example: ;;; LF for """"An object is thrown with a horizontal speed of 20 m/s from a cliff that is 125 m high."""" (DECL ((VAR _X1 """"an"""" """"object"""") (VAR _X3 NIL (PLUR-N """"20"""" """"m/s"""")) (VAR _X2 """"a"""" """"horizontal speed"""" (PP """"of"""" _X3)) (VAR _X4 """"a"""" """"cliff"""" (DECL NIL (S (PRESENT) _X4 """"be"""" (S-ADJ _X4 (DEGREE (MEASUREMENT """"125"""" """"m"""") """"high"""")))))) (S (PRESENT) NIL """"throw"""" _X1 (PP """"with"""" _X2) (PP """"from"""" _X4))) 1.3 The Initial Logic Generator The LF is then used to generate ground logical assertions of the form r(x,y), containing Skolem instances (denoting existentially quantified variables) by applying a set of simple, syntactic rewrite rules recursively to it. Verbs are reified as individuals, Davidsonian-style. At this stage of processing, the binary predicates are: subject (syntactic subject), sobject (syntactic object), mod (modifier), all the prepositions, value (for physical quantities), number-of-elements (for numbered plurals), and named (for proper names). For example, the above LF is translated into “syntactic logic” (additional predicates indicating part of speech, tense, aspect, determiners, and polarity are not shown): ;;; """"An object is thrown with a horizontal speed ;;; of 20 m/s from a cliff that is 125 m high."""" """"object""""(object01), value(quantity01,[20,m/s_n1]), """"m/s""""(m/s_n1), """"speed""""(speed01), """"horizontal""""(horizontal01), mod(speed01,horizontal01), """"of""""(speed01,quantity01), """"cliff""""(cliff01), """"be""""(be01), subject(be01,cliff01), sobject(be01,height01), value(height01,[125,m_n1]), """"m""""(m_n1), """"height""""(height01), """"throw""""(throw01), sobject(throw01,object01), """"with""""(throw01,speed01), """"from""""(throw01,cliff01). 1.4 Subsequent Processing Modules While the output of the basic system is in a logic syntax, it is not coherent enough to support inference as it preserves many difficult linguistic phenomena (ambiguity, metonymy, etc.). Further semantic interpretation involves disambiguation and aligning the interpretation with the target ontology we are using. In general, this is a complex task and our system only makes limited steps in this direction using five modules: word sense disambiguation (WSD); semantic role labeling (SRL); coreference resolution (including across different parts of speech); metonymy resolution (with respect to the target ontology); and structural transformations. We describe these modules below. Word Sense Disambiguation When using WordNet's ontology, each synset in WordNet is a target concept for WSD. BLUE currently performs naive word sense disambiguation by simply selecting the most common synset for a given word+part-of-speech using contextindependent frequency statistics. When using the Component Library (CLib) ontology, BLUE exploits hand-authored mappings between WordNet synsets and CLib concepts: Given a word, e.g., """"cliff"""", BLUE first finds WordNet synsets for the word, then climbs WordNet's taxonomy from those synsets until it finds synsets mapped to CLib concepts, and returns those CLib concepts, again using preference based on context-independent frequency statistics. Verb nominalizations map to the denominalized verb, thus """"fall""""(n) and """"falling""""(n) both map to synsets for """"fall""""(v). Semantic Role Labeling With both ontologies, BLUE uses the same relational vocabulary of approximately 100 binary semantic relations, drawn from the relation set used by UT's Component Library. Semantic role labeling (SRL), for both for verb-noun and noun-noun relationships, is performed using a set of handauthored SRL rules, e.g., """"from""""(x,y) is labeled as origin(x,y) if x is a movement event and y is an object. In cases where the rules are not adequate to clearly identify a semantic relation, the relation is left as a syntactic relation. Coreference Coreference (e.g., """"the ball"""") is computed by searching for a previous entity in the discourse with the same word and qualifiers as in the referring noun phrase. (Coreference using synonyms or types produced more errors than it removed). Metonymy Often a sentence relates entities in a way inconsistent with the target ontology. For example, with the Component Library (CLib) ontology, movement properties (e.g., speed, acceleration) are defined as properties of the movement events, rather than of the object moving. Thus a phrase like """"the initial speed of the ball"""" is metonymous (with respect to CLib) for """"the initial speed of the movement of the ball"""". This module spots and corrects such metonymies using a small set of metonymy resolution rules. Note that metonymy resolution is ontologyspecific, reflecting design decisions about what is and is not an allowable expression in the target ontology. Structural Transformations Often, the structure of the syntactic and (desired) semantic representations differ, and so some structural transformations are necessary. For example, in the basic processing, verbs (e.g., """"weigh"""") are reified as individuals with semantic roles, e.g., """"weigh""""(w), subject(w,x), sobject(w,y), whereas the target ontology stipulates that some particular verbs denote relations e.g., """"weigh"""" corresponds to the CLib relation weight(x,y), not a Weigh event. (This is indicated in CLib by the relation weigh() being associated with synsets for the verb """"weigh""""). Similarly, nouns associated with relations will be transformed to introduce that relation into the representation, e.g., """"weight""""(y), """"of""""(y,x) will be transformed to weight(x,y). This module makes these and other transformations. The verbs """"be"""" and """"have"""" are similarly mapped to relations, but with the extra step that the target relation depends on the arguments. A small set of rules determines the appropriate relation to use. 2 Semantic Formalism"""	bottom-up parsing;cpl;chart parser;coherence (physics);controlled natural language;emoticon;entity;existential quantification;logic gate;maxwell (microarchitecture);modifier key;natural language processing;ontology (information science);ontology components;ordinal data;pp (complexity);pipeline (computing);quantifier (logic);recursion;reification (computer science);rewrite (programming);rewriting;scope (computer science);semantic data model;semantic interpretation;semantic role labeling;semantics (computer science);semiconductor industry;synonym ring;syntactic monoid;tree structure;vocabulary;word sense;word-sense disambiguation;wordnet	Peter Clark;Philip Harrison	2008		10.3115/1626481.1626502	semantic similarity;computer science;artificial intelligence;data mining;algorithm	NLP	-31.996941082866307	-82.31292307530857	89640
8baf26b94bfe4f2764bd0ae2272a940c1d0f7329	utterance classification using linguistic and non-linguistic information for network-based speech-to-speech translation systems	pragmatics;vocabulary;language translation;smart phones;speech;gis speech to speech translation smartphone;vocabulary audio databases language translation linguistics pattern classification smart phones spatiotemporal phenomena speech recognition;vectors;gis;business;mobile communication;pattern classification;spatiotemporal phenomena;speech recognition;speech to speech translation;audio databases;smartphone;voicetra log corpus utterance classification linguistic information nonlinguistic information network based speech to speech translation system network based mobile services large scale log database smartphone application spatiotemporal information spatiotemporal corpus speech recognition accuracy improvement machine translation accuracy improvement location dependency vocabulary location based services l2 regularized logistic regression;pragmatics business speech mobile communication speech recognition knowledge discovery vectors;knowledge discovery;linguistics	Network-based mobile services, such as speech-to-speech translation and voice search, enable the construction of large-scale log database including speech. We have developed a smartphone application called VoiceTra for speech-to-speech translation and have collected 10,000,000 utterances so far. This huge corpus is unique in size and spatio-temporal information; it contains information on anonymized user locations. This spatiotemporal corpus can be used for improving the accuracy of its speech recognition and machine translation, and it will open the door for the study of the location dependency of vocabulary and new applications for location-based services. This paper first analyzes the corpus and then presents a novel method for classifying utterances using linguistic and non-linguistic information. L2-regularized Logistic Regression is used for utterance classification. Our experiments performed on the VoiceTra log corpus revealed that our proposed method outperformed baseline methods in terms of F measure.	baseline (configuration management);experiment;location-based service;logistic regression;machine translation;mobile app;smartphone;speech recognition;vocabulary	Komei Sugiura;Ryong Lee;Hideki Kashioka;Koji Zettsu;Yutaka Kidawara	2013	2013 IEEE 14th International Conference on Mobile Data Management	10.1109/MDM.2013.96	natural language processing;speech recognition;example-based machine translation;mobile telephony;computer science;speech;pragmatics	NLP	-22.49060781912672	-88.65032403146776	89677
b2fcc5964ec4953b35c107f55aad1318a9addc37	an automatic dialog simulation technique to develop and evaluate interactive conversational agents	agent-based dialog simulation technique;evaluate interactive conversational agents;different dialog corpus;conversational agent;new coherent answer;dialog simulation technique show;dialog strategy;dialog model;automatic dialog simulation technique;new situation;possible dialog strategy;new dialog strategy	During recent years, conversational agents have become a solution to provide straightforward and more natural ways of retrieving information in the digital domain. In this article, we present an agent-based dialog simulation technique for learning new dialog strategies and evaluating conversational agents. Using this technique, the effort necessary to acquire data required to train the dialog model and then explore new dialog strategies is considerably reduced. A set of measures has also been defined to evaluate the dialog strategy that is automatically learned and to compare different dialog corpora. We have applied this technique to explore the space of possible dialog strategies and evaluate the dialogs acquired for a conversational agent that collects monitored data from patients suffering from diabetes. The results of the comparison of these measures for an initial corpus and a corpus acquired using the dialog simulation technique show that the conversational agent reduces the time needed to complete the dialogs and improve their quality, thereby allowing the conversational agent to tackle new situations and generate new coherent answers for the situations already present in an initial model.	agent-based model;coherence (physics);context-aware network;develop;dialog manager;dialog system;high- and low-level;requirement;simulation;text corpus	David Griol;Javier Ignacio Carbó Rubiera;José M. Molina López	2013	Applied Artificial Intelligence	10.1080/08839514.2013.835230	natural language processing;speech recognition;computer science;dialog system	AI	-27.364532945298116	-86.13711899968172	90122
1214f8cc4f6abe60208924e5d79564f20f735216	implementation of rule based algorithm for sandhi-vicheda of compound hindi words	rule based;language	Sandhi means to join two or more words to coin new word. Sandhi literally means `putting together' or combining (of sounds), It denotes all combinatory sound-changes effected (spontaneously) for ease of pronunciation. Sandhi-vicheda describes [5] the process by which one letter (whether single or cojoined) is broken to form two words. Part of the broken letter remains as the last letter of the first word and part of the letter forms the first letter of the next letter. SandhiVicheda is an easy and interesting way that can give entirely new dimension that add new way to traditional approach to Hindi Teaching. In this paper using the Rule based algorithm we have reported an accuracy of 60-80% depending upon the number of rules to be implemented.	algorithm;computer science;join (sql);protologism	Priyanka Gupta;Vishal Goyal	2009	CoRR		natural language processing;speech recognition;computer science;artificial intelligence;linguistics;language	DB	-26.631585135982274	-81.80279229735373	90331
ab9c51bfae705df4d4144e93f07d614671cb3a4d	some comments on algorithm and grammar in the automatic parsing of natural languages	increasing complexity;elementary natural-language condition;final version;automatic parsing;simple parsing algorithm;machine translation;appropriate table;natural language;different grammar;concrete example;oft-repeated assertion;nonelementary table	"""The purpose of this paper is to examine the oft-repeated assertion regarding the efficiency of a """"simple parsing algorithm"""" combinable with a variety of different grammars written in the form of appropriate tables of rules. The paper raises the question of the increasing complexity of the tables when more than the most elementary natural-language conditions are included, as well as the question of the ordering of the rules within such nonelementary tables. Some concrete examples from the field of machine translation will be given in the final version of the paper. Some conclusions are presented."""	algorithm;assertion (software development);machine translation;natural language;parsing	Paul L. Garvin	1965	Mech. Translat. & Comp. Linguistics		natural language processing;synchronous context-free grammar;computer science;s-attributed grammar;linguistics;machine translation;natural language;programming language	NLP	-30.45513328542215	-81.63223448238443	90382
9beb63a5bab34083ef9480aa301b94667e275b22	automated two-way entrainment to improve spoken dialog system performance	speech based user interfaces error statistics interactive systems speaker recognition;relative reduction automated two way entrainment spoken dialog system performance lexical entrainment dialog success rate lexical choices task completion dialog estimated error rate reduction;entrainment spoken dialog systems;speech based user interfaces;speaker recognition;error statistics;system performance speech error analysis speech recognition convergence context schedules;interactive systems	This paper proposes an approach to the use of lexical entrainment in Spoken Dialog Systems. This approach aims to increase the dialog success rate by adapting the lexical choices of the system to the user's lexical choices. If the system finds that the users lexical choice degrades the performance, it will try to establish a new conceptual pact, proposing other words that the user may adopt, in order to be more successful in task completion. The approach was implemented and tested in two different systems. Tests showed a relative dialog estimated error rate reduction of 10% and a relative reduction in the average number of turns per session of 6%.	brainwave entrainment;dialog system;lexical choice;spoken dialog systems	José Lopes;Maxine Eskénazi;Isabel Trancoso	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639298	natural language processing;speaker recognition;speech recognition;computer science;dialog system	NLP	-25.86886688667977	-86.9065693640012	90651
405c514fc4d1dc5c9bdb7ce1a85bdcb5192536d9	modular recurrent neural networks for mandarin syllable recognition	mce gpd algorithms;architectural design;syllabe;modular recurrent neural networks;mandarina;mandarin;reconocimiento palabra;recidivante;mandarine;etude experimentale;systeme modulaire;calcul erreur;sistema modular;indexing terms;classification;mandarin speech recognition;error analysis;syllable;recidivant;recurrent;modular system;calculo error;speech recognition;recurrent neural networks hidden markov models artificial neural networks speech recognition vocabulary mutual information councils pattern recognition neural networks error analysis;recurrent neural nets;reconnaissance parole;recurrent neural network;weight function;reseau neuronal;silaba;article;estudio experimental;clasificacion;red neuronal;recurrent neural nets speech recognition;system complexity modular recurrent neural network mandarin syllable recognition mrnn based speech recognition feature domain temporal domain subsyllable discrimination tone discrimination dynamic weighting functions linguistic knowledge initial final structures mrnn architecture design time alignment scaling problem large vocabulary speech recognition reverse time mrnn rev mrnn bidirection mrnn bi mrnn recognition rate complexity;neural network	A new modular recurrent neural network (MRNN)- based speech-recognition method that can recognize the entire vocabulary of 1280 highly confusable Mandarin syllables is proposed in this paper. The basic idea is to first split the complicated task, in both feature and temporal domains, into several much simpler subtasks involving subsyllable and tone discrimination, and then to use two weighting RNN's to generate several dynamic weighting functions to integrate the subsolutions into a complete solution. The novelty of the proposed method lies mainly in the use of appropriate a priori linguistic knowledge of simple initial-final structures of Mandarin syllables in the architecture design of the MRNN. The resulting MRNN is therefore effective and efficient in discriminating among highly confusable Mandarin syllables. Thus both the time-alignment and scaling problems of the ANN-based approach for large-vocabulary speech-recognition can be addressed. Experimental results show that the proposed method and its extensions, the reverse-time MRNN (Rev-MRNN) and bidirection MRNN (Bi-MRNN), all outperform an advanced HMM method trained with the MCE/GPD algorithm in both recognition-rate and system complexity.	algorithm;artificial neural network;biological neural networks;hiv rev-responsive element;hidden markov model;iso/iec 11404;image scaling;linguistics;linuxmce;loudspeaker time alignment;rev;random neural network;recurrent neural network;speech recognition;super robot monkey team hyperforce go!;syllable;vocabulary	Sin-Horng Chen;Yuan-Fu Liao	1998	IEEE transactions on neural networks	10.1109/72.728393	weight function;speech recognition;index term;mandarin chinese;biological classification;computer science;artificial intelligence;recurrent neural network;machine learning;syllable;artificial neural network	AI	-19.830733140767972	-89.2331707329889	90679
d9beae4745da65cdf7057fddc485aa399a4b290a	sharing problems and solutions for machine translation of spoken and written interaction	chat interaction;segmenting utterance;common solution;small translation unit;written interaction share;tag input;machine translation	Examples from chat interaction are presented to demonstrate that machine translation of written interaction shares many problems with translation of spoken interaction. The potential for common solutions to the problems is illustrated by describing operations that normalize and tag input before translation. Segmenting utterances into small translation units and processing short turns separately are also motivated using data from chat.	database normalization;instant messaging;language model;machine translation;online chat;preprocessor;speech recognition	Sherri Lee Condon;Keith Miller	2002			dynamic and formal equivalence;computer-assisted translation;natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;machine translation;rule-based machine translation;machine translation software usability	NLP	-21.613608452333075	-84.74795484985559	91035
4145ff2db3a3b8c8e46a8f078604ee4d59c172f0	learning similarity functions for pronunciation variations		A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods.	artificial neural network;automated system recovery;bayesian network;binary classification;lexicon;microsoft word for mac;recurrent neural network;similarity measure;speech corpus;speech recognition;spontaneous order;telephone switchboard	Einat Naaman;Yossi Adi;Joseph Keshet	2017			artificial intelligence;natural language processing;speech recognition;speech corpus;binary classification;recurrent neural network;lexicon;computer science;pronunciation;bayesian probability;ranking	NLP	-19.603953374694996	-85.87849356367954	91148
1d57d54279e6da9c573c73b6cef1e6411cb0ae42	slovenian text-to-speech synthesis for speech user interfaces.	text to speech synthesis;unit selection;user interface;speech user interface;efficient implementation;subset selection;international telecommunication union;embedded device	The paper presents the design concept of a unitselection text-to-speech synthesis system for the Slovenian language. Due to its modular and upgradable architecture, the system can be used in a variety of speech user interface applications, ranging from server carrier-grade voice portal applications, desktop user interfaces to specialized embedded devices. Since memory and processing power requirements are important factors for a possible implementation in embedded devices, lexica and speech corpora need to be reduced. We describe a simple and efficient implementation of a greedy subset selection algorithm that extracts a compact subset of high coverage text sentences. The experiment on a reference text corpus showed that the subset selection algorithm produced a compact sentence subset with a small redundancy. The adequacy of the spoken output was evaluated by several subjective tests as they are recommended by the International Telecommunication Union ITU. Keywords—text-to-speech synthesis, prosody modeling, speech user interface.	automatic sounding;baudot code;carrier grade;circa;collocation;desktop computer;embedded system;greedy algorithm;intelligibility (philosophy);lexicon;netware file system;requirement;selection algorithm;semantic prosody;server (computing);speech corpus;speech synthesis;text corpus;the sentence;transcription (software);triphone;ub-tree;user interface	Jerneja Zganec-Gros;Ales Mihelic;Nikola Pavesic;Mario Zganec;Stanislav Gruden	2005			user interface design;natural language processing;speech recognition;computer science;multimedia;user interface	Embedded	-20.18994105983165	-83.53128405232417	91256
0ef3b192c7262f39bd5181163b9c4595348bc16b	a combined approach to the polysemy problems in a chinese to taiwanese tts system	speech synthesis;polysemy problems;chinese to taiwanese tts system;text to speech systems;combined approach;text analysis;speech;taiwanese tts systems;training data;accuracy;correct pronunciation combined approach polysemy problems chinese to taiwanese tts system text to speech systems polysemy means;polysemy means;classification algorithms;text to speech;correct pronunciation;computational linguistics;taiwanese tts systems polysemy problems;decision trees;language model;accuracy decision trees training data computational linguistics text analysis classification algorithms speech	This paper proposes a combined approach to the polysemy problems in a Chinese to Taiwanese text-to-speech (TTS) system. Polysemy means there are words with more than one meaning or pronunciation. For example, there are two kinds of pronunciation for the word (he) in Taiwanese. They are /yi7/ and /yin7/. The first pronunciation, /yi7/, can mean ‘he’ or ‘him’; and the other one, /yin7/, means ‘his’. The correct pronunciation of a word affects the comprehensibility (or clarity) and fluency of Taiwanese speech. We had shown that using language models to solve the polysemy problems can have very good results. We propose a combined approach to the polysemy problems in this paper. There are six words with polysemy problems to be solved in this paper. They are (you), (I), (he), (no), (up), and (down). The numbers of pronunciation of these six words are 2, 2, 2, 6, 3, and 4, respectively. Results show that the proposed combined approach can achieve higher accuracy than the existing methods, WU and DLC.	language model;netware file system;windows update	Yih-Jeng Lin;Ming-Shing Yu;Chin-Yu Lin	2010	2010 7th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2010.5684482	natural language processing;training set;speech recognition;computer science;speech;machine learning;decision tree;accuracy and precision;linguistics;speech synthesis	NLP	-20.902719701751277	-82.07345979683431	91278
bf8bdb36f1b48bd1a5cad4624c04b735f277d7f9	accenting and deaccenting: a declarative approach	declarative approach	"""One of the problems that must be addressed by a textto-speech system is the derivation of pitch accent, marking the distinction between """"given"""" and """"new"""" information in an ut terance. This paper discusses a languageindependent approach to this problem, which is based on focus-accent theory (e.g. Ladd 1978, Gussenhoven 1984, t3aart t987), and implemented in my program P R o s a . This program has been developed as part of the ESPRIT-pro jec t POLYGLOT, and provides an integrated environment for modelling the syntaxto-prosody interface of a multi-lingual textto-speech system. The program operates in the following manner . First , the input text is parsed using a variation of context-free phrase-structure rules, at tgmented with information about """"argument"""" s t ructure of phrases. Next, the syntactic representat ion is mapped onto a metrical tree. The metrical tree is then used to derive locations for pitch accents, as well as phonological and intonational phrase boundaries. in this approach, differences between law guages are modelled entirely by the syntactic rules. Also, the system is strictly declaratiw:, in the sense that once a piece of information is added by a rule, it is never removed. In this respect, our approach differs radically from systems which make use of derivational rules (e.g. Quend & Kager 1992). Such systems tend to become extremely complex, hard to verify and almost impossible to maintain or extend (Quenb & Dirksen 1990, Dirksen & Quen6 in press). By contrast , in PROS-3 there is a conspicuous relation between theory and implementat ion, attd the program can be extended in a number of ways ) Below, 1 will focus on two major rules from focus-accent theory: Default Accent and l/.hythn~ic Deaccenting. The tirst rule is used to model deaccenting of """"given"""" information, e.g. the pronouns it, her and cs in the English, l)utch and German sentences of (1), (2) and (3), respectively."""	comparison of command shells;context-free language;declarative programming;emso simulator;item unique identification;parsing;phrase structure rules;polyglot (computing);semantic prosody	Arthur Dirksen	1992			natural language processing;syntax;polyglot;artificial intelligence;computer science;parsing;derivation;phrase;pitch accent;german	NLP	-30.50383558108921	-82.3069411916948	91432
00ed8ac511564c0e944e1495f20f96804749d15e	filtering errors and repairing linguistic anomalies for spoken dialogue systems	lexi-calized tree grammar;speech recognition;recognizer output hypothesis;on-line computing;filtering error;ill-recognized word;bottom-up syntactico-semantic robust parsing;word confidence score;dialogue system;linguistic anomaly;retained sequence;language processing;bottom up	Our work addresses the integration of speech recognition and language processing for whole spoken dialogue systems. To filter ill-recognized words, we design an on-line computing of word confidence scores based on the recognizer output hypothesis. To infer as much information as possible from the retained sequence of words, we propose a bottom-up syntacticosemantic robust parsing relying on a lexicalized tree grammar and on integrated repairing strategies. 1 I n t r o d u c t i o n Spoken dialogue systems enable people to interact with computers using speech. However, a key challenge for such interfaces is to couple successfully automatic speech recognition (ASR) and natural language processing modules (NLP) given their limits. Several collaboration modalities between ASR and NLP have been investigated. On the one hand, the speech recognition task can benefit from linguistic decision to uncover the correct utterance, see (Rayner et al., 1994) among others. On the other hand, NLP components can be robust with respect to recognition errors. The straightforward approach is to be robust by focusing only on informative words (Lamel et al., 1995; Meteer and Rohlicek, 1994). By nature, it misses some existing information in the sentence and it can be misled in case of errors on informative words. A more controlled robustness is expected with a complete linguistic analysis (Young, 1994; Hanrieder and GSrz, 1995; Dowding et al., 1994). In a practical application, a dialogue module *with Lab. CLIPS IMAG, Grenoble twith Dept. Signal, ENST Paris can then handle interactive recovery, as illustrated by (Suhm, Myers, and Waibel, 1996). The current work attempts to repair misrecognitions by mobilising available acoustic cues and by using linguistic abstraction and syntactico-semantic predictions. We present a filtering method and a repairing parsing strategy which fit in a complete system architecture. An advantage of our approach is the use of a core module that is independent from any application. Another advantage, for real applications, is to be aware of the expected performances of the ASR systems. Indeed, there are obstacles that prevent ASR systems to be fully reliable. In particular, the decoding algorithms enforce models which do not exploit all linguistic knowledge, mainly due to computational complexity. This hinders somehow the decoding so that the right solution is sometimes just not available. 2 S y s t e m a r c h i t e c t u r e The system architecture consists in a speech recognizer, a word confidence scoring module, a robust parsing module and higher modules -around a dialogue module (Normand, Pernel, and Bacconnet, 1997). The modules of the system articulate in a complementary way. The scoring module goal is to provide word acoustic confidence scores to help the robust parser in its task. The parsing module takes the best recognition hypothesis. It attempts to repair recognition errors and transmits a semantic representation of the sentence to the dialogue module. It relies on a lexicalized tree grammar and on integrated repairing rules. They make use of the knowledge embedded in the lexical grammar and of candidates present in the N-best hypothesis. We have studied its capacities to detect and predict missing elements and to select syntactically and semantically well-formed sentences. The robust parser needs con-	acoustic cryptanalysis;algorithm;automated system recovery;bottom-up parsing;clips;computational complexity theory;computer;dialog system;embedded system;finite-state machine;information;lexical grammar;natural language processing;online and offline;performance;robustness (computer science);speech recognition;systems architecture;well-formed element;word lists by frequency	David Roussel;Ariane Halber	1997			natural language processing;speech recognition;computer science;linguistics	NLP	-19.798546596932418	-81.39888851379963	91460
033337b901b32d44fac956ae20b891aca1fa6e4d	phone boundary refinement using ranking methods	ranking svm;rankboost;nonboundary signal characteristics;decision tree;hidden markov models support vector machines training speech training data feature extraction speech recognition;support vector machines;multiple phone transition dependent ranker;hmm based viterbi forced alignment;speech processing;training;hypothesized boundary;k means;phone boundary refinement;k mean based clustering;speech;decision tree based clustering;automatic phone segmentation;automatic phone alignment;maximum likelihood estimation;hmm svm based two stage framework;training data;timit corpus phone boundary refinement hmm svm based two stage framework automatic phone alignment svm classifier hypothesized boundary hmm based viterbi forced alignment nonboundary signal characteristics class imbalanced training problem ranking method multiple phone transition dependent ranker k mean based clustering decision tree based clustering;hidden markov models;support vector machines decision trees hidden markov models maximum likelihood estimation pattern classification speech processing;rankboost automatic phone segmentation ranking method ranking svm;feature extraction;svm classifier;pattern classification;ranking method;speech recognition;class imbalanced training problem;timit corpus;decision trees	The HMM/SVM-based two-stage framework has been widely used for automatic phone alignment. The two-stage method uses SVM classifiers to refine the hypothesized boundaries given by the HMM-based Viterbi forced alignment. However, there are two drawbacks in using the classification model for detecting the phone boundaries. First, the training data contains only information about the boundary and far away non-boundary signal characteristics. Second, the classification model suffers from the class-imbalanced training problem. To overcome these drawbacks, we propose using ranking methods to refine the hypothesized boundaries. We train multiple phone-transition-dependent rankers by using K-means-based and decision-tree-based clustering. Both Ranking SVM and RankBoost are evaluated. The results of experiments on the TIMIT corpus demonstrate that the proposed ranking method outperforms the classification method. The best accuracy achieved is 84.20% within a tolerance of 10 ms. The mean boundary distance is 6.66 ms.	cluster analysis;decision tree;experiment;hidden markov model;information retrieval;k-means clustering;learning to rank;machine learning;national supercomputer centre in sweden;ranking svm;refinement (computing);sensor;timit;viterbi algorithm	Hung-Yi Lo;Hsin-Min Wang	2010	2010 7th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2010.5684874	speech recognition;computer science;machine learning;decision tree;pattern recognition;speech processing;ranking svm;hidden markov model	NLP	-20.30817922643123	-89.34764856220275	91481
96536e450871134fa32084fda00211c095acbb6f	theoretical error prediction for a language identification system using optimal phoneme clustering.	language identification	using Optimal Phoneme Clustering Kay M. Berkling, Etienne Barnard (berkling,barnard)@cse.ogi.edu Center for Spoken Language Understanding, Oregon Graduate Institute of Science and Technology Abstract A neural network based language identi cation system is described, which uses language independent phoneme clusters as speech units to recognize the language spoken by native speakers over the telephone. We extend our previous work comparing phoneme-cluster and phoneme based approaches to language identi cation [1]. By creating a new speech unit valid across all languages in a theoreticallymotivatedmanner, we circumvent problems that are associated with ne phonemic modelling such as high complexity [4], extensive training requirements [2], and the linguistically arbitrary reduction to subsets of phonemes [4]. A common set of speech units across languages allows us to automatically derive discriminating sequences of any length and theoretically estimate the language identi cationerror. We demonstrateour implemented system for German vs. English on the OGI-TS database.	artificial neural network;cluster analysis;language identification;ne (complexity);natural language understanding;phoneme;requirement	Kay M. Berkling;Etienne Barnard	1995			natural language processing;language identification;speech recognition;computer science;pattern recognition	NLP	-19.695338215322067	-86.92909000651531	91791
361ca6a00b8e10dd075bc807ad0ab77351002106	progress in the cu-htk broadcast news transcription system	broadcasting training data error analysis acoustic testing speech recognition ear loudspeakers availability real time systems performance gain;cable television;broadcast news;roughly transcribed acoustic training data;evaluation performance;appareillage essai;news;diarization;document audiovisuel;performance evaluation;learning;availability;taux erreur;disponibilidad;real time;evaluacion prestacion;acoustic modeling;speech processing;tratamiento palabra;traitement parole;methode acoustique;segmentation;speech recognition broadcasting;indexing terms;aprendizaje;superears system;automatic speech recognition;apprentissage;broadcast news bn transcription;automatic recognition;acoustic method;reconocimiento voz;diarization automatic speech recognition broadcast news bn transcription;cu htk broadcast news transcription system;recognition configurations cu htk broadcast news transcription system roughly transcribed acoustic training data advanced acoustic model training techniques error rate reduction lightly supervised training data real time broadcast news recognition system multiple recognition outputs multiple segmentations cross site combination superears system;multiple segmentations;aparato ensayo;documento audiovisual;advanced acoustic model training techniques;metodo acustico;audiovisual document;teledistribution;testing equipment;error rate;speech recognition;lightly supervised training data;noticias;system development;cross site combination;multiple recognition outputs;reconnaissance parole;broadcasting;indice error;actualites;recognition configurations;disponibilite;teledistribucion;error rate reduction;segmentacion;real time broadcast news recognition system;reconocimiento automatico;reconnaissance automatique	"""Broadcast news (BN) transcription has been a challenging research area for many years. In the last couple of years, the availability of large amounts of roughly transcribed acoustic training data and advanced model training techniques has offered the opportunity to greatly reduce the error rate on this task. This paper describes the design and performance of BN transcription systems which make use of these developments. First, the effects of using lightly supervised training data and advanced acoustic modeling techniques are discussed. The design of a real-time broadcast news recognition system is then detailed using these new models. As system combination has been found to yield large gains in performance, a range of frameworks that allow multiple recognition outputs to be combined are next described. These include the use of multiple types of acoustic models and multiple segmentations. As a contrast a system developed by multiple sites allowing cross-site combination, the """"SuperEARS"""" system, is also described. The various models and recognition configurations are evaluated using several recent BN development and evaluation test sets. These new BN transcription systems can give gains of over 25% relative to the CU-HTK 2003 BN system"""	acoustic cryptanalysis;acoustic model;cross-site cooking;discriminative model;htk (software);language model;medical transcription;real-time clock;real-time computing;real-time locating system;tip (unix utility);transcription (software)	Mark J. F. Gales;Do Yeong Kim;Philip C. Woodland;Ricky Ho Yin Chan;David Mrva;Rohit Sinha;Sue Tranter	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.878264	availability;speech recognition;telecommunications;news;computer science;speech processing;segmentation;broadcasting	Visualization	-20.922153920861394	-87.04336717553709	92207
53bd85f38926cb785542bcf5e731cbec1b053cbc	machine learning for spoken dialogue systems	statistical machine learning;indexing terms;spoken dialogue system;automatic speech recognition;robot control;machine learning;language processing;natural language generation;text to speech;database search	During the last decade, research in the field of Spoken Dialogue Systems (SDS) has experienced increasing growth. However, the design and optimization of SDS is not only about combining speech and language processing systems such as Automatic Speech Recognition (ASR), parsers, Natural Language Generation (NLG), and Text-to-Speech (TTS) synthesis systems. It also requires the development of dialogue strategies taking at least into account the performances of these subsystems (and others), the nature of the task (e.g. form filling, tutoring, robot control, or database search/browsing), and the user’s behaviour (e.g. cooperativeness, expertise). Due to the great variability of these factors, reuse of previous hand-crafted designs is also made very difficult. For these reasons, statistical machine learning (ML) methods applied to automatic SDS optimization have been a leading research area for the last few years. In this paper, we provide a short review of the field and of recent advances.	dialog system;heart rate variability;machine learning;mathematical optimization;natural language generation;parsing;performance;robot control;speech recognition;speech synthesis	Oliver Lemon;Olivier Pietquin	2007			natural language processing;database search engine;speech recognition;index term;computer science;machine learning;robot control;linguistics;speech synthesis;world wide web	NLP	-22.416453340226735	-86.17058229371317	92446
326d67fce8dab09f84981b7c595e6da247c7009a	unsupervised and active learning in automatic speech recognition for call classification	unsupervised learning;natural language interfaces;iterative process;spoken natural language dialog applications;routing;active learning;natural languages;spoken language dialog systems;data mining;at t voicetone unsupervised learning active learning automatic speech recognition call classification spoken natural language dialog applications transcribing speech data labeling speech data spoken language dialog systems data mining conversational systems web sites iterative process;speech based user interfaces;call classification;labeling speech data;upper bound;iterative methods;automatic speech recognition;natural language;web sites;automatic speech recognition natural languages labeling routing data mining unsupervised learning robustness upper bound humans iterative methods;speech based user interfaces unsupervised learning speech recognition natural languages natural language interfaces interactive systems data mining iterative methods;speech recognition;at t voicetone;robustness;conversational systems;humans;interactive systems;labeling;transcribing speech data	A key challenge in rapidly building spoken natural language dialog applications is minimizing the manual effort required in transcribing and labeling speech data. This task is not only expensive but also time consuming. We present a novel approach that aims at reducing the amount of manually transcribed in-domain data required for building automatic speech recognition (ASR) models in spoken language dialog systems. Our method is based on mining relevant text from various conversational systems and Web sites. An iterative process is employed where the performance of the models can be improved through both unsupervised and active learning of the ASR models. We have evaluated the robustness of our approach on a call classification task that has been selected from AT&T VoiceTone/sup SM/ customer care. Our results indicate that with unsupervised learning it is possible to achieve a call classification performance that is only 1.5% lower than the upper bound set when using all available in-domain transcribed data.	automated system recovery;dialog system;iteration;natural language;speech recognition;unsupervised learning	Dilek Z. Hakkani-Tür;Gökhan Tür;Mazin G. Rahim;Giuseppe Riccardi	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326014	natural language processing;unsupervised learning;speech recognition;computer science;pattern recognition;natural language	NLP	-21.19357789789009	-86.42673424994673	92631
3ce50a0161911b5c91a3d32c891d93a1f74cec36	comparing models of phonotactics for word segmentation		Developmental research indicates that infants use low-level statistical regularities, or phonotactics, to segment words from continuous speech. In this paper, we present a segmentation framework that enables the direct comparison of different phonotactic models for segmentation. We compare a model using phoneme transitional probabilities, which have been widely used in computational models, to syllable-based bigram models, which have played a prominent role in the developmental literature. We also introduce a novel estimation method, and compare it to other strategies for estimating the parameters of the phonotactic models from unsegmented data. The results show that syllable-based models outperform the phoneme models, specifically in the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon.	bigram;computational model;estimation theory;f1 score;high- and low-level;lexicon;markov chain;syllable;text segmentation	Natalie Schrimpf;Gaja Jarosz	2014		10.3115/v1/W14-2803	natural language processing;speech recognition;computer science;communication	NLP	-19.71198055805867	-84.99030588600665	92639
34b2e643e0db48ec87b8d9dad8fdf240eb9ad6c9	improved katz smoothing for language modeling in speech recogniton	language model	In this paper, a new method is proposed to improve the canonical Katz back-off smoothing technique in language modeling. The process of Katz smoothing is detailedly analyzed and the global discounting parameters are selected for discounting. Further more, a modified version of the formula for discounting parameters is proposed, in which the discounting parameters are determined by not only the occurring counts of the n-gram units but also the low-order history frequencies. This modification makes the smoothing more reasonable for those n-gram units that have homophonic (same in pronunciation) histories. The new method is tested on a Chinese Pinyin-to-character (where Pinyin is the pronunciation string) conversion system and the results show that the improved method can achieve a surprising reduction both in perplexity and Chinese character error rate.	chinese room;experiment;language model;n-gram;neural coding;perplexity;smoothing;software bug;speech recognition	Genqing Wu;Fang Zheng;Wenhu Wu;Mingxing Xu;Ling Jin	2002			pinyin;perplexity;speech recognition;word error rate;discounting;smoothing;language model;computer science;katz's back-off model;pronunciation	NLP	-19.352685574419926	-85.54116715478659	92743
caf0c676567e1ce31b921565ea83b17fb86591bd	impact of automatic sentence segmentation on meeting summarization	mmr meeting summarization sentence segmentation rouge;human annotated sentences;pause information;humans natural languages hidden markov models data mining broadcasting degradation speech recognition testing statistics ear;hidden markov model;decision thresholds;speech processing;n gram language model;maximum marginal relevance;hmm;rouge;indexing terms;mmr;speech recognition errors;evaluation metric;rouge scores;hidden markov models;meeting summarization;pause information automatic sentence segmentation meeting summarization speech summarization hidden markov model hmm n gram language model maximum marginal relevance extractive summarization method rouge scores decision thresholds human annotated sentences speech recognition errors icsi meeting corpus;icsi meeting corpus;extractive summarization method;speech recognition;sentence segmentation;speech summarization;automatic sentence segmentation;language model;speech recognition hidden markov models speech processing	This paper investigates the impact of automatic sentence segmentation on speech summarization using the ICSI meeting corpus. We use a hidden Markov model (HMM) for sentence segmentation that integrates the N-gram language model and pause information, and a maximum marginal relevance (MMR) based extractive summarization method. The system-generated summaries are compared to multiple human summaries using the ROUGE scores. The decision thresholds from the segmentation system are varied to examine the impact of different segments on summarization. We find that (1) using system generated utterance segments degrades summarization performance compared to using human annotated sentences; (2) segmentation needs to be optimized for summarization instead of the segmentation task itself, however, the patterns are slightly different from prior work for other tasks such as parsing; and (3) there are effects from different summarization evaluation metrics as well as speech recognition errors.	automatic summarization;hidden markov model;language model;marginal model;markov chain;multi-master replication;n-gram;parsing;rouge (metric);relevance;sentence boundary disambiguation;speech recognition	Yang Liu;Shasha Xie	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518783	rouge;natural language processing;speech recognition;index term;computer science;automatic summarization;pattern recognition;hidden markov model	NLP	-20.749213856157958	-82.65211968288753	92808
0e4c47d3bb8d51849e43b5257adbe7e65f2ca8b9	stochastic finite state automata language model triggered by dialogue states	hierarchical clustering;statistical significance;n gram model;spoken dialogue system;finite state automata;dynamic adaptation;language model	Within the framework of Natural Spoken Dialogue systems, this paper describes a method for dynamically adapting a Language Model (LM) to the dialogue states detected. This LM combines a standard n-gram model with Stochastic Finite State Automata (SFSAs). During the training process, the sentence corpus used to train the LM is split into several hierarchical clusters in a 2-step process which involves both explicit knowledge and statistical criteria. All the clusters are stored in a binary tree where the whole corpus is attached to the root node. Each level of the tree corresponds to a higher specialization of the sub-corpora attached to the nodes and each node corresponds to a different dialogue state. From the same sentence corpus, SFSAs are extracted in order to model longer contexts than the ones used in the standard n-gram model. A set of SFSAs is attached to each node of the tree as well as a sub-LM which combines a bigram trained on the sub-corpus of the node and the SFSAs selected. A first decoding process calculates a word-graph as well as a first sentence hypothesis. This first hypothesis will be used to find the optimal node in the LM tree. Then, a rescoring process of the word graph using the LM attached to the node selected is performed. By adapting the LM to the dialogue state detected, we show a statistically significant gain in WER on a dialogue corpus collected by France Telecom R&D .	automata theory;bigram;binary tree;dialog system;finite-state machine;language model;microsoft word for mac;n-gram;partial template specialization;text corpus;tree (data structure);word error rate	Yannick Estève;Frédéric Béchet;Alexis Nasr;Renato De Mori	2001			natural language processing;speech recognition;computer science;machine learning;hierarchical clustering;statistical significance;statistics;language model	NLP	-21.29435304269231	-81.2008123891935	93077
3778dfb2b35c5301ab5441205b284522685b47a0	a rule-based shallow-transfer machine translation system for scots and english		An open-source rule-based machine translation system is developed for Scots, a low-resourced minor language closely related to English and spoken in Scotland and Ireland. By concentrating on translation for assimilation (gist comprehension) from Scots to English, it is proposed that the development of dictionaries designed to be used within the Apertium platform will be sufficient to produce translations that improve non-Scots speakers understanding of the language. Monoand bilingual Scots dictionaries are constructed using lexical items gathered from a variety of resources across several domains. Although the primary goal of this project is translation for gisting, the system is evaluated for both assimilation and dissemination (publication-ready translations). A variety of evaluation methods are used, including a cloze test undertaken by human volunteers. While evaluation results are comparable to, and in some cases superior to, those of other language pairs within the Apertium platform, room for improvement is identified in several areas of the system.	apertium;data assimilation;dictionary;gist;logic programming;open-source software;rule-based machine translation;scottish corpus of texts and speech;usability	Gavin Abercrombie	2016			rule-based system;machine translation;natural language processing;artificial intelligence;speech recognition;scots;computer science;example-based machine translation	NLP	-24.302899423446142	-83.59259768810276	93130
2d024326e82f1fe82af3d8226e8ee1e19717bd62	progress in automatic meeting transcription.	broadcast news;duration model;model combination;large vocabulary conversational speech recognition;temporal constraints;system development;domain specificity;conversational telephone speech	In this paper we report recent developments on the meeting transcription task, a large vocabulary conversational speech rec ognition task. Previous experiments showed this is a very challenging task, with about 50% word error rate (WER) using existing recognizers. The difficulty mostly comes from highly disfluent/conversational nature of meetings, and lack of domain specific training data. For the first problem, our SWB(Switchboard) system — a conversational telephone sp eech rec ognizer — was used to recognize wide-band meeting data; for the latter, we leveraged the large amount of Broadcast News (BN) data to build a robust system. This paper will especially focus on two experiments in the BN system development: model combination and HMM topology/duration modeling. Model combination can be done at various stages of recognition: post-processing schemes such as ROVER can lead to significant improvements; to reduce computation we tried model combination at acoustic score level. We will also show the importance of temporal constraints in decoding, present some HMM topology/duration modeling experiments. Finally, the meeting browser system and meeting room setup will be reviewed.	acoustic cryptanalysis;computation;experiment;finite-state machine;hidden markov model;transcription (software);video post-processing;vocabulary;word error rate	Hua Yu;Michael Finke;Alexander H. Waibel	1999			natural language processing;speech recognition;computer science;communication	NLP	-21.19269011370556	-85.84252518988806	93553
2814609e3daf669214a8880d4dffb74881c8ab8b	intonational boundaries, speech repairs and discourse markers: modeling spoken dialog		To understand a speaker’s turn of a conversation, one needs to segment it into intonational phrases, clean up any speech repairs that might have occurred, and identify discourse markers. In this paper, we argue that these problems must be resolved together, and that they must be resolved early in the processing stream. We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer. We find that by accounting for the interactions between these tasks that the performance on each task improves, as does POS tagging and perplexity.	finite-state machine;interaction;language model;part-of-speech tagging;perplexity;point of sale;speech recognition;word error rate;dialog	Peter A. Heeman;James F. Allen	1997			natural language processing;discourse marker;dialog box;computer science;artificial intelligence	NLP	-25.633217727949056	-82.99963963301494	93638
3e770fe36b677ef9bd7effc0239688c374de6c15	contextual vector quantization for speech recognition with discrete hidden markov model	modelizacion;front end;modelo markov;reconocimiento palabra;hidden markov model;acoustic modeling;champ aleatoire markov;contextual information;theorie modeles;markov random field;modelisation;feature vector;automatic speech recognition;automatic recognition;markov model;speaker independent;word recognition;mot isole;palabra aislada;speech recognition;vector quantizer;reconnaissance parole;modele markov;isolated word;modeling;teoria modelos;speech production;reconocimiento automatico;reconnaissance automatique;model theory;finite mixture	By using formulation of the finite mixture distribution identification, in this paper, several alternatives to the conventional LBG VQ method are investigated. A contextual VQ method based on the Markov Random Field (MRF) theory is proposed to model the speech feature vector space. Its superiority is confirmed by a series of comparative experiments in a speaker independent isolated word recognition task by using different VQ schemes as the front-end of DHMM. The motivation to use MRF to model the contextual dependence information in the underlying speech production process can be readily extended to acoustic modeling of the basic speech units in speech recognition. Contextual information Vector quantization Hidden Markov model Markov random field Automatic speech recognition	acoustic cryptanalysis;experiment;feature vector;hidden markov model;location-based game;markov chain;markov random field;speech recognition;vector quantization	Qiang Huo;Chorkin Chan	1995	Pattern Recognition	10.1016/0031-3203(94)00117-5	speaker recognition;speech production;speech recognition;systems modeling;feature vector;word recognition;computer science;front and back ends;machine learning;pattern recognition;markov model;hidden markov model;model theory	ML	-20.899370845628642	-92.25687095959348	93918
2bc09e294bc83d1412aa791315ffd8f0203f55a3	at&t at trec-6: sdr track	automatic speech recognition	In the spoken document retrieval track, we study how higher word-recall|recognizing many of the spoken words|aaects the retrieval eeectiveness for speech documents, given that high word-recall comes at a cost of low word-precision|recognizing many words that were not actually spoken. We hypothesize that information retrieval algorithms would beneet from a higher word-recall and are robust against poor word-precision. Start-up diiculties with recognition for this task kept us from doing an systematic study of the eeect of varying levels of word-recall and word-precision on retrieval eeectiveness from speech. We simulated a high word-recall and a poor word-precision system by merging the output of several recognizers. Experiments suggest that having higher word-recall does improve the retrieval eeectiveness from speech.	algorithm;document retrieval;etsi satellite digital radio;experiment;finite-state machine;information retrieval;precision and recall	Amit Singhal;John Choi;Donald Hindle;Fernando Pereira	1997				Web+IR	-21.355435306774297	-83.83750418992317	94309
c6053a529e5916ac4a755e5a23ad9df12384c3c7	oriental cocosda: past, present and future		The purpose of Oriental COCOSDA is to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition/synthesis systems as well as to promote speech research on oriental languages. A series of International Workshop on East Asian Language Resources and Evaluation (EALREW) or Oriental COCOSDA Workshop has been held annually since the preparatory meeting held in 1997. After that, we have had a series of workshops every year in Japan, Taiwan, China, Korea, Thailand, Singapore, India and Indonesia. The Oriental COCOSDA is managed by a convener, three advisory members, and 21 representatives from ten regions in Oriental countries. We need much more Pan-Asia collaboration with research organizations and consortia, though there are some domestic activities in Oriental countries. We note that speech research has become popular gradually in Oriental countries including Malaysia, Vietnam, Xinjang Uygur Autonomous Region of China, etc. We plan to hold future Oriental COCOSDA meetings in these places in order to promote speech research there.	gtk#;informatics;linguistic data consortium;speech corpus;speech recognition;speech synthesis;spontaneous order;suzhou oriental semiconductor;text corpus	Shuichi Itahashi;Chiu-yu Tseng;Satoshi Nakamura	2006			artificial intelligence;library science;natural language processing;computer science;china;spoken language;east asia	NLP	-24.493147121655277	-85.0683797050972	94864
f28a68ec81989381044554d6d66ae1beec398faa	towards optimal tts corpora	text to speech;finite state transducer;speech segmentation	Unit selection text-to-speech systems currently pro duce very natural synthesized phrases by concatenat ing speech segments from a large database. Recently, increasing demand for desi gning high quality voices with less data has create d n ed for further optimization of the textual corpus recorded by the speaker. This corpus is traditionally the result of a condensati on process: sentences are selected from a reference corpus, using an optimization algo rithm (generally greedy) guided by the coverage rat of classic units (diphones, triphones, words...). Such an approach is, however, s trongly constrained by the finite content of the re fe nce corpus, providing limited language possibilities. To gain flexibility in the optimization process, in this paper, we int roduce a new corpus building procedure based on sentence construction rather tha n sentence selection. Sentences are generated using Finite State Transducers, assisted by a human operator and guided by a new fr quency-weighted coverage criterion based on Vocali c Sandwiches. This semi-automatic process requires time-consuming huma n intervention but seems to give access to much den ser corpora, with a density increase of 30 to 40% for a given coverage rate.	display resolution;field electron emission;finite-state transducer;greedy algorithm;mathematical optimization;netware file system;semiconductor industry;speech synthesis;text corpus;uniform memory access	Didier Cadic;Cédric Boidin;Christophe d'Alessandro	2010			artificial intelligence;speech recognition;natural language processing;operator (computer programming);concatenation;finite state transducer;sentence;computer science;pattern recognition;speech synthesis;speech segmentation	NLP	-20.5014916939009	-83.37930950739685	94901
ee33140fb01ad84df4e95f5f20ac0c916a33a946	ann and rule based method for english to arabic machine translation		Machine translation is the process by which computer software is used to translate a text from one natural language into another language with or without minimal human intervention. This definition involves accounting for the grammatical structure of each language and using rules and grammars to transfer the grammatical structure of the source language into the target language. This paper presents an English into Arabic Machine Translation (MT) system for translating simple well-structured English sentences into well-structured Arabic sentences using a rule-based approach and feed-forward back-propagation Artificial Neural Network (ANN). Our system is able to translate sentences having gerunds, infinitives, prepositions and prepositional objects, direct objects, indirect objects, etc. Neural network works as bilingual dictionary, which does not only store the meaning of English word in Arabic but it also stores linguistic features attached to the word. The performance is evaluated by different MT evaluation methods. The n-gram blue score achieved by the system is 0.6029, METEOR score achieved is 0.8221 and 0.8386 on F-measure.	arabic machine translation;artificial neural network;bleu;backpropagation;bilingual dictionary;compiler;imperative programming;logic programming;meteor;n-gram;natural language;printing;software propagation;structured english	Marwan Akeel;Ravi Mishra	2014	Int. Arab J. Inf. Technol.		natural language processing;structured english;speech recognition;transfer-based machine translation;computer science;machine translation;rule-based machine translation;machine translation software usability	NLP	-26.556049780644557	-80.69350434505674	95082
3a8a9bca8a487ebec3dc693bc1d9d18a299c3005	exploring features for localized detection of speech recognition errors		We address the problem of localized error detection in Automatic Speech Recognition (ASR) output to support the generation of targeted clarifications in spoken dialogue systems. Localized error detection finds specific mis-recognized words in a user utterance. Targeted clarifications, in contrast with generic ‘please repeat/rephrase’ clarifications, target a specific mis-recognized word in an utterance (Stoyanchev et al., 2012a) and require accurate detection of such words. We extend and modify work presented in (Stoyanchev et al., 2012b) by experimenting with a new set of features for predicting the likelihood of a local error in an ASR hypothesis on an unsifted version of the original dataset. We improve over baseline results, where only ASRgenerated features are used, by constructing optimal feature sets for utterance and word mis-recognition prediction. The f-measure for identifying incorrect utterances improves by 2.2% and by 3.9% for identifiying incorrect words.	baseline (configuration management);dialog system;error detection and correction;experiment;f1 score;speech recognition	Eli Pincus;Svetlana Stoyanchev;Julia Hirschberg	2013			natural language processing;speech recognition;computer science;pattern recognition	NLP	-19.726547728490345	-81.69079141117304	95099
7f17637e3c6d6937aa05b155847c9d0423a47a2a	"""word order and the principle of """"early immediate constituents"""" (eic)"""	word order	EIC (the principle of “Early Immediate Constituents”) seeks to explain word order variation at the phrase level in performance and processes of grammaticalisation on the basis of human information processing. It is embedded in functionally oriented linguistic modelling and typological argumentation. By quantification and empirical testing of EIC driven hypotheses, the principle proves rather plausible although some questions are still to be answered and integration into a wider linguistic model will have to be accomplished. * Address correspondence to: Christiane Hoffmann, Gervasiusstr. 10, D-54290 Trier, Germany. E-mail: hoffmanc @uni-trier.de 1. Frame, Janet (1992, 1961): Faces in the Water. London: Women’s Press. 2. Of course, the two versions are not totally equivalent. The first occurrence of the possessive pronoun “our” in version M has no antecedent but is used cataphorically. Likewise, the two references to “Safety”, one by the noun itself and one by the anaphoric pronoun, had to be reversed in order of appearance – thanks to Patrick Juola for pointing this out to me. D ow nl oa de d by [ E in dh ov en T ec hn ic al U ni ve rs ity ] at 0 7: 07 2 3 N ov em be r 20 14 109 “EARLY IMMEDIATE CONSTITUENTS” (EIC) On the first level we have two complements of the verb “say”, that are separated by a comma. On the second level we may discern two verbal phrases in the second complement that are conjoined by “and”. The second verbal phrase shows a third level of interest. Here, the verb “remove” has three direct objects that appear in mere juxtaposition (no comma or “and”), a stylistic means of the author. What is remarkable about this sentence? On each level, respectively, there is a conjunction of two or more type equivalent phrases/clauses. Among these we find the following strong dependency: the longer one (length here is measured in words) of the type equivalent phrases follows the shorter one. If there are three phrases/clauses, this holds for the two pairs involved. This means that the longer “that”-complement follows the shorter “that”complement. In the second “that”-complement the longer verbal phrase follows the shorter verbal phrase and in the second verbal phrase the longer the object, the farther down in the linearisation of the three direct objects it is found. In the modified version all the components on each level are reversed to generate a sentence that runs counter the principle on all the three levels we just established and it should be experienced as more or maximally difficult to understand/process: They have said [ that we owe allegiance to Safety ] [ that he is our Red Cross who will [ provide us with ointment and bandages for our wounds ] and	anaphora (linguistics);complement (complexity);earth inductor compass;embedded system;information processing;nl (complexity);turing test	Christiane Hoffmann	1999	Journal of Quantitative Linguistics	10.1076/jqul.6.2.108.4133	word order;natural language processing;computer science;mathematics;linguistics	NLP	-31.22333528771163	-83.36193484934022	95291
827549c8a60cd8a2c17c05df1e28ac2bdc5c4abb	a tentative study on language model based solution to multiple choice of cet-4	dynamic programming;dynamic programmming;n gram language model;multiple choice of cet 4;writing dynamic programming mathematical model educational institutions natural language processing filling computational linguistics;internet;natural language processing dynamic programming internet;dynamic programmming multiple choice of cet 4 n gram language model;5 gram model cet 4 language model based solution multiple choice test item web scale english language data n grams dynamic programming searching 4 gram model;natural language processing	The paper presents a language model based solution to the test item of Multiple Choice of CET-4. Trained on the web scale English language data, different n-grams are examined under a dynamic programming searching for the best answers. Experimental results indicate that both 4-gram and 5-gram model could generate an average of 81% precision for 16 test items.	dynamic programming;grams;information retrieval;language model;n-gram;natural language;norm (social);robot;scalability;yahoo! answers	Zhihang Fan;Muyun Yang;Tiejun Zhao;Sheng Li	2013	2013 International Conference on Asian Language Processing	10.1109/IALP.2013.35	natural language processing;language identification;cache language model;first-generation programming language;natural language programming;the internet;speech recognition;language primitive;data manipulation language;specification language;programming domain;data control language;computer science;machine learning;dynamic programming;fifth-generation programming language;programming language specification	Robotics	-23.93794415811622	-80.62750725538093	95294
2a096cdcc3bf78eafdc8b0ff5345e91d8e46e9b2	hierarchical conversation structure prediction in multi-party chat	human accuracy;sentence-level annotation;hierarchical conversation structure prediction;sentence-level behavior;annotation scheme;conversational practice;information sharing;real-time analysis;conversation structure;multi-party chat;integer linear programming	Conversational practices do not occur at a single unit of analysis. To understand the interplay between social positioning, information sharing, and rhetorical strategy in language, various granularities are necessary. In this work we present a machine learning model for multi-party chat which predicts conversation structure across differing units of analysis. First, we mark sentence-level behavior using an information sharing annotation scheme. By taking advantage of Integer Linear Programming and a sociolinguistic framework, we enforce structural relationships between sentence-level annotations and sequences of interaction. Then, we show that clustering these sequences can effectively disentangle the threads of conversation. This model is highly accurate, performing near human accuracy, and performs analysis on-line, opening the door to real-time analysis of the discourse of conversation.	baseline (configuration management);chat room;cluster analysis;coherent;integer programming;interaction;linear programming;machine learning;online and offline;population;preprocessor;real-time transcription;thread (computing);tokenization (data security);vocabulary;world-system	Elijah Mayfield;David Adamson;Carolyn Penstein Rosé	2012			natural language processing;computer science;artificial intelligence;communication	NLP	-32.977422315248006	-83.7256867314262	95406
ea705143f3531aaf40e3c7968cc9a865aba26beb	adaptive feedback message generation for second language learners of arabic		This paper addresses issues related to generating feedback messages to errors related to Arabic verbs made by second language learners (SLLs). The proposed approach allows for individualization. When a SLL of Arabic writes a wrong verb, it performs analysis of the input and distinguishes between different lexical error types. The proposed system issues the intelligent feedback that conforms to the learner’s proficiency level for each class of error. The proposed system has been effectively evaluated using real test data and achieved satisfactory results.	feedback;speech corpus;spell checker;test data;test set;turing test	Khaled F. Shaalan;Marwa Magdy	2011			natural language processing;speech recognition;computer science;linguistics	SE	-27.093568702142043	-83.23707431486726	95424
0cb23de00ed96782a156564beb332553190f16a4	the artiphon task at evalita 2016		English. Despite the impressive results achieved by ASR technology in the last few years, state-of-the-art ASR systems can still perform poorly when training and testing conditions are different (e.g., different acoustic environments). This is usually referred to as the mismatch problem. In the ArtiPhon task at Evalita 2016 we wanted to evaluate phone recognition systems in mismatched speaking styles. While training data consisted of read speech, most of testing data consisted of single-speaker hypoand hyper-articulated speech. A second goal of the task was to investigate whether the use of speech production knowledge, in the form of measured articulatory movements, could help in building ASR systems that are more robust to the effects of the mismatch problem. Here I report the result of the only entry of the task and of baseline systems. Italiano. Nonostante i notevoli risultati ottenuti recentemente nel riconoscimento automatico del parlato (ASR) le prestazioni dei sistemi ASR peggiorano significativamente in quando le condizioni di testing sono differenti da quelle di training (per esempio quando il tipo di rumore acustico è differente). Un primo gol della ArtiPhon task ad Evalita 2016 è quello di valutare il comportamento di sistemi di riconoscimento fonetico in presenza di un mismatch in termini di registro del parlato. Mentre il parlato di training consiste di frasi lette ad un velocita; di eloquio “standard”, il parlato di testing consiste di frasi sia iperche ipo-articolate. Un secondo gol della task è quello di analizzare se e come l’utilizzo di informazione concernente la produzione del parlato migliora l’accuratezza dell’ASR e in particolare nel caso di mismatch a livello di registri del parlato. Qui riporto risultati dell’unico sistema che è stato sottomesso e di una baseline.	acoustic cryptanalysis;automated system recovery;automatic system recovery;baseline (configuration management);conway's game of life;hyper-heuristic;istituto di scienza e tecnologie dell'informazione;linear algebra;primos;unique name assumption	Leonardo Badino	2016			computer science	AI	-21.08808042935849	-85.41905369820957	95446
8816a3fe5b34c8f89269999dc4091a7941bb1fea	advanced speech communication system for deaf people	telecomunicaciones;sign language;indexing terms;spoken language translation;system integration;natural language;text to speech;speech communication;visual interfaces	This paper describes the development of an Advanced Speech Communication System for Deaf People and its field evaluation in a real application domain: the renewal of Driver’s License. The system is composed of two modules. The first one is a Spanish into Spanish Sign Language (LSE: Lengua de Signos Española) translation module made up of a speech recognizer, a natural language translator (for converting a word sequence into a sequence of signs), and a 3D avatar animation module (for playing back the signs). The second module is a Spoken Spanish generator from signwriting composed of a visual interface (for specifying a sequence of signs), a language translator (for generating the sequence of words in Spanish), and finally, a text to speech converter. For language translation, the system integrates three technologies: an example-based strategy, a rule-based translation method and a statistical translator. This paper also includes a detailed description of the evaluation carried out in the Local Traffic Office in the city of Toledo (Spain) involving real government employees and deaf people. This evaluation includes objective measurements from the system and subjective information from questionnaires.	application domain;finite-state machine;logic programming;natural language;speech recognition;speech synthesis	Rubén San-Segundo-Hernández;Verónica López-Ludeña;Raquel Martín;Syaheerah L. Lutfi;Javier Ferreiros;Ricardo de Córdoba;José Manuel Pardo	2010			natural language processing;manually coded language;cued speech;speech recognition;index term;sign language;language interpretation;computer science;speech;linguistics;natural language;speech synthesis;system integration	NLP	-24.044164481494825	-85.23946721473295	95600
44e67bbadf045e3126953480ab949d8405a9d518	grounding speech utterances in robotics affordances: an embodied statistical language model	robot sensing systems;pragmatics;grounding;semantics;speech;syntactics	We propose an embodied statistical language model for learning the meaning of word sequences, which incorporate action words (i.e. push, pull, tap, etc.,). The model attempts to unify embodied grounding and dis-embodied processing by representing the meaning of word sequences in both the sensorimotor and linguistic knowledge of a humanoid robot. The modeling of word sequences enables to distinguish the meaning of utterances containing the same categories and number of words but different structures. Once the robot has learned the semantics of word sequences and derived the affordances from words physical referents, has at its disposal a grounded language model that enables the mapping of linguistic commands provided by a human tutor into the appropriate behaviors. Moreover, the affordance model enables the robot to solve inference queries for filling in the information missing in the verbal description of the task.	humanoid robot;language model;limbo;robotics	Francesca Stramandinoli;Vadim Tikhanoff;Ugo Pattacini;Francesco Nori	2016	2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)	10.1109/DEVLRN.2016.7846794	natural language processing;computer science;linguistics;communication	Robotics	-31.9845956048178	-81.00367596324837	95814
925d49a81750f12baffa1cc43f42eeb8ce114ef3	a study of sindhi related and arabic script adapted languages recognition		"""1. INTRODUCTION The character recognition of the Roman type of languages especially English has come near to perfection and it is also considered as one of the successful application in the field of computer vision. The work on Arabic script and other scripts is being continued on; but the languages adopting Arabic script is very little while the work on Sindhi language is near to its origin. The Arabic script has more complexities as it's an entirely different script as compared to Roman script. A significant work also has been done on Indian local but Sindhi is lacking its fully functional OCR, although the remarkable work has been done on Sindhi Computing(Bhatti et al., 2014). This paper presents a review of the character recognition processes and image processing techniques applied in OCR systems. The techniques include text line segmentation, word and character segmentation and classification. The paper also looks in to the choices of researchers, made for their research in various languages all around the world. 2. PROPERTIES OF SINDHI LANGUAGE According to Moulana Ubedullah Sindhi a well know Islamic Philosopher and Scholar wrote in his book about Sindhi language, """" The seven languages are the main languages in which Holy Books were sent and the remaining world languages are derived from these seven languages. Sindhi is one these languages with Arabic and Hebrew """" (Allana, 2004). The rich historical background of Sindhi language can be inferred from the 5000 years Indus Civilization of Moen-jo-Daro near Larkana district of Sindh (AboutIndus, 2014). In (Al-lana, 2004) Dr. Nabi Bux Khan Baloch; a well-known Sindhi historian and scholar has categorized Originity of Sindhi into different opinions in which one of the opinion explains that Sindhi is a Sansakrit branch via Varchada Apabharansha. Sindhi Language is spoken by 18 million people in Pakistan as well as 2.8 million in India. Two common scripts, Arabic and Devanagri are used for writing Sindhi language. Arabic is the most common script used, by adding some modified letters to the Ara-bic letters. In India Sindhi is written in both scripts because it can also be written with Devanagri script. Sin-dhi Language has 24 more letters (total 52) than Arabic language with 28; some modified letters have been added with four dots to accommodate the different sounds. Sindhi has more vowels and consonant than Arabic and its neighbor language Urdu. The writing system follows the same style of …"""	capability maturity model;categorization;computer vision;feature extraction;image processing;named-entity recognition;optical character recognition;statistical classification	Dil Nawaz Hakro;Abdullah Zawawi Talib;Zeeshan Bhatti;G. N. Moja	2014	CoRR		natural language processing;speech recognition;computer science;modern arabic mathematical notation	NLP	-25.64951864571427	-81.06593334326449	95882
44487121ca400b5e0485b4fd645d3b23724e7845	a new phonetic candidate generator for improving search query efficiency	kullback-leibler divergence;letter-to-sound;phonetic candidate generator;phonetic similarity	Misspelled query due to homophones or mispronunciation is difficult to be corrected in the conventional spelling correction methods. In phonetic candidate generation, the generator is to produce candidates which are phonetically similar to a given query. In this paper, we present a new phonetic candidate generator for improving the search efficiency of a query. The proposed generator consists of three modules: letter-to-sound (LTS) conversion, phonetic “trie” and phonetic similarity estimator based upon Levenshtein distance and KullbackLeibler Divergence (KLD) between phones. This generator yields a significant improvement over Double-metaphone in terms of candidate accuracy and effective candidate set size.	levenshtein distance;metaphone;trie	Bo Peng;Yao Qian;Frank K. Soong;Bo Zhang	2011			query expansion;artificial intelligence;web search query;information retrieval;pattern recognition;computer science	Web+IR	-22.13855776349666	-82.90403782587211	96480
261bb02e9ad706ef5a4b0cf07bfc9929b988418e	context-dependent acoustic modeling using graphemes for large vocabulary speech recognition	decision tree;acoustic modeling;ions;vocabulary;speech;hidden markov models;pipelines;speech recognition;context dependent;europe;speech recognition hidden markov models speech vocabulary europe ions pipelines	In this paper we propose to use a decision tree based on graphemic acoustic sub-word units together with phonetic questions. We also show that automatic question generation can be used to completely eliminate any manual effort.	acoustic cryptanalysis;acoustic model;decision tree;speech recognition;vocabulary	Stephan Kanthak;Hermann Ney	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743871	natural language processing;speech recognition;speech corpus;computer science;speech;machine learning;decision tree;context-dependent memory;acoustic model;pipeline transport;hidden markov model;ion;speech analytics	Robotics	-19.99284483808666	-85.35922560140986	96641
008d13833c2efb653c8f341205deb5eb3ead9954	beyond prefix-based interactive translation prediction		Current automatic machine translation systems require heavy human proofreading to produce high-quality translations. We present a new interactive machine translation approach aimed at providing a natural collaboration between humans and translation systems. As such, we grant the user complete freedom to validate and correct any part of the translations suggested by the system. Our approach is then designed according to the requirements placed by this unrestricted proofreading protocol. In particular, the ability of the system to suggest new translations coherent with the set of potentially disjoint translation segments validated by the user. We evaluate our approach in a usersimulated setting where reference translations are considered the output desired by a human expert. Results show important reductions in the number of edits in comparison to decoupled post-editing and conventional prefix-based interactive translation prediction. Additionally, we provide evidence that it can also reduce the cognitive overload reported for interactive translation systems in previous user studies.	cognitive science;coherence (physics);error detection and correction;experiment;interactive machine translation;postediting;requirement	Jesús González-Rubio;Daniel Ortiz-Martínez;Francisco Casacuberta;José-Miguel Benedí	2016			natural language processing;machine translation;artificial intelligence;computer science;prefix	NLP	-30.895827475455857	-85.18732373217453	97264
e4537a2c65109bde88ca7a8510e8000cf6332e95	language identification of individual words with joint sequence models	multilingual speech recognition;joint sequence models;presentation;text based language identification	Within a multilingual automatic speech recognition (ASR) system, knowledge of the language of origin of unknown words can improve pronunciation modelling accuracy. This is of particular importance for ASR systems required to deal with codeswitched speech or proper names of foreign origin. For words that occur in the language model, but do not occur in the pronunciation lexicon, text-based language identification (T-LID) of a single word in isolation may be required. This is a challenging task, especially for short words. We motivate for the importance of accurate T-LID in speech processing systems and introduce a novel way of applying Joint Sequence Models to the T-LID task. We obtain competitive results on a real-world 4language task: for our best JSM system, an F-measure of 97.2% is obtained, compared to a F-measure of 95.2% obtained with a state-of-the-art Support Vector Machine (SVM).	f1 score;language identification;language model;lexicon;speech processing;speech recognition;support vector machine;text-based (computing)	Oluwapelumi Giwa;Marelie H. Davel	2014			natural language processing;language identification;cache language model;speech recognition;linguistics;language model	NLP	-19.167542252878977	-86.17025004738373	97347
2d930fd8718646064c05d6f4405892bc3d068f4e	a simple surface realizer for filipino	conference paper	Surface realizers are used at the final phase of natural language text generation to convert abstract or symbolic representations of information to linguistic forms in a particular human language. Rules of grammar for the target language are applied to produce text that is syntactically, morphologically and orthographically correct. In this paper, we present the development of FilSuRe, a simple surface realizer for Filipino. We also present how the Booklat system was produced by revising the realization phase of a prototype automatic story generator system, Picture Books, to use the API library of FilSuRe in order to generate children’s stories in Filipino.	application programming interface;ca-realizer;compiler;natural language generation;prototype	Ethel Ong;Stephanie Abella;Lawrence Santos;Dennis Tiu	2011			visual arts;alternative medicine;cartography	NLP	-30.073807999612264	-80.85695768703762	97393
4a361774dd35beb3c062fc29c2bae872724ba5b0	parsing conversational speech using enhanced segmentation	sentence boundary;probabilistic parser;state-of-the-art segmenter;possible error reduction;conversational speech;interruption point;pause-based segmenter;enhanced segmentation;parser performance;sound;information retrieval;parsers;speech;probability;records;extraction;reduction;boundaries;language	The lack of sentence boundaries and presence of disfluencies pose difficulties for parsing conversational speech. This work investigates the effects of automatically detecting these phenomena on a probabilistic parser’s performance. We demonstrate that a state-of-the-art segmenter, relative to a pause-based segmenter, gives more than 45% of the possible error reduction in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone.	computer performance;interrupt;parsing;probabilistic automaton;sensor	Jeremy G. Kahn;Mari Ostendorf;Ciprian Chelba	2004			natural language processing;extraction;speech recognition;reduction;computer science;speech;parsing;probability;linguistics;language;personal boundaries;sound	NLP	-20.319215755806304	-82.46168513737031	97563
21f929a3b062f712f7c0e64b706e23b4f36e4038	jurilinguistic engineering in cantonese chinese: an n-gram-based speech to text transcription system	statistical modelling;system performance;chinese;speech to text	A Cantonese Chinese transcription system to automatically convert stenograph code to Chinese characters is reported. The major challenge in developing such a system is the critical homocode problem because of homonymy. The statistical N-gram model is used to compute the best combination of characters. Supplemented with a 0.85 milli on character corpus of domain-specific training data and enhancement measures, the bigram and trigram implementations achieve 95% and 96% accuracy respectively, as compared with 78% accuracy in the baseline model. The system performance is comparable with other advanced Chinese Speech-to-Text input applications under development. The system meets an urgent need of the Judiciary of post1997 Hong Kong. Keyword: Speech to Text, Statistical Modelli ng, Cantonese, Chinese, Language Engineering	baseline (configuration management);bigram;medical transcription;n-gram;speech recognition;statistical model;transcription (software);trigram;viterbi algorithm	Benjamin Ka-Yin T'sou;King Kui Sin;Samuel W. K. Chan;Tom B. Y. Lai;Caesar Suen Lun;K. T. Ko;Gary K. K. Chan;Lawrence Y. L. Cheung	2000				NLP	-21.473133437233532	-81.80163434273469	97942
29ba6d9dbeeb1294cc646dc45710637ba2a1d475	improved model architecture and training phase in an off-line hmm-based word recognition system	notice of violation;image recognition;interpolation;off line hmm based word recognition system;handwriting recognition;image segmentation;testing;accuracy;hidden markov models;feature extraction;model architecture;accuracy model architecture training phase off line hmm based word recognition system;word recognition;pattern recognition;interpolation parameter estimation hidden markov models character recognition feature extraction image segmentation;speech recognition;handwritten word recognition;parameter estimation;training phase;oral communication;character recognition;hidden markov models speech recognition handwriting recognition character recognition pattern recognition image recognition testing image segmentation oral communication	Describes the latest developments to enhance the performance of our HMM-based handwritten word recognition system. These methods only deal with the recognition phase and involve the improvement of the HMM architecture as well as the optimization of the training phase. Experiments carried out on real data show that the proposed approaches lead to significant improvements in the accuracy of the system.		Abdenaim El Yacoubi;Robert Sabourin;Michel Gilloux;Ching Y. Suen	1998		10.1109/ICPR.1998.711997	computer vision;speech recognition;feature extraction;word recognition;interpolation;computer science;machine learning;pattern recognition;accuracy and precision;software testing;handwriting recognition;image segmentation;estimation theory;statistics	Vision	-19.589522963524164	-91.06539727208828	97970
8bdc88d97aac60987d4ad0e5a9ca2a94a87ba55b	generalized quantifiers, exception phrases, and logicality		A method of sealing an underwater diving suit constructed of foam core material having a cover material on both sides, including steps of removing a portion of the cover material on the inside of the suit along the seam to expose the foam core and adhering the foam portion of a sealing strip to the foam core.	exception handling	Shalom Lappin	1995	Logic Journal of the IGPL	10.1093/jigpal/3.2-3.203	generative grammar;philosophy;semantics;semantic property;linguistics	Logic	-32.43076577926167	-83.85556885539246	97972
44c0a22dd0d31acf8ede46ac12066b964e713578	graphic linguistics and its terminology		"""DURING the past thirty years great advances have been made towards making the study of language a science, but leading linguists have been mainly concerned with spoken language. There has been a certain tendency to suggest that the study of written documents should always be subsidiary to that of some spoken idiom, or even that it is bound to be less scientific than that of spoken idioms, and perhaps not a proper part of """"linguistics"""" at all. These suggestions should be opposed. """"Linguistics"""" should include the study of written languages as well as that of spoken; the former study can and should be as scientific as the latter, and it needs its own terminology which should be basically independent of that of the study of spoken languages. Much confusion, and some mistrust, if not antagonism, among linguists would seem to have resulted from lack of agreed distinct terminologies for the two studies, which might well be called respectively phonic and graphic or epigraphic linguistics. The problems of graphic linguistics are probably best approached through consideration of what writing is. A script may be defined as a system of visual symbols whose purpose is to convey the thought of one individual or group to another. Writing is often treated as a means of representing a spoken utterance or utterances by visual symbols, but this is not its primary purpose, except where phonetic or phonemic transcription in linguistic work is concerned. Representation of actual, contemplated or imagined utterance is a particular mechanism for conveying meaning by graphic signals, one whose convenience lies in the small number of signs required. The adoption of a particular form of it, alphabetic writing, in Western Europe, has led to its being widely regarded as the normal and natural mechanism, and some of those who have discussed the analysis of systems of writing have tended to write as if they were all more or less satisfactory systems of phonemic transcription of utterances. This attitude leads to or supports the view that the study of written documents should always be subsidiary to the study of some spoken idiom, or as an extreme to the idea that """"texts"""" are not """"language"""". One must leave to psychologists the question whether it is possible to read or write without some thought of phonic realization, whether based on a known spoken idiom or not. But it can hardly be denied that the users of a system of graphic communication may develop for it conventions of vocabulary and grammar which differ from those of any spoken language which they use, or on which the system was originally based. A group of texts showing similar conventions of grammar and vocabulary may reasonably be termed a """"written language"""". Most of this will probably be accepted by the majority of those concerned with the study of spoken languages, though in some cases with the proviso that the study of written language should be considered a discipline separate from """"linguistics"""" and """"philology."""" Such differentiation, however, has the disadvantage of tending to dissociate the study of the spoken form of a Ian -"""	distrust;mereology;transcription (software);vocabulary	R. A. Crossland	1956	Mechanical Translation		clinical linguistics;applied linguistics;terminology;natural language processing;computer science;artificial intelligence;linguistics	NLP	-32.482608629898444	-85.06734938174725	98484
463cf04af2dfd99120f1af9abb00cf20cd5ce892	ciair in-car speech corpus - influence of driving status		CIAIR, Nagoya University, has been compiling an in-car speech database since 1999. This paper discusses the basic information contained in this database and an analysis on the effects of driving status based on the database. We have developed a system called the Data Collection Vehicle (DCV), which supports synchronous recording of multichannel audio data from 12 microphones which can be placed throughout the vehicle, multi-channel video recording from three cameras, and the collection of vehicle-related data. In the compilation process, each subject had conversations with three types of dialog system: a human, a “Wizard of Oz” system, and a spoken dialog system. Vehicle information such as speed, engine RPM, accelerator/brake-pedal pressure, and steering-wheel motion were also recorded. In this paper, we report on the effect that driving status has on phenomena specific to spoken language key words: speech corpus, in-car speech, ITS	compiler;dialog system;microphone;speech corpus;spoken dialog systems;video	Nobuo Kawaguchi;Shigeki Matsubara;Kazuya Takeda;Fumitada Itakura	2005	IEICE Transactions			NLP	-25.002156720909994	-84.83608528618902	98491
21f8c7eea9555bfdf2d3e32c3320f303a5f0e155	inter-annotator agreement on spontaneous czech language - limits of automatic speech recognition accuracy		The goal of this article is to show that for some tasks in automatic speech recognition (ASR), especially for recognition of spontaneous telephony speech, the reference annotation differs substantially among human annotators and thus sets the upper bound of the ASR accuracy. In this paper, we focus on the evaluation of the inter-annotator agreement (IAA) and ASR accuracy in the context of imperfect IAA.We evaluated it using a part of our Czech Switchboardlike spontaneous speech corpus called Toll-free calls. This data set was annotated by three different annotators rendering three parallel transcriptions. The results give us additional insights for understanding the ASR accuracy.	automated system recovery;inter-rater reliability;speech corpus;speech recognition;spontaneous order	Tomás Valenta;Lubos Smídl;Jan Svec;Daniel Soutner	2014		10.1007/978-3-319-10816-2_47	speech recognition;rendering (computer graphics);natural language processing;computer science;artificial intelligence;transcription (linguistics);speech corpus;czech;annotation;telephony	NLP	-20.49087351338075	-83.23008944390803	98805
ca91e34b449753db75476f68a7118bf3b4cafc77	improved speech modelling and recognition using a new training algorithm based on outlier-emphasis for non-stationary state hmm	modelizacion;training analysis;traitement signal;goodness of fit;non stationary states;speech signal;discrimitive training;modelo markov;maximum likelihood;reconocimiento palabra;taux erreur;hidden markov model;speech processing;optimal estimation;maximum vraisemblance;tratamiento palabra;traitement parole;state dependence;stationary state;modelisation;markov model;hidden markov models;viterbi algorithm;signal processing;estimacion parametro;non stationary system;error rate;speech recognition;psychanalyse didactique;discriminative training;outlier emphasis;decodage viterbi;reconnaissance parole;parameter estimation;estimation parametre;modele markov;sistema no estacionario;indice error;procesamiento senal;modeling;systeme non stationnaire;viterbi decoding;maxima verosimilitud;training algorithm;desciframiento viterbi;psicoanalisis didactico	In this study, we develop a modified maximum likelihood algorithm for optimally estimating the state-dependent polynomial parameters in the nonstationary-state HMM. The newly devised training method controls the influence of outliers in the training data on the constructed models. For an alphabet recognition task, outlier emphasis resulted in improved performance. An error rate reduction of 14% is achieved for the linear trend and 7.5% is obtained for the stationary-state HMMs over the conventional models trained by the Viterbi algorithm based on the joint-state maximum likelihood criterion. The properties of the nonstationary-state HMM trained with the proposed approach are analysed by examining goodness-of-fit of the real speech data to the polynomial trajectories in the model.	algorithm;hidden markov model;stationary process;stationary state	Rathinavelu Chengalvarayan	1998	Speech Communication	10.1016/S0167-6393(98)00057-0	optimal estimation;stationary state;speech recognition;systems modeling;viterbi algorithm;word error rate;computer science;machine learning;pattern recognition;maximum likelihood;markov model;goodness of fit;estimation theory;viterbi decoder;hidden markov model;statistics	NLP	-20.19440108293735	-91.89324630825939	99209
8b3dde3ebf24bb55d2b84d81d01c4cab6c75fdd1	spell checker for myanmar language	text analysis;text corpus myanmar spell checker natural language processing string cosine similarity tokenization;text analysis linguistics natural language processing;context natural language processing conferences dictionaries automata educational institutions encoding;user efficiency myanmar spell checker myanmar language natural language processing nlp human language desktop applications machine translation system office automation system myanmar pronunciation myanmar orthography phonetic errors typographic errors nonword errors sequence errors misspelled myanmar words myanmar text corpus string cosine similarity error suggestion list generation official language;natural language processing;office automation;machine translation;linguistics	Natural Language Processing (NLP) is one of the most important research areas carried out in the world of Human language. For every language, spell checker is an essential component of many of the common Desktop applications, Machine Translation system and Office Automation system. In Myanmar, Myanmar Language is used as an official language. Myanmar Pronunciation and orthography has differences because spelling is often not an accurate reflection of pronunciation. In this paper, we developed Myanmar Spell Checker which can handle Typographic Errors (Non-word Errors), Phonetic Errors and Sequence Errors of Myanmar words. If misspelled word contains in the input sentence, this system can provide suggestion for misspelled Myanmar words. We apply Myanmar text Corpus to check Myanmar words. And then we used String Cosine Similarity to generate suggestions list for mistyped Myanmar words. The system can improve the quality of suggestion for misspelled Myanmar words and users' efficiency when the users cannot figure out the correct spelling by themselves.	american and british english pronunciation differences;automation;cosine similarity;natural language processing;spell checker;text corpus;unicode	Aye Myat Mon	2012	2012 International Conference on Information Retrieval & Knowledge Management	10.1109/InfRKM.2012.6204974	natural language processing;speech recognition;computer science;linguistics	NLP	-24.616749576671534	-80.4062011332536	99247
ce297fe7bb961566358704f384f7c7218d18787f	using a serious game to collect a child learner speech corpus	serveur institutionnel;archive institutionnelle;open access;archive ouverte unige;cybertheses;institutional repository	We present an English-L2 child learner speech corpus, produced by Swiss German-L1 students in their third year of learning English, which is currently in the process of being collected. The collection method uses a web-enabled multimodal language game implemented using the CALL-SLT platform, in which subjects hold prompted conversations with an animated agent. Prompts consist of a short animated Engligh-language video clip together with a German-language piece of text indicating the semantic content of the requested response. Grammar-based speech understanding is used to decide whether responses are accepted or rejected, and dialogue flow is controlled using a simple XML-based scripting language; the scripts are written to allow multiple dialogue paths, the choice being made randomly. The system is gamified using a score-and-badge framework with four levels of badges. We describe the application, the data collection and annotation procedures, and the initial tranche of data. The full corpus, when complete, should contain at least 5 000 annotated utterances.	gamification;java annotation;multimodal interaction;randomness;scripting language;simple xml;speech corpus;text corpus;video clip	Claudia Baur;Manny Rayner;Nikos Tsourakis	2014			natural language processing;computer science;linguistics;multimedia	NLP	-27.397829198152866	-84.13410542908447	99669
0b850a0381af1a5519b89b2ee45274660078d39b	mobile texting: can post-asr correction solve the issues? an experimental study on gain vs. costs	word error rate;error reduction;limited enforcement;automotive;evaluation measure;error correction;speech recognition;messaging	"""The next big step in embedded, mobile speech recognition will be to allow completely free input as it is needed for messaging like SMS or email. However, unconstrained dictation remains error-prone, especially when the environment is noisy. In this paper, we compare different methods for improving a given free-text dictation system used to enter textbased messages in embedded mobile scenarios, where distraction, interaction cost, and hardware limitations enforce strict constraints over traditional scenarios. We present a corpus-based evaluation, measuring the trade-off between improvement of the word error rate versus the interaction steps that are required under various parameters. Results show that by post-processing the output of a """"black box"""" speech recognizer (e.g. a web-based speech recognition service), a reduction of word error rate by 55% (10.3% abs.) can be obtained. For further error reduction, however, a richer representation of the original hypotheses (e.g. lattice) is necessary."""	automated system recovery;black box;cognitive dimensions of notations;email;embedded system;experiment;finite-state machine;mac os x 10.3 panther;speech recognition;text corpus;video post-processing;web application;word error rate	Michael Feld;Saeedeh Momtazi;Farina Freigang;Dietrich Klakow;Christian A. Müller	2012		10.1145/2166966.2166974	message;simulation;error detection and correction;speech recognition;word error rate;computer science;artificial intelligence;operating system;machine learning;world wide web	HCI	-24.695913123715272	-87.0598534650484	99838
089770a0fcec38bab3748a6d1240c4c5e466c8f3	an information gain and grammar complexity based approach to attribute selection in speech enabled information retrieval dialogs	electronic commerce;information retrieval system;attribute selection;information retrieval;acoustic modeling;speech processing;speech processing speech recognition information retrieval interactive systems grammars computational complexity;recognition accuracy information gain grammar complexity attribute selection speech enabled information retrieval dialog length reduction acoustic model dynamic dialog driven system;grammars;computational complexity;speech recognition;information gain;interactive systems;information retrieval speech recognition target recognition marketing and sales electronic commerce speech synthesis automatic speech recognition acoustic measurements length measurement gain measurement	An effective dialog driven method is required for today's speech enabled information retrieval systems, such as name dialers. Similar to the dynamic sales dialog for electronic commerce scenarios, information gain measure based approaches are widely used for attribute selection and dialog length reduction. However, for speech enabled information retrieval systems, another important factor influencing attribute selection is speech recognition accuracy. Too low accuracy results in a failed dialog. Recognition accuracy varies with many issues, including acoustic model performance and grammar complexity. The acoustic model is fixed for a whole dialog, while grammar is different for each interaction round, thereby grammar complexity influences the attribute selected for the next question. An approach combining both information gain measurement and grammar complexity is presented for a dynamic dialog driven system. Offline evaluations show that this approach can give a trade-off between the target of faster discrimination of the candidates for retrieval and higher recognition accuracy.	acoustic cryptanalysis;acoustic model;e-commerce;information gain in decision trees;information retrieval;kullback–leibler divergence;online and offline;speech recognition;dialog	Haiping Li;Haixin Chai	2004	2004 International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2004.1409657	voice activity detection;natural language processing;speech recognition;computer science;machine learning;speech processing;kullback–leibler divergence;computational complexity theory;speech analytics	NLP	-21.039143862692786	-88.44679354871617	100409
051bd8b74ddef525ec00db64261796f84758bcef	minimum word error rate training for attention-based sequence-to-sequence models		Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the log-likelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER. In the present work, we explore techniques to train attention-based models to directly minimize expected word error rate. We consider two loss functions which approximate the expected number of word errors: either by sampling from the model, or by using N-best lists of decoded hypotheses, which we find to be more effective than the sampling-based method. In experimental evaluations, we find that the proposed training procedure improves performance by up to 8.2% relative to the baseline system. This allows us to train grapheme-based, uni-directional attention-based models which match the performance of a traditional, state-of-the-art, discriminative sequence-trained system on a mobile voice-search task.	approximation algorithm;automated system recovery;baseline (configuration management);cross entropy;loss function;sampling (signal processing);speech recognition;word error rate	Rohit Prabhavalkar;Tara N. Sainath;Yonghui Wu;Patrick Nguyen;Zhifeng Chen;Chung-Cheng Chiu;Anjuli Kannan	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461809	discriminative model;task analysis;word error rate;machine learning;artificial intelligence;expected value;pattern recognition;sampling (statistics);bayes' theorem;decoding methods;computer science	Robotics	-20.861046618443655	-89.38631914632383	100745
3886ae94e63d01a1ed4c157926de33298dee66e7	the synthesis of lse classifiers: from representation to evaluation		This work presents a first approach to the synthesis of Spanish Sign Language’s (LSE) Classifier Constructions (CCs). All current attempts at the automatic synthesis of LSE simply create the animations corresponding to sequences of signs. This work, however, includes the synthesis of the LSE classification phenomena, defining more complex elements than simple signs, such as Classifier Predicates, Inflective CCs and Affixal classifiers. The intelligibility of our synthetic messages was evaluated by LSE natives, who reported a recognition rate of 93% correct answers.	acm computing classification system;common criteria;functional approach;intelligibility (philosophy);point of view (computer hardware company);synthetic intelligence	Fernando J. López-Colino;José Colás Pasamontes	2011	J. UCS	10.3217/jucs-017-03-0399	computer science;machine learning;artificial intelligence	HCI	-25.696224542300556	-83.81380049161343	100751
e868c1672db9e6cd9409aa97c8931fe2c18a357b	using second-order hidden markov model to improve speaker identification recognition performance under neutral condition	second order;speaker identification;hidden markov model;cepstral analysis speaker recognition hidden markov models computational complexity linear predictive coding speech coding;observation vector second order hidden markov model text dependent speaker identification neutral talking condition recognition performance changing statistical characteristics double stochastic process state transition matrix computational complexity linear predictive coding cepstral feature analysis;speech coding;hidden markov models speaker recognition stochastic processes speech analysis electronic mail markov processes probability distribution probability density function computational complexity decoding;speaker recognition;linear predictive coding;cepstral analysis;first order;hidden markov models;computational complexity	In this paper, second-order hidden Markov model (HMM2) has been used and implemented to improve the recognition performance of textdependent speaker identification systems under neutral talking condition. Our results show that HMM2 improves the recognition performance under neutral talking condition compared to the first-order hidden Markov model (HMM1). The recognition performance has been improved by 9%.	first-order predicate;hidden markov model;markov chain;speaker recognition	Ismail Shahin	2003		10.1109/ICECS.2003.1301992	forward algorithm;speaker recognition;maximum-entropy markov model;linear predictive coding;speech recognition;viterbi algorithm;computer science;machine learning;speech coding;hidden semi-markov model;pattern recognition;first-order logic;markov model;computational complexity theory;second-order logic;hidden markov model;variable-order markov model	Metrics	-19.75973280592398	-92.35111756061926	100753
09d5afe31d2158503a8248fa76915b3076f2f123	alisp-based data compression for generic audio indexing	audio indexing alisp units data driven audio sequencing data compression;approximation algorithms;automatic language independent speech processing approach data compression generic audio indexing blast technique yacast audio retrieval alisp tools audio data approximate matching algorithm audio items radio stream private radio broadcast database audio identification advertisement songs audio motif discovery speaker diarization laughter detection etape 2011 evaluation campaign speaker diarization task evaluations en treatment automatique de la parole evaluation campaign;hidden markov models;vectors;indexing;speaker recognition data compression indexing;stability analysis;hidden markov models indexing vectors data models stability analysis approximation algorithms;data models	In this paper we propose a generic framework to index and retrieve audio. In this framework, audio data is transformed into a sequence of symbols using the ALISP tools. In such a way the audio data is represented in a compact way. Then an approximate matching algorithm inspired from the BLAST technique is exploited to retrieve the majority of audio items that could be present in radio stream. The evaluations of the proposed systems are done on a private radio broadcast database provided by YACAST and other publicly available corpora. The experimental results show an excellent performance in audio identification (for advertisement and songs), audio motif discovery (for advertisement and songs), speaker diarization and laughter detection. Moreover, the ALISP-based system has obtained the best results in ETAPE 2011 (Evaluations en Treatment Automatique de la Parole) evaluation campaign for the speaker diarization task.	approximation algorithm;blast;data compression;hidden markov model;linear algebra;radio broadcasting;regular expression;sequence motif;speaker diarisation;string (computer science);text corpus	Houssemeddine Khemiri;Dijana Petrovska-Delacrétaz;Gérard Chollet	2014	2014 Data Compression Conference	10.1109/DCC.2014.81	natural language processing;data modeling;search engine indexing;von neumann stability analysis;audio mining;speech recognition;computer science;speech coding;pattern recognition;approximation algorithm;hidden markov model	ML	-20.736497530891622	-87.93002236070434	101147
98192432302a439dd8a17a5e6c5fbeae7c3a2aef	eye gaze for understanding conversational speech	speech processing gaze tracking human computer interaction natural language processing;slu eye gaze;link click event prediction eye gaze feature conversational speech understanding spoken language understanding human computer interaction;slu;eye gaze;speech filling boats conferences human computer interaction fasteners face	Eye gaze is a useful indication of attention and, as such, can be a valuable feature to improve spoken-language understanding in human-computer interaction. Based on the hypothesis that users look at a link before selecting it, we investigate the use of novel eye-gaze features to improve link click event prediction. Our data comprises users performing a variety of online tasks such as form filling and web browsing, and we show significant performance improvement by incorporating the use of gaze features. In addition, our analysis shows that there is much user-specific variation in gaze, so we are also looking to improve the modeling of gaze by user- and task-specific adaptation.	eb-eye;human–computer interaction;natural language understanding	Anna Prokofieva;Dilek Z. Hakkani-Tür;Malcolm Slaney	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078637	computer vision;speech recognition;eye tracking;computer science;linguistics	NLP	-25.87007402987484	-87.48971180960127	101206
c1f1f09afd2cd5bb378d9c1d4678df16066dd7b8	word, phrase and sentence		A f i f t h s tudy a rgues t h a t a n a l y s i s of e x i s t i n g n a t u r a l l anguage d i c t i o n a r i e s can be e x p e c t e d to c o n t r i b u t e i m p o r t a n t l y to what i s needed f o r t e x t u n d e r s t a n d i n g programs. The f i n a l s t udy i s an expe r imen t w i t h a s e n t e n c e l e v e l t r a n s l a t o r a p p l i e d to a l a r g e German-Engl i sh t r a n s l a t i o n t a s k . These two s t u d i e s a r e p r i m a r i l y concerned w i t h a n a l y s i s of l anguage a t the s e n t e n c e l e v e l . The most glamourous a r e a s of n a t u r a l l anguage r e s e a r c h a r e a t l e v e l s above the s e n t e n c e , concerned w i t h d i a l o g u e s and d i s c o u r s e , f r e q u e n t l y d i s d a i n f u l of m o r p h o l o g i c a l o r even g rammat i ca l a n a l y s i s i n t h e i r s e a r c h fo r e f f e c t i v e s t r u c t u r e s f o r u n d e r s t a n d i n g what the d i s c o u r s e i s abou t . S c r i p t s , f r ames , s t e r e o t y p e s , schemas a re a l l s t u d i e d i n t h e s e a r e a s ; and o f t e n m o r p h o l o g i c a l and g r a ~ n a t i c a l a n a l y s i s i s bypassed i n f a v o r of keyword s c a n n i n g to e x t r a c t some smal l r e l e v a n t p o r t i o n of the t e x t to be bound as v a l u e s f o r s l o t s i n t h e s e l a r g e r d a t a forms. This s e s s i o n reminds us t h a t much can be accompl i shed w i t h v o c a b u l a r y a n a l y s i s , w i t h keyword s c a n n i n g and statistical treatment of text and with semantic analysis at the single sentence level. Yet, with regard to most of the topics in this and other sessions, there is a stronK sense of de~a vu; the earliest natural language studies featured automatic extracting and information retrieval based on statistical, lexical and associational properties of keywords. Mechanical translation of sentences without regard for larger contexts marked the late sixties high point of MT research amid contemporaneous studies of the English dictionary and thesaurus. Competition among sentence parsing algorithms is an ACL tradition celebrated annually, while psycholinguistics has traditionally applied chronometric studies, and recordings of eye movements to measure this or that aspect of human linguistic processing throughout the	algorithm;artificial intelligence;dictionary;digital-to-analog converter;fo (complexity);information retrieval;natural language;parsing;semantic analysis (compilers);thesaurus	Robert F. Simmons	1980			nominal group;noun phrase;endocentric and exocentric;inverted sentence;verb phrase ellipsis;determiner phrase;balanced sentence	AI	-33.35065649407611	-82.47861661203792	101744
f307a49adf71236dacbe35d3536abafe1b002c3e	a system for creating personalized synthetic voices	speech synthesis;lou gehrig s disease;amyotrophic lateral sclerosis als;augmentative and alternative communication;augmentative and alternative communication aac;amyotrophic lateral sclerosis;synthetic voices	We will be demonstrating the ModelTalker Voice Creation System, which allows users to create a personalized synthetic voice with an unrestricted vocabulary. The system includes a tool for recording a speech inventory and a program that converts the recorded inventory into a synthetic voice for the ModelTalker TTS engine. The entire system can be downloaded for use on a home PC or in a clinical setting, and the resulting synthetic voices can be used with any SAPI compliant system.We will demonstrate the recording process, and convert the recordings to a mini-database with a limited vocabulary for participants to hear.	netware file system;personalization;speech synthesis;synthetic intelligence;vocabulary	Debra Yarrington;Christopher A. Pennington;John Gray;H. Timothy Bunnell	2005		10.1145/1090785.1090827	speech recognition;computer science;speech synthesis	HCI	-24.81625939677171	-86.55619563103296	102186
207ea0c0d306090cc7e0a1bf2fe00230248a2df6	nist speaker recognition evaluation chronicles		NIST has coordinated annual evaluations of textindependent speaker recognition since 1996. During the course of this series of evaluations there have been notable milestones related to the development of the evaluation paradigm and the performance achievements of state-of-the-art systems. We document here the variants of the speaker detection task that have been included in the evaluations and the history of the best performance results for this task. Finally, we discuss the data collection and protocols for the 2004 evaluation and beyond.	programming paradigm;speaker recognition	Mark A. Przybocki;Alvin F. Martin	2004			nist;speaker recognition;speaker diarisation;speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-24.212810655025297	-83.6513050937763	102301
3633e2bbd6a12e43a085e159e5a92950b434c0f8	automatic title generation for chinese spoken documents with a delicate scored viterbi algorithm	selection model;design model;title length automatic title generation chinese spoken document delicate scored viterbi algorithm text summary viterbi beam search term selection term ordering;speech processing;text analysis;viterbi algorithm automatic testing speech navigation performance evaluation educational institutions anthropometry humans bandwidth ip networks;indexing terms;maximum likelihood estimation;automatic generation;viterbi algorithm;title generation spoken documents;title generation;text analysis maximum likelihood estimation natural language processing speech processing;subject areas;spoken documents;natural language processing	Automatic title generation for spoken documents is believed to be an important key for browsing and navigation over huge quantities of multimedia content. A new framework of automatic title generation for Chinese spoken documents is proposed in this paper using a delicate scored Viterbi algorithm performed over automatically generated text summaries of the testing spoken documents. The Viterbi beam search is guided by a delicate score evaluated from three sets of models: term selection model tells the most suitable terms to be included in the title, term ordering model gives the best ordering of the terms to make the title readable, and title length model tells the reasonable length of the title. The models are trained from a training corpus which is not required to be matched with the testing spoken documents. Both objective evaluation based on F1 measure and subjective human evaluation for relevance and readability indicated the approach is very attractive.	automatic summarization;beam search;f1 score;human-readable medium;relevance;text corpus;viterbi algorithm	Sheng-yi Kong;Chien-Chih Wang;Ko-chien Kuo;Lin-Shan Lee	2008	2008 IEEE Spoken Language Technology Workshop	10.1109/SLT.2008.4777866	natural language processing;speech recognition;index term;viterbi algorithm;computer science;pattern recognition;speech processing;maximum likelihood	NLP	-22.285402307962144	-82.47752250794863	102467
a4f952cbba302c16f4bca9a254e1cc9e4607ee6d	segmenting unrestricted chinese text into prosodic words instead of lexical words	tts systems;stress;classification and regression tree;rhythm;zirconium;speech synthesis;tts systems surface differences text to speech systems perceptual differences statistical rule based method cart based method complicatedset based cart method simpleset cart method lexicon word segmentation pos tagging unrestricted chinese text preference test classification and regression tree lexical word strings prosodic word strings part of speech tagging;text to speech systems;rule based;statistical rule based method;simpleset cart method;text analysis;unrestricted chinese text;natural languages;testing;lexical word strings;complicatedset based cart method;preference test;statistical analysis speech synthesis;word segmentation;statistical analysis;prosodic word strings;perceptual differences;part of speech tagging;part of speech;text to speech;speech synthesis testing natural languages zirconium stress computational efficiency tagging asia text analysis rhythm;computational efficiency;lexicon word segmentation;cart based method;asia;surface differences;tagging;pos tagging	This paper stresses the importance of converting a string of lexical words to that of prosodic words in TTS systems by presenting the surface differences and perceptual differences between them. A statistical rule based method and a CART based method are proposed as solutions. Though ComplicatedSet based CART method performs the best, the achievement is obtained at the cost of heavy computation workloads needed by a parser. Statistical rule based method results higher recall but lower precision, comparing to SimpleSet CART method. It is very difficult to tell which is better, since we don’t know which affects naturalness more, precision or recall. Both of them require only lexicon word segmentation and POS tagging in the preprocessing stage, and are easily realized in TTS systems. Results of the preference test discloses that significant improvements on naturalness are perceived when lexical word strings are converted into prosodic word strings by our approach.	computation;decision tree learning;lexicon;netware file system;newton's method;part-of-speech tagging;preprocessor;text segmentation	Yao Qian;Min Chu;Hu Peng	2001		10.1109/ICASSP.2001.941042	natural language processing;text segmentation;speech recognition;part of speech;computer science;rhythm;zirconium;pattern recognition;software testing;stress;natural language;speech synthesis	NLP	-20.64865801023569	-81.92360459975937	102549
06a4c90a55250a2644ca4c5b34f04f509b5124fa	separating surface order and syntactic relations in a dependency grammar	word order domain structure;syntactic dependency tree;word order;lexicalized approach;lexical ambiguity;syntactic relation;proposal result;previous proposal;dependency tree;precise description;dependency grammar;separating surface order	This paper proposes decoupling the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree. We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree. The proposal results in a lexicalized, declarative, and formally precise description of word order; features which lack previous proposals for dependency grammars. Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives. 1 I n t r o d u c t i o n Recently, the concept of valency has gained considerable attention. Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990). Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965). This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997). Our position will be that dependency relations are motivated semantically (Tesni~re, 1959), and need not be projective (i.e., may cross if projected onto the surface ordering). We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree. These order domains constitute a tree defined by set inclusion, and surface word order is determined by traversing this tree. A syntactic analysis therefor consists of two linked, but dissimilar trees. Sec. 2 will briefly review approaches to word order in DG. In Sec. 3, word order domains will be defined, and Sec. 4 introduces a modal logic to describe dependency structures. Sec. 5 applies our approach to the German clause and Sec. 6 relates it to some PSG approaches. 2 W o r d O r d e r in D G A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993). The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals. F u n c t i o n a l G e n e r a t i v e D e s c r i p t i o n (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that	categorial grammar;context-free grammar;context-free language;coupling (computer programming);declarative programming;dependency grammar;discontinuous galerkin method;hudson;language-independent specification;locality of reference;modal logic;parsing;phrase structure grammar;theory;tracing (software);turing completeness	Norbert Bröker	1998			word order;natural language processing;computer science;join dependency;regular tree grammar;linguistics;word grammar;algorithm;dependency grammar	AI	-32.44060347470116	-82.46773814011051	102577
335ace8f1cd3d589fddcfc960d3a9bd4de72442b	automatic chord estimation on seventhsbass chord vocabulary using deep neural network	deep belief network;deep neural network;automatic chord estimation deep neural network ace two layer architecture chord smoothing gmm hmm approach chord type balanced dataset mirex seventhsbass chord vocabulary;hidden markov models vocabulary estimation neural networks smoothing methods standards measurement;vocabulary gaussian processes hidden markov models mixture models natural language processing neural nets text analysis;automatic chord estimation;deep belief network automatic chord estimation deep neural network	"""This paper proposes an automatic chord estimation (ACE) system with a two-layer architecture. The first layer performs chord smoothing with """"GMM + HMM"""" approach. Then given the results of the first layer, the second layer performs chord estimation using a deep neural network, which is trained on a well chord-type balanced dataset. The system accepts exactly the """"SeventhsBass"""" vocabulary. Three approaches with different configurations of the system are compared with Chordino, which is probably the only both MIREX evaluated and """"SeventhsBass"""" acceptable ACE system. Evaluation results on """"The Beatles"""" dataset show that the best approach outperforms Chordino in the most difficult """"SeventhsBass"""" metric in a significant way."""	ace;artificial neural network;deep learning;estimation of distribution algorithm;hidden markov model;smoothing;vocabulary	Jun-qi Deng;Yu-Kwong Kwok	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471677	speech recognition;computer science;machine learning;pattern recognition;deep belief network	Robotics	-21.955046122332902	-89.91317939707022	102579
9ceff2ba2f5e4f12a1f79c1bd6ddcf45a981dae1	speech recognition of spontaneous, noisy speech using auxiliary information in bayesian networks	belief networks;bayesian network;spontaneous noisy speech;hmm automatic speech recognition asr spontaneous noisy speech auxiliary information bayesian networks pitch rate of speech spontaneous speech performance;speech recognition intelligent networks bayesian methods automatic speech recognition hidden markov models speech enhancement artificial intelligence system testing cepstral analysis distributed computing;stephenson;auxiliary information;spontaneous speech performance;distributed computing;bayesian methods;hmm;speech;speech enhancement;rate of speech;asr;automatic speech recognition;cepstral analysis;hidden markov models;belief networks speech recognition noise;speech recognition;system testing;artificial intelligence;pitch;intelligent networks;spontaneous speech;noise;bayesian networks	Automatic speech recognition (ASR) currently performs well in the case of clean, read speech. It performs worse, however, when the speech is spontaneous and in noisy conditions. In previous work we showed the improvement that using auxiliary information in the framework of Bayesian networks (BNs) can bring to ASR in clean, read speech. Here we show that auxiliary information of pitch or rate-of-speech in the context of BNs also helps performance in spontaneous speech with noise.	bayesian network;pitch (music);speech recognition;spontaneous order	Todd A. Stephenson;Mathew Magimai-Doss;Hervé Bourlard	2003		10.1109/ICASSP.2003.1198706	voice activity detection;natural language processing;speech recognition;computer science;machine learning;pattern recognition;bayesian network;speech processing;hidden markov model	NLP	-19.61845302780589	-87.65610345145407	102732
383a89bdd653233a7ff8c1a0e8c78463f1a183e3	speaker intention modeling for large vocabulary mandarin spoken dialogues	information science;natural languages;telephony;computer science;speech;predictive models;speech processing;statistical analysis;mutual information;directory service;sat;data mining;databases;redundancy	This paper presents a statistical speaker intention modeling approach of speech act types (SAT’s)[1] prediction for large vocabulary Mandarin spoken dialogues. A SAT is an abstraction of speaker’s intention in terms of the type of action that the speaker intends by the utterance. With this approach, spoken dialogue systems can be constructed to predict speaker’s intention and make a proper action in advance.	dialog system;super robot monkey team hyperforce go!;vocabulary	Yen-Ju Yang;Lee-Feng Chien;Lin-Shan Lee	1996			natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;linguistics	NLP	-20.20633231463829	-86.33488578025748	102852
1dbcde223eb49076bffdd6378cae6be7fbac51ef	building dialogue corpora for nursing activity analysis		In th is paper, w e in troduce our corpora under developm ent, w hich are recorded in a real environm ent. T hese corpora com prise dialogues collected in hospitals w ith the aim of developing a nursing serv ice support system through a com prehensive understanding of nursing activ ities. We use the corpora to analyze how nurses perform their nursing duties and how they express the perform ance of their tasks. To understand nursing activ ities, w e investigated nursing serv ices and the relevant m edical charts by using the corpora. In the paper, w e show features and prom ising applications of the corpora.	chart;ising model;text corpus	Hiromi Itoh Ozaku;Akinori Abe;Noriaki Kuwahara;Futoshi Naya;Kiyoshi Kogure;Kaoru Sagara	2005			nursing;prom;computer science	ML	-25.547958746523534	-84.2830611928039	102914
264b78f110226ef32f54d1dd7a550fa19ef51020	underspecifying and predicting voice for surface realisation ranking	data-driven surface realisation model;realistic german corpus data;surface realisation ranking;surface realisation performance;syntactic surface realisation system;underspecified input;realisation model;original voice;word order alternation;word order variant;candidate set	This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice.	alternation (formal language theory);bleu;baseline (configuration management);experiment;heuristic;norm (social);text corpus	Sina Zarrieß;Aoife Cahill;Jonas Kuhn	2011			natural language processing;speech recognition;computer science;machine learning	NLP	-19.501956853396788	-80.24025602299757	102994
1e8a89d2c7e2517ca9b774567f55e536caddf0c8	the acquisition of a lexicon from paired phoneme sequences and semantic representations	semantic representation	We present an algorithm that acquires words (pairings of phonological forms and semantic representations) from larger utterances of unsegmented phoneme sequences and semantic representations. The algorithm maintains from utterance to utterance only a single coherent dictionary, and learns in the presence of homonymy, synonymy, and noise. Test results over a corpus of utterances generated from the Childes database of mother-child interactions are presented.	algorithm;childes;coherence (physics);dictionary;interaction;lexicon;speech corpus;text corpus	Carl de Marcken	1994		10.1007/3-540-58473-0_138	natural language processing;speech recognition;computer science;linguistics	NLP	-20.96854918716021	-82.77441207408195	103064
4eb9f0da956c4fdf6920823fea34e39f3226a27b	lattice parsing and application of integrated language models for speech recognition	language model;speech recognition		language model;parsing;speech recognition	Gareth J. F. Jones;Harvey Lloyd-Thomas;Jeremy H. Wright	1995			voxforge;speech recognition;language model;speech corpus;cache language model;parsing;natural language processing;computational linguistics;speaker recognition;artificial intelligence;computer science;speech segmentation	NLP	-19.58958043083714	-85.13275901129012	103238
c16afe2799acb8efab1b21713a105227397dacf7	the nite xml toolkit: demonstration from five corpora	multimodal reference;novel feature;different corpus;classical hebrew;switchboard dialogue;nite xml toolkit;cross-annotated data set;discourse annotation;text language corpus;human annotators	The NITE XML Toolkit (NXT) is open source software for working with multimodal, spoken, or text language corpora. It is specifically designed to support the tasks of human annotators and analysts of heavily cross-annotated data sets, and has been used successfully on a range of projects with varying needs. In this text to accompany a demonstration, we describe NXT along with four uses on different corpora that together span its most novel features. The examples involve the AMI and ICSI Meeting Corpora; a study of multimodal reference; a syntactic analysis of Genesis in classical Hebrew; and discourse annotation of Switchboard dialogues.	genesis;multimodal interaction;nxt;open-source software;parsing;sms language;telephone switchboard;text corpus;xml	Jonathan Kilgour;Jean Carletta	2006			natural language processing;computer science;database;world wide web	NLP	-24.24684777586873	-84.24601665925869	103362
f8711c600f728786e1435a1b964b446b4d94c449	sassc: a standard arabic single speaker corpus.		This paper describes the process of collecting and recording a large scale Arabic single speaker speech corpus. The collection and recording of the corpus was supervised by professional linguists and was recorded by a professional speaker in a soundproof studio using specialized equipments and stored in high quality formats. The pitch of the speaker (EGG) was also recorded and synchronized with the speech signal. Careful attempts were taken to insure the quality and diversity of the read text to insure maximum presence and combinations of words and phonemes. The corpus consists of 51 thousand words that required 7 hours of recording, and it is freely available for academic and research purposes.	display resolution;speech corpus;text corpus	Ibrahim Almosallam;Atheer Alkhalifa;Mansour Al-Ghamdi;Mohamed I. Alkanhal;Ashraf Alkhairy	2013			arabic;speech recognition;computer science	NLP	-23.152273630446423	-84.05068398704063	103646
a2c3a9c5e1ce0441be655ad73d42cd398f60b7b6	perceptual features for off-line handwritten word recognition: a framework for heuristic prediction, representation and matching	acoplamiento grafo;handwriting recognition;analisis forma;approche heuristique;holistic approach;graph matching;couplage graphe;reconnaissance ecriture;reconnaissance caractere;word recognition;pattern recognition;enfoque heuristico;handwritten word recognition;pattern analysis;reconnaissance forme;heuristic approach;reconocimiento patron;character recognition;analyse forme;reconocimiento caracter	Abs t r ac t . Perceptual holistic features are visually conspicuous features of the word shape that have been cited in reading studies as being utilized in fluent reading. While these features have been used for word recognition when the lexicon of possible words is small and static, their application to the general problem of omni-scriptor handwritten word recognition is limited by their variability at the word level and the paucity of samples for word-level training. A methodology of coarse holistic features and heuristic prediction of ideal features from ASCII is proposed to address these issues. The methodology is based on the view that realworld examples of handwritten words are instances of the ideal exemplar of the word class distorted by the scriptor, stylus, medium and intervening electronic imaging processes, and has applications in verification and lexicon reduction for handwritten word recognition.	heuristic;holism;lexicon;spatial variability;speedscript;stylus (computing)	Sriganesh Madhvanath;Venu Govindaraju	1998		10.1007/BFb0033275	natural language processing;speech recognition;word recognition;computer science;intelligent word recognition;pattern recognition;handwriting recognition;matching	AI	-23.287991597287537	-80.999932263981	103664
34db4861fa769834f593d9c41b415d32348bfd57	design of double talk sequences in different languages to harmonize third party listening test results		In [1] a Third Party Listening Test (TPLT) was introduced using pre-recorded conversations in Russian, German and English language to assess and verify the quality of eCall systems (In-vehicle Systems, IVS). The comparison of different languages shall verify the possibility for IVS manufacturers to optimize performance of their systems for the homologation tests according to MGS R55531 [2] based on different languages. First tests lead to similar results for Russian, English and German recordings for most of the tested devices and parameters [1]. Only for particular devices, showing fast switching characteristic during double talk, ambiguous results were found under double talk conditions. The double talk sequence, in particular the timing of uplink and downlink signals, was adapted for the German sequence and the test was repeated in all three languages. Furthermore two groups on test persons, naïve and expert subjects, participated in the German test session in order to verify the consistency of results between both groups. The adapted speech sequences and the results of the recent listening test are discussed here. [1] F. Kettler, S. Poschen, R. Serafimov, “Third Party Listening Test in Emergency Call Scenarios using Different Languages, DAGA Aachen, March 2016 [2] ERA-GLONASS Specification MGS R55531, In-vehicle emergency call system compliant test methods for quality of speaker phone in a vehicle, 2015	device under test;glonass;modelling of general systems;naivety;telecommunications link;thyristor	Frank Kettler;Silvia Poschen;Radi Serafimov	2016			communication;active listening;computer science	AI	-25.35887904498315	-86.81617525309436	103833
8503b7971ae98fbd7efaf697430ca390770b8037	development of indonesian handwritten text database for offline character recognition	databases;visual databases data acquisition handwriting recognition text analysis;lowercase words;indonesian handwritten text database development;isolated uppercase letters;handwriting recognition;culture specific scripts;cultural diversity;vocabulary;database;college students;text analysis;cursive words;accuracy;research and development;english foreign language;text database;databases character recognition handwriting recognition vocabulary accuracy writing educational institutions;database handwriting recognition offline character recognition;writing;mixed digits;cursive words indonesian handwritten text database development offline character recognition latin character isolation culture specific scripts geographic area distribution data acquisition english foreign language mixed digits isolated digits isolated lowercase letters isolated uppercase letters lowercase words uppercase words;latin character isolation;foreign language;isolated lowercase letters;geographic area distribution;data acquisition;isolated digits;character recognition;offline character recognition;handwritten character recognition;uppercase words;visual databases	Research and development of a character recognition systems in the past years has been matured with publicly available either free or commercial character recognition systems especially for isolated Latin characters in multiple languages. The character recognition problem itself can be considered as a mostly solved. Some problems that are still being worked on in this community are unconstrained handwritten text and culture specific scripts. Handwritten text is a special case when the script is used in more than one language; therefore the accuracy of the recognition depends on the knowledge of the language in which the text is written. Indonesia has a large cultural diversity especially languages because of its heterogeneous distribution of geographic areas. The implementation of handwritten character recognition for data acquisition in Indonesia still does not accommodate this background culture to improve accuracy since commercially available systems are developed with foreign language (English) although using the same alphabet. Therefore a local version of offline handwritten database (Indonesian) is required. The purpose of this paper is to report the collection of an Indonesian Handwritten text database. We collected various handwritten text samples which are isolated digits, mixed digits, isolated lowercase letters, isolated uppercase letters, lowercase words, uppercase words, and cursive words from college student source. We also present our method to select words which is used on the database.	data acquisition;handwriting recognition;online and offline;optical character recognition;scripting language	Peb Ruswono Aryan;Iping Supriana;Ayu Purwarianti	2011	Proceedings of the 2011 International Conference on Electrical Engineering and Informatics	10.1109/ICEEI.2011.6021582	foreign language;natural language processing;speech recognition;document processing;computer science;intelligent word recognition;database;accuracy and precision;handwriting recognition;data acquisition;writing;cultural diversity	DB	-23.60733498987064	-85.07370223142715	103940
2c222c3c4cda9b2c11f2dfd4690c9aa4b3382777	study on similarity among indian languages using language verification framework		Majority of Indian languages have originated from two language families, namely, Indo-European and Dravidian. Therefore, certain kind of similarity among languages of a particular family can be expected to exist. Also, languages spoken in neighboring regions show certain similarity since there happens to be a lot of intermingling between population of neighboring regions. This paper develops a technique to measure similarity among Indian languages in a novel way, using language verification framework. Four verification systems are designed for each language. Acceptance of one language as another, which relates to false acceptance in language verification framework, is used as a measure of similarity. If language A shows false acceptance more than a predefined threshold with language B, in at least three out of the four systems, then languages A and B are considered to be similar in this work. It is expected that the languages belonging to the same family should manifest their similarity in experimental results. Also, similarity between neighboring languages should be detected through experiments. Any deviation from such fact should be due to specific linguistic or historical reasons. This work analyzes any such scenario.		Debapriya Sengupta;Goutam Saha	2015	Adv. Artificial Intellegence	10.1155/2015/325703	natural language processing;computer science	NLP	-31.047988490539677	-86.07872362105742	104123
b3188568df1d2f876635d3fc1bd2f0984315b6ee	tibetan language speech recognition model based on active learning and semi-supervised learning	supervised learning;active learning;speech;tibetan language speech recognition model;semi supervised learning;accuracy;computational modeling;tibetan speech utterances;speech recognition supervised learning entropy accuracy speech computational modeling labeling;speech recognition;entropy;semi supervised learning tibetan language speech recognition active learning;tibetan language speech recognition;learning artificial intelligence;tibetan speech utterances tibetan language speech recognition model active learning semisupervised learning;semisupervised learning;labeling;speech recognition learning artificial intelligence	In the researches on Tibetan language speech recognition, accurate labeling of Tibetan speech utterances is extremely time consuming and requires trained linguists. For alleviate this problem, we present an approach that can use few labeled Tibetan speech utterances to construct the effective recognition model. The experimental results show that our approach has better performance than traditional methods based on semi-supervised learning and supervised learning.	semi-supervised learning;semiconductor industry;speech recognition;supervised learning	Xiuqin Pan;Yongcun Cao;Yong Lu;Yue Zhao	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.221	natural language processing;entropy;labeling theory;speech recognition;computer science;speech;machine learning;pattern recognition;accuracy and precision;active learning;supervised learning;computational model	Vision	-19.683879260504607	-88.0988525163436	104135
64bac723c3f0f802b2638d597fb763e15e2a1aa9	session 1: spoken language systems i		The papers in this session addressed issues in combining speech recognition with natural language systems. The first three papers concern the use of grammars. Speech recognizers and Natural Language parsers make different requirements of language knowledge. Recognizers need efficient methods for constraining the search space, while parsers need detailed analytical knowledge. One solution to the problem of integrating speech recognizers with NL processors is to use different language constraints in the two modules. This in effect means using different grammars for recognizing and parsing. The recognizer may use no grammar or simple, efficient grammars, while the parser uses a more complete representation of the language. This means that the recognizer can overgenerate, or produce strings not acceptable to the parser. In this case, a recognition error can lead to a failure to parse the utterance. One solution to this problem is to use an N-Best recognizer. Such a recognizer produces the N (where N is preset) best scoring hypotheses for an utterance. These hypotheses are passed to the parser which can then pick the overall best one.	central processing unit;finite-state machine;nl (complexity);natural language;parsing;requirement;speech recognition;speech synthesis	Wayne Ward	1990			natural language processing;natural language;parsing;utterance;spoken language;artificial intelligence;computer science;grammar;rule-based machine translation	NLP	-21.639152070424384	-85.30180841150485	104148
c7572cf5dd13b2dd820fc19d20e00a3f7942c814	impact of the approaches involved on word-graph derivation from the asr system	automatic speech recognition;word graphs;lattice	Finding the most likely sequence of symbols given a sequence of observations is a classical pattern recognition problem. This problem is frequently approached by means of the Viterbi algorithm, which aims at finding the most likely sequence of states within a trellis given a sequence of observations. Viterbi algorithm is widely used within the automatic speech recognition (ASR) framework to find the expected sequence of words given the acoustic utterance in spite of providing a suboptimal result. Word-graphs (WGs) are also frequently provided as the ASR output as a means of obtaining alternative hypotheses, hopefully more accurate than the one provided by the Viterbi algorithm. The trouble is that WGs can grow up in a very computationally inefficient manner. The aim of this work is to fully describe a specific method, computationally affordable, for getting a WG given the input utterance. The paper focuses specifically on the underlying approaches and their influence on both the spatial cost and the performance.	automated system recovery	Raquel Justo;Alicia Pérez;M. Inés Torres	2011		10.1007/978-3-642-21257-4_83	speech recognition;viterbi algorithm;computer science;machine learning;pattern recognition;lattice;mathematics	NLP	-21.13558732943946	-88.94336965110797	104287
bd5b102b8b6b73801ba5f82f8c8ed197d1000c1d	towards a proper treatment of coercion phenomena	proper treatment;coercion phenomenon;articulates syntax;lexical description;type o;commencer case;coercion interpretation result;coercion construction;lexical level;abstract predicate;hpsg formalism;empirical difficulty;semantic processing	The interpretation of coercion constructions (to begin a book) has been recently considered as resulting from the operation of type changing. For instance, a phrase of type o (object) is coerced to a phrase of type e (event) under the influence of the predicate. We show that this procedure encounters empirical difficulties. Focussing on the begin/commencer case, we show that the coercion interpretation results both from general semantic processes and properties of the predicate, and we argue that it is best represented at the lexical level. The solution is formulated in the HPSG formalism, where the lexical description of heads includes a specification of the argument and articulates syntax and semantics. We propose that the properties attached to the complement remain the same as they are oustside the construction, but that the semantics of the predicate is enriched to include an abstract predicate of which the complement is an argument.	formal system;head-driven phrase structure grammar	Danièle Godard;Jacques Jayez	1993			natural language processing;predicate;semantic memory;computer science;linguistics;predicate;programming language;algorithm	NLP	-33.056871251828056	-81.54582949366566	104422
c17068473debb0f867891274e4dc375d5da39b19	experiments with training corpora for statistical text-to-speech systems		Common text-to-speech (TTS) systems rely on training data for modelling human speech. The quality of this data can range from professional voice actors recording hand-curated sentences in high-quality studio conditions, to found voice data representing arbitrary domains. For years, the unit selection technology dominant in the field required many hours of data that was expensive and time-consuming to collect. With the advancement of statistical methods of waveform generation, there have been experiments with more noisy and often much larger datasets, testing the inherent flexibility of such systems. In this paper we examine the relationship between training data and speech synthesis quality. We then hypothesise that statistical text-to-speech benefits from high acoustic quality corpora with high level of prosodic variation, but that beyond the first few hours of training data we do not observe quality gains. We then describe how we engineered a training dataset containing optimized distribution of features, and how these features were defined. Lastly, we present results from a series of evaluation tests. These confirm our hypothesis and show how a carefully engineered training corpus of a smaller size yields the same speech quality as much larger datasets, particularly for voices that use WaveNet.	acoustic cryptanalysis;experiment;high-level programming language;lazy evaluation;speech synthesis;text corpus;waveform	Monika Podsiadlo;Victor Ungureanu	2018		10.21437/Interspeech.2018-2400	speech recognition;speech synthesis;computer science	NLP	-19.31957331992127	-84.13183600492229	104602
fad4414592210599b1606a89b23b2805c7ffae6f	the icsi+ multilingual sentence segmentation system	broadcast news;indexing terms;automatic speech recognizer;computational linguistics;maximum entropy;language model;machine translation	The ICSI+ multilingual sentence segmentation with results for English and Mandarin broadcast news automatic speech recognizer transcriptions represents a joint effort involving ICSI, SRI, and UT Dallas. Our approach is based on using hidden event language models for exploiting lexical information, and maximum entropy and boosting classifiers for exploiting lexical, as well as prosodic, speaker change and syntactic information. We demonstrate that the proposed methodology including pitchand energyrelated prosodic features performs significantly better than a baseline system that uses words and simple pause features only. Furthermore, the obtained improvements are consistent across both languages, and no language-specific adaptation of the methodology is necessary. The best results were achieved by combining hidden event language models with a boosting-based classifier that to our knowledge has not previously been applied for this task.	baseline (configuration management);boosting (machine learning);finite-state machine;language model;speech recognition;statistical classification;super robot monkey team hyperforce go!	M. Zimmerman;Dilek Z. Hakkani-Tür;James G. Fung;Nikki Mirghafori;L. Gottlieb;Elizabeth Shriberg;Yang Liu	2006			natural language processing;speech recognition;index term;computer science;principle of maximum entropy;computational linguistics;pattern recognition;linguistics;machine translation;language model	NLP	-19.751603448947723	-86.13788192363556	104762
5484d31fb1e9f725b3e3ecc8a1361a13eb522929	constructing a corpus that indicates patterns of modification between draft and final translations by human translators		In human translation, translators first make draft translations and then modify and edit them. In the case of experienced translators, this process involves the use of wide-ranging expert knowledge, which has mostly remained implicit so far. Describing the difference between draft and final translations, therefore, should contribute to making this knowledge explicit. If we could clarify the expert knowledge of translators, hopefully in a computationally tractable way, we would be able to contribute to the automatic notification of awkward translations to assist inexperienced translators, improving the quality of MT output, etc. Against this backdrop, we have started constructing a corpus that indicates patterns of modification between draft and final translations made by human translators. This paper reports on our progress to date.	backdrop cms;cobham's thesis;experience;text corpus	Takeshi Abekawa;Kyo Kageura	2008			natural language processing;artificial intelligence;computer science;linguistics	NLP	-27.624704783411406	-82.73672317152435	104797
bf6be715e317f4f0271081ae4d8f20d77aa7673c	enhancing foreign language tutors - in search of the golden speaker	language tutor;foreign language teaching;language learning;pronunciation correction;foreign language;computer assisted language learning	In the past, educators relied on classroom observation to determine the relevance of various pedagogical techniques. Automated language learning now allows us to examine pedagogical questions in a much more rigorous manner. We can use a computer-assisted language learning (CALL) system as a base, tracing all user responses and controlling the information given out. We have thus used the Fluency system [Proceedings of Speech Technology in Language and Learning, 1998, p. 77] to answer the question of what voice a language learner should imitate when working on pronunciation. In this article, we will examine whether there should be a choice of model speakers and what characteristics of a model's voice may be important to match when there is a choice.		Katharina Probst;Yan Ke;Maxine Eskénazi	2002	Speech Communication	10.1016/S0167-6393(01)00009-7	direct method;foreign language;language pedagogy;natural language processing;language identification;speech recognition;universal networking language;language assessment;second-language acquisition;computer science;language industry;language transfer;linguistics;natural language;language technology;second-language attrition;comprehension approach	NLP	-28.008733318638253	-82.7489506811702	104867
64db64f50c47d6548b11361f0220fd1ffa78fdda	a lattice-based approach to query-by-example spoken document retrieval	language modeling;spoken document retrieval;statistical model;lattice based spoken document retrieval;probabilistic retrieval approach;query by example;language model;retrieval of conversational telephone speech;conversational telephone speech	Recent efforts on the task of spoken document retrieval (SDR) have made use of speech lattices: speech lattices contain information about alternative speech transcription hypotheses other than the 1-best transcripts, and this information can improve retrieval accuracy by overcoming recognition errors present in the 1-best transcription. In this paper, we look at using lattices for the query-by-example spoken document retrieval task - retrieving documents from a speech corpus, where the queries are themselves in the form of complete spoken documents (query exemplars). We extend a previously proposed method for SDR with short queries to the query-by-example task. Specifically, we use a retrieval method based on statistical modeling: we compute expected word counts from document and query lattices, estimate statistical models from these counts, and compute relevance scores as divergences between these models. Experimental results on a speech corpus of conversational English show that the use of statistics from lattices for both documents and query exemplars results in better retrieval accuracy than using only 1-best transcripts for either documents, or queries, or both. In addition, we investigate the effect of stop word removal which further improves retrieval accuracy. To our knowledge, our work is the first to have used a lattice-based approach to query-by-example spoken document retrieval.	document retrieval;etsi satellite digital radio;lattice-based cryptography;query by example;relevance;speech corpus;statistical model;transcription (software)	Tee Kiah Chia;Khe Chai Sim;Haizhou Li;Hwee Tou Ng	2008		10.1145/1390334.1390397	natural language processing;document retrieval;statistical model;query expansion;visual word;speech recognition;computer science;query by example;information retrieval;statistics;language model	Web+IR	-22.683760371814742	-82.81836031699565	104943
94b9bb391fcc40d4e2ff447f97aacbf9e054e4ea	complement of incomplete task results for real-time crowdsourcing interpretation		On the crowdsourcing, there is a problem of unstable task results because of an unspecified number of people entrying the task. It is important for the quality improvement of task result to integrate incomplete task results. However, it is difficult to perform majority vote method for free-text-input style task. In addition, it couldn't be the quality improvement in the cases of many incomplete task results. In this report, we propose an integration method for real-time crowdsourcing interpretation. The present method complements incomplete sentences using co-occurrence words in the other task results. The evaluation results show that the proposed method improves subjective appraisals about sentence complement results.	complement (complexity);control theory;crowdsourcing;real-time clock;real-time locating system	Takeaki Shionome;Hirotaka Hashimoto;Jianwei Zhang;Yuhki Shiraishi;Daisuke Wakatsuki;Yohei Seki;Atsuyuki Morishima	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300617	pattern recognition;quality management;artificial intelligence;computer science;crowdsourcing;sentence	Robotics	-19.381066803229647	-82.26508171987247	106396
034d44207b592825975e08db08c56cc94f24e63d	covert communication by exploring statistical and linguistical distortion in text		Most state-of-the-art text steganography algorithms are designed based on synonym substitution with the concern of simplicity and robustness. However, synonym substitution will cause some detectable impact on cover texts. In this paper, we propose an content-adaptive text steganography to minimize the impact caused by embedding process. We believe that synonym substitution will cause a hybird distortion consists of statistical distortion and linguistical distortion. We design a double-layered STC embedding algorithm (HSL) to minimize the distortion. Experiments results indicate that the security performance of HSL is better compared with traditional methods based on synonym substitution.	distortion	Huanhuan Hu;Xin Zuo;Weiming Zhang;Nenghai Yu	2018		10.1007/978-3-030-00015-8_25	steganography;computer science;robustness (computer science);synonym;theoretical computer science;embedding;distortion;distributed computing;covert	HCI	-31.920775846288844	-88.40620751450878	106404
4997a3c646a7b54cd63fefb9c0adfea0529d1670	the frobenius anatomy of relative pronouns		This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics.	matrix multiplication;theory	Stephen Clark;Bob Coecke;Mehrnoosh Sadrzadeh	2013			natural language processing;mathematics;linguistics	NLP	-32.177677787712035	-82.27253403559449	106757
5c44a00ca323d7c91bc181e094a4ed9a0c43ef1c	end-to-end asr-free keyword search from speech		Conventional keyword search (KWS) systems for speech databases match the input text query to the set of word hypotheses generated by an automatic speech recognition (ASR) system from utterances in the database. Hence, such KWS systems attempt to solve the complex problem of ASR as a precursor. Training an ASR system itself is a time-consuming process requiring transcribed speech data. Our prior work presented an ASR-free end-to-end system that needed minimal supervision and trained significantly faster than an ASR-based KWS system. The ASR-free KWS system consisted of three subsystems. The first subsystem was a recurrent neural network based acoustic encoder that extracted a finite-dimensional embedding of the speech utterance. The second subsystem was a query encoder that produced an embedding of the input text query. The acoustic and query embeddings were input to a feedforward neural network that predicted whether the query occurred in the acoustic utterance or not. This paper extends our prior work in several ways. First, we significantly improve upon our previous ASR-free KWS results by nearly 20% relative through improvements to the acoustic encoder. Next, we show that it is possible to train the acoustic encoder on languages other than the language of interest with only a small drop in KWS performance. Finally, we attempt to predict the location of the detected keywords by training a location-sensitive KWS network.	acoustic cryptanalysis;artificial neural network;automated system recovery;database;encoder;end system;end-to-end encryption;feedforward neural network;recurrent neural network;search algorithm;speech recognition	Kartik Audhkhasi;Andrew Rosenberg;Abhinav Sethy;Bhuvana Ramabhadran;Brian Kingsbury	2017	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2017.2759726	natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-19.513209403186462	-86.73474948938525	106801
8a700807332a7b4b4a3aabbeff0639fbe87a64ff	an environment-compensated minimum classification error training approach based on stochastic vector mapping	front end;evaluation performance;interfase usuario;appareillage essai;robust speech recognition;digit recognition error rate minimum classification error training approach stochastic vector mapping environment compensated training approach robust automatic speech recognition hidden markov model hmm maximum likelihood criterion splice auroral connected digits database;base donnee;robust automatic speech recognition;performance evaluation;feature compensation;learning;maximum likelihood;user interface;taux erreur;hidden markov model;man machine dialogue;correction erreur;evaluacion prestacion;speech processing;modele markov variable cachee;maximum vraisemblance;database;tratamiento palabra;traitement parole;base dato;hmm;inmunidad ruido;probabilistic approach;maximum likelihood estimation;carta de datos;noise robustness;minimum classification error training mce;aprendizaje;automatic speech recognition;apprentissage;automatic recognition;digit recognition error rate;hidden markov models;noise immunity;reconocimiento voz;enfoque probabilista;approche probabiliste;error correction;aparato ensayo;mappage;auroral connected digits database;testing equipment;hidden markov model hmm;error rate;speech recognition;stochastic vector mapping;dialogo hombre maquina;interface utilisateur;environment compensated training approach;mapping;stochastic vector mapping feature compensation hidden markov model hmm minimum classification error training mce noise robustness robust speech recognition;maximum likelihood criterion;reconnaissance parole;stochastic processes hidden markov models testing automatic speech recognition error analysis noise robustness speech recognition network address translation computer science piecewise linear approximation;stochastic model;correccion error;minimum classification error;immunite bruit;splice;indice error;modelo estocastico;minimum classification error training approach;article;modele stochastique;maxima verosimilitud;reconocimiento automatico	A conventional feature compensation module for robust automatic speech recognition is usually designed separately from the training of hidden Markov model (HMM) parameters of the recognizer, albeit a maximum-likelihood (ML) criterion might be used in both designs. In this paper, we present an environment-compensated minimum classification error (MCE) training approach for the joint design of the feature compensation module and the recognizer itself. The feature compensation module is based on a stochastic vector mapping function whose parameters have to be learned from stereo data in a previous approach called SPLICE. In our proposed MCE joint design approach, by initializing the parameters with an approximate ML training procedure, the requirement of stereo data can be removed. By evaluating the proposed approach on Auroral connected digits database, a digit recognition error rate, averaged on all three test sets, of 5.66% is achieved for multicondition training. In comparison with the performance achieved by the baseline system using ETSI advanced front-end, our approach achieves an additional overall error rate reduction of 12.4%	approximation algorithm;baseline (configuration management);benchmark (computing);elegant degradation;finite-state machine;hidden markov model;linuxmce;markov chain;speech recognition;statistical classification;substring;vocabulary;word error rate	Jian Wu;Qiang Huo	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.872616	speech recognition;computer science;machine learning;pattern recognition;maximum likelihood;hidden markov model;statistics	Vision	-19.863457557133316	-91.25280640420618	106920
2d1adf74c7e04d8a8d60c09944b0dd5524a2e1b1	generating readable texts for readers with low basic skills		Most NLG systems generate texts for readers with good reading ability, but SkillSum adapts its output for readers with poor literacy. Evaluation with lowskilled readers confirms that SkillSum’s knowledge-based microplanning choices enhance readability. We also discuss future readability improvements.	human-readable medium;natural language generation	Sandra Williams;Ehud Reiter	2005			natural language processing;computer science;multimedia;pedagogy	AI	-28.481363941050372	-83.75336498223605	107161
d9034feb08bc4b2bcb0cf203b5fdb069b7fef738	automatic transcription of conversational telephone speech	desciframiento;modelizacion;vocal tract length normalization;front end;evaluation performance;interfase usuario;word error rate;model selection;speech coding natural languages decoding interpolation error statistics speech recognition hidden markov models;fiabilidad;interpolation;reliability;modelo reticular;prononciation;large vocabulary conversational speech recognition;performance evaluation;transcription automatique;decodage;intervalo confianza;learning;decoding;automatic system;user interface;taux erreur;man machine dialogue;rich transcription;minimum phone error;evaluacion prestacion;acoustic modeling;interpolacion;model adaptation;vocal tract;conversacion;confusion network;speech coding;tratamiento lenguaje;statistical significance;natural languages;telephone speech recognition;methode acoustique;selection modele;heteroscedastic linear discriminant analysis;telephony speech natural languages cepstral analysis linear discriminant analysis loudspeakers adaptation model decoding interpolation nist;transcripcion automatica;aprendizaje;discriminant analysis;analyse discriminante;modelisation;analisis discriminante;confidence interval;apprentissage;canal vocal;cepstral analysis;acoustic method;hidden markov models;sistema automatico;speaker adaptive training;anglais;seleccion modelo;conducto vocal;language processing;analyse cepstrale;pronunciation;fiabilite;intervalle confiance;speech recognition cambridge university htk hidden markov model automatic transcription conversational telephone speech front end processing acoustic modeling language pronunciation modeling cepstral normalization heteroscedastic linear discriminant analysis minimum phone error training speaker adaptive training lattice based model adaptation decoding score estimation interpolation word error rate;metodo acustico;traitement langage;conversation;systeme automatique;modele reticulaire;error rate;speech recognition;error statistics;dialogo hombre maquina;system development	This paper discusses the Cambridge University HTK (CU-HTK) system for the automatic transcription of conversational telephone speech. A detailed discussion of the most important techniques in front-end processing, acoustic modeling and model training, language and pronunciation modeling are presented. These include the use of conversation side based cepstral normalization, vocal tract length normalization, heteroscedastic linear discriminant analysis for feature projection, minimum phone error training and speaker adaptive training, lattice-based model adaptation, confusion network based decoding and confidence score estimation, pronunciation selection, language model interpolation, and class based language models. The transcription system developed for participation in the 2002 NIST Rich Transcription evaluations of English conversational telephone speech data is presented in detail. In this evaluation the CU-HTK system gave an overall word error rate of 23.9%, which was the best performance by a statistically significant margin. Further details on the derivation of faster systems with moderate performance degradation are discussed in the context of the 2002 CU-HTK 10 /spl times/ RT conversational speech transcription system.	acoustic cryptanalysis;acoustic model;cell (microprocessor);cepstrum;database normalization;discriminative model;elegant degradation;hp multi-programming executive;htk (software);interpolation;language model;lattice model (finance);linear discriminant analysis;medical transcription;speech recognition;tip (unix utility);tract (literature);transcription (software);word error rate	Thomas Hain;Philip C. Woodland;Gunnar Evermann;Mark J. F. Gales;Xunying Liu;Gareth L. Moore;Daniel Povey;Lan Wang	2005	IEEE Transactions on Speech and Audio Processing	10.1109/TSA.2005.852999	natural language processing;speech recognition;acoustics;interpolation;word error rate;computer science;hidden markov model;statistics;language model	NLP	-20.772670494005595	-90.51003924386725	107256
39079f7229ef72f448d348fbd998529967bd2130	speech recognition system of arabic digits based on a telephony arabic corpus	arabic;recognition;telephony corpus.;hmm;saavb;digits;arabic language;speech recognition;hidden markov model	Automatic recognition of spoken digits is one of the difficult tasks in the field of computer speech recognition. Spoken digits recognition process is required in many applications such as speech based telephone dialing, airline reservation, automatic directory to retrieve or send information, etc. These applications take numbers and alphabets as input. Arabic language is a Semitic language that differs from European languages such as English. One of these differences is how to pronounce the ten digits, zero through nine. In this research, spoken Arabic digits are investigated from the speech recognition problem point of view. The system is designed to recognize an isolated whole-word speech. The Hidden Markov Model Toolkit (HTK) is used to implement the isolated word recognizer with phoneme based HMM models. In the training and testing phase of this system, isolated digits data sets are taken from the telephony Arabic speech corpus, SAAVB. This standard corpus was developed by KACST and it is classified as a noisy speech database. A hidden Markov model based speech recognition system was designed and tested with automatic Arabic digits recognition. This recognition system achieved 93.72% overall correct rate of digit recognition.	directory service;finite-state machine;hidden markov model;markov chain;speech corpus;speech recognition	Yousef Ajami Alotaibi;Mansour Al-Ghamdi;Fahad Alotaiby	2008			natural language processing;arabic numerals;hidden markov model;semitic languages;speech corpus;numerical digit;speech recognition;telephony;directory;artificial intelligence;computer science;reservation	NLP	-22.052965236773026	-85.78578897266658	107368
133a406b35342b5766e853f8581f32b4cf17c86b	towards phone segmentation for concatenative speech synthesis		We present a new approach to solve the problem of phone segmentation when preparing databases for concatenative Text-to-Speech synthesis. First, we describe the problem and review the state of the art. Then we present some already existing techniques to perform this segmentation and present our approach based on a Regression Tree to perform Boundary Specific Correction of the HMM segmentation. We discus different evaluation procedures. Finally, we compare some systems and we show how our system improves the system based on HMMs setting 94% of the boundaries within a tolerance of 20ms compared to a manual segmentation, and how phonetic rather than acoustical features are better suited for this task.	discus;database;decision tree learning;hidden markov model;speech synthesis	Jordi Adell;Antonio Bonafonte	2004			phone;speech recognition;speech synthesis;segmentation;computer science	NLP	-20.25388237673675	-84.29565679821638	107626
b3687a31f67bd6ecd8dd99d8db172d80b3245e85	quality assessment for speaker diarization and its application in speaker characterization	hypothetical diarization quality assessment speaker diarization speaker characterization telephone environment;speaker recognition;telephone conversations speaker diarization confidence measures speaker characterization;reliability speech accuracy density estimation robust algorithm quality assessment nist materials	There are many applications related to speaker characterization, specially in telephone environments, where large datasets are available but not directly useful since there are two speakers involved in every recording. Even with very accurate speaker diarization systems, we can expect to find some recordings with low diarization accuracy. The use of these recordings may reduce the accuracy of any speaker characterization technology. Therefore, it is highly desirable to detect those recordings where the speakers are correctly segmented, in order to discard or process manually the remaining ones before feeding them into the application. In this work we propose a set of confidence measures to assess the quality of a hypothetical diarization output, in order to detect those recordings that are correctly segmented. We show that these confidence measures enable us to retrieve most of the desired recordings from a given dataset, discarding those recordings that degrade the overall accuracy of an application that make use of speaker characterization technologies.	cluster analysis;logistic regression;nist rbac model;speaker diarisation;speaker recognition;speech synthesis	Carlos Vaquero;Alfonso Ortega;Antonio Miguel;Eduardo Lleida	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2236317	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	Visualization	-19.860355121775026	-83.88773263797991	108334
2a0e0629dbca2592084955fd809ebe18ad1ce105	linguatronic: product-level speech system for mercedes-benz car		2. LINGUATRONIC In the S-Class car of 1996, Mercedes-Benz introduced the first generation of Linguatronic. Linguatronic is the brand name used in Europe of a speech dialogue system that allows completely hands-free operation of the car’s mobile phone, including number dialing (with connected digit dialog), number storing, userdefined telephone directory entry name, name dialing, and directory editing. Linguatronic I has a vocabulary of about 30 speaker-independent words (digits and control words). The second version has a vocabulary of about 300 words, and, in addition, allows for operation of comfort electronics (radio, CDplayer/changer, air condition etc). The system is now available for German, US English, UK English, Italian, Spanish, French and Swiss-German. Japanese and Dutch are currently under development. 3. ORGANIZATION The basic algorithms incorporated in Linguatronic are developed by the Speech Understanding group of DaimlerChrysler Research and Technology in Ulm, Germany. These algorithms then are taken up by the Speech Processing Division of DaimlerChrysler’s TEMIC business unit and put into products. These products are first marketed exclusively to the Mercedes-Benz premium brand of DaimlerChrysler, but in time, they are available to other brands and manufacturers as well. This reflects the Mercedes-Benz philosophy that safety-enhancing technology should be deployed by everybody. Thus, the speech technology from Temic is currently also available in BMW’s and Audi’s, with other car makers and suppliers to follow shortly with their top products..	algorithm;comand aps;dialog system;directory service;mercedes-euklid;mobile phone;speech processing;speech technology;vocabulary;voice command device	Paul Heisterkamp	2001				NLP	-23.782006789523898	-85.6143422152731	108634
48777b5b23add01c3ff1753be4d876cdce5f24dd	recognition of non-native german speech with multilingual recognizers		In this study we present di erent approaches to the recognition of non-natives. With a corpus in German spoken by speakers with 56 di erent rst languages, the Strange Corpus, we perform recognition experiments with monolingual and multilingual recognizers. Among other, we compared two German recognizers, one that was trained in addition with non-native (Italian) speech and the other trained with German speakers only. We found that best performance is achieved with the recognizer trained with German including non-native speech, followed by a bilingual recognizer and an Italian recognizer trained with German and Italian natives.	experiment;finite-state machine;text corpus	Ulla Uebler;Manuela Boros	1999			speech recognition;natural language processing;artificial intelligence;german;computer science	NLP	-21.08982822616079	-85.247270217655	108644
2c8a377ac10e12754808f92f7fe65ec82be71cb0	automatic language identification using acoustic sub-word units	language identification	We propose a parallel sub-word recognition system (PSWR) as an alternative to the parallel phone recognition (PPR) system conventionally reported for language identification (LID) task. The sub-word recognizer (SWR) used in the PSWR system can be obtained from training data without phonetic transcription in any of the languages in the task. It is based on automatic segmentation followed by segment clustering and segment HMM modeling. The SWR can replace the front-end phone recognizer (PR) in the PPR system as well as in the PRLM and P-PRLM systems which constitute two other well accepted frameworks in LID system design. This allows easy expansion of these systems to a large number of languages without requiring tedious manually labeled training speech data in any of the languages in the task. On a 6 language LID task, using the OGI-TS database, we show that the PSWR system performs comparably to the PPR system, thus providing an efficient automatic alternative.	acoustic cryptanalysis;cluster analysis;finite-state machine;hidden markov model;language identification;portland pattern repository;systems design;transcription (software)	A. K. V. Sai Jayram;V. Ramasubramanian;Thippur V. Sreenivas	2002			speech recognition;language identification;computer science	NLP	-21.201741692774707	-85.43065001623289	108721
7ecf2487250c3be1d946592b90908f629f9d391b	experiments in automatic meeting transcription using jrtk	natural language interfaces;automatic summarization;online front ends;language modelling automatic meeting transcription jrtk experiments automatic recognition conversational speech acoustic modelling optimal performance english recognizers performance human speech noisy speech lapel microphones background noise error rates callhome conversational database meeting browser recognition errors;error rate;speech recognition;user interfaces;natural language interfaces speech recognition user interfaces online front ends;humans background noise automatic speech recognition minutes testing speech enhancement working environment noise microphones error analysis databases	In this paper we describe our early exploration of automatic recognition of conversational speech in meetings for use in automatic summarizers and browsers to produce meeting minutes effectively and rapidly. To achieve optimal perfor mance we started from two different baseline English recognizers adapted to meeting conditions and tested resulting performance. The data were found to be highly disfluent (conversational human to human speech), noisy (due to lapel microphones and environment), and overlapped with background noise, resulting in error rates comparable so fa r to those on the CallHome conversational database (40-50% WER). A meeting browser is presented that allows the user to search and skim through highlights from a meeting efficiently despite the recognition errors.	baseline (configuration management);finite-state machine;microphone;smart common input method;specular highlight;transcription (software);word error rate	Hua Yu;C. Clark;Robert G. Malkin;Alexander H. Waibel	1998		10.1109/ICASSP.1998.675416	natural language processing;speech technology;speech recognition;word error rate;computer science;automatic summarization;speech processing;user interface;speech analytics	NLP	-21.40152561188731	-85.69317386516359	109134
9ce4ae8e37c72abe71db790062f091204093d693	topic estimation with domain extensibility for guiding user's out-of-grammar utterances in multi-domain spoken dialogue systems	out of grammar utterance;indexing terms;spoken dialogue system;semantic mapping;multi domain;single domain;multi domain spoken dialogue system;topic estimation	In a multi-domain spoken dialogue system, a user’s utterances are more prone to be out-of-grammar, because this kind of system deals with more tasks than a single-domain system. We defined a topic as a domain about which users want to find more information, and we developed a method of recovering out-ofgrammar utterances based on topic estimation, i.e., by providing a help message in the estimated domain. Moreover, the domain extensibility, that is, to facilitate adding new domains, should be inherently retained in multi-domain systems. We therefore collected documents from the Web as training data for topic estimation. Because the data contained not a few noises, we used Latent Semantic Mapping (LSM), which enables robust topic estimation by removing the effect of noise from the data. The experimental results based on using 272 utterances collected with a Woz-like method showed that our method increased the topic estimation accuracy by 23.1 points from the baseline.	baseline (configuration management);dialog system;extensibility;latent semantic mapping;off topic;semantic mapper;spoken dialog systems;world wide web	Satoshi Ikeda;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno	2007			natural language processing;single domain;speech recognition;index term;computer science	NLP	-21.432391419143514	-82.59872583913932	109509
dec6a6176f13223b4e054540639f400b074774b3	query-driven strategy for on-the-fly term spotting in spontaneous speech	signal image and speech processing;acoustics;mathematics in music;engineering acoustics;on the fly;spontaneous speech	Spoken utterance retrieval was largely studied in the last decades, with the purpose of indexing large audio databases or of detecting keywords in continuous speech streams. While the indexing of closed corpora can be performed via a batch process, on-line spotting systems have to synchronously detect the targeted spoken utterances. We propose a two-level architecture for on-the-fly term spotting. The first level performs a fast detection of the speech segments that probably contain the targeted utterance. The second level refines the detection on the selected segments, by using a speech recognizer based on a query-driven decoding algorithm. Experiments are conducted on both broadcast and spontaneous speech corpora. We investigate the impact of the spontaneity level on system performance. Results show that our method remains effective even if the recognition rates are significantly degraded by disfluencies.	acoustic cryptanalysis;algorithm;alpha–beta pruning;automatic system recovery;batch processing;broadcasting (networking);database;emergence;experiment;finite-state machine;n-gram;online and offline;performance;phonetic algorithm;real-time clock;sql;sensor;speech recognition;spontaneous order;text corpus	Mickael Rouvier;Georges Linarès;Benjamin Lecouteux	2010	EURASIP J. Audio, Speech and Music Processing	10.1155/2010/326578	voice activity detection;natural language processing;audio mining;speech recognition;acoustics;speech corpus;computer science;speech processing;physics;speech analytics	NLP	-21.20655435905654	-84.579306420385	109797
0e508c6b9a9df45691346981dbb481b40c934082	reinforcement learning for turn-taking management in incremental spoken dialogue systems		In this article, reinforcement learning is used to learn an optimal turn-taking strategy for vocal human-machine dialogue. The Orange Labs’ Majordomo dialogue system, which allows the users to have conversations within a smart home, has been upgraded to an incremental version. First, a user simulator is built in order to generate a dialogue corpus which thereafter is used to optimise the turn-taking strategy from delayed rewards with the Fitted-Q reinforcement learning algorithm. Real users test and evaluate the new learnt strategy, versus a non-incremental and a handcrafted incremental strategies. The data-driven strategy is shown to significantly improve the task completion ratio and to be preferred by the users according to subjective metrics.	algorithm;baseline (configuration management);dialog system;dialog tree;home automation;interaction;interrupt;logic programming;majordomo;online and offline;rl (complexity);reinforcement learning;simulation	Hatim Khouzaimi;Romain Laroche;Fabrice Lefèvre	2016			machine learning;artificial intelligence;home automation;simulation;turn-taking;reinforcement learning;computer science	NLP	-27.343829948015888	-86.86166746688127	110007
9d31b6f84fab22510d3e3358126df96d0e8d2832	probabilistic state machines: dialog management for inputs with uncertainty	state machine;model uncertainty;command objects;user interface management systems;change propagation	Traditional models of input work on the assumption that inputs delivered to a system are fairly certain to have occurred as they are reported. However, a number of new input modalities, such as pen-based inputs, hand and body gesture inputs, and voice input, do not share this property. Inputs under these techniques are normally acquired by a process of recognition. As a result, each of these techniques makes mistakes and provides inputs which are approximate or uncertain. This paper considers some preliminry techniques for dialog management in the presence of this uncertainty. These techniques—including a new input model and a set of extended state machine abstractions—will explicitly model uncertainty and handle it as a normal and expected part of the input process.	approximation algorithm;dialog manager;finite-state machine	Scott E. Hudson;Gary L. Newell	1992		10.1145/142621.142650	simulation;computer science;machine learning;data mining;finite-state machine	SE	-26.94577872365622	-86.50473228566182	110331
423de2543fabfac85bc7c21db51099938a91a725	the quality of multilingual automatic segmentation using german maus		 The goal of this work is to demonstrate the quality ofmultilingual automatic segmentations using the GermanMAUS system ([1, 5]) in order to substitute costly manuallysegmented data by automatically segmented corpora.In this study we investigated the inuence of languagespecic HMMs in a cross-language task namely the automaticsegmentations of English, French and Japanese withHMMs trained on German acoustic data. Given the orthographictranscription of an utterance we were able toproduce... 		Nicole Beringer;Florian Schiel	2000			speech recognition;computer vision;artificial intelligence;computer science;segmentation;german	NLP	-21.454267896767334	-84.18602809600951	110456
d8b835c6631a0d390c823dc72cc4bf1a09b0841e	improving students' pronunciation through accent reduction software	software;prononciation;laboratoire de langues;logiciel;enseignement des langues;language laboratory;research design;conventional instruction;technologie de l education;foreign countries;student;pronunciation;experimental groups;turkey;computer software;educational technology;linguistic input;etudiant;accent;language teaching;english second language;control groups;language laboratories;turquie	This study aimed to find out whether integrating accent reduction software in advanced English language classes at the university level would result in improvements in students’ pronunciation at the segmental and suprasegmental levels. The study made use of a quasi-experimental research design. Two classes at the Department of Foreign Language Education at Middle East Technical University in Turkey participated in the study. Whilst one class (the control group) followed traditional instruction, the other class (experimental group) followed instruction which integrated use of accent reduction software in a multimedia language laboratory. Based on the results of the study, it is suggested that especially in English as a Foreign Language (EFL) settings where natural target language input is scarce, technology has a lot to offer, and EFL learners may be provided with exposure and practice/ interaction opportunities in the target language through specifically designed software programs. Introduction In second or foreign language acquisition research, it has been observed that compared to receptive skills, productive skills are acquired later and through a more difficult process. Especially in speaking skills and pronunciation, learners almost never reach native-like mastery. In English as a Foreign Language (EFL) situations, this problem is experienced even more severely as learners usually do not have natural exposure to the target language out of the classroom. Learning target language pronunciation, however, is an indispensable part of mastering a foreign or second language (Celce-Murcia & Goodwin, 1991; Pennington, 1996). The 304 British Journal of Educational Technology Vol 36 No 2 2005 © British Educational Communications and Technology Agency, 2005. importance of pronunciation has been recognised better since the 1980s after communicative approaches to language teaching became influential in language teaching. Background of the study In linguistics, the sound system of English is studied under two headings: segmental phonology and suprasegmental phonology (Kreidler, 1989). Segmental aspect of the sound system includes individual vowels and consonants, and the suprasegmental aspect comprises word, phrase, and sentence stress, pitch contour or intonation, and rhythm. McDonough (1999, p. 265) lists the key features of the English sound system as follows: 1. Individual sounds: vowels and consonants where there are phonemic distinctions, such as between the English words bit and bet, or shop and chop, or meat and neat, for example. (This is in contrast to allophonic distinctions which make no difference to meaning, but where the pronunciation of an individual sound varies according to phonetic context.). 2. a. Diphthongs: vowels in combination, as in near or boy. b. Consonant clusters, as in school /sk/, train, or empty. 3. Linkage of sounds, an important phenomenon in English and a frequent source of difficulty for learner–listeners. For example, the phrase “Put it on” will not be heard as three separate words. 4. STRESS pattern in polysyllabic words, which themselves are related to word grammar (as in “responsible” and “responsibility,” for example). 5. Sentence stress and rhthym, and the related phenomenon of weak forms, whereby unstressed syllables are most frequently reduced to schwa / ∂ /. Regular stress in English tends to fall on nouns, adjectives, adverbs and main verbs (as in “I’ve been líving hére for over a yéar,” for instance). Contractions (as in won’t or would’ve) are also to be included here. 6. INTONATION, and the use of varying pitch to formulate meaning and intention. The traditional approach to teaching target language pronunciation was based on practising segmental phonology by focusing particularly on sounds which do not exist in the learner’s native tongue and using patterns of minimal pairs (Bronstein, 1960). In this approach learners were expected to practise the target sounds in isolation by imitating the model provided by the teacher or a cassette to reach the desired accuracy. Recent approaches to teaching pronunciation, however, emphasise suprasegmental aspects of pronunciation over segmental features. Since the communicative approach, which aims to help learners acquire communicative competence, became influential in language teaching, a “holistic” top–down approach to teaching pronunciation has been adopted instead of the “atomistic” bottom–up approach. With the communicative paradigm, it has been recognised that the goal of getting foreign/second language learners to have perfect pronunciation may be unrealistic and inappropriate (Jenkins, 1998). Instead, it has been suggested that the goal in teaching pronunciation should be “intelligibility” (Kenworthy, 1987) and “communicative Accent reduction software in language education 305 © British Educational Communications and Technology Agency, 2005. efficiency” (Harmer, 1993). As Harmer (1993, p. 22) states, “Our aim should be to make sure that students can always be understood to say what they want to say. They will need good pronunciation for this, though they may not need to have perfect accents.” Computer assisted language learning Although computer technology has been used for language learning and teaching since the 1960s, the use of computers for language teaching and learning has gained more importance only in the last decade or so. “A decade ago, the use of computers in the language classroom was of concern only to a small number of specialists. However, with the advent of multimedia computing and the Internet, the role of computers in language instruction has now become an important issue confronting large numbers of language teachers throughout the world” (Warschauer & Healey, 1998, p. 57). When we look at the developments in Computer Assisted Language Learning (CALL), we see that there have been changes in the various aspects of CALL since the 1970s including not only how language is viewed and the English teaching paradigm followed, but also the type of technology used, the type of activities provided, and the main objectives of teaching and learning. Warschauer identifies three phases of CALL (Warschauer, 2000; Warschauer & Healey, 1998), as outlined in Table 1. Warschauer and Healey (1998) accept that the three stages outlined in Table 1 “do not fall into neatly contained timelines” because previous stages can still be in practice whilst a new stage emerges. Therefore, they argue that all three coexist today and “current uses of computers in the language classroom correspond to all three of the paradigms” (Warschauer & Healey, 1998). Indeed, when we look at computer-assisted pronunciation packages in particular, we see that most of the available software programs display features of the “1970s–1980s: Structural CALL” stage. Therefore, almost all software packages to teach pronunciation focus on accuracy at the expense of fluency and extended discourse. Sefero lu (2003, p. 5) claims that “one of the main ğ Table 1: The three stages of Computer Assisted Language Learning (CALL) Stage 1970s–1980s: Structural CALL 1980s–1990s: Communicative CALL 21st century: Integrative CALL Technology Mainframe PCs Multimedia and Internet English teaching paradigm Grammar-translation and audio-lingual Communicate [sic] language teaching Content-based, ESP/EAP View of language Structural (a formal structural system) Cognitive (a mentally constructed system) Socio-cognitive (developed in social interaction) Principal use of computers Drill and practice Communicative exercises Authentic discourse Principal objective Accuracy [Accuracy] and fluency [Accuracy] and agency Note . ESP = English for Specific Purposes; EAP = English for Academic Purposes. From Warschauer, 2000. 306 British Journal of Educational Technology Vol 36 No 2 2005 © British Educational Communications and Technology Agency, 2005. limitations of many of the computer assisted pronunciation software packages is that they are limited to presenting and practicing of segmental aspects (ie, individual sounds) of the language rather than suprasegmental aspects and connected speech.” Moreover, many commercially available software packages have been criticised because they were not designed based on sound pedagogical guidelines (Pennington, 1999; Warschauer & Healey, 1998) or they failed to adopt a multidisciplinary approach that includes teachers, linguists, speech experts, etc in the design process (Cole et al , 1998; Price, 1998). There have been very few studies so far which tested the effectiveness of computerassisted pronunciation training. One study conducted with learners of Italian as a foreign language reported positive results on the effectiveness of using computerassisted language learning environment on the quality of pronunciation achieved by those attending a beginners’ course in Italian (Rostron & Kinsella, 1995). Another study, by Sullivan & Czigler (2002), reported that the student and lecturer reflections on a technology-supported introductory undergraduate phonetics course were positive. Yet, another study conducted by Zhang (1998) with learners of Chinese as a foreign language provided support for the application of computer technology in learning foreign language pronunciation. Zhang used a multimedia browser designed to develop listening comprehension skills through a self-study approach based on the exploration of authentic audio and/or video text, which also provided articulatory training by letting students record, play, and compare their pronunciation. She concludes that the results of her study demonstrates: how computer technology can be usefully and successfully incorporated into a teaching curriculum. Technology of this kind enables learners to take risks and follow their own path without the scrutiny of the teacher. It also allows the native speaker model readily available in proper contexts at any time. However, this is not to say that computer techno	amiga reflections;bus mastering;coexist (image);compact cassette;compiler;contraction mapping;design of experiments;enlightenment foundation libraries;experiment;holism;intelligibility (philosophy);internet;jenkins;lu decomposition;linkage (software);mainframe computer;molecular dynamics;pitch (music);programming paradigm;socio-cognitive;sullivan conjecture;timeline;while;word grammar	Gölge Seferoglu	2005	BJET	10.1111/j.1467-8535.2005.00459.x	psychology;natural language processing;educational technology;speech recognition;language assessment;computer science;language education;linguistics;world wide web;pedagogy	NLP	-28.593311858599826	-83.85375175569573	110636
cd6cfcd3425734596f199f1434f343d43c426cf1	several measures for selecting suitable speech corpora.		Wemake statistical investigations of various speech corpora to extract useful information re ecting the contents of the corpus so that we can create a sort of guidelines for selecting the most suitable corpus. A word is not separated by spaces in the Japanese text. Accordingly, we adopt n-gram counting methods to extract frequent mora sequences instead of words. A mora roughly corresponds to a syllable. By investigating the frequencies of 1 to 10-mora sequences in the existing six corpora, we can nd the distinction between the written and the spoken languages, keywords and topics of dialogues. This paper shows that the simple statistical investigation makes it possible to represent the contents of the corpus to some extent without conducting a complicated job such as morphological analysis.	n-gram;speech synthesis;syllable;text corpus	Shuichi Itahashi;Naoko Ueda;Mikio Yamamoto	1997			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-25.489334335745408	-80.48235937608813	110717
fbbbf61391d00d094078a1442ec920134885c111	virtual polysemy	knowledge representation;virtual polysemy;single meta-entry;lexical ambiguity;underspecified word entry;usage extensibility;language processing;contextual information;different use;feature structure formalism	"""We present an approach to lexical knowledge representation where different uses of the same word can be conflated into a single meta-entry which encodes regnlarities about sense/usage extensibility. This approach makes it possible to solve lexical anrbiguities by using contextual information (luring language processing to ground underspecified word entries, and can be efficiently implemented within a typed feature structure formalism. 1 I n t r o d u c t i o n One of the central aspects of lexical knowledge, perhaps the most significant in characterizing the creative aspect of language use, is our ability to generate appro~ priate uses of words in coutext. This ability is usually exercized by manipulating semantic and/or syntactic properties of words to achieve desirable collocational settings. Some illustrative examples are given in (1) where ® move can be interpreted as a psychological verb when used transitively with a sentient direct object, * enjoy can take either a noun or verb phrase complement when used in the expeT~ence sense (Pustejovsky, 1991, 1993; Briscoe, Copestake & Boguraev, 1990), , accord is synonymous with either agree or give/granl depending on its valency (Poznafiski & Sanfilippo, 1993), and * the occurrence of a directional argument with swim triggers a shift in aspectual interpretation. (1) a. Please move your car Her sadness moves him b. John enjoys the book John enjoys reading the book e. The two alibis do not accord They accorded him a warm welcome d. John swam for hours John swam across the channel Although the precise nrechanisms which govern lexical knowledge are still largely unknown, there is strong evidence that word sense extensibi[ity is not arbitrary (Atkins &: Levin, 1991; Pustejovsky, 1991, 1994; Ostler Atkius, 1991). [,'or example, the amenability of a *This work was carried out as part of the M'F project at SIIARP Laboratories of Europe. We would like to thank all members of the NLP groul), and in particular Iatl Johnson and Pete Whitelock, for helpful comments and advice. transitive verb such as move to yield either a movement or psychological interpretation ean be generalized to most predicates of caused motion (e.g. agitate, crash, cross, lift, slrike, sweep, unwind) with the causer c o l responding to the stimulus argument and the theme to the experieneer. Similarly, the option of either a noun or verb phrase complement for enjoy can be extended to many other psychological verbs with experiencer subjects (e.g. hale, like, lnvfeO, and verbs of undirected motion in English (e.g. carry, drive, float, push, run, swim, walk) can subcategorize for an expression of completed path so as to yield a telic/directed interpretation (Tahny, 1985; Sanfilippo el al., 1992; Sanfilippo, 11994). Moreover, the metonymical and metaphoric processes which are responsible for sense/usage extcnsious appear to be sul)ject to crosslinguistic variatiou. For example, the """"meat vs. animal"""" alternatkm that is found in English -viz. feed lh.e lamb vs. eal lamb is absent in Eskimo (Nunberg &. Zaencn, 1992) as well as in l)utch where nominal compomlding is used instead -. e.g. lain vs. lamvlees (Copestake & Saniilippo, 1993). Exanrples of this sort show that our ability to exteud word use in context is often systematic or conventiom alized. As Pustejovsky and Boguraev (1993) point out, traditional approaches to lexical representation assume that word use extensibility can be modeled by exhaustively describiug the meaning of a word through closed enumeration of its senses: each sense corresponds to a predefined context. This practice has largely characterized the compilation of dictionary entries in the texicographic tradition and has consequently iniluenced the shape of comlmtational lexicons since the large scale construction of such lexicons has typically involved semiautomatic knowledge acquisition from machine readable dictionaries (Carroll & Grover, 1989). Word sense enumeration provides highly specialized lexical entries, but • it fails to make explicit regularities about word sense cxtensibility which are necessary in promoting compactedness in lexical description, ® it is at odds with our ability to create new word uses in novel contexts, and • it generates massive lexic~d ambiguity. The use of lexical rules to generate different uses of a word fl'om a kernel entry (Copestake gg Briscoe, 199l; Sanfilippo, 1994) provides a 1)rincipled alternative to word sense enumeration and can be made to eater for uovel uses of words. Ilowever, it is not clear whether this practice can address the question of lexical ambiguity suc(:essfully as there is no known general control"""	a* search algorithm;carroll morgan (computer scientist);collocation;compiler;dictionary;extensibility;fido alliance;graph (discrete mathematics);human-readable medium;james pustejovsky;john collison;knowledge acquisition;knowledge representation and reasoning;lexicon;natural language processing;parsing;pete finnigan;protologism;sadness;semantics (computer science);sentience;serial experiments lain;word sense	Antonio Sanfilippo;Kerima Benkerimi;Dagmar Dwehus	1994			natural language processing;computer science;linguistics	NLP	-32.149505821890024	-82.34124917662577	110888
fbcdab5afc0553b85325fe18cd346bdbf0fbb74c	icdar 2009 handwriting recognition competition	databases;rimes database handwriting recognition competition;investments;text analysis handwriting recognition natural language processing;handwriting recognition;french written text documents;training;text analysis;training data;artificial neural networks;postal services;hidden markov models;large scale integration;handwriting recognition competition;feature extraction;handwriting recognition databases hidden markov models postal services text analysis dictionaries system testing training data production investments;dictionaries;production;system testing;rimes database;natural language processing;french written text documents handwriting recognition rimes database	This paper describes the handwriting recognition competitionheld at ICDAR 2009. This competition is based onthe RIMES-database, with French written text documents.These document are classified in three different categories,complete text pages, words, and isolated characters. Thisyear 10 systems were submitted for the handwritten recognitioncompetition on snippets of French words. The systemswere evaluated in three subtask depending of the sizes ofthe used dictionary. A comparison between different classificationand recognition systems show interesting results. Ashort description of the participating groups, their systems,and the results achieved are presented.	artificial neural network;dictionary;handwriting recognition;hidden markov model;international conference on document analysis and recognition;recurrent neural network	Emmanuèle Grosicki;Haikal El Abed	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.184	natural language processing;training set;speech recognition;feature extraction;computer science;machine learning;pattern recognition;handwriting recognition;system testing	Robotics	-22.209174382252105	-84.72620785179792	110939
c9f347143e43ff7273f32e7695cf566e023f4b4f	a finite state approach to german verb morphology	rewrite rule;tree structure;finite state transducer	"""This paper presents a new, language independent model for analysis and generation of word forms based on Finite State Transducers (FSTs). It has been completely implemented on a PC and successfully tested with lexicons and rules covering all of German verb morphology and the most interesting subsets of French and Spanish verbs as well. The linguistic databases consist of a'letter-tree structured lexicon with annc~ tated feature lists and a FST which is constructed from a set of morphophonological rules. These rewriting rules operate on complete words unlike other FST-based systems. 1 I n t r o d u c t i o n Until the beginning of this decade, morphological parsers usually were restricted to on e particular language; in fact we do not know of any one which was language independent or even applicable to a wide class of non-trivial inflected languages. In the meantime, the situation has changed a lot through the usage of Finite State Transducers (FSTs). Although the formalism of generative phonology seems to be powerful enough to cover almost any language, it is very difficult to implement it computationally. Recent approaches to compile the rules of generative phonology into finite au tomata offer solutions to both problems. In the following, we report on a successful and complete application of this technique to the morphology of German verbs. To demonstrate its generality, it has also been applied to a large subset of French in fact the most interesting cases and some Spanish verbs. 2 The Finite State Approach As Gazdar /1985 / [1] observed, only very few papers on the mathematical foundations of modern phonology exist. He quotes from J o h n s o n ' s / 1 9 7 0 / P h D thesis [2], the earliest s tudy of this kind, who states that """"any theory which allows phonological rules to simulate arbitrary rewriting systems is seriously defective, for it asserts next to nothing about the sorts of mappings the rules can perform"""" ( /Johnson 1970] [2], p. 42). According to Gazdar /1985/ ([1], p. 2) Johnson """"proves that a phonology that permits only simultaneous rule application, as opposed to iterative derivational application, is equivalent to an FST. And he then argues that most of the phonology current around 1970 could eitlmr be formalized or reanalyzed in terms of simultaneous rule application, and could thus be reduced to FSTs."""" At the Winter LSA meeting at New York in December 1981, R. Kaplan and M. Kay gave a talk a written account does nat exist in which they showed """"how the iteratively applied rules of standard generative phonology could, individually, be algorithmically compiled into FSTs"""" ( /Gazdar 1985/[1], p. 2) under the constraint that rules may not be reapplied to their own outputs. Such a finite ordered cascade of FSTs can be collapsed into a single FST whose behavior is equivalent to that of the original generative rules ( c f . /Kay 1983/ [4], p. 100-104). A FST is a special kind of finite automaton which operates simultaneously on an input and an output tape such that it inspects two symbols at a time. In Kay's approach, the FSTs carry two labels, each label referring to one of the two tapes. In general, a FST is said to accept a pair of tapes if the symbols on them match a sequence of transitions starting in an initial state and ending in one of the designated final states. If no such sequence can be found, the tapes are rejected. To allow tapes of different length to be accepted, a symbol to be matched against one or other of the tapes to do a transition may be empty, in which case the corresponding tape is ignored. There are two advantages with this approach: The first is, that such a combined FST can be implemented easily as a simple and very efficient program. Second, unlike ordered sets of rewriting rules, there is no directionality in principle, so that the same machine can be used for analysis and generation as well. Kimmo Koskenniemi /1983a, 1983b, 1984, 1985/] ([5], [6], [7], [8]) took up this approach and applied a variation of it to some heavily inflected languages, first of all to Finnish. His """"two-level"""" model proposes parallel rules instead of successive ones like those of generative phonology. The term """"two-level"""" is supposed to express that there are only two levels, the lexicai and the surface level, and that there are no intermediate ones, even logically. Besides its simplicity in particular with respect to implementation the problematic ordering of rules is avoided. 3 A Paral le l t t ewr i t ing Variant of FST s w i t h Feature Unification Although Koskenniemi's machinery works in parallel with respect to the rules, rewriting is still performed in a sequential manner: each word form is processed letter by letter (or morpheme by morpheme) such that all replacements are done one at a time. Certainly this model does not depend on the processing direction from left to right, but at any time during processing it focusses on only one symbol on the input tape. It is precisely this feature, where our approach, based on a suggestion by Kay, differs from Koskenniemi's. Our work grew out of discussions with M. Kay, to which the first author had the opportunity during a research stay at CSLI, Stanford, in summer 1985. Without his help oar investigations would not have been possible. In oul system, rewriting is performed over complete surface words, not letters or morphemes. There is no translation from lexical to surface strings, because there is only one level, the level of surface strings. Rewriting is defined by rules satisfying the scheme Pattern -+ Replacement where both, Pattern and Replacement, are strings that are allowed t o contain the wild card """"?"""" character which matches exactly one (and the same) letter. Let a,b, wl,w2 E ~* where ]E is an alphabet. For all wl, w2 the rule a --~ b, with Pat tern= a and Replacement= b, rewrites wlaw2 to wlbw2. It should be noted that only one occurrence of the Pattern is rewritten. Furthermore, it can be specified whether the search is to be conducted from left to right or vice versa. Hence, it is possible to perform rewriting in parallel in contrast to Koskenniemi's sequential mode. The rules are attached to the edges of a FST; hence the application order of the rules is determined by the sequence of admissible transitions. Conflicts arising from the fact that at a given state the patterns of several rules match are resolved by the strategy described in sec. 5."""	admissible heuristic;algorithm;automaton;compiler;database;finite-state machine;finite-state transducer;galaxy morphological classification;generative grammar;han unification;iterative method;kaplan–meier estimator;lexicon;mathematical morphology;network address translation;opensimulator;parsing;r language;rewriting;semantics (computer science);simulation;stanford university centers and institutes	Günther Görz;Dietrich Paulus	1988		10.3115/991635.991678	natural language processing;finite state transducer;computer science;linguistics;tree structure;algorithm	NLP	-30.404198475616067	-82.36580447065305	111066
ba087225de3d25899a9e13e66e0fba7f067ef1bc	maximum entropy direct models for speech recognition	bayesian framework;bayes estimation;desciframiento;modelizacion;evaluation performance;metodo estadistico;word error rate;maximum entropy methods;performance evaluation;metodo entropia maxima;modelo markov;decodage;generic model;decoding;taux erreur;hidden markov model;statistical independence;evaluacion prestacion;acoustic modeling;speech processing;modele markov variable cachee;tratamiento palabra;traitement parole;maximum entropy acoustic modeling;statistical method;methode acoustique;probabilistic approach;hidden markov models maximum entropy methods speech recognition;direct modeling;statistical model;modelisation;estimacion bayes;acoustic method;markov model;hidden markov models;reconocimiento voz;methode statistique;enfoque probabilista;approche probabiliste;metodo acustico;modele statistique;error rate;speech recognition;nongenerative modeling;modelo estadistico;reconnaissance parole;modele markov;methode entropie maximum;indice error;modeling;method of maximum entropy;entropy speech recognition hidden markov models decoding natural languages probability bayesian methods error analysis state space methods data mining;maximum entropy;language model;nongenerative modeling direct modeling maximum entropy acoustic modeling;estimation bayes;word error rate maximum entropy direct models speech recognition maximum entropy markov model stand alone acoustic models	Traditional statistical models for speech recognition have mostly been based on a Bayesian framework using generative models such as hidden Markov models (HMMs). This paper focuses on a new framework for speech recognition using maximum entropy direct modeling, where the probability of a state or word sequence given an observation sequence is computed directly from the model. In contrast to HMMs, features can be asynchronous and overlapping. This model therefore allows for the potential combination of many different types of features, which need not be statistically independent of each other. In this paper, a specific kind of direct model, the maximum entropy Markov model (MEMM), is studied. Even with conventional acoustic features, the approach already shows promising results for phone level decoding. The MEMM significantly outperforms traditional HMMs in word error rate when used as stand-alone acoustic models. Preliminary results combining the MEMM scores with HMM and language model scores show modest improvements over the best HMM speech recognizer.	acoustic cryptanalysis;acoustic model;asynchronous i/o;explicit modeling;finite-state machine;generative model;hidden markov model;language model;markov chain;maximum-entropy markov model;parse tree;parsing;principle of maximum entropy;simple features;speech recognition;statistical model;word error rate	Hong-Kwang Jeff Kuo;Yuqing Gao	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.858064	maximum-entropy markov model;speech recognition;word error rate;computer science;pattern recognition;mathematics;hidden markov model;statistics	ML	-20.82599004090678	-90.67992063404962	111271
7b404e80ba91970f111b7f4e426600b1c3dc9374	a syllable-based turkish speech recognition system by using time delay neural networks (tdnns)	neural networks;speech recognition neural nets;training;speech;accuracy;syllable based speech recognition artificial intelligence machine learning artificial neural networks time delay neural networks speech recognition;vectors;feature extraction;speech recognition;speech recognition speech feature extraction training neural networks accuracy vectors;tdnn syllable based turkish speech recognition system time delay neural networks speech recognition units ortographically transparent language syllabified language automatic syllable boundary detection module	In this paper, we present a model for Turkish speech recognition. The model is syllable-based, where the recognition is performed through syllables as speech recognition units. The main goal of the model is to recognize as much as possible of a given continuous speech by identifying only a small set of syllables in the language. For that purpose, only the syllable types with a higher frequency are selected for the recognition. The use of longer recognition units in speech recognition systems increases the success of the recognition since it is easier to detect the endpoints of syllables when compared to phonemes. On the other side, word-based recognition requires a very large dataset that includes all the words and word forms in the language, which is also another challenge. Hereby, we take the advantage of Turkish being an ortographically transparent and syllabified language. Our model employs time delay neural networks (TDNNs) for learning syllables. We achieve an accuracy of %65.6 on our large vocabulary continuous speech corpus. In addition, we define an algorithm for the automatic detection of syllable boundaries which gives an accuracy of %44. The automatic syllable boundary detection module is used for the recognition of isolated syllables rather than a continuous speech.	acoustic cryptanalysis;acoustic model;algorithm;artificial neural network;broadcast delay;experiment;language model;pattern recognition;soft computing;speech corpus;speech recognition;syllable;test set;time delay neural network;vocabulary	Burcu Can;Harun Artuner	2013	2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2013.7054130	voice activity detection;natural language processing;speaker recognition;audio mining;speech recognition;feature;feature extraction;intelligent character recognition;computer science;speech;machine learning;speech processing;time delay neural network;acoustic model;accuracy and precision;logogen model;artificial neural network;language model	NLP	-19.24186616923278	-86.66113421401268	111272
29e0dc857b64ac83643496be22d1f8a312768124	fully lexicalized head-driven syntactic generation.		We describe a new approach to syntactic generation with Head-Driven Phrase Structure Grammars (HPSG) that uses an extensive off–line preprocessing step. Direct generation algorithms apply the phrase-structure rules (schemata) of the grammar on–line which is an computationally expensive step. Instead, we collect off-line for every lexical type of the HPSG grammar all minimally complete projections (called elementary trees) that can be derived with the schemata. This process is known as ‘compiling HPSG to TAG’ and derives a Lexicalized Tree-Adjoining Grammar (LTAG). The representation as an LTAG is ‘fully lexicalized’ in the sense that all grammatical information is directly encoded with the lexical item (as a set of elementary trees) and the combination operations are reduced from schema applications to the TAG primitives of adjunction and substitution. Given this LTAG, the generation task has a very different search space that can be traversed very efficiently, avoiding the costly on–line applications of HPSG unification. The entire generation task from a semantic representation to a surface string is split into two tasks, a microplanner and a syntactic realizer. This paper discusses the syntactic generator and the preprocessing steps as implemented in the Verbmobil system. 1 Generation in a Speech–to–Speech System The syntactic generation algorithm and the preprocessing steps presented in this paper are integrated into the Verbmobil system (see [Wahlster 1993, Bub, Wahlster, and Waibel 1997]). It is a system for speech–to–speech dialog translation. The input for the generation module VM–GECO is generated by a semantic–based transfer component (see [Dorna and Emele 1996]). The interface language chosen comprises the encoding of target language–specific semantic information in a combination of Underspecified Discourse Representation Theory and Minimal Recursion Semantics (see [Bos et al. 1996] and [Copestake, Flickinger, and Sag 1997]). The internal architecture of the generation module is modularized: it is separated into two phases, a microplanner and a syntactic generator. Throughout the system, we emphasize declarativity, which is also a necessary precondition for a comprehensive off–line preprocessing of external knowledge bases–in particular the preprocessing of the underlying Head–Driven Phrase Structure Grammar (HPSG, see [Pollard and Sag 1994]) which has been developed at CSLI, reflecting the latest developments in the linguistic theory and with a fairly wide coverage and also covering phenomena of spoken language. 1 VerbMobil GEneration COmponents 2 Microplanning and Syntactic Generation Starting from the semantic representation, the microplanning component generates an annotated dependency structure which is used by the syntactic generation component to realize a surface string. The microplanner also carries out word–choice. One goal of this modularization is a stepwise constraining of the search–space of alternative linguistic realizations, using different views in the different modules. In each step, only an abstraction of the multitude of information contained in an alternative needs to be considered. Another aspect of this architecture is the separation into a kernel system, i.e., the language independent core algorithms (a constraint-solver for microplanning and the search and combination algorithms for syntactic generation described in section 5) and declarative knowledge bases, e.g., the language specific word-choice constraints in microplanning and the TAG grammars used in syntactic realization. This separation allows for an easy adaptation of the system to other languages and domains (see [Becker et al. 1998]). 3 Declarativity in the Syntactic Generator All modules of the generator utilize external, declarative knowledge bases. For the syntactic generator, extensive off-line preprocessing of the highly declarative HPSG grammar for English is applied. The grammar has not even been written exclusively as a generation grammar. It is specialized, however, in that it covers phenomena of spoken language. The high level of abstraction which is achieved in the hierarchically organized grammar description (see [Flickinger 1987]) allows for easy maintenance as well as off-line preprocessing. The off-line preprocessing steps described in the next section keep the declarative nature of the grammar intact, i.e. they retain explicitly the phrase structures and syntactic features as defined by the HPSG grammar. In general, declarative knowledge bases allow for an easier adaptation of the system to other domains and languages. This is a huge benefit in the current second phase of the Verbmobil project [Becker et al. 1996] where the generator is extended to cover German, English and Japanese as well as additional and extended domains with a considerably larger vocabulary. 4 Off–Line Preprocessing: HPSG to TAG Compilation The subtasks in a direct syntactic generator based on an HPSG grammar will always include the application of schemata (the HPSG equivalent of phrase structure rules) such that all syntactic constraints introduced by a lexical item (especially its SUBCAT list) are fulfilled. This results in a constant repetition of, e.g., building up the projection of a verb in a declarative sentence. In preprocessing the HPSG grammar we aim at computing all possible partial phrase structures which can be derived from the information in a lexicon entry. Given such sets of possible syntactic realization together with a set of selected lexicon entries for an utterance and finally their dependencies, the task of a syntactic generator is simplified considerably. Instead of exploring all The HPSG grammar is being developed at CSLI, Stanford University. Development is carried out on a grammar development platform which is based on TDL [Krieger and Schäfer 1994]. In fact, most of the testing during grammar development depends on the use of a parser. possible, computationally expensive applications of HPSG schemata, it merely has to find suitable precomputed syntactic structures for each lexical item and combine them appropriately. For this preprocessing of the HPSG grammar, we adapted the ‘HPSG to TAG compilation’ process described in [Kasper et al. 1995]. The basis for the compilation is an identification of syntactically relevant selector features which express subcategorization requirements of a lexical item, e.g. the VALENCE features. In general, a phrase structure is complete when these selector features are empty. Starting from the feature structure for a lexical item, HPSG schemata are applied such that the current structure is unified with a daughter feature of the schema. The resulting structure is again subject to this process. This compilation process stops when certain termination criteria are met, e.g., when all selector features are empty. Thus, all projections from the lexical item are collected as a set of minimally complete phrase structures which can also be interpreted as elementary trees of a Tree–Adjoining Grammar (TAG). Instead of actually applying this compilation process to all lexical items, certain abstractions over the lexical entries are specified in the HPSG grammar. In fact, the needs of the compilation process have led to a clear–cut separation of lexical types and lexical entries as shown in Figure 1. A typical lexical entry is shown in Figure 2 and demonstrates that only three kinds of information are stored: the lexical type MV NP TRANS LE, the semantic contribution (the relation SUIT REL) and morphological information (the stem and potentially irregular forms). By expanding the lexical type, the full feature structure can be obtained. Schemata Phrase Structure Lexicon Hierarchy Lexical Instance Morphological Information Lexical Type Types Semantic Syntactic Types HPSG Principles	algorithm;analysis of algorithms;ca-realizer;compiler;emoticon;han unification;head-driven phrase structure grammar;minimal recursion semantics;online and offline;precondition;preprocessor;stanford university centers and institutes;tree-adjoining grammar;unification (computer science);verbmobil;dialog	Tilman Becker	1998			natural language processing;generative grammar;artificial intelligence;head-driven phrase structure grammar;relational grammar;computer science;categorial grammar;phrase structure rules;mildly context-sensitive grammar formalism;emergent grammar;generalized phrase structure grammar	NLP	-30.42918845458462	-81.85298143346694	112008
a069aedf6810fd58ec384c78a713c4722ce8799a	concept-to-speech generation with knowledge sharing for acoustic modelling and utterance filtering	speech synthesis;hidden markov model;natural language generation;concept to speech	HighlightsWe present two knowledge sharing approaches for CTS generation.Syntactic features replace prosody phrasing features in HMM-based speech synthesis.Acoustic features are used to filter synthetic utterances for one input concept.The HMM-based acoustic model yields comparable results without prosodic phrasing.Utterance filtering can remove inferior synthetic utterances for the input concept. A Concept-to-Speech (CTS) system converts the conceptual representation of a sentence-to-be-spoken into speech. While some CTS systems consist of independently built text generation and Text-to-Speech (TTS) modules, the majority of the existing CTS systems enhance the connection between these two modules with a prosodic prediction module that utilizes linguistic knowledge from the text generator to predict prosodic features for TTS generation. However, knowledge embodied within the individual modules has the potential to be shared in more ways. This paper describes knowledge sharing for acoustic modelling and utterance filtering in a Mandarin CTS system. First, syntactic information generated by the text generator is propagated to a hidden Markov model (HMM) based acoustic model within the TTS module and replaces the symbolic prosodic phrasing features therein. Our experimental results show that this approach alleviates the local hard-decision problem in automatic prosodic phrasing for Mandarin CTS systems and achieves a comparable performance to the traditional approach without explicit prosodic phrasing. Second, the acoustic features of multiple synthetic utterances expressing the same input concept are utilized to evaluate the utterance candidates. With this 'post-processing' mechanism, our CTS system is able to filter out inferior synthetic utterances and find an acceptable candidate to express the input concept.	acoustic cryptanalysis	Xin Wang;Zhen-Hua Ling;Li-Rong Dai	2016	Computer Speech & Language	10.1016/j.csl.2015.12.003	natural language processing;speech recognition;computer science;speech synthesis;hidden markov model	NLP	-19.20644212717859	-84.00696547713835	112486
d909997fe133198646ab5820136c712448ab7d50	is natural language an inconvenience or an opportunity for ir?	language technology;knowledge management;websom;data mining;natural language;visual search;self organizing map;document classification;document mapping	"""Natural language (NL) has evolved to facilitate human communication. It enables the speaker to make the listener's mind wander among her experiences and mental associations roughly according to the intentions of the speaker. The speaker and the listener usually share experiences and expectations, and they use mostly the same units and rules of a shared NL. Written language functions similarly, but in a less interactive way, with fewer possibilities for feedback.Both the symbols of NL (i.e. words or morphemes), and their arrangements are meaningful. Not with universal and precise meanings, but similar enough among different speakers and accurate enough for the communication mostly to succeed.NLs are mostly very large systems. Hundreds of thousands of words and infinitely many possible utterances. Even inflection alone might produce huge numbers of forms, e.g. more than ten thousand distinct forms out of every Finnish verb entry.NL processing (for IR or any other purpose) must cope with phenomena like (1) inflection and compounding, (2) synonymy, (3) polysemy, (4) ambiguity, (5) anaphora and (6) head-modifier relations among words and phrases.Language technology can neutralize much of the effect of these 'inconveniences' inherent with NL, but what kinds of advantages could NL have? Redundant use of synonymous expressions can effectively identify new concepts. Multilingual parallel documents may help in identifying their exact content. NLs typically carry connotations, i.e. what is implied but not explicitly said (e.g. attitudes, politeness). Vague associations are easy to express in NL, but not always in formal systems (e.g. """"a few years ago there was an article about the rival of Yeltsin - I don't remember his name but - he then went over to some region in Siberia - but what did the guy promise?"""") Jokes and humor belong to NLs, not to formal systems.  .Are there any alternatives for NL? Not really, because any artificial and more precise formalisms fail to adapt to new concepts and they do not easily allow restructuring of previous ideas.One challenge for language technology is to find better solutions for the above 'inconveniences' in order to provide various IR, document classification, indexing and summarizing methods with more accurate and adequate input data. With more accurate input some of the more demanding tasks of IR can perhaps be solved."""	anaphora (linguistics);document classification;experience;feedback;formal system;image processing;language technology;modifier key;nl (complexity);natural language;vagueness	Kimmo Koskenniemi	2002		10.1145/564376.564378	natural language processing;speech recognition;self-organizing map;visual search;computer science;machine learning;natural language;language technology;world wide web;information retrieval	NLP	-32.47778241541004	-84.9170929664744	112529
964056f89065da6a2b9250e7c84ce314f9104020	rules and generalization capacity extraction from ann with gp	genetic program;activation function;algoritmo genetico;algorithme genetique;rule discovery;genetic algorithm;reseau neuronal;red neuronal;artificial neural network;neural network	"""In language engineering, language models are employed in order to improve system performance. These language models are usually N-gram models which are estimated from large text databases using the occurrence frequencies of these N-grams. An alternative to conventional frequency-based estimation of N-gram probabilities consists in using neural networks to this end. These """"connectionist N-gram models"""", although their training is very time-consuming, present a pair of interesting advantages over the conventional approach: networks provide an implicit smoothing in their estimations and the number of free parameters does not grow exponentially with N.#R##N##R##N#Some experimental works provide empirical evidence on the capability of multilayer perceptrons and simple recurrent networks to emulate N-gram models, and proposes new directions for extending neural networks-based language models."""		Juan R. Rabuñal;Julian Dorado;Alejandro Pazos;Daniel Rivero	2003		10.1007/3-540-44868-3_77	genetic algorithm;computer science;artificial intelligence;machine learning;activation function;artificial neural network;algorithm	NLP	-19.493389088581953	-87.68322933452859	112694
3d83b0f680cb92130b55959f367c9cefcfaba9c9	an ir-inspired approach to recovering named entity tags in broadcast news		We propose a new approach to improving named entity recognition (NER) in broadcast news speech data. The approach proceeds in two key steps: (1) we automatically detect document alignments between highly similar speech documents and corresponding written news stories that are easily obtainable from the Web; (2) we employ term expansion techniques commonly used in information retrieval to recover named entities that were initially missed by the speech transcriber. We show that our method is able to find named entities missing in the transcribed speech data, and additionally to correct incorrectly assigned named entity tags. Consequently, our novel approach improves state-of-the-art NER results from speech data both in terms of recall and precision.	domain adaptation;information retrieval;medical transcription;ner model;named entity;precision and recall;transcriber;transcription (software);world wide web	Niraj Shrestha;Ivan Vulic;Marie-Francine Moens	2013		10.1007/978-3-642-41057-4_6	natural language processing;speech recognition;computer science;information retrieval	NLP	-24.032174197688203	-80.62018703820297	113048
780ff5f0fd7ac34b32d5e05c86f0e789cfdae2c9	measuring semantic complexity	prepositional phrase;natural language interface;natural language processing	We define semantic complexity using a new concept of meaning automata. We measure the semantic complexity of understanding of prepositional phrases, of an ”in depth understanding system”, and of a natural language interface to an on-line calendar. We argue that it is possible to measure some semantic complexities of natural language processing systems before building them, and that systems that exhibit relatively complex behavior can be built from semantically simple components.	automata theory;natural language processing;natural language user interface;online and offline	Wlodek Zadrozny	1995	CoRR		natural language processing;semantic interpretation;semantic similarity;semantic computing;natural language user interface;computer science;semantic compression;linguistics	NLP	-32.69021897769939	-80.47723035718018	113283
284871902e65304f0c1ce1628272801579d8a52a	generating adjectives to express the speaker's argumentative intent	information technology;computer science;knowledge base	being modified. In addition, these decisions interact with the lexical properties of adjectives, the syntax of the clause We address the problem of generating adjectives in a text and other factors like collocations. In this paper we theregeneration system. We distinguish between usages of adfore address the following two questions: What should be jectives informing the hearer of a property of an object and the input to a generator capable of producing argumenusages expressing an intention of the speaker, or an artative usages of adjectives? And how should the generator gumentative orientation. For such argumentative usages, combine the many interacting factors constraining the we claim that a generator cannot simply map from inforselection of an adjective? mation in the knowledge base to adjectives. Instead, we After reviewing previous work related to these quesidentify various knowledge sources necessary to decide tions, we present the linguistic data upon which we base whether to use an adjective, what adjective should be our approach and the conclusions we draw from its selected and what syntactic function it should have. We analysis. We then present and justify the input we require show how these decisions interact with lexical properties of to properly select adjectives and discuss how adjective adjectives and the syntax of the clause. We propose a selection is constrained by the lexical properties of adjecmechanism for adjective selection and illustrate it in the tives and interacts with other surface decisions. The paper context of the explanation component of the ADVISOR expert illustrates the key features of our implementation of adjecsystem. We describe an implementation of adjective selective selection in the context of the ADVISOR explanation tion using a version of Functional Unification Grammars. component. Introduction Previous Work Traditionally, an adjective is defined as ‘‘serving as a In previous work in generation, adjectives have been modifier of a noun to denote a quality of the thing named, studied as a tool for producing descriptions of objects. It is to indicate its quantity or extent, or to specify a thing as important to distinguish usages of descriptive noun-phrases distinct from something else’’ (Webster, 1963). Analysis to either refer to objects or to attribute a property to objects of human conversations however shows that adjectives of(Donnellan, 1966, Kronfeld, 1981, Searle, 1979). In a ten loosely relate to actual properties of the objects being referential usage, a noun-phrase is used when the speaker modified but are used to express a speaker’s intention or wants the hearer to identify some object. In this case, argumentative orientation. The work we present here is adjectives are used to contrast the target object from other developed in the context of the explanation component of potential referents. The proper adjectives are chosen based the ADVISOR expert system (McKeown et al, 1985, on their discriminatory power. For example, in a backMcKeown, 1988), a question answering system advising ground containing blocks of different forms and colors, the university students which courses to select. In this context, generator will pick a combination of form and color that when an academic advisor tells a student that a course is can be used to uniquely identify the referent and differenvery hard, he often does not refer to a property of the tiate it from all other blocks in the background. Different course, but rather expresses his evaluation of the course. mechanisms for such a selection are presented in (Dale, This creates problems for text generation. The first 1988, pp.249-262) for the EPICURE system, (Appelt, problem we face is that the information needed to choose 1985) for the KAMP system and in (Reiter, 1990). whether to use an adjective playing an argumentative role In attributive usages, the goal of the speaker is to inform cannot be found directly in a knowledge-base describing the hearer of some property of an object. In (McKeown, objects of the domain. Instead, the decisions must be 1985) and (Appelt, 1985) for example, adjectives are used based on the speaker’s goals, a hearer model and the object to perform inform speech-acts. In this case, the generator 1Reprinted from Proceedings of the 9th National Conference on Artificial Intelligence (AAAI 91), 1991, Anaheim CA, pp98-104 simply maps from the information in the knowledge-base (McKeown & Elhadad, 1991, Elhadad & McKeown, describing the object to an adjective denoting the property 1990). We identified a set of pragmatic features necessary being attributed. Note that in KAMP (Appelt, 1985), the to distinguish between these connectives, including arnotion of action subsumption was introduced to account for gumentative features. In this paper, we refine this work cases where a particular noun-phrase simultaneously and identify features to adequately select a certain class of served as a referring and attributive expression and the adjectives. adjective was selected both because of its contrastive value and of its informative value. The Problem: Data and Motivation Other works have studied usages that are neither attribuOriginally, our task was to extend the linguistic coverage tive nor referential in the sense discussed above. With of the generator for the explanation component of the PAULINE, Hovy (Hovy, 1988) discussed the use of adjecADVISOR expert system to select adjectives based on tives to satisfy pragmatic constraints. For example, the general principles. ADVISOR is a system that assists univergenerator could produce a sentence like poor John was sity students select courses and plan their semester severely beaten by the police where poor does not denote (McKeown, 1988). any information about John but rather expresses the orienWe performed an analysis of a corpus of 40,000 words tation of the speaker. Hovy covered many different lincontaining transcripts of recordings of advising sessions guistic devices satisfying pragmatic constraints and as a with human academic advisors. In this corpus, we idenresult provides only a very superficial treatment of adjectified approximately 700 occurrences of 150 distinct adjective selection (he devotes a single paragraph to its discustives. We focused our analysis on all occurrences of adjecsion). tives modifying a course, in both predicative and attribuIn (Bruxelles et al, 1989) and (Bruxelles & Raccah, tive positions. We found 69 such occurrences, of 26 dis1991), a model for describing the argumentative potential tinct adjectives. Figure 1 shows a break down of these of lexical items is introduced. This model aims at explainoccurrences in semantic classes. ing how adjectives like courageous express both a property of the modified object and an argumentative orientation of Of the 69 occurrences listed in Figure 1, 58 express a the speaker (a favorable evaluation of the object), whereas property of a course that one cannot reasonably expect to adjectives like intrepid or bold while conveying roughly find in the knowledge-base describing courses. For exthe same information also convey a different orientation. ample, it is problematic to describe a course as good or The reported work is still at early stages and is oriented hard in absolute terms. For most of the occurrences theretowards interpretation. We use here many concepts fore, the technique of mapping from a semantic property in derived from this work and examine its implications on the knowledge-base to an adjective, as used in previous generation. generation systems to produce attributive noun-phrases, In earlier work, we have studied the problem of generatwould not be applicable. Most of the usages of adjectives ing certain connectives like but, although, because or since in the corpus correspond to an argumentative usage. For Semantic class Adjective Occurrences Semantic class Adjective Occurrences Difficulty [24] advanced 1 Importance [24] important 10	artificial intelligence;collocation;color;expert system;han unification;impredicativity;information;interaction;knowledge base;logical connective;modifier key;natural language generation;nick mckeown;question answering;subsumption architecture;text corpus;the superficial;word lists by frequency	Michael Elhadad	1991			natural language processing;knowledge base;computer science;artificial intelligence;information technology	AI	-31.06925550754365	-83.14288166179115	113381
289c92b685c50f0bfc34c040bfc8a2727ef07823	datr: a language for lexical knowledge representation	morphologie;representation des connaissances;lexicon;artificial intelligent;morphology;natural language;q100 linguistics;g400 computing;computational linguistics;analyzer;knowledge representation;linguistique informatique;formalisme;analyseur;representation lexicale;lexique	Much recent research on the design of natural language lexicons has made use of nonmonotonic inheritance networks as originally developed for general knowledge representation purposes in Artificial Intelligence. DATR is a simple, spartan language for defining nonmonotonic inheritance networks with path~value equations, one that has been designed specifically for lexical knowledge representation. In keeping with its intendedly minimalist character, it lacks many of the constructs embodied either in general-purpose knowledge representation languages or in contemporary grammar formalisms. The present paper shows that the language is nonetheless sufficiently expressive to represent concisely the structure of lexical information at a variety of levels of linguistic analysis. The paper provides an informal example-based introduction to DATR and to techniques for its use, including finite-state transduction, the encoding of DA Gs and lexical rules, and the representation of ambiguity and alternation. Sample analyses of phenomena such as inflectional syncretism and verbal subcategorization are given that show how the language can be used to squeeze out redundancy from lexical descriptions.	alternation (formal language theory);artificial intelligence;datr;general-purpose markup language;knowledge representation and reasoning;lexicon;natural language;spartan;transduction (machine learning)	Roger Evans;Gerald Gazdar	1996	Computational Linguistics		natural language processing;spectrum analyzer;morphology;computer science;computational linguistics;linguistics;natural language;lexical functional grammar;lexical grammar	AI	-32.660876063228656	-80.52623511966259	113445
6fa83f9db43b2d3f433358429e4cf9fdbdc9d657	reducing redundant computation in hmm evaluation	computational complexity;data compression;hidden markov models;redundancy;speech coding;speech recognition;hmm evaluation;complexity;compression index;hidden markov models;isolated-word recognition experiments;observation string;redundant computation reduction	Redundant computations occur when a set of hidden Markov models (HMMs) is evaluated with respect to an observation string. A formal restructuring of the HMM allows the redundancy to be identified and removed. An 풪([1-κ]NT) complexity HMM results, with the compression index κ ∈ [0,1) depending upon several factors. Isolated-word recognition experiments illustrate the results	computation;hidden markov model	John R. Deller;R. K. Snider	1993	IEEE Trans. Speech and Audio Processing	10.1109/89.242493	speech recognition;artificial intelligence;redundancy (engineering);kappa;computer science;speech coding;hidden markov model;computation;computational complexity theory;pattern recognition;data compression;markov model	Arch	-20.73766794050991	-88.60391767593971	113555
0cdfc2aa2afee2f6f323f6f24d6c59e5257304da	towards automatic mispronunciation detection in singing		A tool for automatic pronunciation evaluation of singing is desirable for those learning a second language. However, efforts to obtain pronunciation rules for such a tool have been hindered by a lack of data; while many spokenword datasets exist that can be used in developing the tool, there are relatively few sung-lyrics datasets for such a purpose. In this paper, we demonstrate a proof-of-principle for automatic pronunciation evaluation in singing using a knowledge-based approach with limited data in an automatic speech recognition (ASR) framework. To demonstrate our approach, we derive mispronunciation rules specific to South-East Asian English accents in singing based on a comparative study of the pronunciation error patterns in singing versus speech. Using training data restricted to American English speech, we evaluate different methods involving the deduced L1-specific (native language) rules for singing. In the absence of L1 phone models, we incorporate the derived pronunciation variations in the ASR framework via a novel approach that combines acoustic models for sub-phonetic segments to represent the missing L1 phones. The word-level assessment achieved by the system on singing and speech is similar, indicating that it is a promising scheme for realizing a full-fledged pronunciation evaluation system for singing in future.	acoustic cryptanalysis;acoustic model;speech recognition	Chitralekha Gupta;David Grunberg;Preeti Rao;Ye Wang	2017			speech recognition;computer science;singing	NLP	-19.806369645949275	-84.29643968781727	113900
50c427e2e97101cfc9ef49002142c147dca34047	indirect text entry using one or two keys	empirical study;voice output communication aids voca;text entry;indirect text entry;huffman codes;augmentative and alternative communication;interventions for communication disorders;augmentative and alternative communication aac;interface evaluation;scanning;information theory;speech generating devices sgd	This paper introduces a new descriptive model for indirect text composition facilities that is based on the notion of a containment hierarchy. This paper also demonstrates a novel, computer-aided technique for the design of indirect text selection interfaces -- one in which Huffman coding is used for the derivation of the containment hierarchy. This approach guarantees the derivation of optimal containment hierarchies, insofar as mean encoding length. This paper describes an empirical study of two two-key indirect text entry variants and compares them to one another and to the predictive model. The intended application of these techniques is the design of improved indirect text entry facilities for the users of AAC systems.	advanced audio coding;huffman coding;key (cryptography);predictive modelling;run-length encoding;selection (user interface)	Melanie Baljko;Andrew Tam	2006		10.1145/1168987.1168992	natural language processing;speech recognition;information theory;computer science;empirical research;huffman coding	HCI	-26.406798077136663	-88.66903445791962	114071
3c37223e2c710055d44f141fc401081ff418e687	translating names and technical terms in arabic text	machine translation;context effect	It is challenging to translate names and technical terms from English into Arabic. Translation is usually done phonetically: different alphabets and sound inventories force various compromises. For example, Peter Streams may come out as hr..~ ~ bytr szrymz. This process is called transliteration. We address here the reverse problem: given a foreign name or loanword in Arabic text, we want to recover the original in Roman script. For example, an input like .~..A~ bytr strymz should yield an output like Peter Streams. Arabic presents special challenges due to unwritten vowels and phonetic-context effects. We present results and examples of use in an Arabic-to-English machine translator. Introduction It is not trivial to write an algorithm for turning Translators must deal with many problems, and one of the most frequent is translating proper names and technical terms. For language pairs like Spanish/English, this presents no great challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil. However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English. Phonetic translation across these pairs is called transliteration. (Knight and Graehl, 1997) present a computational treatment of Japanese/English transliteration, which we adapt here to the case in Arabic. Arabic text, like Japanese, frequently contains foreign names and technical terms that are translated phonetically. Here are some examples from newspaper text: a	algorithm;alphabet;arabic;computation;device translator device component;formal language;inventory;machine translation;name	Bonnie Glover;Kevin Knight	1998		10.3115/1621753.1621760	natural language processing;speech recognition;computer science;modern arabic mathematical notation;linguistics	NLP	-25.909068850940834	-81.76907647029182	114246
2a1f61ae2c896d301fae364db50879bbfa130699	incremental reference resolution: the task, metrics for evaluation, and a bayesian filtering model that is sensitive to disfluencies	dialogue system;bayesian filtering	In this paper we do two things: a) we discuss in general terms the task of incremental reference resolution (IRR), in particular resolution of exophoric reference, and specify metrics for measuring the performance of dialogue system components tackling this task, and b) we present a simple Bayesian filtering model of IRR that performs reasonably well just using words directly (no structure information and no hand-coded semantics): it picks the right referent out of 12 for around 50% of realworld dialogue utterances in our test corpus. It is also able to learn to interpret not only words but also hesitations, just as humans have shown to do in similar situations, namely as markers of references to hard-to-describe entities.	approximation;dialog system;dialog tree;emoticon;entity;experiment;hierarchical database model;parsing;part-of-speech tagging;particle filter;statistical model	David Schlangen;Timo Baumann;Michaela Atterer	2009		10.3115/1708376.1708381	speech recognition;computer science;artificial intelligence;communication	NLP	-19.650172532517868	-80.98625513291843	114783
05bbcd7e50fa0630b7092e04322cfe5679dc27d2	an std system for oov query terms integrating multiple std results of various subword units		We have been proposing a Spoken Term Detection (STD) method for Out-Of-Vocabulary (OOV) query terms integrating various subword recognition results using monophone, triphone, demiphone, one third phone, and Sub-phonetic segment (SPS) models. In the proposed method, subword-based ASR (Automatic Speech Recognition) is performed for all spoken documents and subword recognition results are generated using subword acoustic models and subword language models. When a query term is given, the subword sequence of the query term is searched for all subword sequences of subword recognition results of spoken documents. Here, we use acoustical distances between subwords when matching the two subword sequences by Continuous Dynamic Programming. We have also proposed the method rescoring and integrating multiple STD results obtained using various subword units. Each candidate segment has a distance, the segment number and the document number. Re-scoring is performed using distances each of high ranked candidate segments, and the last distance is obtained by integrating then linearly using weighting factors. In STD tasks (SDPWS) of IR for Spoken Documents in NTCIR-10, we apply various subword models to the STD tasks and integrate multiple STD results obtained from these subword models.	acoustic cryptanalysis;dynamic programming;ibm 1401 symbolic programming system;language model;std bus;speech recognition;substring;triphone;vocabulary	Kazuma Konno;Hiroyuki Saito;Shiro Narumi;Kenta Sugawara;Kesuke Kamata;Manabu Kon'no;Jinki Takahashi;Yoshiaki Itoh	2013			phone;dynamic programming;triphone;ranking;computer science;artificial intelligence;pattern recognition;language model;weighting	Web+IR	-22.247738890935068	-83.61950579176163	115058
94c715538053e9ea5e1d3811e0f1bca2dc336515	lexical prefix tree and wfst: a comparison of two dynamic search concepts for lvcsr	dynamic programming;weighted finite state transducer wfst;trees mathematics decoding dynamic programming finite state machines search problems speech recognition;history;cover structure;decoding;implementation independent properties;hidden markov models transducers search problems decoding history context context modeling;weighted finite state transducer wfst beam search history conditioned lexical tree hclt search lvcsr;on the fly transducer composition;hypotheses recombination;complex language models;history conditioned lexical tree hclt search;transducers;trees mathematics;language model look ahead techniques;wfst;asr;automatic speech recognition;finite state machines;weighted finite state transducer based search;hidden markov models;static network decoders;history conditioned lexical tree search strategy;lexical prefix tree;beam search;lexical prefix tree asr automatic speech recognition lattice generation language model look ahead techniques hypotheses recombination cover structure history conditioned lexical tree search strategy implementation independent properties beam search dynamic programming on the fly transducer composition weighted finite state transducer based search complex language models static network decoders dynamic network decoders lvcsr dynamic search concepts wfst;speech recognition;dynamic network decoders;lvcsr;search problems;dynamic search concepts;context modeling;context;lattice generation	Dynamic network decoders have the advantage of significantly lower memory consumption compared to static network decoders, especially when huge vocabularies and complex language models are required. This paper compares the properties of two well-known search strategies for dynamic network decoding, namely history conditioned lexical tree search and weighted finite-state transducer-based search using on-the-fly transducer composition. The two search strategies share many common principles like the use of dynamic programming, beam search, and many more. We point out the similarities of both approaches and investigate the implications of their differing features, both formally and experimentally, with a focus on implementation independent properties. Therefore, experimental results are obtained with a single decoder by representing the history conditioned lexical tree search strategy in the transducer framework. The properties analyzed cover structure and size of the search space, differences in hypotheses recombination, language model look-ahead techniques, and lattice generation.	approximation algorithm;beam search;binary decoder;codec;composition filters;computation;dynamic programming;experiment;finite-state transducer;history monoid;language model;mathematical optimization;overhead (computing);scalability;sparse matrix;speech analytics;table (database);trie;vocabulary	David Rybach;Hermann Ney;Ralf Schlüter	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2013.2248723	natural language processing;beam search;speech recognition;transducer;computer science;machine learning;dynamic programming;context model;finite-state machine;hidden markov model	NLP	-20.872195724946362	-88.13804917806455	115162
07dc417612e528c052e6064f7725133f58e6cbe7	prosodic phrases and semantic accents in speech corpus for czech tts synthesis	katedra kybernetiky;maximum likelihood;kybernetika;publikace prosodic phrases and semantic accents in speech corpus for czech tts synthesis;informacni a řidici systemy;automaticke řizeni;statistical method;publications prosodic phrases and semantic accents in speech corpus for czech tts synthesis;uměla inteligence;parameter estimation;em algorithm	We describe a statistical method for assignment of prosodic phrases and semantic accents in read speech data. The method is based on statistical evaluation of listening test data by a maximum-likelihood approach with parameters estimated by an EM algorithm. We also present linguistically relevant quantitative results about the prosodic phrase and semantic accent distribution in 250 Czech	expectation–maximization algorithm;speech corpus;speech synthesis;test data	Jan Romportl	2008		10.1007/978-3-540-87391-4_63	natural language processing;speech recognition;expectation–maximization algorithm;linguistics;maximum likelihood;estimation theory;statistics	NLP	-20.606943329065246	-82.0348361495869	115695
45155f67c684cf9496233e7e56b03226223ecc48	japanese lexical simplification for non-native speakers		This paper introduces Japanese lexical simplification. Japanese lexical simplification is the task of replacing complex words in a given sentence with simple words to produce a new sentence without changing the original meaning of the sentence. We propose a method of supervised regression learning to estimate complexity ordering of words with statistical features obtained from two types of Japanese corpora. For the similarity of words, we use a Japanese thesaurus and dependency-based word embeddings. Evaluation of the proposed method is performed by comparing the complexity ordering of the words.	lexical simplification;supervised learning;text corpus;text simplification;thesaurus;word embedding	Muhaimin Hading;Yuji Matsumoto;Maki Sakamoto	2016			artificial intelligence;natural language processing;computer science;lexical simplification	NLP	-20.958534371036002	-80.4633963528764	115745
e7f77f643126f210610f0835541aae789f2dd7b1	optimal probabilistic evaluation functions for search controlled by stochastic context-free grammars	partial word sequences;evaluation function;stochastic context free grammar;optimal control stochastic processes automatic speech recognition context modeling intelligent robots hidden markov models natural language processing automatic control councils cognitive robotics;stochastic context free grammars;probability;time complexity;language modeling;natural languages;cubic time complexity;word sequences;upper bound;search;context sensitive grammars;computational complexity;probability context sensitive grammars computational complexity natural languages;probabilities;optimal probabilistic evaluation functions;probability computation;natural language processing optimal probabilistic evaluation functions search stochastic context free grammars language modeling probabilities word sequences probability computation partial word sequences best parse tree cubic time complexity;natural language processing;language model;best parse tree	The possibility of using stochastic context-free grammars (SCFG's) in language modeling (LM) has been considered previously. When these grammars are used, search can be directed by evaluation functions based on the probabilities that a SCFG generates a sentence, given only some words in it. Expressions for computing the evaluation function have been proposed by Jelinek and Lafferty (1991) for the recognition of word sequences in the case in which only the prefix of a sequence is known. Corazza et al. (1991) have proposed methods for probability computation in the more general case in which partial word sequences interleaved by gaps are known. This computation is too complex in practice unless the lengths of the gaps are known. This paper proposes a method for computing the probability of the best parse tree that can generate a sentence only part of which (consisting of islands and gaps) is known. This probability is the minimum possible, and thus the most informative, upper-bound that can be used in the evaluation function. The computation of the proposed upper-bound has cubic time complexity even if the lengths of the gaps are unknown. This makes possible the practical use of SCFG for driving interpretations of sentences in natural language processing. >	context-free grammar	Anna Corazza;Renato De Mori;Roberto Gretter;Giorgio Satta	1994	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.329008	natural language processing;computer science;theoretical computer science;machine learning;probability;statistics;language model	Vision	-22.04658540662227	-91.71478027943355	115794
898b1bc4003edb940987a38ad50fa86724c57578	neural network recognition of spelling errors	output representation;neural network recognition;natural language word;output form;different form;ann input;different type;word representation;new word representation;ann performance;adequate word representation;neural network;input output;natural language;artificial neural network	One area in which artificial neural networks (ANNs) may strengthen NLP systems is in the identification of words under noisy conditions. In order to achieve this benefit when spelling errors or spelling variants are present, variable-length strings of symbols must be converted to ANN input/otttput form--fixed-length arrays of numbers. A common view in the neural network community has been that different forms of input/output representations have negligible effect on ANN performance. This paper, however, shows that input/output representations can in fact affect the performance of ANNs in the case of natural language words. Minimum properties for an adequate word representation are proposed, as well as new methods of word representation. To test the hypothesis that word representations significantly affect ANN performance, traditional and new word representations are evaluated for their ability to recognize words in the presence of four types of typographical noise: substitutions, insertions, deletions and reversals of letters. The results indicate that word representations have a significant effect on ANN performance. Additionally, different types of word representation are shown to perform better on different types of error.	artificial neural network;input/output;natural language processing;protologism	Mark Lewellen	1998			natural language processing;input/output;speech recognition;computer science;machine learning;natural language;artificial neural network	NLP	-20.449440792467186	-80.90210408382318	115920
a8ed9ba251befbfeb391a1c599ab2dde7126b085	consistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation	user modelling;optimisation;reinforcement learning;speech processing;realistic man machine simulation;user modeling technique;spoken dialogue system;user modelling interactive systems learning artificial intelligence man machine systems optimisation speech processing stochastic processes;machine learning;stochastic processes;simulation technique;task oriented dialogue simulation technique;machine learning method;optimization;man machine systems speech processing automatic speech recognition learning systems stochastic processes sociotechnical systems optimization methods training data history acoustic noise;learning artificial intelligence;stochastic description spoken dialogue system machine learning method optimization task oriented dialogue simulation technique user modeling technique realistic man machine simulation;interactive systems;man machine systems;user model;stochastic description	Because of the great variability of factors to take into account, designing a spoken dialogue system is still a tailoring task. Rapid design and reusability of previous work is made very difficult. For these reasons, the application of machine learning methods to dialogue strategy optimization has become a leading subject of researches this last decade. Yet, techniques such as reinforcement learning are very demanding in training data while obtaining a substantial amount of data in the particular case of spoken dialogues is time-consuming and therefore expansive. In order to expand existing data sets, dialogue simulation techniques are becoming a standard solution. In this paper we describe a user modeling technique for realistic simulation of man-machine goal-directed spoken dialogues. This model, based on a stochastic description of man-machine communication, unlike previously proposed models, is consistent along the interaction according to its history and a predefined user goal	dialog system;heart rate variability;machine learning;mathematical optimization;reinforcement learning;simulation;spoken dialog systems;user modeling	Olivier Pietquin	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262563	natural language processing;user modeling;computer science;artificial intelligence;machine learning;speech processing;reinforcement learning	Robotics	-25.391732355437796	-88.30370286911838	116007
19876cd79e37f947c75358a07d727bc5fd59e26d	optimal selection of limited vocabulary speech corpora		We address the problem of finding a subset of a large speech data corpus that is useful for accurately and rapidly prototyping novel and computationally expensive speech recognition architectures. To solve this problem, we express it as an optimization problem over submodular functions. Quantities such as vocabulary size (or quality) of a set of utterances, or quality of a bundle of word types are submodular functions which make finding the optimal solutions possible. We, moreover, are able to express our approach using graph cuts leading to a very fast implementation even on large initial corpora. We show results on the Switchboard-I corpus, demonstrating improved results over previous techniques for this purpose. We also demonstrate the variety of the resulting corpora that may be produced using our method.	analysis of algorithms;cut (graph theory);mathematical optimization;optimization problem;rapid application development;speech recognition;submodular set function;telephone switchboard;text corpus;vocabulary	Hui Lin;Jeff A. Bilmes	2011			pattern recognition;artificial intelligence;speech corpus;submodular set function;bundle;computer science;vocabulary;cut;optimization problem	ML	-21.197592736341377	-86.13903363003638	116407
a02b5a6ae6739d01c3eb06ca73cc4349570a3746	a knowledge representation method to implement a taiwanese tone group parser		A tone group parser could be one of the most important components of the Taiwanese text-to-speech system. In this paper, we offered the hypothesis of tonal government to emphasis the idea that if the allotone selection can be made for each word in a sentence then the tone groups will be separated within the sentence and supported our viewpoint with the implementation of a Taiwanese tone group parser. In addition to the description of using the symbol system to convert language expertise and heuristic knowledge into a knowledge base to cope with a frame-based corpus and a tone sandhi processor, the procedure of connecting the inference engine and the knowledge 1 The 2017 Conference on Computational Linguistics and Speech Processing ROCLING 2017, pp. 1-4  The Association for Computational Linguistics and Chinese Language Processing	computation;computational linguistics;heuristic;inference engine;knowledge base;knowledge representation and reasoning;outline of software engineering;parser;speech processing;speech synthesis	Yu-Chu Chang	2017			knowledge representation and reasoning;natural language processing;parsing;artificial intelligence;computer science	NLP	-23.975386659314513	-83.46977879653306	116573
1e5ddab92e115dfacc0b0f451f5d8b38955529f3	porting tonnew domains using the learner		Acquiring syntactic and semantic information about a new application domain for a natural language processing system is often a time-consuming task. To address this problem, various researchers have developed acquisition tools to speed the process. While such tools are very useful, they are typically tied to particular systems and so their benefits cannot be shared by other researchers. In this paper, we discuss an experiment using the Leamer-a software tool for acquiring information about a new task domain for Parlance, l an ATN-based natural language system---to configure a quite different natural language system, the BBN ACFG, a unification-based system. We have used the Learner to produce information in three major areas: syntactic and semantic information about the lexical items used in the new domain; translation rules from the parser output to the application system; and a class grammar for use in the speech recognition component of HARC, the BBN spoken language system. Initial results are encouraging: 1499 lexical items have been acquired, of which 91% were directly usable, without any manual editing; all of the translation rules are usable; and a speech vocabulary of 2170 items, with an associated class grammar with a perplexity of 89, has been acquired with a small amount of manual editing.	application domain;natural language processing;perplexity;programming tool;speech recognition;unification (computer science);vocabulary	Robert J. P. Ingna;Lance A. Ramshaw	1989				NLP	-23.256210913610104	-85.07692615928993	116678
0c5ede45a2aba7f4cd5b83298d9360bad01416af	lucid: a corpus of spontaneous and read clear speech in british english	speaking styles;speech production	This paper describes LUCID, the London UCL Clear Speech in Interaction Database, which contains spontaneous and read speech in clear and casual speaking styles for 40 Southern British English speakers. The problem-solving task used to collect the spontaneous speech, the DiapixUK task, is also described, along with ways of using the task to elicit different types of clear speech without explicit instruction, e,g. using different ‘barriers’ to communication. Applications of the corpus and of the task materials for future research projects are discussed. The corpus and materials will be available online to the research community at the end of the project.	problem solving;spontaneous order;text corpus	Rachel Baker;Valérie Hazan	2010			natural language processing;speech recognition;speech corpus;computer science;communication	NLP	-24.493300395932916	-84.3401568162243	116751
908aefbd784707dc4d123a72785932a022f1d5ec	topic mining based on word posterior probability in spoken document	confusion network;modified weight;posterior probability;spoken document;topic mining	For speech recognition system, there are three kinds of result representations as one-best, N-best and Lattice. Since lattice has multi-path which can reduce the effect of recognition error rate, it is widely applied nowadays. In fact, there are amount of redundancies in lattice, which leads to the increasing of complexity of latter algorithm based on it. Additionally, for the decoding algorithm, it is acted as maximum a posterior probability (MAP) which can only guarantee the posterior probability of the whole sentence is of maximum. For MAP does not mean the highest syllable recognition rate, here, confusion network is introduced in topic mining system. In the clustering during confusion network, the minimum word error rule is adopted, which is proper to topic mining system since the least meaningful unit is word in Chinese and word information is most important in topic mining. In this paper, a simplified confusion network generation algorithm is proposed to handle some problems caused by insertion error during recognition. Then based on the confusion network, a word list extraction approach is proposed, in which, the dictionary is adopted to judge whether the consecutive arc in confusion sets is a word. At this stage, the error word information produced by error recognition rate can be corrected to some extent. After the competition part in word list extraction on confusion network, a final word list with posterior probability can be obtained. Furthermore, this kind of posterior probability can be combined in topic mining system. SVD and NMF are adopted here to decompose the term-document matrix on the word list of confusion network. From the experiments, it can be drawn that the proposed approach based on confusion network can achieve better performance than that of one-best and N-best. Additionally, the modified weight which combined posterior probability into term-document matrix can further improve the system performance.	algorithm;cluster analysis;dictionary;document-term matrix;experiment;non-negative matrix factorization;singular value decomposition;speech recognition;syllable	Guo-xing Chen;Xue-Zhi Xiang;Jing-xin Chang	2011	JSW		natural language processing;speech recognition;word error rate;computer science;artificial intelligence;machine learning;pattern recognition;database;posterior probability;statistics	ML	-21.80560663710095	-81.25199166334778	116788
3998d424ccbebbb5b33ff386b521762006cf0379	an empirical study of multilingual natural language generation: what should a text planner do?		We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations. We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems. 1 I n t r o d u c t i o n The natural language generation community has emphasized for a number of years the strengths of multilingual generation (MGEN) systems (Iordanskaja et al., 1992; RTsner and Stede, 1992; Reiter and Mellish, 1993; Goldberg et al., 1994; Paris et al., 1995; Power and Scott, 1998). These strengths concern the reuse of knowledge, the support for early drafts in several languages, the support for maintaining consistency when making changes, the support for producing alternative formulations, and the potential for producing higher quality outputs than machine translation. (The weaknesses concern the high-cost of building large, language-independent knowledge bases, and the dilficulty of producing high-quality. broad-coverage generation algorithms.) From an economic perspective, the more a system can rely on language independent modules for the purpose of multilingual generatiom the better. If an MGEN system needs to develop language dependent knowledge bases, and language dependent algorithms for content selection, text planning, and sentence planning, i t is difficult to justify its economic viability. However, if most of these components are language independent and/or much of the code can be re-used, an MGEN system becomes a viable option. .Many of the earl3 implementations of MGEN systems have adopted the perspective that text planners can be implemented as language-independent modules (lordanskaja el, ;11., 1992: Goldberg et el., 1994), possibly followed by a hm:aricatwn stage, in which discourse l.rees are re-written to refleet~ language-specific constraints (R6sner and Stede. 1992; St,ede, 1999). Although such an approach may 17 be adequate for highly restricted text genres, such as weather forecasts, it usually poses problems for less restricted genres. Studies of instruction manuals (RTsner and Stede, 1992; Delin et al., 1994: Delin et al., 1996) suggest that there are variations with respect to the way high-level communicative g o a l s are realized across languages. For example, Delin et al. (1994) noticed that sentences (1), (2), and (3), which were taken from a trilingual instruction manual for a step-aerobics machine, yield nonisomorphic Rhetorical Structure (Mann and Thompson, 1988) analyses in English, French, and German respectively (see Figure 1). English: [The stepping load can be altered I ] [by loosening the locking lever 2] [and changing the position of the cylinder foota]. (1) French: [Pour modifier la charge d'appui, l] [desserrer les levieres 2] [puis d6placer le pied des v6rins a] ([To modify the load stepping ~] [loosen the levers 2] [then change .the foot of the cylinder foot.el) (2) German: [,Nach Lockern der Klelnmhebel 2] (3) [kann t ] [durch Verschieben des Zylinderfudes 3 ] [die Tretbelastung verS.ndert werden.~ ] ([After loosening of the levers 2] [can'] [by puslfing of the cylinder foot 3] [the load changed be, ~]) Hmvever, previous.discourse ,studies do .not estimate how ubiquitous such non-isomorphic analyses are. Are the examples above an exception or the norm? Are non-isomorphic analyses specific to discourse structures built, across elementary discourse units of single sentences, or do they also occur across sentences and paragraphs? If nonisomorphism is ubiquitous, how should an MGEN system be designed in order to effectively deal wit h non-isomorphic discourse structures when mapping knowledge bases into multiple languages? In this paper, we describe an experiment that was designed to answer these questions. To investigate English French German	algorithm;box counting;cylinder seal;ede;exception handling;high- and low-level;knowledge base;language-independent specification;linear algebra;lock (computer science);machine translation;modifier key;natural language generation;owner's manual;parallel text;stepping level	Daniel Marcu;Lynn Carlson;Maki Watanabe	2000			natural language processing;language identification;speech recognition;computer science;linguistics;empirical research	NLP	-31.722518059024356	-82.28162218035081	116812
046c2498dfae896b2087ed10595b135f7f6988b5	computational modeling of timing control and its application to objective evaluation of the second language proficiency.				Yoshinori Sagisaka;Hiroaki Kato;Minoru Tsuzaki;Shizuka Nakamura;Chatchawarn Hansakunbuntheung	2010			programming language;language proficiency;natural language processing;artificial intelligence;computer science	Robotics	-29.943213354026994	-80.33455572437605	117069
3aa330f170710dd3495fd423cbd468c36acbda91	bilingual audio-subtitle extraction using automatic segmentation of movie audio	speech motion pictures accuracy training data mining acoustics conferences;movie audio;audio segmentation;s2s systems;audio signal processing;motion pictures;automatic segmentation;acoustics;speech processing;training;speech;data mining;accuracy;bilingual movie audio;speech to speech systems;audio segmentation bilingual movie audio movie subtitle;bilingual audio subtitle extraction;multilingual audio stream segmentation;text data;multilingual audio stream segmentation bilingual audio subtitle extraction automatic segmentation movie audio text data bilingual audio data speech to speech systems s2s systems;conferences;bilingual audio data;speech processing audio signal processing;movie subtitle	Extraction of bilingual audio and text data is crucial for designing Speech to Speech (S2S) systems. In this work, we propose an automatic method to segment multilingual audio streams from movies. In addition, the audio streams are aligned with the corresponding subtitles. We found that the proposed method gives 89% perfectly segmented bilingual audio and 6% partially segmented bilingual audio. In addition, the mapping of the audio to the corresponding subtitles has accuracy 91%.	image segmentation;text corpus	Andreas Tsiartas;Prasanta Kumar Ghosh;Panayiotis G. Georgiou;Shrikanth (Shri) Narayanan	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947635	natural language processing;audio mining;speech recognition;audio signal processing;computer science;speech;speech processing;acoustic model;accuracy and precision;multimedia	Robotics	-22.10344012824874	-83.45319535571339	117676
06f5c74352cf5a33b717943e2fbfa027c982e650	syntactic alternations of hindi verbs with reference to the morphological paradigm	natural languages computational linguistics mathematical morphology;mathematical morphology;natural languages;computer science speech natural languages;semantic category;computational linguistics;nominal variants syntactic alternation pattern hindi verbs argument structure semantic nature phonological changes morphological paradigms intransitive causative variants transitive causative variants;syntactic alternation;morphological paradigm	The aim of this paper is to show the alternation pattern of verbs in Hindi. The work is guided by Beth Levin’s work on English verb classes and alternations where English verbs are classified semantically according to their argument structure [1]. There is a strong belief that the semantic nature of verbs is largely dependent on its argument structure. The nature of the Hindi verbs shows that along with the argument structure, attention should also be paid to the phonological changes which influence the morphological paradigms. Preliminary attempts have been made to class the simple verbs of Hindi into different morphological paradigms along with the phonological changes. At present, the work is limited only to the syntactic structure. Semantic characteristics have not been dealt with. Focus is mainly given on nominal, transitive-causative, and intransitive-causative variants.	categorization;dictionary;evert willem beth;galaxy morphological classification;lexicon;location-based game;machine translation;natural language processing;nominal type system;programming paradigm;syntactic predicate;transitive closure;vdr;word-sense disambiguation	Debasri Chakrabarti;Pushpak Bhattacharyya	2002		10.1109/LEC.2002.1182294	natural language processing;modal verb;computer science;linguistics;communication	NLP	-33.6005715581797	-81.7267889658493	118029
9705a7e68949eb50d5643433ab2038ad1a5e1cbd	automatic determination of parts of speech of english words	part of speech	The classifying of words according to syntactic usage is basic to language handling; this paper describes an algorithm for automatically classifying words according to thirteen commonly used parts of speech: noun, adjective, verb, past verb, adverb, preposition, conjunction, pronoun, interjection, present participle, past participle, auxiliary verb, and plural or collective noun. The algorithm was derived by a computerized study of the words in The Shorter Oxford English Dictionary. In its operation it utilizes a prepared dictionary of around nine hundred words to assign parts of speech to special or exceptional words. Other words are split into affix and kernel parts and assigned a part of speech on the basis of the part-of-speech implications of the affixes and the length of the remaining kernel. An accuracy of 95 per cent is achieved from the point of view of inclusive part of speech, where inclusive part of speech is defined as that string which contains all the parts of speech attributed to the word by the dictionary but which may also contain one or two more parts of speech. Introduction	algorithm;compiler;dictionary;point of view (computer hardware company)	Lois L. Earl	1967	Mech. Translat. & Comp. Linguistics		speech production;cued speech;audio mining;speech corpus;speech;acoustic model;linguistics;chinese speech synthesis;speech synthesis	NLP	-27.221034956091927	-80.69156854000936	118377
cd181e1ba32f6c3cb6b90e92a9e43ee82c985f3c	data-driven interaction patterns: authority and information sharing in dialogue		We explore the utility of a computational framework for social authority in dialogue, codified as utterance-level annotations. We first use these annotations at a macro level, compiling aggregate statistics and showing that the resulting features are predictive of group performance in a task-based dialogue. Then, at a micro level, we introduce the notion of an interaction pattern, a formulation of speaker interactions over multiple turns. We use these patterns to characterize situations where speakers do not share information equally. These patterns are found to be more discriminative at this task than similar patterns using standard dialogue acts.	aggregate data;compiler;interaction design pattern	Elijah Mayfield;Michael Garbus;David Adamson;Carolyn Penstein Rosé	2011			natural language processing;computer science;artificial intelligence;data mining	NLP	-33.08096430476543	-83.58199009223397	118531
75b265926f8896af46d30ccdd35503accfcf0c9b	sample selection for automatic language identification	unsupervised learning;sample selection;likelihood ratio;spoken language identification;active learning;speech processing;statistical classifiers;selective sampling;natural languages;natural languages speech processing unsupervised learning;natural language;entropy criterion;language identification;speech recognition;entropy;likelihood ratio criterion;speech recognition entropy natural language processing;natural language processing;automatic language identification;natural languages training data sampling methods partitioning algorithms error analysis costs speech processing uncertainty iterative algorithms iterative methods;language labeled speech samples;likelihood ratio criterion sample selection automatic language identification spoken language identification language labeled speech samples statistical classifiers entropy criterion	Current approaches to automatic spoken language identification (LID) assume the availability of a large corpus of manually language-labeled speech samples for training statistical classifiers. We investigate two methods of active learning to significantly reduce the amount of labeled speech needed for training LID systems. Starting with a small training set, an automated method is used to select samples from a corpus of unlabeled speech, which are then labeled and added to the training pool - one selection method is based on a previously known entropy criterion, and another on a novel likelihood-ratio criterion. We demonstrate LID performance comparable to a large training corpus using only a tenth of the training data. A further 40% improvement in LID performance is obtained using a third of the training data. Finally, we show that our novel selection method is more robust to variance in the unlabeled pool than the entropy based method.	language identification;selection (genetic algorithm);statistical classification;test set;text corpus	David Farris;Christopher M. White;Sanjeev Khudanpur	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518587	natural language processing;unsupervised learning;speech recognition;computer science;machine learning;pattern recognition;speech processing;natural language	NLP	-20.495103179323223	-89.72373582788688	118799
3dea29c2c398d96b4880f8322b72859f3b23f020	improving acoustic based keyword spotting using lvcsr lattices	confidence measure cm;lattices;decoding;acoustics;vocabulary;cascading style sheets;vocabulary acoustic signal processing natural language processing speech recognition;speech;acoustic signal processing;large vocabulary continuous speech recognition lvcsr lattice english keyword detection conversational scenario acoustic based keyword spotting system lvcsr based keyword spotting system spoken data likelihood ratios;artificial neural networks;hidden markov models;keyword spotting kws;spoken term detection std;speech recognition;confidence measure cm keyword spotting kws spoken term detection std;acoustics lattices hidden markov models speech artificial neural networks cascading style sheets decoding;natural language processing	This paper investigates detection of English keywords in a conversational scenario using a combination of acoustic and LVCSR based keyword spotting systems. Acoustic KWS systems search predefined words in parameterized spoken data. Corresponding confidences are represented by likelihood ratios given the keyword models and a background model. First, due to the especially high number of false-alarms, the acoustic KWS system is augmented with confidence measures estimated from corresponding LVCSR lattices. Then, various strategies to combine scores estimated by the acoustic and several LVCSR based KWS systems are explored. We show that a linear regression based combination significantly outperforms other (model-based) techniques. Due to that, the relative number of false-alarms of the combined KWS system decreased by more than 50% compared to the acoustic KWS system. Finally, an attention is also paid to the complexities of the KWS systems enabling them to potentially be exploited in real-detection tasks.	acoustic cryptanalysis;speech analytics	Petr Motlícek;Fabio Valente;Igor Szöke	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288898	natural language processing;speech recognition;computer science;speech;machine learning;pattern recognition;lattice;cascading style sheets;artificial neural network;hidden markov model	Robotics	-20.177894629127287	-86.9073246687658	119369
7bc89cc99a300b6aa704645b76813c03cfb56221	cyberpunc: a lightweight punctuation annotation system for speech	nonspeech applications;trigram language model;image recognition;intra sentence punctuation;text;speech dictation system lightweight punctuation annotation system cyberpunc automatic insertion intra sentence punctuation text acoustic stream lexical information speech recognition system nonspeech applications automatic grammar correction word processor spoken text parsing punctuation restoration system trigram language model viterbi algorithm quantitative performance subjective performance viterbi decoder;punctuation restoration system;automatic grammar correction;decoding;prototypes;speech processing;subjective performance;automatic insertion;acoustic stream;dictation speech recognition speech processing viterbi decoding grammars word processing;spoken text parsing;telephony;word processor;automatic speech recognition;grammars;dictation;cyberpunc;lexical information;viterbi algorithm;speech recognition system;quantitative performance;viterbi decoder;viterbi algorithm computer science speech recognition prototypes broadcasting telephony image recognition automatic speech recognition decoding humans;speech recognition;humans;computer science;broadcasting;lightweight punctuation annotation system;speech dictation system;viterbi decoding;language model;word processing	Doug Beeferman Adam Berger John La erty School of Computer Science Carnegie Mellon University Pittsburgh PA 15213 dougb,aberger,la erty@cs.cmu.edu ABSTRACT This paper describes a lightweight method for the automatic insertion of intra-sentence punctuation into text. Despite the intuition that pauses in an acoustic stream are a positive indicator for some types of punctuation, this work will demonstrate the feasibility of a system which relies solely on lexical information. Besides its potential role in a speech recognition system, such a system could serve equally well in non-speech applications such as automatic grammar correction in a word processor and parsing of spoken text. After describing the design of a punctuationrestoration system, which relies on a trigram language model and a straightforward application of the Viterbi algorithm, we summarize results, both quantitative and subjective, of the performance and behavior of a prototype system.	acoustic cryptanalysis;computer science;language model;linear algebra;parsing;prototype;speech recognition;trigram;viterbi algorithm	Doug Beeferman;Adam L. Berger;John D. Lafferty	1998		10.1109/ICASSP.1998.675358	natural language processing;speech recognition;computer science;speech processing;viterbi decoder;language model	NLP	-22.161771890836143	-81.52315822599391	119415
1caf8d2296aabd96115c7269f97c32655277cc61	justification of plan recognition results	user model;expert system	Expert systems are typically expected to be able to justify their decisions to the user. This paper argues that help syst ems or tutoring systems based on a plan recognizer can equally bene fit from an explanation component . To this end a plan recognition system equipped with a usermodel is presented and the techniques re quired to generate precise and useful justifications of its results ar e int oduced. The system answers questions about various aspects of a deci sion and allows its user to check and adjust the user model if necessar y.	c4.5 algorithm;expert system;finite-state machine;machine learning;sion's minimax theorem;user modeling	M. Bauer	1996			user modeling;artificial intelligence;machine learning;simulation;computer science;expert system	AI	-27.151060249659103	-86.76147786529226	120097
c31b811a77369b4263e0523a652625a553fd2b44	the lexicon in text generation	generation task;lexical phenomenon;generation lexicon;primary goal;alternatives designer;text generation lexicon;computer files;syntax;lexicography;generative lexicon;natural language;semantics;structures	1. I n t r o d u c t i o n 1 In this paper I will review the state of the text generation lexicon. I have two primary goals: 1) to give the reader an idea of what is currently being done, by setting out some of the alternatives designers of generation lexicons have faced, the choices they have made, and the implications of these choices for the types of lexical phenomena they have been able to represent. 2) to suggest what a generation lexicon could do, i.e. what range of lexical phenomena is relevant to the generation task. These issues will be addressed more or less in parallel throughout this paper, with more attention to the first goal in the first two sections, and to the second in the last three sections.	lexicon;natural language generation	Susanna Cumming	1986			natural language processing;speech recognition;generative lexicon;computer science;linguistics	NLP	-32.69974445772258	-80.42471541147846	120714
00735b56d9a206efa0f8fa66d9375933b30193e7	natural language processing for transparent communication between public administration and citizens	dialogue system;access to information;multilingual nl generation;dialogue systems;natural language processing;public administration	This paper presents two projects concerned with the application of natural language processing technology for improving communication between Public Administration and citizens. The first project, GIST,is concerned with automatic multilingual generation of instructional texts for form-filling. The second project, TAMIC, aims at providing an interface for interactive access to information, centered on natural language processing and supposed to be used by the clerk but with the active participation of the citizen.	freedom of information laws by country;gist;natural language processing	Bernardo Magnini;Elena Not;Oliviero Stock;Carlo Strapparava	2000	Artificial Intelligence and Law	10.1023/A:1008394902165	natural language processing;computer science;multimedia	AI	-29.370995018343606	-84.0035614936464	120791
763f7e6fb046b5f8dba81aa6faaa72199fc98b27	learning speech semantics with keyword classification trees	classification tree;local parser;speech recognition grammars learning artificial intelligence;performance evaluation;semantic representation;noun phrase;air travel information system;speech classification tree analysis training data robustness educational institutions computer science impedance matching displays testing performance evaluation;speech;testing;speech semantics;training data;grammars;speech recognition system;displays;impedance matching;keyword classification trees;speech recognition;robustness;classification tree analysis;computer science;learning artificial intelligence;linguistic analyzer;speech recognition system speech semantics keyword classification trees linguistic analyzer air travel information system local parser semantic representation performance level;performance level	A linguistic analyzer based on KCTs (keyword classification trees) was trained on sentences from the ATIS (Air Travel Information System) air travel task and incorporated into the system (CHANEL) built at CRIM (Centre de Recherche Informatique de Montreal) for the Nov. 1992 ATIS benchmarks. Word sequences were processed by a local parser that identified semantically important noun phrases and then passed through a forest of KCTs, each responsible for generating a different aspect of the semantic representation. CHANEL attained a reasonable performance level, despite its heavy reliance on KCTs rather than on handcoded linguistic rules. The CRIM speech recognition system had a recognition rate of 88.9% words correct; CHANEL is clearly robust. >		Roland Kuhn;Renato De Mori	1993		10.1109/ICASSP.1993.319228	natural language processing;training set;impedance matching;noun phrase;speech recognition;decision tree learning;computer science;speech;machine learning;pattern recognition;software testing;robustness	NLP	-21.5089275425863	-86.09221567036442	120882
63a9a39808d52ef137357f60dcf559ff2e533f3b	topic detection in read documents	broadcast news;description systeme;unsupervised clustering;system description;distance measure;reconocimiento palabra;deteccion;hidden markov model;information retrieval;diario;speech retrieval;communication orale;annotation;detection;system performance;statistical model;journal;topic detection;bd publico;sound recording;enregistrement son;recherche information;registro sonido;newspaper;comunicacion oral;sujet;portugues;speech recognition;nearest neighbour search;descripcion sistema;recuperacion informacion;reconnaissance parole;subject;oral communication;portuguese;language model;portugais;kullback leibler	In this paper, we address the importance and the problems involved in topic annotation in the speech retrieval domain. Identified the problem, an algorithm developed to perform automatic topic annotation of broadcast news (BN) speech corpora is described. The approach adopted is based in Hidden Markov Models (HMM) and topic language models, to solve topic segmentation and labelli ng tasks simultaneously. To overcome the lack of topic labelled material to train the statistical models, a two-stage unsupervised clustering was developed. Both stages are based on the nearest-neighbour search method, using the Kullback-Leibler as a distance measure. On-going experiments to evaluate the system performance are also described.	algorithm;cluster analysis;experiment;hidden markov model;kullback–leibler divergence;language model;markov chain;nearest neighbor search;statistical model;text corpus;text segmentation	Rui Amaral;Isabel Trancoso	2000		10.1007/3-540-45268-0_29	statistical model;speech recognition;newspaper;document clustering;computer science;artificial intelligence;hidden markov model;statistics;language model;portuguese	NLP	-24.056196238875415	-81.96311751803059	120923
3a1add2849d345ba2234af87d0f3dd7044f89d49	automatic creation of scenarios for evaluating spoken dialogue systems via user-simulation	speech recognition;spoken dialogue systems;dialogue management;natural language processing;spoken language understanding	This paper proposes a novel technique to create scenarios that can be used by a user simulator for exhaustively evaluating spoken dialogue systems. The scenarios are automatically created from simple scenario-templates that the systems' developers create manually employing their knowledge about typical goals of the system's users. The scenarios contain goals, which the user simulator will try to achieve through the interaction with the systems. The goals are represented in the form of semantic frames, which are associated with user utterances of sentences and are taken from utterance corpora. In this way, the scenarios enable speech-based interaction between the simulator and the spoken dialogue systems to be evaluated. Experiments have been carried out employing two spoken dialogue systems (Saplen and Viajero), a user simulator and two utterance corpora previously collected for two different application domains: fast-food ordering and bus travel information. Experimental results show that the technique has been useful for exhaustively evaluating the systems and finding out problems in their performance that must addressed to improve them. Some of these problems are caused by acoustic similarity between some uttered words and strong speaker accents. Thus, we think these problems would have been difficult to uncover employing the user simulation techniques typically used nowadays, as they do not employ real speech and just consider semantics of user intentions.	dialog system;simulation	Ramón López-Cózar	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.05.026	natural language processing;speech recognition;computer science	NLP	-25.924707703985675	-86.7318676742295	121382
54783554b83a2a2c1b12a6d2d0360128357bf097	speech recognition using on-line estimation of speaking rate.	word error rate;on line estimation;rate of speech;speech recognition	In this paper, we describe a rate of speech estima-tor that is derived directly from the acoustic signal. This measure has been developed as an alternative to lexical measures of speaking rate such as phones or syllables per second, which, in previous work, we estimated using a rst recognition pass; the accuracy of our earlier lexical rate estimate depended on the quality of recognition. Here we show that our new measure is a good predictor of word error rate, and in addition, correlates moderately well with lexical speech rate. We also show that a simple modiication of the model transition probabilities based on this measure can reduce the error rate almost as much as using lexical phones per second calculated from manually transcribed data. When we categorized test utterances based on speaking rate thresholds computed from the training set, we observed that a diierent transition probability value was required to minimize the error rate in each speaking rate bin. However, the reduction of error provided by this approach is still small in comparison with the increases in error observed for unusually fast or slow speech.	acoustic fingerprint;categorization;kerrison predictor;markov chain;online and offline;speech recognition;test set;tor messenger;word error rate	Nelson Morgan;Eric Fosler-Lussier;Nikki Mirghafori	1997			voice activity detection;speaker recognition;linear predictive coding;speech recognition;word error rate;acoustic model	NLP	-20.200548923997328	-88.28414099477448	121430
726b5111612d6dc336dc00799a3725f1139ad1d9	the predictive power of game structure in dialogue act recognition: experimental results using maximum entropy estimation	bottom up;maximum entropy;information extraction;top down	Recognizing the dialogue act(s) performed by means of an utt erance involves combining top-down expectations about the next li kely ‘move’ in a dialogue with bottom-up information extracted f rom the speech signal. We compare two ways of generating expectatio ns: one which makes the expectations depend only on the previous act, and one which also takes into account the fact that individua l ialogue acts play a role as part of larger conversational stru ctu es (‘games’). Our results indicate that exploiting game struc ture does lead to improved expectations.	entropy estimation;top-down and bottom-up design	Massimo Poesio;Andrei Mikheev	1998			information extraction;joint entropy;artificial intelligence;principle of maximum entropy;pattern recognition;maximum entropy probability distribution;predictive power;maximum-entropy markov model;utterance;computer science;machine learning;maximum entropy spectral estimation	NLP	-20.758134680964496	-87.87784437653295	121872
fe4f94985ebf5868ee6cf6b79f47b3b60c401017	a new method for automatic generation of speaker-dependent phonological rules	bottom up;top down;speech processing;acoustic signal processing;speaker adaptation speaker dependent phonological rules automatic generation recognition errors pronunciation variability continuous speech standard pronunciation multiple pronunciation dictionary single pronunciation dictionary recognizer dependent phonological rules top down recognizer bottom up recognizer phrase recognition experiments concatenated phoneme hmm;automatic generation;hidden markov models;glossaries;speaker dependent;speech recognition;hidden markov models dictionaries concatenated codes automatic speech recognition speech recognition broadcasting laboratories electronic mail loudspeakers tv;glossaries speech recognition speech processing hidden markov models acoustic signal processing;speaker adaptation	This paper presents a new method for automatic generation of speaker-dependent phonological rules in order to decrease recognition errors caused by pronunciation variability. The proposed method generates phonological rules by using objective speaker's continuous speech and corresponding standard pronunciation, resulting in forming a multiple-pronunciation dictionary from a single-pronunciation dictionary. The method makes it possible to generate automatically speaker-dependent and recognizer-dependent phonological rules, and be applied to both a top-down recognizer and a bottom-up recognizer, while conventional methods are based on hand-derived general phonological rules such as coarticulation knowledge or are applied only to a bottom-up recognizer. Phrase recognition experiments with concatenated phoneme HMMs showed that the generated rules can decrease recognition errors and play a role of speaker adaptation at the phonological level.		Toru Imai;Akio Ando;Eiichi Miyasaka	1995		10.1109/ICASSP.1995.479831	natural language processing;speaker recognition;speech recognition;computer science;top-down and bottom-up design;speech processing	NLP	-19.912239162715395	-86.26145974424918	122011
6c74b290e8f7f479e5385911e1a75d045fdf73f4	learning better lexical properties for recurrent oov words	recurrent oov words;pattern clustering;oov word learning;lexical properties speech recognition systems recovery rate pos label accuracy pronunciation accuracy bottom up clustering approach hybrid decoder out of vocabulary words recurrent oov words;speech recognition learning artificial intelligence pattern clustering;oov word learning oov word detection recurrent oov words distributed evidence;oov word detection;speech recognition;distributed evidence;learning artificial intelligence;acoustics accuracy speech speech recognition context feature extraction testing	Out-of-vocabulary (OOV) words can appear more than once in a conversation or over a period of time. Such multiple instances of the same OOV word provide valuable information for learning the lexical properties of the word. Therefore, we investigated how to estimate better pronunciation, spelling and part-of-speech (POS) label for recurrent OOV words. We first identified recurrent OOV words from the output of a hybrid decoder by applying a bottom-up clustering approach. Then, multiple instances of the same OOV word were used simultaneously to learn properties of the OOV word. The experimental results showed that the bottom-up clustering approach is very effective at detecting the recurrence of OOV words. Furthermore, by using evidence from multiple instances of the same word, the pronunciation accuracy, recovery rate and POS label accuracy of recurrent OOV words can be substantially improved.	bottom-up parsing;cluster analysis;part-of-speech tagging;sensor;vocabulary	Long Qin;Alexander I. Rudnicky	2013	2013 IEEE Workshop on Automatic Speech Recognition and Understanding	10.1109/ASRU.2013.6707699	natural language processing;speech recognition;computer science;pattern recognition	NLP	-19.518980130489744	-80.84691970916488	122283
6b75e5b9db17eed72447609064a021dd046c0b15	data-driven generation of pronunciation dictionaries in the german verbmobil project: discussion of experimental results	databases;pronunciation variants;graph theory;orthographic transcription;speech signal;speech recognition systems;dictionaries speech recognition lattices laboratories acoustics oral communication databases graphics counting circuits educational products;lattices;data driven generation;training sample;canonical pronunciation form;german verbmobil project;acoustics;graph theory natural languages speech recognition dictionaries multimedia databases;natural languages;training procedure;pronunciation dictionaries;counting circuits;dictionaries;multimedia databases;graph representation;speech recognition;training procedure data driven generation pronunciation dictionaries german verbmobil project speech recognition systems canonical pronunciation form pronunciation variants speech database training algorithm canonical dictionary graph representation training sample speech signal orthographic transcription;speech database;oral communication;educational products;canonical dictionary;graphics;training algorithm	In the framework of the German Verbmobil project we developed a procedure for the automatic, data driven generation of pronunciation dictionaries for speech recognition systems. In most recognizers only simple dictionaries containing the canonical pronunciation form are used. They represent the correct pronunciation, but in most cases the canonical pronunciation does not match the actual realization of the word. To solve this problem we chose an approach to derive pronunciation variants automatically from a speech database. The training algorithm bases on a canonical dictionary which is compiled into a graph representation in a first stage. Pronunciation variants are then learned from a training sample consisting of speech signal and its orthographic transcription. In this paper we will focus on the experimental results obtained in the Verbmobil framework and introduce methods to evaluate pronunciation dictionaries generated by the training procedure.	algorithm;compiler;dictionary;finite-state machine;graph (abstract data type);medical transcription;orthographic projection;speech recognition;transcription (software);verbmobil	Matthias Eichner;Matthias Wolff	2000		10.1109/ICASSP.2000.862075	natural language processing;speech recognition;computer science;graphics;graph theory;lattice;graph;natural language	NLP	-20.361760324953902	-85.94911897645937	122306
435ffa0e868b4e7502384c12203ab0df0ecdc1b2	context-sensitive error correction: using topic models to improve ocr	context-sensitive error correction;lary model;low-confidence output;rect mistake;improve ocr;mantic context;important step;misrecognized charac;global word distribution;human interaction;modern optical character recognition;topic models;simulated ocr correction task;optical character recognition;error correction	Modern optical, character recognition software relies on human interaction to correct mis recognized characters. Even though the software often reliably identifies low-confidence output, the simple language and vocabulary models employed are insufficient to automatically correct mistakes. This paper demonstrates that topic models, which automatically detect and represent an article's semantic context, reduces error by 7% over a global word distribution in a simulated OCR correction task. Detecting and leveraging context in this manner is an important step towards improving OCR.	error detection and correction;optical character recognition;qr code;topic model;vocabulary	Michael L. Wick;Michael G. Ross;Erik G. Learned-Miller	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.91	natural language processing;interpersonal relationship;error detection and correction;speech recognition;intelligent character recognition;computer science;intelligent word recognition;machine learning;pattern recognition;topic model;optical character recognition	Vision	-22.972088162643395	-80.60800543302787	122496
eacd60fa626b5d2ab0c1cfc55030bec88a58cd71	hybrid statistical/unit-selection turkish speech synthesis using suffix units	signal image and speech processing;hybrid speech synthesis;acoustics;statistical speech synthesis;mathematics in music;engineering acoustics;suffix selection;turkish;article	Unit selection based text-to-speech synthesis (TTS) has been the dominant TTS approach of the last decade. Despite its success, unit selection approach has its disadvantages. One of the most significant disadvantages is the sudden discontinuities in speech that distract the listeners (Speech Commun 51:1039–1064, 2009). The second disadvantage is that significant expertise and large amounts of data is needed for building a high-quality synthesis system which is costly and time-consuming. The statistical speech synthesis (SSS) approach is a promising alternative synthesis technique. Not only that the spurious errors that are observed in the unit selection system are mostly not observed in SSS but also building voice models is far less expensive and faster compared to the unit selection system. However, the resulting speech is typically not as natural-sounding as speech that is synthesized with a high-quality unit selection system. There are hybrid methods that attempt to take advantage of both SSS and unit selection systems. However, existing hybrid methods still require development of a high-quality unit selection system. Here, we propose a novel hybrid statistical/unit selection system for Turkish that aims at improving the quality of the baseline SSS system by improving the prosodic parameters such as intonation and stress. Commonly occurring suffixes in Turkish are stored in the unit selection database and used in the proposed system. As opposed to existing hybrid systems, the proposed system was developed without building a complete unit selection synthesis system. Therefore, the proposed method can be used without collecting large amounts of data or utilizing substantial expertise or time-consuming tuning that is typically required in building unit selection systems. Listeners preferred the hybrid system over the baseline system in the AB preference tests.	automatic sounding;baseline (configuration management);characterization test;hybrid system;speech synthesis	Cenk Demiroglu;Ekrem Güner	2016	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-016-0082-0	speech recognition;acoustics;physics	AI	-19.545295903898783	-84.30267130650789	122497
c8ec320e24039ede7d6833d79340b7db274925b5	a textual processor to handle atis queries	air travel information system;natural language;official airline guide;spontaneous speech	This paper describes the initial development of a natural language text processor, as the first step in an INRS dialogue-by-voice system. The eventual system will accept natural, spontaneous speech from users and produce responses from the databases in the form of synthetic speech. This paper reports results in processing the textual version of ATIS (Air Travel Information System) queries. The current system (programmed in C) accepts as input the cleaned-up text (SNOR) version of the spoken queries, and produces the desired Official Airline Guide (OAG) information as output. It uses only the words in the input text, and not any punctuation marks, on the assumption that such marks are difficult to obtain directly from speech input. Based on the training text data, the system correctly interprets a large majority of the textual queries. I N T R O D U C T I O N Speech recognition systems have made significant progress in recent years toward the goal of correctly interpreting continuously-spoken utterances. However, substantial restrictions are usually imposed upon the speaker, to guarantee success. Typically, one must either pause after each word, restrict one's choice of words to a small vocabulary, and/or train the system to adapt to one's voice. In many systems, it is not feasible to insist on vocabulary restrictions, nor can the system always be trained ahead of time to a user's voice. Many applications over the telephone to serve the general public will be of this latter type~ Furthermore, most users do not like altering their speaking style, and especially not speaking in isolated-word formant. Thus most practical applications of the future will have to be speaker-independent (i.e., trained ahead of time by other speakers), without major restrictions in vocabulary, and be able to accept normal, spontaneous speech. In particular, one major application is allowing the general public to do transactions directly with computer databases (including over the telephone). As an example of this type of interaction, we are currently examining a system to permit direct access for a user to air travel information. A user can pose natural questions to the database and receive answers just as a travel agent does. The database is that of the Official Airline Guide (OAG). To simplify the task slightly, we use a subset of the flights in the OAG: only those for airports at nine major US cities (Atlanta, Boston, Baltimore, Denver, Dallas, Oakland, Philadelphia, Pittsburgh, and San Francisco). Otherwise, the entire OAG database is used. In the future, we will investigate actual voice dialogues between a user and the database, but for now the subject of this study is limited to the analysis of individual queries by users. We wish to design an automatic system to correctly respond to the user with the desired OAG information. As a first step toward this goal, the current study is further limited to the analysis of textual versions of the user's utterances, rather than the speech itself. Thus we assume perfect operation of an initial speech recognizer, which would accept the spontaneous queries of a user and output the word sequence corresponding to the speech. Such word sequences can have grammatical mistakes and repeated words, as often occur in natural speech. Our textual processor must handle such deviations from normal written text that occur with spontaneous speech. In particular, this means that one cannot rely directly on standard English text processors, which presume grammatical input text. T E X T P R O C E S S I N G FOR OAG QUERIES The task of natural language processing (including deviations as found in spontaneous speech) is difficult (e.g., witness the difficulty of automatic machine translation of natural languages). However, we have simplified the task here by assuming that the user is querying the OAG database. Thus we have a good idea of the type of questions that will usually be asked, and of the typical subjects of those questions. We do not, however, know the format that any individual user may employ. Furthermore, each user is free to use one's own style of speaking and one's own choice of words. We staxt out with a vocabulary that includes all the words (including names) in the OAG database, and extend that vocabulary to include words discovered during training sessions with trial users. While one could theoretically access a dictionary of over 100,000 words (as might be found in a large English dictionary, augmented by the names found in the OAG database), such an approach is probably inefficient for this OAG application (especially if such a large vocabulary had to be searched in a full speech recognition task). Thus we have chosen to limit ourselves to a dictionary of about 700 words (with separate entries for parts of contractions, e.g., 're, 'll). We also employ a list of 47 common word suffixes (e.g., -s, -ed); unrecognized words with such endings have corresponding final letters removed before trying the dictionary again. For example, the word ~cities' is not in the dictionary; so the final -s is removed (and the 'ie' changed to 'y') to locate 'ci ty ' in the lexicon (the plural nature of the located noun is also	automatic transmitter identification system (television);central processing unit;data dictionary;database;finite-state machine;information system;lexicon;machine translation;natural language processing;random access;speech recognition;spontaneous order;synthetic intelligence;text corpus;vocabulary	Douglas D. O'Shaughnessy	1991			natural language processing;speech recognition;computer science;linguistics;natural language	NLP	-25.42437928113689	-82.67576363832963	122763
745506e828b9c037e072fccf912b264d085d4fa6	constraint based syntax of modifiers	grammar;syntactics grammar semantics context humans computational modeling presses;types;iterative modification grammar rule licence post premodifled phrase constraint based lexicalized grammar;constraint based lexicalized grammar;computer model;semantics;iterative methods computational linguistics grammars;presses;feature value structure;spurious syntactic ambiguity formal grammar constraint based lexicalized grammar modifiers feature value structure types constraints;modifiers;spurious syntactic ambiguity;iterative methods;grammars;computational modeling;syntactics;formal grammar;humans;computational linguistics;context;constraints	The paper introduces a technique for grammar rules that licence post-and pre-modified phrases, by using a generalized approach to constraint based lexicalized grammar. The rules generate iterative modification by avoiding spurious syntactic ambiguity. The rules of syntactic modification are lexically restricted by the lexical head of the modifier expression via a grammatical feature and its value.	iterative method;let expression;modifier key	Roussanka Loukanova	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.229	natural language processing;syntax;computer science;computational linguistics;phrase structure rules;grammar;semantics;iterative method;relational grammar;formal grammar;programming language;computational model	NLP	-31.188473541424468	-82.86859977956546	122954
0e4d042b668805e19f097b7eb0f223babec68f67	performance prediction for exponential language models	n-gram language model;training set performance;language model;test set performance;performance prediction;minimum discrimination information model;exponential language model;language model performance;class-based model;test set cross-entropy;exponential family;cross entropy;linear regression;information model	We investigate the task of performance prediction for language models belonging to the exponential family. First, we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models. We build models over varying domains, data set sizes, and n-gram orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics. Remarkably, we find a simple relationship that predicts test set performance with a correlation of 0.9997. We analyze why this relationship holds and show that it holds for other exponential language models as well, including class-based models and minimum discrimination information models. Finally, we discuss how this relationship can be applied to improve language model performance.	cross entropy;information model;kullback–leibler divergence;language model;n-gram;performance prediction;test set;time complexity	Stanley F. Chen	2009			exponential family;information model;computer science;linear regression;machine learning;exponential random graph models;cross entropy;statistics;language model	ML	-22.477410395395246	-89.52763617254546	123058
16dc6366b1de9632b79a0306d56284d2cd5138b7	a contextual postprocessing system for error correction using binary n-grams	information science;contextual information;natural languages;character recognition context contextual post processor dictionary error correction error detection pattern classification pattern recognition positional binary n grams;positional binary n grams;error analysis;error correction;dictionaries;pattern classification;statistics;dictionary;pattern recognition;humans;error detection;character recognition;context;contextual post processor	The effectiveness of various forms of contextual information in a postprocessing system for detection and correction of errors in words is examined. Various algorithms utilizing context are considered, from a dictionary algorithm which has available the maximum amount of information, to a set of contextual algorithms utilizing positional binary n-gram statistics. The latter information differs from the usual n-gram letter statistics in that the probabilities are position-dependent and each is quantized to 1 or 0, depending upon whether or not it is nonzero. This type of information is extremely compact and the computation for error correction is orders of magnitude less than that required by the dictionary algorithm.	algorithm;computation;dictionary;error detection and correction;grams;n-gram;qr code;tag (game)	Edward M. Riseman;Allen R. Hanson	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223971	error detection and correction;speech recognition;information science;computer science;machine learning;pattern recognition;statistics	Vision	-22.98651793064782	-80.35131759277282	123168
1536b502746e7556c5f9e41b77ed21307360cf96	the casia statistical machine translation system for iwslt 2008	statistical machine translation;spoken language translation	This paper describes our statistical machine translation system (CASIA) used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. In this year's evaluation, we participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Here, we mainly introduce the overview of our system, the primary modules, the key techniques, and the evaluation results.	statistical machine translation	Yanqing He;Jiajun Zhang;Maoxi Li;Licheng Fang;Yufeng Chen;Yu Zhou;Chengqing Zong	2008			natural language processing;speech recognition;computer science;evaluation of machine translation;linguistics;machine translation;machine translation software usability	NLP	-24.09819963970292	-82.99529099932782	123347
395f4b41578c3ff5139ddcf9e90eb60801b50394	statistical language modeling using the cmu-cambridge toolkit	cmu;statistical language model;language model	The CMU Statistical Language Modeling toolkit was re leased in in order to facilitate the construction and testing of bigram and trigram language models It is currently in use in over academic government and industrial laboratories in over countries This paper presents a new version of the toolkit We outline the con ventional language modeling technology as implemented in the toolkit and describe the extra e ciency and func tionality that the new toolkit provides as compared to previous software for this task Finally we give an exam ple of the use of the toolkit in constructing and testing a simple language model	bigram;language model;naruto shippuden: clash of ninja revolution 3;trigram	Philip Clarkson;Ronald Rosenfeld	1997			natural language processing;speech recognition;linguistics;modeling language	NLP	-23.88308119817497	-83.40214102241595	123393
cd2fa0b55e262901b7fe892bf54534cfd583498e	an approach to intelligent speech production system	information processing;text to speech;text generation;speech production	In the paper an intelligent speech production system is established by using language information processing technology. The concept of bi-directional grammar is proposed in Chinese language information processing and a corresponding Chinese characteristic network is completed. Correct text can be generated through grammar parsing and some additional rules. According to the generated text the system generates speech which has good quality in naturalness and intelligibility using Chinese Text-to-Speech Conversion System.	information processing;intelligibility (philosophy);parsing;production system (computer science)	Fang Chen;Baozong Yuan	1997	Journal of Computer Science and Technology	10.1007/BF02951338	voice activity detection;natural language processing;speech technology;speech production;speech recognition;information processing;speech corpus;computer science;noisy text analytics;speech processing;chinese speech synthesis;speech synthesis;speech analytics	NLP	-23.4750226927719	-84.03804990328845	123531
d36d4ae7cbd512e5906935a39035624574ae6487	automatic text generation by learning from literary structures		Most of the work dealing with automatic story production is based on a generic architecture for text generation; however, the resulting stories still lack a style that can be called literary. We believe that in order to generate automatically stories that could be compared with those by human authors, a specific methodology for fiction text generation should be defined. We also believe that it is essential for a story to convey the effect of originality to the person who is reading it. Our methodology proposes corpus-based generation of stories that could be called creative and also have a style similar to human fiction texts. We also show how these stories have plausible syntax and coherence, and are perceived as interesting by human evaluators.	algorithm;coherence (physics);experiment;intentionality;mind;natural language generation;structured text;text corpus	Angel Daza;Hiram Calvo;Jesús Figueroa-Nazuno	2016			natural language processing;linguistics	NLP	-32.84275520281231	-80.25529612859005	123687
578b272419a469cd354ca7e15268aee56e363ba3	the cmu-avenue french-english translation system	seventh workshop;system building;training data selection;statistical machine translation;french-english translation system;carnegie mellon university;avenue research group;hierarchical phrase-based translation system;data size;cmu-avenue french-english translation system;naacl wmt12	This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12). We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.	baseline (configuration management);best practice;statistical machine translation	Michael J. Denkowski;Greg Hanneman;Alon Lavie	2012			speech recognition;computer science;data science;machine translation;machine translation software usability	NLP	-24.2816914950106	-83.0009721233575	123886
1166374546625be512ae5563ebf561bc2525fc49	clustering voices in the waste land		T.S. Eliot’s modernist poem The Waste Land is often interpreted as collection of voices which appear multiple times throughout the text. Here, we investigate whether we can automatically cluster existing segmentations of the text into coherent, expert-identified characters. We show that clustering The Waste Land is a fairly difficult task, though we can do much better than random baselines, particularly if we begin with a good initial segmentation.	cluster analysis;coherence (physics);waste	Julian Brooke;Graeme Hirst;Adam Hammond	2013			artificial intelligence	NLP	-25.649858835580545	-80.93623080989404	123980
683fb43d3de8499b78665b2613a0383365952b6a	improving spoken language understanding using word confusion networks	natural language;automatic speech recognition	A natural language spoken dialog system includes a large vocabulary automatic speech recognition (ASR) engine, whose output is used as the input of a spoken language understanding component. Two challenges in such a framework are that the ASR component is far from being perfect and the users can say the same thing in very different ways. So, it is very important to be tolerant to recognition errors and some amount of orthographic variability. In this paper, we present our work on developing new methods and investigating various ways of robust recognition and understanding of an utterance. To this end, we exploit word-level confusion networks (sausages), obtained from ASR word graphs (lattices) instead of the ASR 1-best hypothesis. Using sausages with an improved confidence model, we decreased the calltype classification error rate for AT&T’s How May I Help You (HMIHY ) natural dialog system by 38%.	automated system recovery;dialog system;natural language understanding;orthographic projection;spatial variability;speech recognition;spoken dialog systems;statistical classification;vocabulary	Gökhan Tür;Jeremy H. Wright;Allen L. Gorin;Giuseppe Riccardi;Dilek Z. Hakkani-Tür	2002			confusion;speech recognition;linguistics;habit;natural language;argyranthemum;colored;spoken language;computer science	NLP	-19.793006218741215	-84.73164733320938	124044
26bb7dccd42d8b789037ecd07235912b613b0926	unsupervised stream-weights computation in classification and recognition tasks	unsupervised learning;multistream weights estimation;modelizacion;robust speech recognition;reliability;document audiovisuel;background modeling;procesamiento informacion;funcion no lineal;learning;etude theorique;classification non supervisee;speech processing;tratamiento palabra;traitement parole;non linear function;speech;arriere plan;testing;data fusion;indexing terms;robust speech recognition decision fusion multistream weights estimation;estimation algorithm;audio visual speech recognition;linear functionals;unsupervised learning nonlinear functions signal classification speech recognition;algorithme;aprendizaje;modelisation;algorithm;nonlinear functions;distance measurement;apprentissage;background;adaptation model;model error;reconocimiento voz;machine learning unsupervised stream weights computation stream classification problem nonlinear function audiovisual speech classification audiovisual speech recognition;erreur estimation;fusion donnee;documento audiovisual;clasificacion no supervisada;information processing;signal classification;classification algorithms;estudio teorico;decision fusion;fonction non lineaire;audiovisual document;classification error;classification signal;error estimacion;unsupervised classification;speech recognition;reconnaissance parole;estimation error;theoretical study;traitement information;fusion datos;modeling;streaming media speech recognition signal processing algorithms estimation error pattern recognition automatic speech recognition signal to noise ratio speech analysis machine learning algorithms testing;algoritmo	In this paper, we provide theoretical results on the problem of optimal stream weight selection for the two stream classification problem. It is shown that in the presence of estimation or modeling errors using stream weights can decrease the total classification error. Specifically, we show that stream weights should be selected to be proportional to the feature stream reliability and informativeness. Next, we turn our attention to the problem of unsupervised stream weights computation in real tasks. Based on the theoretical results we propose to use models and ldquoanti-modelsrdquo (class-specific background models) to estimate stream weights. A nonlinear function of the ratio of the inter- to intra-class distance is proposed for stream weight estimation. The resulting unsupervised stream weight estimation algorithm is evaluated on both artificial data and on the problem of audiovisual speech classification. Finally, the proposed algorithm is extended to the problem of audiovisual speech recognition. It is shown that the proposed algorithms achieve results comparable to the supervised minimum-error training approach for classification tasks under most testing conditions.	acoustic cryptanalysis;acoustic model;algorithm;authorization;cluster analysis;computation;experiment;human body weight;ieee xplore;multiclass classification;nonlinear system;speech recognition;stream cipher;supervised learning;unsupervised learning;ur, ur/web	Eduardo Sánchez-Soto;Alexandros Potamianos;Khalid Daoudi	2009	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2008.2011513	unsupervised learning;data stream clustering;speech recognition;systems modeling;index term;information processing;computer science;speech;machine learning;pattern recognition;errors-in-variables models;reliability;speech processing;sensor fusion;software testing	ML	-21.21031006522796	-91.29229738577278	124668
cf93d391a20e95e3d12f95bf987fed28d6f6a3a5	discriminative training of dynamic programming based speech recognizers	dynamic programming;dynamic programming speech recognition error analysis speech analysis character recognition artificial neural networks probability distribution algorithm design and analysis error correction vocabulary;generalized probabilistic descent;dynamic program;highly confusible english e set dynamic programming minimum recognition error formulation generalized probabilistic descent discriminative training speech recognizer optimization dynamic time warping linear discriminant function dtw distortion sequence speaker independent recognition;speaker independent;linear discriminant function;error rate;speech recognition;discriminative training;learning artificial intelligence;dynamic time warping;speech recognition dynamic programming learning artificial intelligence	AbstrucfIn this paper, a newly proposed minimum recognition error formulation and a generalized probabilistic descent (GPD) algorithm are analyzed and employed to accomplish discriminative training of a conventional dynamic programming based speech recognizer. Unlike many other approaches, the objective of discriminative training in the new framework is to directly minimize the recognition error rate. Such an attempt, i.e., direct minimization of the recognition error rate, has never been addressed due to the fact that the error rate (or error count) of a given finite set of data is a piecewise-constant function of the recognizer parameters and thus cannot be easily optimized. In order to achieve minimum recognition error, a formulation that allows controlled approximation to the exact error rate and renders optimization possible is discussed. Specifically, we present an implementation of the GPD method in a dynamic time warping (DTW) based system. A modification, besides discriminative training of the reference templates, is made in that a linear discriminant function on the DTW distortion sequence is used to replace the conventional average DTW path distance. A series of speaker independent recognition experiments using the highly confusible English E-set as the vocabulary were conducted to examine the characteristics of the GPD method for discriminative training. Without ad hoc supplementary schemes, the method achieved a recognition rate of 84.4%, a remarkable performance improvement compared to 60% with the traditional template training via clustering. Furthermore, the experimental results verify that the GPD algorithm with the new minimum recognition error formulation indeed converges to a solution that accomplishes the objective of minimum error rate.	algorithm;approximation;cluster analysis;constant function;discriminative model;distortion;dynamic programming;dynamic time warping;experiment;finite-state machine;gradient descent;hoc (programming language);iso/iec 11404;linear discriminant analysis;mathematical optimization;rendering (computer graphics);speech recognition;speech synthesis;vocabulary	Pao-Chung Chang;Biing-Hwang Juang	1993	IEEE Trans. Speech and Audio Processing	10.1109/89.222873	speaker recognition;speech recognition;word error rate;computer science;machine learning;dynamic programming;dynamic time warping;pattern recognition	Vision	-19.264853657842405	-91.90022302771307	124950
f1eefd8a6ce6f5c492d5d87b64cdf1d83c959ec9	problems of music information retrieval in the real world	information retrieval;musica;problems;searching;large scale;musique;recherche information;audio;midi;music information retrieval;approximate matching;recuperacion informacion;relevance information retrieval;computer interfaces;human perception;music;notation	Although a substantial number of research projects have addressed music information retrieval over the past three decades, the field is still very immature. Few of these projects involve complex (polyphonic) music; methods for evaluation are at a very primitive stage of development; none of the projects tackles the problem of realistically large-scale databases. Many problems to be faced are due to the nature of music itself. Among these are issues in human perception and cognition of music, especially as they concern the recognizability of a musical phrase. This paper considers some of the most fundamental problems in music information retrieval, challenging the common assumption that searching on pitch (or pitch-contour) alone is likely to be satisfactory for all purposes. This assumption may indeed be true for most monophonic (single-voice) music, but it is certainly inadequate for polyphonic (multi-voice) music. Even in the monophonic case it can lead to misleading results. The fact, long recognized in projects involving monophonic music, that a recognizable passage is usually not identical with the search pattern means that approximate matching is almost always necessary, yet this too is severely complicated by the demands of polyphonic music. Almost all text-IR methods rely on identifying approximate units of meaning, that is, words. A fundamental problem in music IR is that locating such units is extremely difficult, perhaps impossible.	approximation algorithm;cognition;database;information retrieval;regular expression;text-based user interface	Donald Byrd;Tim Crawford	2002	Inf. Process. Manage.	10.1016/S0306-4573(01)00033-4	midi;speech recognition;computer science;machine learning;notation;music;linguistics;multimedia;world wide web;information retrieval	Web+IR	-24.5004651249833	-89.05986303364277	125046
3b512b47cc80ab8e5bc8b821a36514ed6428edf8	independent automatic segmentation by self-learning categorial pronunciation rules		The goal of this paper is to present a new method to automatically generate pronunciation rules for automatic segmentation of speech the German MAUSER system. MAUSER is an algorithm which generates pronunciation rules independently of any domain dependent training data either by clustering and statistically weighting self-learned rules according to a small set of phonological rules clustered by categories or by re-weighting “seen” phonological rules. By this method we are able to automatically segment cost-effectively large corpora of mainly unprompted speech.	algorithm;categorial grammar;cluster analysis;text corpus	Nicole Beringer	2003			artificial intelligence;segmentation-based object categorization;scale-space segmentation;pattern recognition;computer science;pronunciation;segmentation	NLP	-21.130015351818603	-81.13517370841925	125160
4be254bc3d0fdc751261eb06686e0700baf8484a	measuring domain similarity for statistical machine translation	training;language translation;frequency measurement;training data;business;transportation;adaptation models;statistical machine translation smt domain adaptation domain similarity;domain similarity measurement skew divergence relative word frequency similarity functions feature representations cosine similarity function training domain test domain statistical machine translation;data models;training adaptation models training data frequency measurement data models business transportation	It is well known that the statistical machine translation (SMT) performance suffers when a model is applied to out-of-domain data. It is also known that the more similar the test domain and the training domain are, the more efficient the training data are for SMT performance. Hence, measuring the similarity of domains is an important task to select appropriate training data. The most widely used method uses the cosine similarity function and word frequency. The lack of exploring other approaches motivates us to propose and compare several similarity measures. Aiming for better SMT performance, we compared 10 similarity measures, which are a combination of 2 feature representations and 5 similarity functions. The results show that using the relative word frequency as the feature representation and using the skew divergence as the similarity function performs the best amongst the 10 measures and outperforms random data selection.	cosine similarity;domain adaptation;experiment;randomness;similarity measure;statistical machine translation;word lists by frequency	Lin Liu;Hailong Cao;Tiejun Zhao	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816269	data modeling;training set;transport;speech recognition;computer science;machine learning;pattern recognition	NLP	-21.945565518256977	-89.53672744405591	125408
274b94881f0dcc7d5c0892f098ee81ddd08b697c	pragma - a flexible bidirectional dialogue system		This paper gives an overview of a natural language dialogue system called PRAGMA. This system contains a number of novel and important features, as well as integrating previous work into a unified mechanism. The most important advance that PRAGMA represents compared with previous systems is the high degree of bidirectional@ employed in its design. A single grammar is used for interpretation and generation, and the same knowledge sources are used for plan recognition and response generation. The system is also flexible, in that it generates useful extended responses, not only to queries which allow the user’s plan to be inferred, but also to queries which do not allow this.	dialog system;dialog tree;directive (programming);natural language	John Levine	1990				AI	-31.640006477048892	-80.90889321417632	125458
83fc9056bcb0a79e54b3b43463a79715a9142ff6	finite state automata and arabic writing	right shape;contextual rule;arabic writing;moore automaton;contextual analysis;careful linguistic analysis;finite state automaton;arabicized software;computational overload;complex problem;finite state automata	Arabic writing has specific features, which imply computational overload for any arabicized software. Finite state automata are well known to give efficient solutions for translation problems which can be formalized as regular languages. These automata are as more easily built that their alphabet have been reduced through a careful linguistic analysis. This reduction makes it possible to write directly an automaton without going through the intermediate stage of contextual rules, which have to be translated into an automaton for the sake of efficiency. This paper presents two Moore automata, the first one, taken as an example, gives a solution to the choice of right shape for a letter to be printed or displayed (usually known as contextual analysis), the second one studies the more complex problem of determining the right carrying letter for hamza. Every arabicized software has to face these questions and finite state automata are certainly a good answer to them. I N T R O D U C T I O N Arabic writing has specific features, which imply computational overload for any arabicized software. The first one, well known now for many years, is the fact that Arabic printing tries to imitate handwriting. Because of this, consonants and long vowels can have four or only two shapes depending of their ability to be bound to the following letter and of where they appear in the word. These shapes can be very different : for example letter o 2 (h) ICERTAL : Centre d'l~tudes et de Recherche en Traitement Automatique des Langues, I N A L C O : Institut National des Langues et Civilisations Orientales ~the Arabic parts of this paper have been typeset isolated final medial initial or present only small variations : for example letter ~r* (s) isolated final medial initial Letters which cannot be bound to the next one have only two shapes, for example letters (d) and .~ (w and fi) isolated final isolated final During the seventies and the beginning of the eighties, hard controversies took place within the Arabs concerned with these questions, linguists and computer scientists. Finally in 1983 the ASMO (Arab Society for Normalization which unfortunately does not exist any more), influenced by Pr. Lakhdar-Ghazal from IERA (Rabat Morocco) chose to give a unique code to all shapes of one particular letter. This is certainly a good choice from a linguistic point of view, but even so, compromises had to be made to take into account writing habits that conflicted with it. Letter hamza is the most noticeable example of such a compromise for reasons we shall explain later. 1 C O N T E X T U A L A N A L Y S I S Whatever be the choice made for coding, from a typesetting or a computational point of view, there must be different codes for the different shapes of a letter. So every arabicized software has to use two systems for coding : the reduced code we have just introduced and the extended code in which the different shapes have different using Klaus Lagally's ArabTEX	automaton;bibliothèque de l'école des chartes;bibliothèque des ecoles françaises d'athènes et de rome;code;computation;computer scientist;finite-state machine;medial graph;moore machine;point of view (computer hardware company);printing;regular language	Michel Fanton	1998			arithmetic;nondeterministic finite automaton with ε-moves;nondeterministic finite automaton;quantum finite automata;computer science;nested word;artificial intelligence;two-way deterministic finite automaton;deterministic finite automaton;deterministic automaton;automata theory;ω-automaton;dfa minimization;mobile automaton;algorithm	Theory	-32.476117183689006	-85.40623887452696	125683
17b0f147a7d8b0060f944d4eb110d3f559f63531	naturalistic dialogue management for noisy speech recognition	human computer interaction;human computer interaction speech recognition decision trees context machine learning robustness semantics;machine learned models naturalistic dialogue management noisy speech recognition spoken dialogue system naturalistic clarification strategies wizard of oz corpus wizard data clarification decisions clarification module library domain decision tree;speech recognition;learning artificial intelligence;speech recognition decision trees human computer interaction learning artificial intelligence;decision trees;system performance human computer interaction machine learning robustness speech	With naturalistic dialogue management, a spoken dialogue system behaves as a human would under similar conditions. This paper reports on an experiment to develop naturalistic clarification strategies for noisy speech recognition in the context of spoken dialogue systems. We collected a wizard-of-Oz corpus in which human wizards with access to a rich set of clarification actions made clarification decisions online, based on human-readable versions of system data. The experiment compares an evaluation of calls to a baseline system in a library domain with calls to an enhanced version of the system. The new system has a clarification module based on the wizard data that is a decision tree constructed from three machine-learned models. It replicates the wizards' ability to ground partial understandings of noisy input and to build upon them. The enhanced system has a significantly higher rate of task completion, greater task success and improved efficiency.	baseline (configuration management);decision tree;dialog system;evaluation function;human-readable medium;interpretation (logic);reinforcement learning;robot learning;speech recognition;spoken dialog systems;usability;wizard (software)	Rebecca J. Passonneau;Susan L. Epstein;Tiziana Ligorio	2012	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2012.2229964	natural language processing;speech recognition;computer science;machine learning;decision tree	NLP	-27.07220709317891	-86.36825027476982	125834
5ef18eef1707daf98361e52fa7a06c1dcf6aa42d	neural-network based measures of confidence for word recognition	estimation theory;automatic speech recognition neural networks speech recognition laboratories speaker recognition databases loudspeakers acoustic applications adaptation model bars;multilayer perceptrons speech recognition estimation theory;confidence measure;multilayer perceptrons;switchboard corpus neural network based confidence measures word recognition probabilistic framework knowledge sources word hypothesis joint performance;word recognition;speech recognition;neural network	This paper proposes a probabilistic framework to de ne and evaluate con dence measures for word recognition. We describe a novel method to combine di erent knowledge sources and estimate the con dence in a word hypothesis, via a neural network. We also propose a measure of the joint performance of the recognition and con dence systems. The de nitions and algorithms are illustrated with results on the Switchboard Corpus.	algorithm;artificial neural network;cross entropy;mean squared error;naruto shippuden: clash of ninja revolution 3;speech corpus;telephone switchboard	Mitch Weintraub;Françoise Beaufays;Zeév Rivlin;Yochai Konig;Andreas Stolcke	1997		10.1109/ICASSP.1997.596078	natural language processing;speaker recognition;speech recognition;word recognition;word error rate;computer science;machine learning;pattern recognition;time delay neural network;mathematics;estimation theory;artificial neural network;statistics	ML	-19.31958504998535	-90.494054111377	125836
73600a3ce3733d6569824ebd16269a40f0fcaff5	enhanced spoken sentence retrieval using a conventional automatic speech recognizer in smart home	query term generation;spoken sentence retrieval;re ranking	With the rapid evolution of smart home environment, the demand for spoken information retrieval (e.g., voice-activated FAQ retrieval) on information appliances is increasing. In spoken information retrieval, users’ spoken queries are converted into text queries using automatic speech recognition (ASR) engines. If top-1 results of the ASR engines are incorrect, the errors are propagated to information retrieval systems. If a document collection is a small set of sentences such as frequently asked questions (FAQs), the errors have additional effect on the performance of information retrieval systems. To improve the performance of such a sentence retrieval system, we propose a post-processing model of an ASR engine. The post-processing model consists of a re-ranking and a query term generation model. The re-ranking model rearranges top-n outputs of the ASR engines using the ranking support vector machine (Ranking SVM). The query term generation model extracts meaningful content words from the re-ranked queries based on term frequencies and query rankings. In the experiments, the re-ranking model improved the top-1 performance results of an underlying ASR engine with 4.4% higher precision and 6.4% higher recall rate. The query term generation model improved the performance results of an underlying information retrieval system with an accuracy 2.4% to 2.6% higher. Based on the experimental result, the proposed model revealed that it could improve the performance of a spoken sentence retrieval system in a restricted domain.	finite-state machine;home automation;speech recognition;speech synthesis	Hyeokju Ahn;Harksoo Kim	2016	International Journal on Artificial Intelligence Tools	10.1142/S0218213016500172	natural language processing;document retrieval;query expansion;visual word;ranking;speech recognition;computer science;vector space model;information retrieval;query language	NLP	-22.006375269485535	-83.19689084653699	125851
1d08e27ea8ff4218f4865326128bcfdb0618685e	i-vector based language modeling for query representation	analytical models;vectors data structures document handling indexing multimedia computing query processing speaker recognition;information retrieval;language modeling;semantics;i vector;spoken document retrieval;maximum likelihood estimation;speaker recognition;indexes information retrieval speaker recognition analytical models semantics maximum likelihood estimation speech recognition;indexes;query representation;query representation spoken document retrieval i vector language modeling;speech recognition;i vector based language modeling framework topic detection and tracking collection tdt 2 collection subword level units word level units speaker recognition language identification query formulation improvement spoken document representation indexing technique sdr spoken document retrieval multimedia data query representation ivlm framework	Since more and more multimedia data associated with spoken documents have been made available to the public, spoken document retrieval (SDR) has become an important research subject in the past two decades. Following the research tendency, many efforts have been devoted towards developing indexing and modeling techniques for representing spoken documents, but only few have been made on improving query formulation for better representing users' information needs. The i-vector based language modeling (IVLM) framework, stemming from the state-of-the-art i-vector framework for language identification and speaker recognition, has been proposed and formulated to represent documents in SDR with good promise recently. However, a major challenge of using IVLM for query modeling is that a query usually consists of only a few words; thus, it is hard to learn a reliable representation accordingly. In this paper, we focus our attention on query reformulation and propose three novel methods on top of IVLM to more accurately represent users' information needs. In addition, we also explore the use of multi-levels of index features, including word- and subword-level units, to work in concert with the proposed methods. A series of empirical SDR experiments conducted on the TDT-2 (Topic Detection and Tracking) collection demonstrate the good effectiveness of our proposed methods as compared to existing state-of-the-art methods.	document retrieval;etsi satellite digital radio;experiment;information needs;language identification;language model;speaker recognition;stemming;substring	Kuan-Yu Chen;Hsin-Min Wang;Berlin Chen;Hsin-Hsi Chen	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178965	natural language processing;database index;speaker recognition;query optimization;query expansion;web query classification;ranking;speech recognition;computer science;pattern recognition;semantics;maximum likelihood;rdf query language;query language	DB	-23.35366717346447	-88.90719419107175	126119
e0cbf4b91baa36b0f5afb23eab68405e17048373	compression of multilingual aligned texts	terminology dictionaries computer science natural languages encoding concurrent computing testing degradation decoding data compression;linguistics data compression encoding;data compression;european union multilingual aligned text compression storage space alignment algorithm english french corpus;english french corpus;text compression;multilingual aligned text compression;storage space;alignment algorithm;european union;encoding;linguistics	Summary form only given. Multilingual text compression depends primarily on the ability to match the corresponding parts of related texts by identifying semantic correspondences across the various sub-texts, a task generally referred to as text alignment. Savings in storage space can be obtained by replacing words and phrases with pointers to their translations, determined by any alignment algorithm. The suggested method was tested on an English-French corpus of the European Union. The French part was compressed using pointers towards the English part. The obtained compression rate (22.0%) is similar to the performances of Bzip and HuffWord and better than that of Gzip. However, Bzip and Gzip's performances degrade when small sub-sections are processed separately, which makes them inappropriate for systems which often decode only small pieces	algorithm;data compression;dictionary coder;netbsd gzip / freebsd gzip;performance;pointer (computer programming);text corpus;bzip2	Ehud S. Conley;Shmuel Tomi Klein	2006	Data Compression Conference (DCC'06)	10.1109/DCC.2006.15	data compression;natural language processing;speech recognition;computer science;theoretical computer science;encoding;statistics	NLP	-23.049544071017593	-81.8320290885579	126162
fba7ad8f63a42111b3618e51e3493ed70aafdcd0	learning influences from word use in polylogue		We propose a probabilistic model for estimating influences among speakers from conversation data with multiple people. In conversations, people tend to mimic their companions’ behavior depending on their level of trust. With the proposed model, we assume that the word use of a speaker depends on the word use of previous speakers as well as their own earlier word use and the general word distribution. The influences can be efficiently estimated by using the expectation maximization (EM) algorithm. Experiments on two meeting data sets in Japanese and in English demonstrate the effectiveness of the proposed method.	expectation–maximization algorithm;statistical model	Tomoharu Iwata;Shinji Watanabe	2011			expectation–maximization algorithm;conversation;artificial intelligence;pattern recognition;statistical model;data set;computer science	Web+IR	-20.08045779005206	-88.7638540901632	126594
6edb7dfe174731a4161e6867da6476478d4c0b50	lexicon optimization for wfst-based speech recognition using acoustic distance based confusability measure and g2p conversion		In this paper, we propose a lexicon optimization method based on a confusability measure (CM) to develop a large vocabulary continuous speech recognition (LVCSR) system with unseen words. When a lexicon is built or expanded for unseen words by using grapheme-to-phoneme (G2P) conversion, the lexicon size increases since G2P is generally realized by 1-to-N-best mapping. Thus, the proposed method attempts to prune the confusable words in the lexicon by a CM defined as an acoustic model distance between two phonemic sequences. It is demonstrated by LVCSR experiments that the proposed lexicon optimization method achieves a relative word error rate (WER) reduction of 14.72% in a Wall Street Journal task compared to the 1-to-4-best G2P converted lexicon approach.	acoustic cryptanalysis;acoustic model;automated system recovery;experiment;lexicon;mathematical optimization;mobile device;speech analytics;speech recognition;spontaneous order;the wall street journal;vocabulary;word error rate;dialog	Nam Kyun Kim;Woo Kyeong Seong;Hong Kook Kim	2015		10.1007/978-3-319-19291-8_12	natural language processing;speech recognition;pattern recognition	NLP	-19.568761817071728	-85.57421909896252	126920
535577d3414dd341e1117c8f0f10655ec7417fc9	a pattern-based analyzer for french in the context of spoken language translation: first prototype and evaluation	chosen situation;trentino area;italian travel agent;french client;instantiated argument;language translation;speech recognition module;dialogue act;argument value;pattern-based analyzer;current version;speech recognition	"""In this paper, we describe a first prototype of a pattern-based analyzer developed in the context of a speech-to-speech translation project using a pivot-based approach (the pivot is called IF). The chosen situation involves a French client talking to an Italian travel agent (both in their own language) to organize a stay in the Trentino area. An IF consists of a dialogue act, and a list, possibly empty, of argument values. The analyzer applies a """"phrase spotting"""" mechanism on the output of the speech recognition module. It finds well-formed phrases corresponding to argument values. A dialogue act is then built according to the instantiated arguments and some other features of the input. The current version of the prototype has been involved in an evaluation campaign on an unseen corpus of four dialogues consisting of 235 speech turns. The results are given and commented in the last part of the paper. We think they pave the way for future enhancements to both the coverage and the development methodology."""	machine translation;prototype;speech recognition;well-formed formula	Hervé Blanchon	2002			natural language processing;speech recognition;computer science;linguistics	NLP	-25.956641028559247	-84.83744917778496	126966
d291f9c39525460c68118b2beb100ac002f6e495	user-guided system development in interactive spoken language education	development;interactive;system;guided;system development;user;spoken;language;user involvement	"""This paper is a Case Study of user involvement in the requirements speciication for project ISLE: Interactive Spoken Language Education. Developers of Spoken Language Dialogue Systems should involve users from the outset, particularly if the aim is to develop novel solutions for a generic target application area or market. As well as target end-users, SLDS developers should identify and consult """"meta-level"""" domain experts with expertise in human-to-human dialogue in the target domain. In our case, English language teachers and publishers provided generic knowledge of learners' dialogue preferences; other applications have analogous domain language experts. These domain language experts can help to pin down a domain-speciic sublanguage which ts the constraints of current speech recognition technology: linguistically-naive end-users may expect unconstrained conversational English, but in practice dialogue interactions have to be constrained in vocabulary and syntax. User consultation also highlighted a need to consider how to integrate speech input and output with other modes of interaction and processing; in our case the input speech signal is processed by speech recogniser, stress and mispronunciation detectors, and output responses are text and graphics as well as speech. This suggests a need to revisit the deenition of """"dialogue"""": other SLDS developers should also consider the merits of multimodality as an adjunct to pure spoken language dialogue, particularly given that current systems are not capable of accurately handling unconstrained English."""	computer;dialog system;emoticon;enlightenment foundation libraries;feedback;graphics;input/output;interaction;interpretation (logic);lexicon;prospective search;requirement;requirements analysis;sensor;speech recognition;sublanguage;user requirements document;vocabulary	Eric Atwell;Peter Howarth;Clive Souter;Patrizio Baldo;Roberto Bisiani;Dario Pezzotta;Patrizia Bonaventura;Wolfgang Menzel;Daniel Herron;Rachel Morton;Juergen A. Schmidt	2000	Natural Language Engineering	10.1017/S1351324900002473	natural language processing;user;speech recognition;computer science;system;linguistics;language;interactivity	NLP	-28.336188405229898	-84.69946109402666	127044
abba70c8d56e6f834d31e08d174d9e2c9b60e057	development of an approach to automatic language identification based on phone recognition	duration model;phoneme;systems;hidden markov model;performance;linguistique appliquee;phonem;system performance;identification de la langue;automatic recognition;prosodie;language identification;error rate;context dependent;computational linguistics;systeme;prosody;linguistique informatique;back propagation;language model;reconnaissance automatique;neural network;applied linguistics;phonotactique	Abstract   An Automatic Language Identification (LID) approach is presented. The baseline LID system consists of three parts: (1) hidden Markov model (HMM) based context-independent phone recognizers, (2) language identification score generators and (3) a linear language classifier. The system exploits language-dependent phonotactic constraints and prosodic information. Four methods are proposed to improve the system performance. Two bigram-based interpolated N-gram language models (forward and backward) are used to model the phone sequence constraints of different spoken languages. A context-dependent duration model interpolated by a context-independent duration model is used to capture the duration information. Comparison experiments between the linear classifier and neural network-based final classifiers were conducted. Finally, optimization of language model based on back propagation is proposed. The improved system was evaluated on an 11-language task, and performance reached 13·3% and 26·2% (error rate) for utterances averaging 45 s duration and 10 s duration, respectively. Compared with the baseline system performance, it shows the importance of the issues addressed in this paper for language identification.	language identification	Yonghong Yan;Etienne Barnard;Ronald A. Cole	1996	Computer Speech & Language	10.1006/csla.1996.0003	natural language processing;language identification;speech recognition;performance;word error rate;computer science;backpropagation;computational linguistics;machine learning;context-dependent memory;applied linguistics;system;linguistics;prosody;hidden markov model;language model	NLP	-19.437577979427655	-87.6121175330578	127151
43e28b41c9fb8a65aa32a426bb11569ab359fdd3	applying dialogue constraints to the understanding process in a dialogue system	europa;probleme confection horaire;problema concepcion horario;espana;man machine dialogue;telephone;train;tren;dialogo hombre maquina;comprehension langage specifique;espagne;europe;telefono;timetabling problem;dialogue homme machine;spain	In this paper, we present an approach to the estimation of a dialogue-dependent understanding component of a dialogue system. This work is developed in the framework of the Basurde Spanish dialogue system, which answers queries about train timetables by telephone in Spanish. A modelization which is specific to each dialogue state is proposed to improve the behaviour of the understanding process. Some experimental results are presented.	dialog system;dialog tree	Emilio Sanchis Arnal;Fernando García;Isabel Galiano;Encarna Segarra	2002		10.1007/3-540-46154-X_55	artificial intelligence;train	NLP	-27.224136820889317	-85.50440428647401	127270
072049f043d4c6b88762c73f77b0e4a2e721e2ca	improved phonotactic analysis in automatic language identification	integrated circuits error analysis speech recognition speech stability analysis matrices accuracy;speech;error analysis;accuracy;matrices;stability analysis;speech recognition;integrated circuits	This paper presents a method for phone-dependent weighting within phonotactic models in automatic language identification. Based on statistical analysis of the phonetic-recognizer behaviour, a phone confidence measure is derived and used to weight the bigram probabilities during testing. The confidence corresponds to the expected decoding stability of individual phones. The proposed method was shown to improve the system performance consistently on a three-language task. The best improvement of the error rate was from 8.4% to 1.8% for the 45-second utterances.	bigram;finite-state machine;language identification	Jirí Navrátil	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.405227124689294	-89.60383593310107	127371
70688e0267cc5bc8ad74bf74fcb39bb985aa6dad	understanding natural language instructions: the case of purpose clauses	natural language instruction;sheds light;instruction understanding;purpose clause;model relation;natural language	This paper presents an analysis of purpose clauses in the context of instruction understanding. Such analysis shows that goals affect the interpretation and / or execution of actions, lends support to the proposal of using generation and enablement to model relations between actions, and sheds light on some inference processes necessary to interpret purpose clauses.	interpretation (logic);natural language	Barbara Di Eugenio	1992			natural language processing;computer science;linguistics;natural language;programming language	NLP	-33.34029287795433	-80.40868522207514	127451
2b18729169d73ca52f9f73350fb9f275c8fe1d27	spoken language corpus for machine interpretation research	article author;interpretive research	This paper describes a database consisting of speech and language, which we are currently constructing for the purpose of the research on machine interpretation. The database contains bilingual data of lectures and dialogues. We have collected the speech of about 72 hours in total and transcribed it into the text manually. We have investigated the database in order to acquire empirical knowledge of human interpreting. In this paper, we report the characteristic features of spoken language by Japanese-to-English interpreters.	database;text corpus	Yasuyuki Aizawa;Shigeki Matsubara;Nobuo Kawaguchi;Katsuhiko Toyama;Yasuyoshi Inagaki	2000			natural language processing;speech recognition;computer science;corpus linguistics;linguistics	NLP	-24.808832825271818	-83.81951064188753	127734
550dfb062786c919eb8ab7e7ec35a4850562dafc	syntactic procedures for the detection of self-repairs in german dialogues	reconocimiento palabra;tratamiento lenguaje;intelligence artificielle;language processing;traitement langage;speech recognition;artificial intelligence;inteligencia artificial;reconnaissance parole;spoken language processing;spontaneous speech	Absbract. One problem of spoken language processing is the handling of self-interruptions and self-repairs in spontaneous speech. Within a sample of negotiation dialogues and free conversations 4300 self-repairs were collected. The repairs were classified by two kinds of covert repair (hesitations, word repetitions) and four kinds of overt repair (retracings, instant repairs, fresh starts, pivot constructions). Self repairs show syntactic regularities which can be used for automatic processing of spontaneous speech (automatic identification of a repair and automatic transformation into the correct utterance). 96% of the repairs are identified and transformed by eleven detection rules. For only 4% of the repairs the rules cannot he applied. For the detection of these rare cases prosodic cues have to be taken into account.		Bernd Tischer	1996		10.1007/3-540-63175-5_41	natural language processing;speech recognition;computer science;artificial intelligence	NLP	-25.820608781930694	-82.94264170832085	128593
51a4d9a7189c1b5a952cdbeb26379101fbb21478	design and analysis of a hyphenation procedure	root word;affixes;rejection rule;hyphenation;break value table;exception dictionary	Abstract#R##N##R##N#A hyphenation procedure is described wherein the search list includes exceptional words, prefixes, suffixes as well as a probabilistic Break-Value-Table. The list of prefixes and suffixes is augmented with what are termed as root words to achieve greater flexibility and accuracy. Importance is given to a number of ways whereby the overall algorithm can be speeded up; in this connection a number of rejection rules are formulated so that only the likely candidates are processed. The order of searching of the various data tables is also considered. A further refinement is tried wherein the common suffixes and common prefixes are given preferential treatment. The algorithm developed was tested on approximately 2,700 common English technical words and an attempt is made to analyse the incorrectly handled words.	hyphenation algorithm	A. W. Narwekar	1979	Softw., Pract. Exper.	10.1002/spe.4380090406	prefix;speech recognition;computer science;root;algorithm	SE	-26.487104696914123	-80.53875323515804	128808
764691f8a7ad444319912c4e1131f3d4d2db5fe6	evaluating productivity gains of hybrid asr-mt systems for translation dictation	integrated approach;words per minute;word error rate;statistical machine translation;statistical significance;automatic speech recognition;error correction;source language;off the shelf;language model	This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.	automated system recovery;automatic system recovery;domain adaptation;emoticon;error detection and correction;experiment;human factors and ergonomics;language model;metal mt;plover;speech recognition;statistical machine translation;vocabulary;word error rate;words per minute	Alain Désilets;Marta Stojanovic;Jean-François Lapointe;Rick Rose;Aarthi M. Reddy	2008			natural language processing;speech recognition;computer science;communication	HCI	-20.227645536684328	-83.47798769277962	128814
b5dfa3ecdef0c3fd50cd7098e91df266f20d29a4	geographic language models for automatic speech recognition		In this paper, we propose improving automatic speech recognition (ASR) accuracy for local points of interest (POI) by leveraging a geo-specific language model (Geo-LM). Geographic regions are defined according to U.S. Census Bureau Combined Statistical Areas. Depending on the user's associated geographic region, for each user a class based Geo-LM is constructerd dynamically within a difference-LM based weighted finite state transducer (WFST) system. The benefits of this approach include: improved accuracy for local POI name recognition, flexibility in training, and efficient LM construction at runtime. Our experiments show that the proposed Geo-Lm achieves an average of over 18 % relative word error rate (WER) reduction on the tasks of local POI search, with no degradation to the general accuracy and very limited latency increase, compared to the baseline nationwide general LM. In addition to accuracy improvement, we also discuss optimization of runtime efficiency.	algorithmic efficiency;baseline (configuration management);elegant degradation;experiment;finite-state transducer;language model;mathematical optimization;point of interest;run time (program lifecycle phase);speech recognition;word error rate	Xiaoqiang Xiao;Hong Chen;Mark Zylak;Daniela Sosa;Suma Desu;Mahesh Krishnamoorthy;Daben Liu;Matthias Paulik;Yuchen Zhang	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462550	latency (engineering);point of interest;language model;decoding methods;speech recognition;training set;word error rate;artificial intelligence;hidden markov model;pattern recognition;computer science;finite state transducer	Robotics	-22.22490163981896	-87.44163943737753	129037
06bdec625a05a41d1428c146cecca6d55a5aaf33	validation of image defect models for optical character recognition	optical distortion;image defect models;degradation;character image generators;optical character recognition;prototypes;optical character recognition software character recognition error analysis optical distortion character generation facsimile degradation predictive models prototypes image generation;ocr error classification;fonts;error analysis;optical character recognition software;image generation;facsimile;character generation;document image defect models;document image processing;predictive models;fonts image defect models optical character recognition character image generators distortions;character sets;character recognition;document image processing optical character recognition character sets;defect model validation;distortions	In this paper, we consider the problem of evaluating character image generators that model distortions encountered in optical character recognition (OCR). While a number of such defect models have been proposed, the contention that they produce the desired result is typically argued in an ad hoc and informal way. We introduce a rigorous and more pragmatic deenition of when a model is accurate: we say a defect model is validated if the OCR errors induced by the model are indistinguishable from the errors encountered when using real scanned documents. We describe four measures to quantify this similarity, and compare and contrast them using over ten million scanned and synthesized characters in three fonts. The measures diierentiate eeectively between different fonts and diierent scans of the same font regardless of the underlying text.	distortion;hoc (programming language);optical character recognition;software bug	Yanhong Li;Daniel P. Lopresti;George Nagy;Andrew Tomkins	1996	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.481536	arithmetic;computer vision;speech recognition;degradation;character encoding;distortion;computer science;prototype;predictive modelling;optical character recognition	Vision	-27.933192155926623	-89.64789746611746	129385
35b9af8df8b46735de50dd938ef61cb94b0514a2	combining pomdps trained with user simulations and rule-based dialogue management in a spoken dialogue system	adaptive computational model;trainable dialogue manager;dialogue manager;rule-based dialogue management;comprehensive dialogue system architecture;standard rule-based model;best dialogue move;computes on-line decision;user simulation;dialogue system;combining pomdps;reinforcement learning-based dialogue system;partially observable markov decision;reinforcement learning;rule based	Over several years, we have developed an approach to spoken dialogue systems that includes rule-based and trainable dialogue managers, spoken language understanding and generation modules, and a comprehensive dialogue system architecture. We present a Reinforcement Learning-based dialogue system that goes beyond standard rule-based models and computes on-line decisions of the best dialogue moves. The key concept of this work is that we bridge the gap between manually written dialog models (e.g. rule-based) and adaptive computational models such as Partially Observable Markov Decision Processes (POMDP) based dialogue managers.	computational model;dialog system;dialog tree;logic programming;markov chain;natural language understanding;online and offline;partially observable markov decision process;reinforcement learning;spoken dialog systems;systems architecture	Sebastian Varges;Silvia Quarteroni;Giuseppe Riccardi;Alexei V. Ivanov;Pierluigi Roberti	2009			natural language processing;speech recognition;computer science;machine learning	NLP	-27.29933070293596	-85.7042306013578	129401
df6aaa8428531aa235e72f0668bc69de504fb2cb	grapheme-to-phoneme conversion using automatically extracted associative rules for korean tts system	association rule	In this paper, we describe a method for automatically extracting grapheme-to-phoneme conversion rules directly from the transcription of speech synthesis database and introduce a weighted score and jamo similarity to overcome the rule application difficulties. We make a structured rule tree by rule pruning and rule association, and can eliminate most of the rules with almost no decrease of the performance. Our system achieves over 99.5 percent of phoneme-level accuracy and this performance is easily achievable even with the small amount of training data.	netware file system;speech synthesis;transcription (software)	Jinsik Lee;Seungwon Kim;Gary Geunbae Lee	2006			association rule learning;computer science;machine learning;pattern recognition;data mining	NLP	-21.730692887508138	-80.92503429838088	129454
3b9e37c363fdbdcf07b3154132877f48902b7a55	recent progress in the sphinx speech recognition system	recent improvement;corrective training;sphinx speech recognition system;darpa resource management task;between-word coarticulation modeling;function-phrase modeling;recent progress;speaker-independent word accuracy	This paper describes recent improvements in the SPHINX Speech Recognition System. These enhancements include function-phrase modeling, between-word coarticulation modeling, and corrective training. On the DARPA resource management task, SPHINX attained a speaker-independent word accuracy of 96% with a grammar (perplexity 60), and 82% without grammar (perplexity 997).	next-generation network;perplexity;programming paradigm;speech recognition;sphinx;test data;vocabulary	Kai-Fu Lee;Hsiao-Wuen Hon;Mei-Yuh Hwang	1989			natural language processing;speech recognition;computer science;linguistics	NLP	-20.382403263892336	-86.33109815498648	129822
b06c59839baf7a12c6eff511a69c9378308af108	improved recognition of contact names in voice commands	decoding;training;transducers;accuracy;live traffic experiments voice commands contact name recognition mobile device voice commands challenging problem infinite vocabularies contact token probability language model false triggering contact voice commands noisy contact name lists out of vocabulary contact name problems class based language models false triggering recognition performance system performance;speech recognition transducers context modeling context training decoding accuracy;speech recognition;fsts speech recognition voice commands contact names;context modeling;context	The recognition of contact names in mobile-device voice commands is a challenging problem. Some of the difficulties include potentially infinite vocabularies, low probability of contact tokens in the language model (LM), increased false triggering of contact voice commands when none are spoken, and very large and noisy contact name lists. In this paper we suggest solutions for each of these difficulties. We address low prior probability and out-of-vocabulary contact name problems by using class-based language models, and creating on-the-fly user dependent small language models containing only relevant names. These models are compiled dynamically based on analysis of the mobile device state. Since these solutions can increase biasing towards contact names during recognition, it is crucial to monitor false triggering. To properly balance this bias we introduce the concept of a contacts insertion reward. This reward is tuned using both positive and negative test sets. We show significant recognition performance improvements on data sets in three languages, without negatively impacting the overall system performance. The improvements are obtained in both offline evaluations as well as on live traffic experiments.	biasing;compiler;experiment;language model;mobile device;online and offline;test set;vocabulary	Petar S. Aleksic;Cyril Allauzen;David Elson;Aleksandar Kracun;Diego Melendo Casado;Pedro J. Moreno	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178957	natural language processing;speech recognition;transducer;computer science;voice tag;mathematics;accuracy and precision;context model;statistics	Robotics	-22.571670205462123	-87.43791749307314	129894
549d0b239081c733343b8f78c8f628e5179beff6	target-text mediated interactive machine translation	target text mediation;machine assisted human translation;word completion;statistical translation models;statistical language models;statistical language model;machine translation;interactive mt	The use of Machine Translation as a tool for professional or other highly skilled translators is for the most part currently limited to postediting arrangements in which the translator invokes MT when desired and then manually cleans up the results. A theoretically promising but hitherto largely unsuccessful alternative to postediting for this application is interactive machine translation (IMT), in which the translator and MT system work in tandem. We argue that past failures to make IMT viable as a tool for skilled translators have been the result of an infelicitous mode of interaction rather than any inherent flaw in the idea. As a solution, we propose a new style of IMT in which the target text under construction serves as the medium of communication between an MT system and its user. We describe the design, implementation, and performance of an automatic word completion system for translators which is intended to demonstrate the feasibility of the proposed approach, albeit in a very rudimentary form.	directive (programming);elliott brothers (computer company);event (computing);flaw hypothesis methodology;interaction;interactive machine translation;postediting;text-based (computing)	George F. Foster;Pierre Isabelle;Pierre Plamondon	1997	Machine Translation	10.1023/A:1007999327580	computer-assisted translation;natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;machine translation;rule-based machine translation;machine translation software usability	HCI	-23.717890852006917	-85.41999925588829	130326
cb608f19e314006f8094e1c3acb2ccde426fc301	a joint learning approach for situated language generation	grammar;engage;di fabbrizio g;weiss n;bangalore s;devault d;surface form;hmms;frezza buet h;referent;observation likelihood;scottd;young s;surface realizer;traum d r;shimodaira h;kollera;pietquin o;adapt;petrie t;lemon o;surface forms;task models;content selection;state spaces;hidden markov model;mdps;watkins c;probability distributions;reinforcement learning;henderson j;soules g;angeli g;simulation;schatzmann j;klein d;van zaanen m;hmm;kruijffg j m;curse of dimensionality;surface realization;annotation;levine;communications and signal processing;conjunction;rabiner l r;comprehend;unigram;aggregation;reward function;incremental;annotated;information presentation;simulator;romary l;salmon alts;mairessef;keizer s;stoial;give;cuayahuitl h;byron d k;rieserv;reward;adaptive;wizard of oz;request;dohsaka k;hidden markov models;requests;machine learning;jurd cek f;reward functions;viethen j;dialogue context;information packaging;reference;byron d;belz a;referring expressions;sutton r s;adaptation;kelleher j d;barto a g;state space;situated interaction;attributes;optimizations;eckert w;shockleyd m;referring expression;cassell j;georgila k;semantic;baum l e;janarthanam s;multimodal;stent a j;dietterich t g;syntactic;repairs;evaluation;effective;optimization;renals s;realization;entropy;learning agent;surface realizations;yu k;mdp;dethlefs n;markov decision process;pieraccini r;korbayovai k;denecke m;referring expression generation;thomson b;grammatical;bigram;realizations;dale r;decision trees;gargett a;williams j d;chandramohan s;artstein r;reboul a;liangp;raux a;geist m;denis a;artificial intelligence and natural language processing;context;constraints;striegnitz k;discourse context;reiter e;language model;comprehension;repair;garoufi k;nakanom;references;fosler lussier e;realizer;tagging;gasic m;moore j d		natural language generation;situated	Nina Dethlefs;Heriberto Cuayáhuitl	2014		10.1017/CBO9780511844492.008	n-gram;natural language processing;markov decision process;probability distribution;entropy;natural language programming;comprehension;conjunction;speech recognition;universal networking language;curse of dimensionality;syntax;referring expression;realization;computer science;state space;artificial intelligence;evaluation;adaptive behavior;decision tree;multimodal interaction;grammar;linguistics;bigram;modeling language;language technology;hidden markov model;adaptation	NLP	-29.441118686418818	-80.4327395325205	130686
eca16c1c776406abd0d966653a705f945bd4b520	ungrammaticality and extra-grammaticality in natural language understanding systems	natural language understanding system	"""Among the components included in Natural Language Understanding (NLU) systems is a grammar which spec i f i es much o f the l i n g u i s t i c s t ruc tu re o f the ut terances tha t can be expected. However, i t is ce r ta in tha t inputs that are ill-formed with respect to the grammar will be received, both because people regularly form ungra=cmatical utterances and because there are a variety of forms that cannot be readily included in current grammatical models and are hence """"extra-grammatical"""". These might be rejected, but as Wilks stresses, """"...understanding requires, at the very least, ... some attempt to interpret, rather than merely reject, what seem to be ill-formed utterances."""" [WIL76]"""	formal grammar;natural language understanding;spec#	Stanley C. Kwasny;Norman K. Sondheimer	1979			natural language processing;computer science	AI	-30.738596560519724	-82.70776744321914	130894
0aac7f25dd59c8fb112edcef01f8b293bf8e4e52	speaker-independent upfront dialect adaptation in a large vocabulary continuous speech recognizer.	continuous speech recognizer;speaker independent	Large vocabulary continuous speech recognition systems show a signi cant decrease in performance if a users pronunciation di ers largely from those observed during system training. This can be considered as the main reason why most commercially available systems recommend| if not enforce | the individual end user to read an enrollment script for the speaker dependent reestimation of acoustic model parameters. Thus, the improvement of recognition rates for dialect speakers is an important issue both with respect to a broader acceptance and a more convenient or natural use of such systems. This paper compares di erent techniques that aim on a better speaker independent recognition of dialect speech in a large vocabulary continuous speech recognizer. The methods discussed comprise Bayesian adaptation and speaker clustering techniques and deal with both the availability and absence of dialect training material. Results are given for a case study that aims on the improvement of a German speech recognizer for Austrian speakers.	acoustic cryptanalysis;acoustic model;cluster analysis;finite-state machine;speech recognition;vocabulary	Volker Fischer;Yuqing Gao;Eric Janke	1998			natural language processing;speech recognition;linguistics	ML	-21.704613343455932	-85.77512568461424	130985
bd5db6eac503b05a46e85259894ab5b2333a7e6b	a comparative study on various confidence measures in large vocabulary speech recognition	interpolation;confidence measure;vocabulary;posterior probability;maximum likelihood estimation;equal error rate large vocabulary speech recognition inter word mutual information high level confidence measures word posterior probabilities n best counting likelihood ratio testing linear interpolation asr switchboard task mandarin dictation task recognition errors baseline recognition systems lsa technique verification performance;linear interpolation;likelihood ratio test;equal error rate;comparative study;vocabulary speech recognition collision mitigation automatic speech recognition chaos asia light rail systems humans acoustic measurements probability;speech recognition;mutual information;error statistics;error statistics speech recognition vocabulary maximum likelihood estimation interpolation	In this paper, we have conducted a comparative study on several confidence measures (CM) for large vocabulary speech recognition. Firstly, we propose a novel high-level CM that is based on the inter-word mutual information (MI). Secondly, we experimentally investigate several popular low-level CM, such as word posterior probabilities, N-best counting, likelihood ratio testing (LRT), etc. Finally, we have studied a simple linear interpolation strategy to combine the best low-level CM with the best high-level CM. All of these CM are examined in two large vocabulary ASR tasks, namely the Switchboard task and a Mandarin dictation task, to verify the recognition errors in baseline recognition systems. Experimental results show: (1) the proposed MI-based CM greatly surpass another existing high-level CM which are based on the LSA technique; (2) among all low-level CM, word posteriori probabilities give the best verification performance; (3) when combining the word posteriori probabilities with the MI-based CM, the equal error rate is reduced from 24.4% to 23.9% in the Switchboard task and from 17.5% to 16.2% in the Mandarin dictation task.	baseline (configuration management);experiment;high- and low-level;linear interpolation;long-running transaction;mutual information;speech recognition;super robot monkey team hyperforce go!;telephone switchboard;vocabulary	Gang Guo;Chao Huang;Hui Jiang;Ren-Hua Wang	2004	2004 International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2004.1409573	natural language processing;speech recognition;likelihood-ratio test;interpolation;word error rate;computer science;comparative research;pattern recognition;maximum likelihood;posterior probability;mutual information;linear interpolation;statistics	NLP	-20.453043738807263	-89.67396706400189	131025
07e8f9294e3a201a6d1530237eb230505b412b39	wiktionary as a source for automatic pronunciation extraction		In this paper, we analyze whether dictionaries from the World Wide Web which contain phonetic notations, may support the rapid creation of pronunciation dictionaries within the speech recognition and speech synthesis system building process. As a representative dictionary, we selected Wiktionary [1] since it is at hand in multiple languages and, in addition to the definitions of the words, many phonetic notations in terms of the International Phonetic Alphabet (IPA) are available. Given word lists in four languages English, French, German, and Spanish, we calculated the percentage of words with phonetic notations in Wiktionary. Furthermore, two quality checks were performed: First, we compared pronunciations from Wiktionary to pronunciations from dictionaries based on the GlobalPhone project, which had been created in a rule-based fashion and were manually cross-checked [2]. Second, we analyzed the impact of Wiktionary pronunciations on automatic speech recognition (ASR) systems. French Wiktionary achieved the best pronunciation coverage, containing 92.58% phonetic notations for the French GlobalPhone word list as well as 76.12% and 30.16% for country and international city names. In our ASR systems evaluation, the Spanish system gained the most improvement from Wiktionary pronunciations with 7.22% relative word error rate reduction.	dictionary attack;logic programming;speech recognition;speech synthesis;word error rate;world wide web	Tim Schlippe;Sebastian Ochs;Tanja Schultz	2010			international phonetic alphabet;word error rate;speech recognition;notation;computer science;pronunciation;speech synthesis;german	NLP	-23.235634998418014	-83.33091827624762	131389
0f1bc8dbbd2da0fbaecaa5c98e252d6bde389bee	using proxies for oov keywords in the keyword search task	indexes lattices speech transducers training vocabulary keyword search;document handling;query processing;vocabulary;vocabulary document handling query processing speech recognition;nist evaluation keyword oov keyword search task weighted finite state transducer wfst based framework out of vocabulary keyword handling speech search task large vocabulary continuous speech recognition kws system conversational telephone speech tagalog word based index phone based index word lattices lvcsr system pronunciation lexicon oov keyword pronunciation standard grapheme to phoneme method in vocabulary proxies word sequences phone sequences phone confusion matrix empirical analysis babel evaluation keyword word proxy search term weighted value;speech recognition;low resource lvcsr speech recognition keyword search oov keywords proxy keywords	We propose a simple but effective weighted finite state transducer (WFST) based framework for handling out-of-vocabulary (OOV) keywords in a speech search task. State-of-the-art large vocabulary continuous speech recognition (LVCSR) and keyword search (KWS) systems are developed for conversational telephone speech in Tagalog. Word-based and phone-based indexes are created from word lattices, the latter by using the LVCSR system's pronunciation lexicon. Pronunciations of OOV keywords are hypothesized via a standard grapheme-to-phoneme method. In-vocabulary proxies (word or phone sequences) are generated for each OOV keyword using WFST techniques that permit incorporation of a phone confusion matrix. Empirical results when searching for the Babel/NIST evaluation keywords in the Babel 10 hour development-test speech collection show that (i) searching for word proxies in the word index significantly outperforms searching for phonetic representations of OOV words in a phone index, and (ii) while phone confusion information yields minor improvement when searching a phone index, it yields up to 40% improvement in actual term weighted value when searching a word index with word proxies.	confusion matrix;finite-state transducer;lexicon;microsoft word for mac;proxy server;search algorithm;speech analytics;speech recognition;vocabulary	Guoguo Chen;Oguz Yilmaz;Jan Trmal;Daniel Povey;Sanjeev Khudanpur	2013	2013 IEEE Workshop on Automatic Speech Recognition and Understanding	10.1109/ASRU.2013.6707766	natural language processing;speech recognition;computer science;linguistics	NLP	-22.311803577089243	-83.34429421807071	131876
3d79c6718b69ed9f7ab08c83ec164fb22f2491e7	natural language understanding considerations for a lifelong learning companion	query processing computer aided instruction continuing professional development natural language processing;open learner modeling;query processing;open learner modeling natural language understanding lifelong learning;computer aided instruction;syntactics training writing accuracy context acceleration equations;open learner model;natural language understanding;nlu capabilities natural language understanding natural language processing lifelong learning companion keyword spotting syntactic information automated assessment;lifelong learning;continuing professional development;natural language processing	In this paper, we present considerations for natural language processing for a lifelong learning companion. In the context of these considerations, we review related work in automated assessment of learner writing and present an idea for augmenting keyword spotting with syntactic information. However, the extra information given by syntax is offset by parser errors and added burden on the author. The results suggest that while standard keyword spotting is a quick approach to adding NLU capabilities it has inherent limitations.	natural language processing;natural language understanding	Mark G. Core	2012	2012 10th International Conference on Creating, Connecting and Collaborating through Computing	10.1109/C5.2012.16	natural language processing;language identification;natural language programming;speech recognition;natural language user interface;computer science;continuing professional development;multimedia;lifelong learning	NLP	-29.04530221784141	-84.71190464156786	132062
c99aee0b33569ddceec9eb3b2484cf802462c1c5	a non-expert kaldi recipe for vietnamese speech recognition system		In this paper we describe a non-expert setup for Vietnamese speech recognition system using Kaldi toolkit. We collected a speech corpus over fifteen hours from about fifty Vietnamese native speakers and using it to test the feasibility of our setup. The essential linguistic components for the Automatic Speech Recognition (ASR) system was prepared basing on the written form of the language instead of expertise knowledge on linguistic and phonology as commonly seen in rich resource languages like English. The modeling of tones by integrating them into the phoneme and using the phonetic decision tree is also discussed. Experimental results showed this setup for ASR systems does yield competitive results while have potentials for further improvements.	decision tree;kaldi;speech corpus;speech recognition;speech synthesis	Hieu-Thi Luong;Hai-Quan Vu	2016			artificial intelligence;natural language processing;vietnamese;computer science;speech recognition;recipe	NLP	-22.46512914453304	-85.09966718447961	132171
20e229e0c3547f981f3694c76d9533cc13b55952	a hybrid language model for open-vocabulary thai lvcsr		This paper investigates the use of a hybrid language model for open-vocabulary Thai LVCSR. Thai text is written without word boundary markers and the definition of word unit is often ambiguous due to the presence of compound words. Hence, to build open-vocabulary LVCSR, a very large lexicon is required to also handle word unit ambiguity. Pseudomorpheme (PM), a syllable-like sub-word unit specifically designed for Thai is considered to be a more well-defined unit. To overcome the problem of out-of-vocabulary words and to also reduce the size of the lexicon, a hybrid language model which combines word and sub-word units is proposed. Words and sub-words frequently found in several domains constitute open-vocabulary for general domain Thai LVCSR. To verify our scheme, we run recognition experiments on data from various tasks including broadcast news transcription, dictation and mobile speech-to-speech translation. Open-vocabulary Thai LVCSR using the hybrid language model obviously reduces the out-of-vocabulary problem. The proposed model having a much smaller lexicon size achieves a comparable recognition error rate to a baseline system using a full-word lexicon.	baseline (configuration management);experiment;language model;lexicon;speech analytics;syllable;transcription (software);vocabulary	Kwanchiva Thangthai;Ananlada Chotimongkol;Chai Wutiwiwatchai	2013			speech recognition;vocabulary;computer science;language model	NLP	-21.203484012548042	-83.80906304339491	132248
fe18dddd444868356d5787d2574d819ca70dce69	analyzer of sentence card set for learning by problem-posing		MONSAKUN is software for learning by problem-posing in arithmetical word problems where a learner poses a problem by selecting and combining sentence cards from a given set of sentence cards. It is not easy task to prepare the sets of the sentence cards manually because it is necessary to evaluate all combinations. This paper describes an analyzer of a set of sentence cards. Experimental evaluation of the analyzer is also reported.		Tsukasa Hirashima;Megumi Kurayama	2013		10.1007/978-3-642-39112-5_75	natural language processing;speech recognition;programming language	NLP	-27.399781427411057	-82.37611701802703	132297
23bc0594a3fd796568cea0b7a4f984a40b433949	recognition of greek phonemes using support vector machines	metodo cuadrado menor;phoneme;modelizacion;reconocimiento lenguaje;phonetique;methode moindre carre;servicio vocal;voice service;analisis estadistico;least squares method;reconnaissance langage;service information;telephone;fonema;intelligence artificielle;aprendizaje probabilidades;classification;language recognition;greek;modelisation;reconocimiento voz;griego;statistical analysis;machine exemple support;analyse statistique;least square;fonetica;servicio informacion;speech recognition;apprentissage probabilites;artificial intelligence;phonetics;inteligencia artificial;grec;reconnaissance parole;support vector machine;information service;maquina ejemplo soporte;vector support machine;telefono;modeling;clasificacion;language model;probability learning;service vocal	In the present work we study the applicability of Support Vector Machines (SVMs) on the phoneme recognition task. Specifically, the Least Squares version of the algorithm (LS-SVM) is employed in recognition of the Greek phonemes in the framework of telephone-driven voice-enabled information service. The N-best candidate phonemes are identified and consequently feed to the speech and language recognition components. In a comparative evaluation of various classification methods, the SVM-based phoneme recognizer demonstrated a superior performance. Recognition rate of 74.2% was achieved from the N-best list, for N=5, prior to applying the language model.	algorithm;feature vector;finite-state machine;language identification;language model;least squares;support vector machine	Iosif Mporas;Todor Ganchev;Panagiotis Zervas;Nikos Fakotakis	2006		10.1007/11752912_30	phonetics;speech recognition;computer science;artificial intelligence;machine learning;greek;least squares;statistics;language model	ML	-20.889043880729414	-90.81368914155075	132451
212ddfa75e58f0d1f0eb0cc1e03d69e9ced44bd9	a self-transcribing speech corpus: collecting continuous speech with an online educational game		We describe a novel approach to collecting orthographically transcribed continuous speech data through the use of an online educational game called Voice Scatter, in which players study flashcards by using speech to match terms with their definitions. We analyze a corpus of 30,938 utterances, totaling 27.63 hours of speech, collected during the first 22 days that Voice Scatter was publicly available. Though each individual game covers only a small vocabulary, in aggregate speech recognition hypotheses in the corpus contain 21,758 distinct words. We show that Amazon Mechanical Turk can be used to orthographically transcribe utterances in the corpus quickly and cheaply, with near-expert accuracy. Moreover, we present a filtering technique that automatically identifies a sub-corpus of 39% of the data for which recognition hypotheses can be considered human-quality transcripts. We demonstrate the usefulness of such self-transcribed data for acoustic model adaptation.	acoustic cryptanalysis;acoustic model;aggregate data;amazon mechanical turk;speech corpus;speech recognition;text corpus;the turk;vocabulary	Alexander Gruenstein;Ian McGraw;Andrew M. Sutherland	2009			natural language processing;speech corpus;speech recognition;artificial intelligence;speech analytics;computer science;transcription (linguistics);acoustic model;vocabulary;speech synthesis	NLP	-22.106072709820175	-84.54781108085953	132508
6abf7319737b83712375089147bc5b46c4c8faca	lexical access using minimum message length encoding	unsupervised learning;lexical access;derived equivalence;minimum message length;speech recognition	A method for deriving equivalence classes for lexical access in speech recognition is considered, which automatically derives equivalence classes from training data using unsupervised learning and the Minimum Message Length Criterion. These classes model insertions, deletions and substitutions in an input phoneme string due to mis-recognition and mis-pronunciation, and allow unlikely word candidates to be eliminated quickly. This in turn allows a more detailed examination of the remaining candidates to be carried out eeciently.	lexicon;minimum message length;speech recognition;turing completeness;unsupervised learning	Ian E. Thomas;Ingrid Zukerman;Jonathan J. Oliver;Bhavani Raskutti	1996		10.1007/3-540-61532-6_20	natural language processing;unsupervised learning;speech recognition;computer science;machine learning;minimum message length;pattern recognition	NLP	-22.110457699284783	-80.88238792251691	132873
a5f8a49b2466def377a16268a395020f7a0bb0a8	measuring performance of virtual keyboards based on cyclic scanning	keyboards computational modeling paper technology circuit topology mathematical model predictive models manuals frequency natural languages application software;manuals;keyboards;application software;virtual keyboard;paper technology;text entry;natural languages;layout;data mining;circuit topology;computational modeling;handicapped aids;cyclic scanning;dictionaries;keyboards handicapped aids;mathematical model;predictive models;frequency;handicapped aids virtual keyboard cyclic scanning mathematical model text entry	This paper presents an exhaustive study into the different topologies of virtual ambiguous keyboards that operate by scanning techniques, analyzing the text entry average time (tc) and the average number of user inputs (UIc) per character. An mathematical model shows that in comparison with unambiguous one, text entry, in multi-tap mode, doesn't offers better performance, because both tc and UIc are greater in them. Another method of text entry, called Tnk (Text in n keys), offers improvement with respect to unambiguous keyboards. But solely highly ambiguous key-board (4-keys keyboards) shows a jointly reduction in tc and UIc. Results obtained with the model do to focus on highly ambiguous keyboard. This paper demonstrate, using simulation with extensive text, that character prediction with TnK mode only have better performance than unambiguous keyboard with character prediction in UIc parameter. Another techniques of text entry are also studied	computer keyboard;mathematical model;multi-tap;simulation	Alberto J. Molina;Octavio Rivera;Isabel M. Gómez	2009	2009 Fifth International Conference on Autonomic and Autonomous Systems	10.1109/ICAS.2009.35	topology;layout;application software;simulation;speech recognition;telecommunications;computer science;artificial intelligence;operating system;frequency;mathematical model;database;predictive modelling;multimedia;natural language;programming language;computational model;computer security	SE	-27.352368865509742	-88.9316336611842	132910
594c2b27bbef7aa15c7ee36b31f27e28a08b1339	alto: rapid prototyping for parsing and translation		We present Alto, a rapid prototyping tool for new grammar formalisms. Alto implements generic but efficient algorithms for parsing, translation, and training for a range of monolingual and synchronous grammar formalisms. It can easily be extended to new formalisms, which makes all of these algorithms immediately available for the new formalism.	algorithm;parsing;rapid prototyping;semantics (computer science);software prototyping;text corpus;xerox alto	Johannes Gontrum;Jonas Groschwitz;Alexander Koller;Christoph Teichmann	2017			programming language;computer science;natural language processing;artificial intelligence;rotation formalisms in three dimensions;parsing;rapid prototyping;formalism (philosophy);grammar	NLP	-30.08617496619671	-80.70856808771426	133070
bf229e7f726a9f792abfa891f313cda328ba389a	n-best error simulation for training spoken dialogue systems	error simulation spoken dialogue systems reinforcement learning pomdp;roc n best error simulation training spoken dialogue systems reinforcement learning simulated environment simulated dialogue performance logistic regression dirichlet distribution receiver operating characteristics;semantics measurement logistics generators maximum likelihood estimation standards training;regression analysis;regression analysis interactive systems learning artificial intelligence natural language processing;learning artificial intelligence;interactive systems;natural language processing	A recent trend in spoken dialogue research is the use of reinforcement learning to train dialogue systems in a simulated environment. Past researchers have shown that the types of errors that are simulated can have a significant effect on simulated dialogue performance. Since modern systems typically receive an N-best list of possible user utterances, it is important to be able to simulate a full N-best list of hypotheses. This paper presents a new method for simulating such errors based on logistic regression, as well as a new method for simulating the structure of N-best lists of semantics and their probabilities, based on the Dirichlet distribution. Off-line evaluations show that the new Dirichlet model results in a much closer match to the receiver operating characteristics (ROC) of the live data. Experiments also show that the logistic model gives confusions that are closer to the type of confusions observed in live situations. The hope is that these new error models will be able to improve the resulting performance of trained dialogue systems.	dialog system;logistic regression;reinforcement learning;simulation;virtual reality	Blaise Thomson;Milica Gasic;Matthew Henderson;Pirros Tsiakoulis;Steve J. Young	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424194	natural language processing;speech recognition;computer science;machine learning;regression analysis	NLP	-25.82964590695713	-87.76170819966023	133102
8d5a343ac02682e53e0f10612cd70590710f08f4	word-graph-based handwriting keyword spotting of out-of-vocabulary queries	measurement;decoding;training;smoothing methods;hidden markov models;vectors;stochastic processes	Thanks to the use of lexical and syntactic information, Word Graphs (WG) have shown to provide a competitive Precision-Recall performance, along with fast lookup times, in comparison to other techniques used for Key-Word Spotting (KWS) in handwritten text images. However, a problem of WG approaches is that they assign a null score to any keyword that was not part of the training data, i.e. Out-of-Vocabulary (OOV) keywords, whereas other techniques are able to estimate a reasonable score even for these kind of keywords. We present a smoothing technique which estimates the score of an OOV keyword based on the scores of similar keywords. This makes the WG-based KWS as flexible as other techniques with the benefit of having much faster lookup times.		Joan Puigcerver;Alejandro Héctor Toselli;Enrique Vidal	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.355	stochastic process;speech recognition;computer science;machine learning;pattern recognition;hidden markov model;measurement;statistics	Vision	-22.493470765674115	-82.7256982931323	133164
5bfc1f74e41e2907862574b3e4278752cbdc76f2	janus: a system for translation of conversational speech		Janus is a large scale system for interactive spoken language translation that has been developed at Carnegie Mellon University and the University of Karlsruhe in the course of the last seven years. The system currently accepts spontaneous conversational speech in a limited domain in English, German or Spanish and produces output in German, English, Spanish, Japanese and Korean. In this overview article of the Janus system we describe how the system has evolved over the years and developed it's current architecture. We brieey describe the current system components, summarize our system development and evaluation methods, and present some of our most recent performance evaluation results. Finally , we discuss our current eeorts to signiicantly expand the domain of coverage of the system, and the future directions we intend to explore within the context of this project.	janus;machine translation;performance evaluation;spontaneous order	Alexander H. Waibel;Alon Lavie;Lori S. Levin	1997	KI		janus;communication;computer science	NLP	-24.300245632514738	-83.87259047674328	133423
296e436c59ce7b489ebd3ca7853e768e85888b9c	modeling cantonese pronunciation variations for large-vocabulary continuous speech recognition	tan;判解;lee;soong;cantonese;frank k;法律詞典;kam;論文;pronunciation variation;automatic speech recognition;大陸法學;法規;patgi;月旦法學;法律題庫;裁判時報;月旦知識庫;法學資料庫;tssci;教學	This paper presents different methods of handling pronunciation variations in Cantonese large-vocabulary continuous speech recognition. In an LVCSR system, three knowledge sources are involved: a pronunciation lexicon, acoustic models and language models. In addition, a decoding algorithm is used to search for the most likely word sequence. Pronunciation variation can be handled by explicitly modifying the knowledge sources or improving the decoding method. Two types of pronunciation variations are defined, namely, phone changes and sound changes. Phone change means that one phoneme is realized as another phoneme. A sound change happens when the acoustic realization is ambiguous between two phonemes. Phone changes are handled by constructing a pronunciation variation dictionary to include alternative pronunciations at the lexical level or dynamically expanding the search space to include those pronunciation variants. Sound changes are handled by adjusting the acoustic models through sharing or adaptation of the Gaussian mixture components. Experimental results show that the use of a pronunciation variation dictionary and the method of dynamic search space expansion can improve speech recognition performance substantially. The methods of acoustic model refinement were found to be relatively less effective in our experiments.	acoustic cryptanalysis;acoustic model;algorithm;ambiguous grammar;calculus of variations;context-sensitive language;dictionary;elegant degradation;experiment;language model;lexicon;mixture model;physical vapor deposition;refinement (computing);speech analytics;speech recognition;threshold model;vocabulary;word error rate	Tan Lee;Patgi Kam;Frank K. Soong	2006	IJCLCLP		natural language processing;speech recognition;computer science;linguistics	NLP	-19.34822299767961	-85.6862169723403	133842
a15ab7f21026ebb159019030659419d11fa6d00a	sentence lipreading using hidden markov model with integrated grammar	modele markov hidden;grammar;vocabulaire;modelo markov;reconocimiento palabra;hidden markov model;frase;extraction forme;vocabulary;hmm;vocabulario;lecture labiale;sentence;automatic recognition;markov model;extraccion forma;grammaire;image sequence;speech recognition;lectura labial;secuencia imagen;phrase;reconnaissance parole;modele markov;pattern extraction;gramatica;sequence image;reconocimiento automatico;reconnaissance automatique;lip reading	In this paper, we describe a systematic approach to the lipreading of whole sentences. A vocabulary of elementary words is considered. Based on the vocabulary, we define a grammar that generates a set of legal sentences. Our lipreading approach is based on a combination of the grammar with hidden Markov models (HMMs). Two different experiments were conducted. In the first experiment a set of e-mail commands is considered, while the set of sentences in the second experiment is given by all English integer numbers up to one million. Both experiments showed promising results, regarding the difficulty of the considered task.	concatenation;email;experiment;fast fourier transform;formal grammar;hidden markov model;lexicon;markov chain;pixel;problem domain;statistical classification;vocabulary	Keren Yu;Xiaoyi Jiang;Horst Bunke	2001	IJPRAI	10.1142/S0218001401000770	natural language processing;speech recognition;computer science;machine learning;pattern recognition;grammar;markov model;hidden markov model	NLP	-22.208254339154163	-82.13986033650293	133885
a0a4a94943abe431635335c53ff69391557c1463	unified task knowledge for spoken language understanding and dialog management		Consider a conversational speech system incorporating spoken language understanding (SLU) and dialog manager (DM) modules. A prerequisite of natural dialog is that understanding takes place in context, which in turn necessitates a sharing of task knowledge between the two modules. However, because they are engaged in different activities it is often the case that the task knowledge representation is different for each of them, an undesirable duplication. In this paper we consider AT&T’s How May I Help You? natural dialog system. We present a method for automatically inducing the task knowledge representation used for spoken language understanding from that used for dialog management. Also, the context information needed by the SLU module for understanding each utterance is generated automatically by the DM from its own knowledge representation and the current dialog situation. This enables understanding-incontext to be implemented while avoiding the duplication involved in creating and maintaining separate task knowledge representations. The system has been evaluated using a database comprising 200k dialogs drawn from live customer traffic.	class hierarchy;consistency model;context-sensitive language;database;dialog manager;dialog system;digraphs and trigraphs;emergence;inductive reasoning;knowledge representation and reasoning;linear algebra;natural language understanding;receiver operating characteristic;rejection sampling;spoken dialog systems	Jeremy H. Wright;Alicia Abella;Allen L. Gorin	2002			speech recognition;semantic interpretation;natural language processing;spoken language;computer science;dialog box;artificial intelligence	NLP	-30.19681755936769	-85.41237307361213	134122
3ad2583e00f42657c799c807e485415172a43637	almannarómur: an open icelandic speech corpus		The purpose of the Almannarómur project is collecting data for a speech corpus (database) for Icelandic. Its main aim is creating an open source speech project to enable research and development for Icelandic language technology. The database is particularly suitable for acoustic modelling for speech recognition but it could also be used for other purposes, such as to develop a speaker recognition system or to analyze prosody. The project is run by Reykjavik University and the Icelandic Centre for Language Technology in cooperation with Google who provided technical support. The number of participants achieved in this effort was 563, providing, on average, around 219 read sentences each. This paper gives a short introduction to Icelandic language technology, describes how the text corpus was constructed for the database, and presents how the recording effort was organized as well as its main results.	acoustic cryptanalysis;language technology;open-source software;semantic prosody;speaker recognition;speech corpus;speech recognition;technical support;text corpus	Jón Guðnason;Oddur Kjartansson;Jökull Jóhannsson;Elín Carstensdóttir;Hannes Högni Vilhjálmsson;Hrafn Loftsson;Sigrún Helgadóttir;Kristín Jóhannsdóttir;Eiríkur Rögnvaldsson	2012			technical support;language technology;speech corpus;natural language processing;icelandic language;speaker recognition;prosody;text corpus;artificial intelligence;icelandic;computer science	NLP	-24.068206053299622	-84.17167516792543	134210
8ed43e9d072e3b4b0f3104832235a4d371a731eb	tibetan multi-word expressions identification framework based on news corpora	context analysis;tibetan mwes;larger corpus;tibetan mwe identification framework	This paper presents an identification framework for extracting Tibetan multi-word expressions. The framework includes two phases. In the first phase, sentences are segmented and high-frequency word-based n-grams are extracted using Nagao’s N-gram statistical algorithm and Statistical Substring Reduction Algorithm. In the second phase, the Tibetan MWEs are identified by the proposed framework which based on the combination of context analysis and language model-based analysis. Context analysis, two-word Coupling Degree and Tibetan syllable inside word probability are three strategies in Tibetan MWE identification framework. In experimental part, we evaluate the effectiveness of three strategies on small test data, and evaluate results of different granularity for Context analysis. On small test corpus, F-score above 75% have been achieved when words are segmented in pre-processing. On larger corpus, the P@N (N is 800) overcomes 85%. It indicates that the identification framework can work well on larger corpus. The experimental result reaches acceptable performance for Tibetan MWEs.	algorithm;f1 score;grams;language model;minimal working example;n-gram;preprocessor;substring;syllable;test data;text corpus	Minghua Nuo;Congjun Lun;Huidan Liu	2016		10.1007/978-3-319-50496-4_2	natural language processing;communication;literature	NLP	-21.098120754701583	-81.71661110209713	134527
590bff6355ccb7ba777d86532da22be5e9e5d173	a modified hme architecture for text-dependent speaker identification	male speakers modified hierarchical mixtures of experts architecture text dependent speaker identification gating network statistical model maximum likelihood problem expectation maximization algorithm isolated digit utterances;speaker identification;architecture systeme;learning;neural nets;maximum likelihood;maximization;maximum vraisemblance;hierarchical mixture of experts;classification;statistical model;feature extraction speaker recognition neural nets;speaker recognition;algorithme;aprendizaje;algorithm;apprentissage;expectation maximization;feature extraction;modele statistique;arquitectura sistema;modelo estadistico;testing neural networks feature extraction neurons databases statistics information science;reseau neuronal;speaker;system architecture;locutor;em algorithm;maximizacion;clasificacion;red neuronal;maxima verosimilitud;locuteur;maximisation;neural network;algoritmo	A modified hierarchical mixtures of experts (HME) architecture is presented for text-dependent speaker identification. A new gating network is introduced to the original HME architecture for the use of instantaneous and transitional spectral information in text-dependent speaker identification. The statistical model underlying the proposed architecture is presented and learning is treated as a maximum likelihood problem; in particular, an expectation-maximization (EM) algorithm is also proposed for adjusting the parameters of the proposed architecture. An evaluation has been carried out using a database of isolated digit utterances by 10 male speakers. Experimental results demonstrate that the proposed architecture outperforms the original HME architecture in text-dependent speaker identification.		Ke Chen;Dahong Xie;Huisheng Chi	1996	IEEE transactions on neural networks	10.1109/72.536325	speech recognition;expectation–maximization algorithm;computer science;machine learning;pattern recognition;artificial neural network;statistics	Vision	-20.98891638165523	-91.65360134716967	134563
1e5126c94fa7731bfd3e41e174752d45291ce895	improved topic-dependent language modeling using information retrieval techniques	wall street journal;context information;information retrieval;natural languages;n gram model;grammars;information retrieval speech recognition natural languages grammars;information retrieval power system modeling predictive models context modeling history natural languages speech recognition entropy training data vocabulary;probability topic dependent language modeling information retrieval techniques n gram language models speech recognition systems long term context information experiments wall street journal text corpus baseline language model model perplexity prediction;speech recognition;language model	N-gram language models are frequently used by the speech recognition systems to constrain and guide the search. N-gram models use only the last N-1 words to predict the next word. Typical values of N that are used range from 2-4. N-gram language models thus lack the long-term context information. We show that the predictive power of the N-gram language models can be improved by using long-term context information about the topic of discussion. We use information retrieval techniques to generalize the available context information for topic-dependent language modeling. We demonstrate the effectiveness of this technique by performing experiments on the Wall Street Journal text corpus, which is a relatively difficult task for topic-dependent language modeling since the text is relatively homogeneous. The proposed method can reduce the perplexity of the baseline language model by 37%, indicating the predictive power of the topic-dependent language model.	baseline (configuration management);experiment;information retrieval;language model;n-gram;perplexity;speech recognition;text corpus;the wall street journal	Milind Mahajan;Doug Beeferman;Xuedong Huang	1999		10.1109/ICASSP.1999.758182	n-gram;natural language processing;language identification;cache language model;speech recognition;universal networking language;question answering;cognitive models of information retrieval;computer science;computational linguistics;concept search;modeling language;natural language;information extraction;query language;language model	NLP	-21.18479920113982	-86.61641541258551	134611
b15661623c19ac437a39b9da42e94841f084fda5	an efficient, fast matching approach using posterior probability estimates in speech recognition	posterior probability;speech recognition	Acoustic fast matching is an effective technique to accelerate the search process in large vocabulary continuous speech recognition. This paper introduces a novel fast matching method. This method is based on the evaluation of future posterior probabilities for a look-ahead number of timeframes in order to exclude unlikely phone models as early as possible during the search. In contrast to the likelihood scores used by more traditional fast matching methods these posterior probabilities are more discriminative by nature as they sum up to unity over all the possible models. By applying the proposed method we managed to reduce by 66% the decoding time consumed in our timesynchronous Viterbi decoder for a recognition task based on the Wall Street Journal database with virtually no additional decoding errors.	fast fourier transform;speech recognition;the wall street journal;viterbi decoder;vocabulary	Sherif Abdou;Michael S. Scordilis	2003			artificial intelligence;speech recognition;pattern recognition;computer science;machine learning;posterior probability	Vision	-21.251553906689	-88.0782121725356	134822
041a8c01c072ee2e857d8d488768825afb51eb73	rhythmic unit extraction and modelling for automatic language identification	modelizacion;rhythm typology;processus gauss;europa;reconocimiento lenguaje;syllabe;phonetique;gaussian mixture;phonotactism;reconnaissance langage;aleman;japonais;french;fonotactismo;methode acoustique;rhythm modelling;chino;vocal;language recognition;algorithme;modelisation;algorithm;frances;phonotactisme;automatic recognition;acoustic method;syllable;anglais;modelling language;francais;metodo acustico;spanish;european languages;fonetica;allemand;language identification;voyelle;teoria mezcla;english;phonetics;gaussian process;europe;espagnol;proceso gauss;german;chinois;mixture theory;discriminacion;silaba;chinese;vowel;ingles;modeling;theorie melange;japones;asian languages;discrimination;reconocimiento automatico;reconnaissance automatique;algoritmo;espanol;japanese	Authors : Jean-Luc Rouas, Jérôme Farinas, François Pellegrino, Régine André-Obrecht 1 Institut de Recherche en Informatique de Toulouse UMR 5505 CNRS – Institut National Polytechique de Toulouse – Université Paul Sabatier – Université Toulouse 1, France 2 Laboratoire Dynamique Du Langage UMR 5596 CNRS – Université Lumière Lyon 2, France {rouas@irit.fr, Jerome.Farinas@irit.fr, Francois.Pellegrino@univ-lyon2.fr, obrecht@irit.fr} Corresponding author : François PELLEGRINO (Francois.Pellegrino@univ-lyon2.fr) Postal Address : DDL – ISH 14, avenue Berthelot 69363 LYON CEDEX 7 FRANCE Tel: +33 4 72 72 64 77 Fax: +33 4 72 72 65 90 ABSTRACT	fax;françois lionet;jean;language identification;postal;uniform resource identifier	Jean-Luc Rouas;Jérôme Farinas;François Pellegrino;Régine André-Obrecht	2005	Speech Communication	10.1016/j.specom.2005.04.012	phonetics;speech recognition;french;computer science;linguistics	Crypto	-29.745228362483186	-86.78774184138943	134878
1778d0829f89b0bdf2cc14a43624590ff8c9668f	k dixez? a corpus study of spanish internet orthography	orthographe;changement orthographique;analyse de corpus;pragmatics;lexical frequency;orthography;information and communication technologies ict;phonology;pragmatique;corpus analysis;spanish;sociolinguistics;communication mediatisee par ordinateur;computer mediated communication;phonologie;espagnol;technologies de l information et de la communication tic;spelling variant;sociolinguistique;frequence lexicale;variante orthographique	New technologies have always influenced communication, by adding new ways of communication to the existing ones and/or changing the ways in which existing forms of communication are utilized. This is particularly obvious in the way in which computer-mediated communication (CMC) has had an impact on communication. In this exploratory article, we are concerned with some characteristics of a newly evolving form of Spanish Internet orthography that differ from standard Spanish spelling. Three types of deviations from ‘the norm’ are considered: a reduction (post-vocalic d/[ ] deletion in -ado), a transformation (namely the spelling change from ch to x), and reduplication (of characters). Based on a corpus of approximately 2.7 million words of regionally balanced informal internet Spanish compiled in 2008, we describe the spelling changes and discuss a variety of sometimes interacting factors governing the rates of spelling variants such as overall frequency effects, functional (pragmatic, sociolinguistic, and iconicity-related) characteristics, and phonological constraints. We also compare our findings to data from Mark Davies’s (2002) Corpus del Español (100 million words, 1200s–1900s, http://www.corpusdelespanol.org) as well as other sources and relate them to the discussion of the register/genre of Internet language. .................................................................................................................................................................................	compiler;computer-mediated communication;emoticon;interaction;internet;mark davies (linguist);text corpus	Mark Myslín;Stefan Th. Gries	2010	LLC	10.1093/llc/fqp037	natural language processing;sociolinguistics;philosophy;orthography;computer science;linguistics;sociology;spanish;phonology;pragmatics	Web+IR	-28.933223203845206	-83.58455469780824	135037
33ade22cb4cf606e8f17b2e60908971a69625d34	a fast and effective state decoding algorithm	frame-synchronous searching;equal feature variance sum;state decoding;speech recognition;frame synchronization;state transition;word recognition;error rate	In this paper a fast and effective algorithm named equal feature variance sum (EFVS) frame-synchronous searching is presented for state decoding. EFVS controls the state transition by using only the feature variance of the speech, instead of by using the state dwell distribution. The basic hypothesis of this new algorithm is the equality of feature variance sum in each state of the speech. Given the boundaries of the speech recognition unit (SRU), EFVS can generate the state sequence without dynamic searching. In a continuous speech word recognition system, this novel algorithm reduces the error rate by 36.8% and speed up the system by 65.6% compared with the traditional state decoding methods.	algorithm;decoding methods;enhanced flight vision system;search/retrieve via url;speech recognition;state transition table	Mingxing Xu;Fang Zheng;Wenhu Wu	1999			artificial intelligence;speech recognition;word error rate;speedup;word recognition;pattern recognition;decoding methods;computer science;frame synchronization;algorithm	NLP	-21.40676614978503	-88.19355435932282	135053
19d6e837b6924c5928d65c643e3b85012bf1246f	improved models for mandarin speech-to-text transcription	mandarin speech to text transcription;mlp features;character error rate;interpolation;cer;decoding;mandarin;limsi;speech processing;training;speech;speech to text transcription;artificial neural networks;artificial neural networks decoding speech recognition adaptation models interpolation speech training;speech to text;n gram language models;character error rate mandarin speech to text transcription speech recognition;speech recognition;cer mandarin speech to text transcription limsi acoustic models mlp features neural network language models n gram language models character based consensus decoding character error rate;acoustic models;adaptation models;neural network language models;artificial neural network;character based consensus decoding	This paper describes recent advances at LIMSI in Mandarin Chinese speech-to-text transcription. A number of novel approaches were introduced in the different system components. The acoustic models are trained on over 1600 hours of audio data from a range of sources, and include pitch and MLP features. N-gram and neural network language models are trained on very large corpora, over 3 billion words of texts; and LM adaptation was explored at different adaptation levels: per show, per snippet, or per speaker cluster. Character-based consensus decoding was found to outperform word-based consensus decoding for Mandarin. The improved system reduces the relative character error rate (CER) by about 10% on previous GALE development and evaluation data sets, obtaining a CER of 9.2% on the P4 broadcast news and broadcast conversation evaluation data.	acoustic cryptanalysis;acoustic model;artificial neural network;consensus (computer science);language model;memory-level parallelism;n-gram;pitch (music);speech recognition;super robot monkey team hyperforce go!;text corpus;transcription (software)	Lori Lamel;Jean-Luc Gauvain;Viet Bac Le;Ilya Oparin;Sha Meng	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947394	natural language processing;speech recognition;mandarin chinese;interpolation;computer science;speech;artificial neural network	NLP	-19.846992140498234	-86.67530033450927	135293
541bcef69a901f1870315bdf2ad8f21d0272d711	a syntactic and morphological analyzer for a text-to-speech system	input text syntactically;dialog system;speech recognition system;syntactic analyzer;chart parsing;morphological analyzer;text-to-speech system;parsing strategy;phonetic representation;parsing experiment;grammar formalism;speech recognition;text to speech	This paper presents a system which analyzes an in'put text syntactically and morphologically and converts the text from the graphemic to the phonetic :representation (or vice versa). We describe the grammar formaSsm used and report a parsing experiment which compared eight parsing strategies within the :h'amework of chart parsing. Although the morphological and syntactic analyzer has been developed for a text-to-speech system for German, it is language independent and general enough to be used for dialog systems, NL-interfaces or speech recognition systems.	alloy analyzer;chart parser;dialog system;nl (complexity);parsing;speech recognition;speech synthesis	Thomas Russi	1990			natural language processing;speech recognition;computer science;bottom-up parsing;s-attributed grammar;linguistics	NLP	-21.80658329669543	-82.7312113981201	135328
cc862da50d6ba401e45d2517c456c9a400553e15	efficient manycore chmm speech recognition for audiovisual and multistream data	speech recognition	Robustness of speech recognition can be significantly improved by multi-stream and especially by audiovisual speech recognition. This is of interest for example for human-machine interaction in noisy reverberant environments, and for transcription of or search in multimedia data. The most robust implementations of audiovisual speech recognition often utilize Coupled Hidden Markov Models (CHMMs), which allow for both modalities to be asynchronous to a certain degree. In contrast to conventional speech recognition, this increases the search space significantly, so current implementations of CHMM systems are often not real-time capable. Thus, for real-time constrained applications such as online transcription of VoIP communication or responsive multi-modal human-machine interaction, using current multiprocessor computing capability is vital. This paper describes how general purpose graphics processors can be used to obtain a real-time implementation of audiovisual and multi-stream speech recognition. The design has been integrated both with a WFST-decoder and a token passing system, with parallelization leading to a maximum speedup factor of 32 and 25, respectively.	central processing unit;graphics processing unit;hidden markov model;human–computer interaction;manycore processor;markov chain;modal logic;multi-core processor;multiprocessing;parallel computing;real-time clock;real-time transcription;speech recognition;speedup;transcription (software)	Dorothea Kolossa;Jike Chong;Steffen Zeiler;Kurt Keutzer	2010			implementation;robustness (computer science);speech recognition;speedup;multiprocessing;asynchronous communication;hidden markov model;token passing;computer science;voice over ip	ML	-21.469186030793534	-88.25929482869259	135777
4e6adcdc0875d76c2d1e9cecb5ed6fba2d0ac0e3	research on confusion network algorithm based on minimum bayes risk decision rule	speech recognition bayes methods speech coding;word error rate;generic algorithm;large vocabulary continuous speech recognition;bayes methods;speech recognition confusion network algorithm minimum bayes risk decision rule word error rate decoding strategy network generation algorithm;network generation algorithm;confusion network;speech coding;decoding strategy;confusion network algorithm;minimum bayes risk decision rule;speech recognition;speech recognition decoding vocabulary lattices error analysis character generation speech analysis measurement minimization methods character recognition;minimum bayes risk;decision rule	In mandarin large vocabulary continuous speech recognition, we can obtain recognition result which word error rate (WER) is minimum by using minimum Bayes risk(MBR) decoding strategy. One method of MBR decoding is that the word lattice can be transformed into confusion network in order to obtain the recognition result with minimum WER. According to the characteristic of mandarin, we proposed an improved confusion network generation algorithm based on prevenient works. The experimental results of mandarin large vocabulary continuous speech recognition show that the improved algorithm yields a lower WER than the MAP recognition and prevenient two confusion network generation algorithms.	algorithm;speech recognition;super robot monkey team hyperforce go!;vocabulary;word error rate	Bin Wu;Gang Liu;Jun Guo	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.618	speech recognition;computer science;machine learning;pattern recognition	Robotics	-19.926483458806135	-90.85394565045867	136205
8be23117187d8018b4220370a5b5757414c7e1dd	parameter selection for isolated word recognition using vector quantization	degradation;information technology;vocabulary;speech coding;code standards;books;vector quantization;parameter selection;word recognition;vector quantizer;computer science;vector quantization books vocabulary speech coding algorithm design and analysis computer science information technology laboratories code standards degradation;algorithm design and analysis;critical parameter	The use of vector quantization (VQ) in isolated-word recognition of a 20-word vocabulary is examined. A separate sequence of VQ code books is designed for each word in the recognition vocabulary and input words are classified by performing VQ and finding the sequence of code books that achieve the smallest average distortion. In this paper, critical parameters are noted and the results of parameter studies are presented.	book;distortion;vector quantization;vocabulary	David K. Burton;Joseph T. Buck;John E. Shore	1984		10.1109/ICASSP.1984.1172347	natural language processing;algorithm design;speech recognition;degradation;learning vector quantization;word recognition;computer science;machine learning;speech coding;pattern recognition;linde–buzo–gray algorithm;information technology;vector quantization	HCI	-20.08252947796097	-90.83634125959749	136387
2754e39be09c3ce474e2e9fb4774b9a93660e7f7	statistical analysis of orthographic and phonemic language corpus for word-based and phoneme-based polish language modelling	signal image and speech processing;acoustics;mathematics in music;engineering acoustics	This article presents the original results of Polish language statistical analysis, based on the orthographic and phonemic language corpus. Phonemic language corpus for Polish was developed by using automatic grapheme-to-phoneme conversion of the source orthographic language corpus, obtained from the National Corpus of Polish (NCP). The corpus contains the most frequently used Polish words, written with the use of phonemic notation. Performed statistical analysis of Polish language based on phonemic language corpus, includes frequency of occurrence calculation of the orthographic and phonemic language components, as well as their sequence. Statistical language data, obtained as a result of performed statistical analysis, enable to develop statistical word-based and phoneme-based language models for Polish. Applying these language models can effectively contribute to efficiency improvement of automatic speech recognition for Polish.	language model;national corpus of polish;orthographic projection;speech recognition	Piotr Klosowski	2017	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-017-0102-8	polish;speech recognition;language identification;computer science;natural language processing;notation;language model;text corpus;artificial intelligence;orthographic projection	NLP	-21.415394720378835	-81.74297304607991	136675
00c7e02520e20103b5e20a30a35a9aeafcfdac32	improving the performance of an lvcsr system through ensembles of acoustic models	system performance lvcsr system acoustic models large vocabulary continuous speech recognition ensembles construction bagging algorithms training sets word error rate reduction supervised learning classifiers boosting style algorithm acoustic model training optimal models large real world corpus carnegie mellon communicator dialog system;optimal models;word error rate;optimisation;acoustic model training;ensembles construction;decoding;supervised learning;selected works;large vocabulary continuous speech recognition;signal classification speech recognition acoustic signal processing optimisation learning artificial intelligence;bagging;acoustic modeling;acoustic signal processing;system performance;word error rate reduction;training sets;bagging algorithms;error analysis;boosting style algorithm;lvcsr system;boosting;voting;probability distribution;signal classification;speech recognition;bepress;classifiers;computer science;acoustic models;carnegie mellon communicator dialog system;learning artificial intelligence;sampling methods;large real world corpus;speech recognition boosting bagging decoding error analysis voting probability distribution computer science sampling methods system performance	This paper describes our work on applying ensembles of acoustic models to the problem of large vocabulary continuous speech recognition (LVCSR). We propose three algorithms for constructing ensembles. The first two have their roots in bagging algorithms; however, instead of randomly sampling examples our algorithms construct training sets based on the word error rate. The third one is a boosting style algorithm. Different from other boosting methods which demand large resources for computation and storage, our method present a more efficient solution suitable for acoustic model training. We also investigate a method that seeks optimal combination for models. We report experimental results on a large real world corpus collected from the Carnegie Mellon Communicator dialog system. Significant improvements on system performance are observed in that up to 15.56% relative reduction on word error rate is achieved.	acoustic cryptanalysis;acoustic model;algorithm;boosting (machine learning);computation;dialog system;randomness;sampling (signal processing);speech analytics;speech recognition;vocabulary;word error rate	Rong Zhang;Alexander I. Rudnicky	2003		10.1109/ICASSP.2003.1198921	probability distribution;sampling;speech recognition;bootstrap aggregating;voting;word error rate;computer science;machine learning;pattern recognition;supervised learning;boosting	NLP	-19.38409181707357	-89.85144697762611	136741
cd913ec47c850f989cdc7d66925b8217bf90d899	speaker indexing and adaptation using speaker clustering based on statistical model selection	processus gauss;evaluation performance;model selection;performance evaluation;gaussian processes;classification non supervisee;indexation automatique;bayes methods;evaluacion prestacion;speech processing;tratamiento palabra;traitement parole;selection modele;signal classification speaker recognition speech recognition statistical analysis gaussian processes vector quantisation bayes methods;archive;statistical model;indexing method;speaker recognition;automatic speech recognition;automatic recognition;reconocimiento voz;statistical analysis;seleccion modelo;adaptacion locutor;archivo;indexation;clasificacion no supervisada;signal classification;automatic indexing;modele statistique;classification signal;unsupervised classification;speech recognition;modelo estadistico;teoria mezcla;gaussian process;reconnaissance parole;classification automatique;vector quantisation;proceso gauss;automatic classification;mixture theory;bayesian information criterion;clasificacion automatica;speaker adaptation;theorie melange;indexing loudspeakers acoustic testing automatic speech recognition robustness informatics speech recognition voice mail bayesian methods gaussian distribution;reconocimiento automatico;bayesian information criterion unsupervised speaker indexing speaker adaptation speaker clustering statistical model selection automatic speech recognition utterance duration optimal speaker model gmm vq bic;indizacion automatica;reconnaissance automatique;adaptation locuteur	The paper addresses unsupervised speaker indexing and automatic speech recognition of discussions. In speaker indexing, there are two cases, where the number of speakers is unknown beforehand and where the number is known. When the specified number is unknown, it is difficult to apply to various data because it needs to determine several parameters like threshold. In addition, serious problems arise in applying a uniform model because variations in the utterance durations of speakers are large. We thus propose a method which can robustly perform speaker indexing for the two cases using a flexible framework in which an optimal speaker model (GMM or VQ) is selected based on the BIC (Bayesian information criterion). Moreover, we propose a combination method of speaker adaptation based on speaker selection and the indexing method. For real discussion archives, we demonstrated that indexing performance is higher than that of conventional methods for the two cases and speech recognition performance was improved by the combination method.	archive;bayesian information criterion;cluster analysis;google map maker;model selection;speaker recognition;speech recognition;statistical model;unsupervised learning;vector quantization	Masafumi Nishida;Tatsuya Kawahara	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1325995	speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;gaussian process;speech processing;mathematics;statistics	Vision	-20.598376937246563	-91.35014338453603	137009
00b86bc42d69e4a5d8fc8a565eac12fdbe2641e2	speaker and session variability in gmm-based speaker verification	mismatching;large scale generative model;processus gauss;evaluation performance;corpus based approach;likelihood ratio;session variability;train factor analysis models;gaussian mixture;analisis factorial;performance evaluation;speaker recognition testing nist performance analysis adaptation model speech recognition large scale systems statistical analysis statistical distributions performance evaluation;generic model;learning;gmm based speaker verification;maximum likelihood;gaussian processes;availability;taux erreur;disponibilidad;gauchissement;enfoque basado en corpus;evaluacion prestacion;speech processing;maximum vraisemblance;tratamiento palabra;traitement parole;maximum likelihood ii criteria;approche basee sur corpus;indexing terms;maximum likelihood estimation;speaker recognition evaluation;speaker verification;speaker recognition;aprendizaje;large scale;apprentissage;desadaptacion;analyse factorielle;posterior distribution;factor analysis;hidden variables;equal error rate;torcimiento;reconnaissance locuteur;feature warping session variability gmm based speaker verification maximum likelihood ii criteria large scale generative model posterior distribution speaker recognition train factor analysis models;ley a posteriori;error rate;teoria mezcla;speaker recognition gaussian processes maximum likelihood estimation;desadaptation;gaussian process;rapport vraisemblance;proceso gauss;indice error;mixture theory;disponibilite;loi a posteriori;theorie melange;joint factor analysis;speaker verification factor analysis gaussian mixture;warping;maxima verosimilitud;feature warping;relacion verosimilitud	We present a corpus-based approach to speaker verification in which maximum-likelihood II criteria are used to train a large-scale generative model of speaker and session variability which we call joint factor analysis. Enrolling a target speaker consists in calculating the posterior distribution of the hidden variables in the factor analysis model and verification tests are conducted using a new type of likelihood II ratio statistic. Using the NIST 1999 and 2000 speaker recognition evaluation data sets, we show that the effectiveness of this approach depends on the availability of a training corpus which is well matched with the evaluation set used for testing. Experiments on the NIST 1999 evaluation set using a mismatched corpus to train factor analysis models did not result in any improvement over standard methods, but we found that, even with this type of mismatch, feature warping performs extremely well in conjunction with the factor analysis model, and this enabled us to obtain very good results (equal error rates of about 6.2%)	factor analysis;generative model;google map maker;heart rate variability;hidden variable theory;software verification;spatial variability;speaker recognition;text corpus	Patrick Kenny;Gilles Boulianne;Pierre Ouellet;Pierre Dumouchel	2007	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2007.894527	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;gaussian process;speech processing;mathematics;maximum likelihood;statistics	Web+IR	-20.288210342241122	-91.3158717306172	137010
3838e5ac00ef83feed4171b3c94f815ef844f002	modelling semantic context of oov words in large vocabulary continuous speech recognition	large vocabulary continuous speech recognition;semantic context;out of vocabulary;training;vocabulary;semantics;computational modeling;context vocabulary context modeling speech recognition semantics training computational modeling;vocabulary indexing speech recognition;proper names;speech recognition;second pass speech recognition semantic context out of vocabulary words oov words diachronic nature broadcast news data large vocabulary continuous speech recognition proper names automatic indexing audio video content reliable automatic transcriptions diachronic audio documents lvcsr transcriptions topic context latent dirichlet allocation continuous word vector representations task specific word context representations neural bag of weighted word model french broadcast news videos raw embeddings skip gram models;context modeling;context;semantic context large vocabulary continuous speech recognition out of vocabulary proper names	The diachronic nature of broadcast news data leads to the problem of out-of-vocabulary OOV words in large vocabulary continuous speech recognition LVCSR systems. Analysis of OOV words reveals that a majority of them are proper names PNs. However, PNs are important for automatic indexing of audio-video content and for obtaining reliable automatic transcriptions. In this paper, we focus on the problem of OOV PNs in diachronic audio documents. To enable the recovery of the PNs missed by the LVCSR system, relevant OOV PNs are retrieved by exploiting the semantic context of the LVCSR transcriptions. For retrieval of OOV PNs, we explore topic and semantic context derived from latent Dirichlet allocation LDA topic models, continuous word vector representations and the neural bag-of-words NBOW model which is capable of learning task specific word and context representations. We propose a neural bag-of-weighted words NBOW2 model which learns to assign higher weights to words that are important for retrieval of an OOV PN. With experiments on French broadcast news videos, we show that the NBOW and NBOW2 models outperform the methods based on raw embeddings from LDA and Skip-gram models. Combining the NBOW and NBOW2 models gives a faster convergence during training. Second pass speech recognition experiments, in which the LVCSR vocabulary and language model are updated with the retrieved OOV PNs, demonstrate the effectiveness of the proposed context models.	bag-of-words model;digital video;experiment;language model;latent dirichlet allocation;local-density approximation;speech analytics;speech recognition;vocabulary;word embedding	Imran A. Sheikh;Dominique Fohr;Irina Illina;Georges Linarès	2017	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2017.2651361	natural language processing;speech recognition;computer science;proper noun;semantics;linguistics;context model;computational model	NLP	-19.471251603817162	-80.35717895779023	137088
33794ae99e46acac41c081c43584df7f7c960246	visual clue: an approach to predict and highlight next character	keyboards;graphically similar characters visual clue next character highlighting approach next character prediction approach user friendly word prediction system virtual keyboard indian languages keystrokes cognitive load alphabets sizes phonetically similar characters;text analysis;predictive virtual keyboard human computer interaction visual clue;keyboards visualization prediction algorithms context color measurement hardware;cognition;search problems;augmented reality;natural language processing;text analysis augmented reality cognition keyboards natural language processing search problems	The motivation of this research is to develop a user friendly word prediction system augmented with virtual keyboard in the context of Indian languages. The objective would be not only to save keystrokes but also to reduce the cognitive load to compose the text accurately. In the context of Indian language, in addition to large alphabets sizes there are a lot of phonetically or graphically similar characters which needs more time to search the desired characters and occasionally leads to taping wrong characters. This issue can be addressed by highlighting the next probable characters and thus offering visual clue and mitigating errors. The proposed approach not only help in avoiding the error, it also highlight the required character when user have misspelled part of word while composing text.	benchmark (computing);character encoding;event (computing);simulation;tree structure;usability;virtual keyboard	Manoj Kumar Sharma;Sayan Sarcar;Pradipta Kumar Saha;Debasis Samanta	2012	2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)	10.1109/IHCI.2012.6481820	natural language processing;computer vision;augmented reality;text mining;speech recognition;cognition;computer science;artificial intelligence;machine learning	Visualization	-27.37547670777657	-88.89326137258283	137934
8ca96d73ac142ead0bf51860e684586dec9f2019	large-vocabulary spoken word recognition using time-delay neural network phoneme spotting and predictive lr-parsing	phoneme;prediccion;learning;reconocimiento palabra;fonema;phonem;backpropagation;algorithme;aprendizaje;algorithm;retropropagation;apprentissage;time delay neural network;spoken word recognition;automatic recognition;palabra;analizador sintaxico;speech recognition;word;parser;reconnaissance parole;reseau neuronal;retropropagacion;analyseur syntaxique;prediction;red neuronal;reconocimiento automatico;mot;reconnaissance automatique;neural network;algoritmo	Abstract#R##N##R##N#This paper proposes a large-vocabulary speech recognition system using a phoneme spotting method by a time-delay neural network (TDNN) and a predictive LR parser. This is the first attempt to recognize large vocabulary speech using neural networks. The prediction of phonemes in words is performed by a predictive LR parser. Time alignment between predicted phonemes by the predicted LR parser and phoneme spotting results via TDNN is realized using a DTW (dynamic time warping) method. Speaker-dependent recognition for a 5240-word vocabulary using 2620 test words uttered by a male announcer resulted in a rate of 92.6 percent for the top choices, rates of 97.6 and 99.1 percent for the second and fifth choices, respectively.	artificial neural network;lr parser;parsing;speech recognition;time delay neural network;vocabulary	Yasuhiro Minami;Hidefumi Sawai;Masanori Miyatake	1991	Systems and Computers in Japan	10.1002/scj.4690220111	natural language processing;speech recognition;prediction;computer science;artificial intelligence;backpropagation;machine learning;word;time delay neural network;artificial neural network	NLP	-19.65280212919186	-88.04002207049541	138018
a98ef90891ab9e6109fa5d7d32b50919a8ae830c	developing a flexible spoken dialog system using simulation	extensive use;dialog system;content-dependent verbal description;boston area;user query;development process;real user interaction;cooperative response;unique dialog;dialog development	In this paper, we describe a new methodology to develop mixed-initiative spoken dialog systems, which is based on the extensive use of simulations to accelerate the development process. With the help of simulations, a system providing information about a database of nearly 1000 restaurants in the Boston area has been developed. The simulator can produce thousands of unique dialogs which benefit not only dialog development but also provide data to train the speech recognizer and understanding components, in preparation for real user interactions. Also described is a strategy for creating cooperative responses to user queries, incorporating an intelligent language generation capability that produces content-dependent verbal descriptions of listed items.	additive white gaussian noise;database;dialog system;finite-state machine;full scale;interaction;language model;natural language generation;simulation;speech recognition;speech synthesis;spoken dialog systems;spontaneous order;synthetic intelligence;utility functions on indivisible goods	Grace Chung	2004			natural language processing;speech recognition;dialog box;computer science;dialog system;software development process	NLP	-24.7239409787844	-85.19389137379719	138164
de18b5d4c285b850e3282f256ffc3066a2feadf5	sensitivity analysis on-line with a conversational dynamic programming system.	dynamic program;sensitivity analysis			Pierre E. Bonzon	1971			machine learning;artificial intelligence;dynamic programming;computer science	NLP	-27.51087536118081	-85.6955013208613	138220
3b516a2fe96d333b502f6831cdd26f699d89a019	reliability-weighted acoustic model adaptation using crowd sourced transcriptions	acoustic modeling	This paper focuses on adaptation of acoustic models using speech transcribed by multiple noisy experts. A simple approach involves combining multiple transcripts using word frequency based Recognizer Output Voting Error Reduction (ROVER) followed by adaptation using the combined transcripts. But this assumes that the transcripts being combined are equally reliable. To overcome this assumption, we use two sets of scores to estimate this reliability. The first set is based on answers to some questions given by the transcribers. The second set is derived in an unsupervised way using the word frequency based ROVER transcripts and baseline acoustic models. The overall confidence is a convex combination of these scores and is used to perform a confidence weighted fusion. We adapt the baseline acoustic models using these combined transcripts. Recognition results for a Mexican Spanish ASR system show an absolute improvement of 0.5% in word error rate and 0.9% in sentence error rate.	acoustic cryptanalysis;acoustic fingerprint;acoustic model;baseline (configuration management);unsupervised learning;word error rate;word lists by frequency	Kartik Audhkhasi;Panayiotis G. Georgiou;Shrikanth (Shri) Narayanan	2011			speech recognition;computer science	NLP	-19.55722368564038	-86.12763209419126	138337
6e704f0c8f78589867f94ac7eecef61f92f96087	a speaker-independent, syntax-directed, connected word recognition system based on hidden markov models and level building	words per minute;reconnaissance multilocuteur;syntax;hidden markov models vocabulary speech recognition natural languages linear predictive coding speech analysis vector quantization system testing laboratories computer networks;hidden markov model;speech analysis;vocabulary;reconocimiento de la palabra;natural languages;syntaxe;connected word;computer networks;linear predictive coding;hidden markov models;vector quantization;speaker independent;mot relie;word recognition;speech recognition;system testing;vector quantizer;reconnaissance parole;sintaxis;state transition	In the last several years, a wide variety of techniques have been developed which make practical the implementation and development of large networks for recognizing connected sequences of words. Included among these techniques are efficient and accurate speech modeling methods (e.g.,, vector quantization, hidden Markov models) and efficient, optimal network search procedures (i.e., level building). In this paper we show how to integrate these techniques to give a speaker-independent, syntax-directed, connected word recognition system which requires only a modest amount of computation, and whose performance is comparable to that of previous recognizers requiring an order of magnitude more computation. In particular, the recognizer we studied was an airlines information and reservation system using a 129 word vocabulary, and a deterministic syntax (grammar) with 144 states, 450 state transitions, and 21 final states, generating more than 6 X lo9 sentences. An evaluation of the system, using six talkers each speaking 51 test sentences, yielded a sentence accuracy of about 75 percent resulting from a word accuracy of about 93 percent, for an average speaking rate of about 210 words per minute.	computation;finite-state machine;hidden markov model;information;markov chain;vector quantization;vocabulary;words per minute	Lawrence R. Rabiner;Stephen E. Levinson	1985	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/TASSP.1985.1164586	natural language processing;linear predictive coding;speech recognition;syntax;word recognition;computer science;pattern recognition;natural language;system testing;vector quantization;hidden markov model	ML	-21.48474512457794	-86.12359183148081	139255
178b499f5aa1c74291d0d05b2be6759dbbb10640	model compression for gmm based speaker recognition systems	compression ratio;speaker recognition;system modeling;gaussian mixture model;network model	For large-scale deployments of speaker verification systems models size can be an important issue for not only minimizing storage requirements but also reducing transfer time of models over networks. Model size is also critical for deployments to small, portable devices. In this paper we present a new model compression technique for Gaussian Mixture Model (GMM) based speaker recognition systems. For GMM systems using adaptation from a background model, the compression technique exploits the fact that speaker models are adapted from a single speakerindependent model and not all parameters need to be stored. We present results on the 2002 NIST speaker recognition evaluation cellular telephone corpus and show that the compression technique provides a good tradeoff of compression ratio to performance loss. We are able to achieve a 56:1 compression (624KB 11KB) with only a 3.2% relative increase in EER (9.1% 9.4%).	enhanced entity–relationship model;google map maker;mixture model;mobile device;mobile phone;requirement;speaker recognition	Douglas A. Reynolds	2003			systems modeling;network model;mixture model;compression ratio;speech recognition;speaker recognition;nist;artificial intelligence;speaker diarisation;compression (physics);pattern recognition;computer science	ML	-19.315756184492955	-90.52643287224754	139520
ac50078e58b33fc98d30187a88b1364aff716ba9	unification-based semantic interpretation in the bbn spoken language system	unification-based semantic interpretation;integrated syntax;semantic selectional restriction;relational noun;semantic interpretation problem;bbn spoken language system;definite clause;current state;recognize continous speech;unification formalism;syntax;noun;natural language;language;speech;semantic interpretation;semantics;definite clause grammar;grammars	"""This paper describes the current state of work on unification-based semantic interpretation in HARC (for Hear and Recognize Continous speech) the BBN Spoken Language System. It presents the implementation of an integrated syntax/semantics grammar written in a unification formalism similar to Definite Clause Grammar. This formalism is described, and its use in solving a number of semantic interpretation problems is shown. These include, among others, the encoding of semantic selectional restrictions and the representation of relational nouns and their modifiers. 1 I N T R O D U C T I O N Over the past year work on semantic interpretation in the BBN Spoken Language System has shifted from a Montague Grammar (Montague, 1973) style rule-for-rule approach to one which attempts to carry out semantic interpretation directly in the unification grammar rules themselves. This is accomplished by adding semantic features to the grammar rules, placing them on the same footing as the existing syntactic features. Meaning representations are thereby constructed, and semantic filtering constraints applied, as part of parsing the utterance. We view such a move as having essentially three advantages: • more information is available to semantic interpretation, so it is possible to gain higher coverage • syntax and semantics are integrated, so semantic filtering constraints can be applied as constituents are built and attached • this integration is simple and does not require any complex engineering of cooperating software modules All three of these advantages are important ones for a spoken language system. The HARC system has the following overall organization. Spoken input is initially analyzed by the """"N-best"""" algorithm(Chow and Schwartz, 1989), converting it into"""" a rank-ordered set of N best word-sequence hypotheses (for a given value of N). These N hypotheses are then analyzed by the parser, using the combined syntactic/semantic grammar. Those hypotheses which are syntactically and semantically allowed emerge from the parser as initial logical forms in which quantifiers are interpreted """"in place"""". Next, the quantifier module assigns scopes and passes the translation to the anaphora component, which then resolves the referent of intraand extra-sentential pronouns. The completed logical form is then passed to the back-end component whose responsibility is to compute the appropriate response to the user's input. The present paper confines itself to a description of the combined syntactic/semantic grammar, along with some discussion of the parsing algorithm. We will first consider the representational framework in which the grammar is written. 2 THE GRAMMAR FORMALISM The BBN grammar formalism, described in detail in (Boisen et al., 1989), is most closely related to Definite Clause Grammar (Pereira and Warren, 1980). Rules consist of a single left-hand term and zero or more right hand terms. Terms can have features, whose values are themselves terms. Variables, indicated by the """":"""" prefix, indicate identity between different slots in and among terms. Here is an example of a simple grammar rule written in this formalism: (VP (AGR :P :N) :MOOD (WH-) :TRX :TRY) (V :CONTRACT (TRANSITIVE) :P :N :MOOD) (NP :NSUBCATFRAME (WH-) :TRX :TRY)"""	advanced graphics riser;algorithm;anaphora (linguistics);definite clause grammar;formal grammar;formal system;han unification;modular programming;montague grammar;parsing;quantifier (logic);semantic interpretation;semantics (computer science);unification (computer science);vp/css;warren abstract machine	David Stallard	1989			natural language processing;noun;semantic interpretation;syntax;speech;semantic compression;semantics;linguistics;language;natural language;definite clause grammar;programming language	NLP	-30.734565640309068	-82.14391789777883	139841
610f893d9dcee2ce2859087122a2a0acca364417	selective training for hidden markov models with applications to speech classification	analyse parole;forward backward;e set alphabet;training analysis;convergence properties training data hidden markov models speech classification maximum likelihood estimation training tokens speech unit hmm parameters training set selective training method outliers feature statistics confusable patterns foreign accent classification language identification speech recognition e set alphabet error rates forward backward training open test conditions maximum mutual information estimation;traitement signal;language identifier;convergence;maximum mutual information;modelo markov;generic model;maximum likelihood;complexite calcul;reconocimiento palabra;hidden markov model;analisis palabra;hmm parameters;convergence of numerical methods;speech processing;speech analysis;convergence of numerical methods speech recognition speech processing hidden markov models maximum likelihood estimation error statistics pattern classification;maximum vraisemblance;training tokens;identificateur langage;speech classification;classifier training;natural languages;testing;indexing terms;maximum likelihood estimation;classification;statistical model;selective training method;maximum mutual information estimation;training data;error analysis;forward backward training;training set;complejidad computacion;markov model;maximum likelihood estimate;hidden markov models;outliers;computational complexity;signal processing;open test conditions;convergence properties;modele statistique;pattern classification;statistics;confusable patterns;feature statistics;language identification;error rate;error rates;speech recognition;mutual information;speech unit;error statistics;psychanalyse didactique;modelo estadistico;reconnaissance parole;foreign accent classification;modele markov;procesamiento senal;hidden markov models maximum likelihood estimation training data statistics natural languages speech recognition error analysis testing mutual information convergence;clasificacion;maxima verosimilitud	Traditional maximum likelihood estimation of hidden Markov model parameters aims at maximizing the overall probability across the training tokens of a given speech unit. Therefore, it disregards any interaction and biases across the models in the training procedure. Often the resulting model parameters do not result in minimum error classiication in the training set. A new selective training method is proposed which controls the innuence of outliers in the training data on the generated models. The resulting models are shown to possess feature statistics which are more clearly separated for confusable patterns. The proposed selective training procedure is used for hidden Markov model training, with application to foreign accent classiication, language identiication, and speech recognition using the E-set alphabet. The resulting error rates are measurably improved over traditional Forward-Backward training under open test conditions. The proposed method is similar in terms of its goal to maximum mutual information estimation training, however it requires less computation, and the convergence properties of maximum likelihood estimation are retained in the new formulation.	computation;feature vector;hidden markov model;markov chain;mutual information;speech recognition;teaching method;test set	Levent M. Arslan;John H. L. Hansen	1999	IEEE Trans. Speech and Audio Processing	10.1109/89.736330	speech recognition;computer science;machine learning;hidden semi-markov model;pattern recognition;maximum likelihood;hidden markov model;statistics	ML	-20.00000849702074	-91.84489015779907	139887
5c736d1bfb2b407c2ee1b372f9a60054767b1ef1	there and back again: a semantic analysis	crosslinguistic study;structure syntaxique;variation semantique;filologias;adverbe;semantic variation;semantics;semantique;semantic interpretation;etude translinguistique;linguistica;anglais;interpretation semantique;ambiguity;english;syntactic structure;grupo a;adverb;semantic analysis;ambiguite	This paper presents a cross-linguistic survey of the interpretations that decomposition adverbs like again permit. The survey distinguishes two different types of predicates that are combined with again: lexical accomplishments like ‘open the door’, on the one hand, and combinations of a motion verb with a directional Prepositional Phrase (PP) on the other (for example, ‘walk to the summit’). There is systematic crosslinguistic variation with the latter type of predicate: a language only permits a restitutive reading of again with such a predicate if the language has resultative constructions. I argue that in these languages the PP can function as a result phrase. In languages without resultatives, the PP cannot be a result phrase, and a restitutive reading is impossible. The data support an analysis of restitutive again that is sensitive to the presence of a result phrase in the syntax.		Sigrid Beck	2005	J. Semantics	10.1093/jos/ffh016	semantic interpretation;philosophy;english;semantics;linguistics;sociology	NLP	-33.48986558200755	-81.73959616907766	140612
4ff43ef0db905e9c8d3ef7594174aea97cfd1868	belief hidden markov model for speech recognition	belief hmm speech recognition hmm theory of belief functions;speech recognition hidden markov models;hidden markov models;hmm belief hidden markov model speech recognition spoken words;speech recognition;hidden markov models speech recognition acoustics speech probabilistic logic training context modeling	Speech Recognition searches to predict the spoken words automatically. These systems are known to be very expensive because of using several pre-recorded hours of speech. Hence, building a model that minimizes the cost of the recognizer will be very interesting. In this paper, we present a new approach for recognizing speech based on belief HMMs instead of probabilistic HMMs. Experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems.	acoustic cryptanalysis;finite-state machine;hidden markov model;markov chain;speech recognition;speech synthesis	Siwar Jendoubi;Boutheina Ben Yaghlane;Arnaud Martin	2013	2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)	10.1109/ICMSAO.2013.6552563	natural language processing;speaker recognition;speech recognition;computer science;pattern recognition;markov model;hidden markov model	Robotics	-20.802977916170164	-87.80076347632405	140631
0308008878d61ea6f67c7e07ea0c0274bc7ae6f7	using reinforcement learning to build a better model of dialogue state	reinforcement learning;spoken dialogue system;tutoring system;dialogue manager	Given the growing complexity of tasks that spoken dialogue systems are trying to handle, Reinforcement Learning (RL) has been increasingly used as a way of automatically learning the best policy for a system to make. While most work has focused on generating better policies for a dialogue manager, very little work has been done in using RL to construct a better dialogue state. This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems.	experiment;reinforcement learning	Joel R. Tetreault;Diane J. Litman	2006			natural language processing;computer science;artificial intelligence;reinforcement learning	NLP	-27.152663441408148	-86.8443508006026	140845
1665000a62f4b94ee5abbff358b4f5c353a33b37	lium's statistical machine translation system for iwslt 2010		This paper describes the systems developed by the LIUM laboratory for the 2009 IWSLT evaluation. We participated in the Arabic and Chinese to English BTEC tasks. We developed three different systems: a statistical phrase-based system using the Moses toolkit, an Statistical Post-Editing system and a hierarchical phrase-based system based on Joshua. A continuous space language model was deployed to improve the modeling of the target language. These systems are combined by a confusion network based approach.	compiler;language model;moses;statistical machine translation	Anthony Rousseau;Loïc Barrault;Paul Deléglise;Yannick Estève	2009			speech recognition;engineering;artificial intelligence;communication	NLP	-23.85643886132669	-83.13878561607171	140876
5e0d1988e8bc5a60f2bf61075a4ddb741d6c7ecf	translation methodology in the spoken language translator: an evaluation	jamforande sprakvetenskap och lingvistik;spoken language translation;speech translation;general language studies and linguistics	In this paper we describe how the translation methodology adopted for the Spoken Language Translator (SLT) addresses the characteristics of the speech translation task in a context where it is essential to achieve easy customization to new languages and new domains. We then discuss the issues that arise in any attempt to evaluate a speech translator, and present the results of such an evaluation carried out on SLT for several language pairs. 1 The nature of the speech	formal language	David M. Carter;Ralph Becket;Manny Rayner;Robert Eklund;Catriona MacDermid;Mats Wirén;Sabine Kirchmeier-Andersen;Christina Philp	1997	CoRR		direct method;computer-assisted translation;natural language processing;speech recognition;universal networking language;example-based machine translation;computer science;language education;interlingual machine translation;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-28.30928436899885	-82.24071846995517	140918
8ebabe07580f5b9355c2e60bb3d7bf72b242e670	prosodic unit selection using an imitation speech database		Starting with a rule based prosody generation system, we try to improve the naturalness of the generated prosody by using a corpus based approach, without losing the advantages of the rule based method. To achieve this, a prosodic unit selection method is introduced, which is similar in its approach to the waveform unit selection used by large unit inventory waveform concatenation systems. Trying to avoid the problem of incomplete unit description in existing prosodic databases, a new method of data collection and labeling is introduced. A small database of the proposed kind was collected, and results of applying selection algorithm to it are given. The approach described in this paper could be useful for improving prosody naturalness and assisting in personalizing prosody. It requires relatively little expert manual work, and can be used for small footprint TTS systems.	concatenation;database;netware file system;selection algorithm;semantic prosody;speech corpus;text corpus;waveform	Joram Meron	2001			prosodic unit;imitation;natural language processing;artificial intelligence;speech recognition;computer science	NLP	-21.08161808613518	-84.00545767797203	141501
46c463cf040f70e96050ba91085bf46ae7f0060b	editing by voice and the role of sequential symbol systems for improved human-to-computer information rates	shorttalk;keyboards;command language;editing language;information rates;natural languages;physics computing;information rates humans natural languages keyboards speech recognition user interfaces physics computing pain automatic speech recognition handicapped aids;speech based user interfaces;automatic speech recognition;dictation;handicapped aids;dictation systems;natural language;nonverbal means sequential symbol systems human to computer information rates text composing voice editing dictation systems automatic speech recognition symbolizations shorttalk language editing command sequences primitive symbol combinations;word processing speech based user interfaces text editing dictation speech recognition;speech based user interfaces man machine systems speech recognition natural languages;pain;information rate;speech recognition;humans;editing language text editing dictation systems automatic speech recognition shorttalk;user interfaces;man machine systems;word processing;text editing	Composing text on the computer is usually heavily dependent on editing, such as moving text around, correcting spacing, and inserting punctuation characters. Dictation systems, based on automatic speech recognition, are not known for their efficiency as an editing tool—something that significantly reduces their potential for freeing the user from the keyboard. Speech recognition has almost invariably been tied to natural language, but we point out that this approach is inherently disadvantageous in important ways. Instead, given the evidence that humans routinely become experts at sequencing signs not related to natural language, we propose that editing command languages should rely on symbolizations similar to that of the keyboard. We introduce ShortTalk, an editing language whose command sequences are primitive symbol combinations that are not confusable with dictation. We argue that ShortTalk by construction may solve common editing situations much more efficiently than by use of keyboard and mouse. Our experimental results for ShortTalk indicate that an average information rate of about 16 bps for editing commands is achievable. Thus, editing by speech may be more efficient than by non-verbal means.	entropy (information theory);game controller;natural language;speech recognition	Nils Klarlund	2003		10.1109/ICME.2003.1221371	natural language processing;speech recognition;computer science;natural language	NLP	-26.18116946529379	-88.0368238415927	141709
4deb324dca009d4a0ee096f6238bfc9b608594e1	a design for a parser for english	natural language understanding;control structure	Most current natural language understanding systems utilize parsers whose control structures are based on simulating non-deterministic machines. This paper presents a design for an implementation of a parser that attempts to operate deterministically. The discussion focuses on mechanisms that allow grammar rules to diagnose what grammatical structure should be built next at each point during the analysis process. An example grammar is presented, and a sample parse is discussed at length to illustrate these mechanisms.	control flow;deterministic algorithm;natural language understanding;parsing;simulation	Mitchell Marcus	1976		10.1145/800191.805527	natural language processing;parser combinator;compiler-compiler;parsing expression grammar;operator-precedence grammar;computer science;parsing;glr parser;emergent grammar;linguistics;programming language;attribute grammar;top-down parsing	NLP	-30.981699061897906	-81.09416589390706	142153
03af1b854ca5a37316c1d30a6ca3c6068f5599d4	ku-ispl speaker recognition systems under language mismatch condition for nist 2016 speaker recognition evaluation		Korea University – Intelligent Signal Processing Lab. (KUISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.	cluster analysis;database normalization;discrepancy function;experiment;information services procurement library;signal processing;spatial variability;speaker recognition;super robot monkey team hyperforce go!	Suwon Shon;Hanseok Ko	2017	CoRR		natural language processing;speaker recognition;speaker diarisation;speech recognition;linguistics	NLP	-23.462967325442587	-84.52779576060973	142471
3e0e613aa968db9490d6e430b3cabe4633d1bb3a	naval ocean systems center (nosc) user interface technology, code 441	application task;naval ocean systems center;user interface technology;speech databases;evaluation speech recognition technology;darpa-developed speech recognition technology;naval application	NOSC has identified the Battle Group Tactical Trainer (BGTT) as an appropriate candidate for the integration of speech recognition in the navy. Current plans are to record speech samples and create transcriptions from the BGTT exercises at NOSC and the Naval Post-Graduate School. We will then evaluate the recordings to determine appropriate tasks for speech recognition, and create a language model to describe the speech. Following this, a speech database will be collected for testing and evaluation.	language model;speech corpus;speech recognition;user interface	Steve Nunn;Nancy Hupp	1989			simulation;speech recognition	NLP	-24.20243695147495	-84.52957055464373	142548
92e1f3ec1c4917a2e36f53a84ae3ff335606ddda	genetic algorithm based optimization of partly-hidden markov model structure using discriminative criterion	acoustic model;optimisation;partly hidden markov model;likelihood ratio;tecnologia electronica telecomunicaciones;optimizacion;hidden markov model;modele markov variable cachee;algoritmo genetico;discriminant analysis;analyse discriminante;analisis discriminante;hidden markov models;reconocimiento voz;weighted likelihood ratio maximization criterion;lecture talk speech recognition;algorithme genetique;speech recognition;genetic algorithm;optimization;reconnaissance parole;tecnologias;grupo a;rapport vraisemblance;relacion verosimilitud	A discriminative modeling is applied to optimize the structure of a Partly-Hidden Markov Model (PHMM). PHMM was proposed in our previous work to deal with the complicated temporal changes of acoustic features. It can represent observation dependent behaviors in both observations and state transitions. In the formulation of the previous PHMM, we used a common structure for all models. However, it is expected that the optimal structure which gives the best performance differs from category to category. In this paper, we designed a new structure optimization method in which the dependence of the states and the observations of PHMM are optimally defined according to each model using the weighted likelihood-ratio maximization (WLRM) criterion. The WLRM criterion gives high discriminability between the correct category and the incorrect categories. Therefore it gives model structures with good discriminative performance. We define the model structure combination which satisfy the WLRM criterion for any possible structure combinations as the optimal structures. A genetic algorithm is also applied to the adequate approximation of a full search. With results of continuous lecture talk speech recognition, the effectiveness of the proposed structure optimization is shown: it reduced the word errors compared to HMM and PHMM with a common structure for all models.	genetic algorithm;hidden markov model;markov chain	Tetsuji Ogawa;Tetsunori Kobayashi	2006	IEICE Transactions	10.1093/ietisy/e89-d.3.939	speech recognition;genetic algorithm;likelihood-ratio test;computer science;artificial intelligence;machine learning;pattern recognition;acoustic model;mathematics;hidden markov model;statistics	Vision	-21.46431226162328	-91.46624614499132	142602
b8b650386097fa6b9b2114d78c7d3cc3832746ff	using finite state transducers for helping foreign language learning		The interest and demand to foreign language learning are increased tremendously along with the globalization and freedom of movement in the world. Today, the technological developments allow the creation of supportive materials for foreign language learners. However, the language acquisition between languages with high typological differences still poses challenges for this area and the learning task it self. This paper introduces our preliminary study for building an educational application to help foreign language learning between Turkish and English. The paper presents the use of finite state technology for building a Turkish word synthesis system (which allows to choose word-related features among predefined grammatical affix categories such as tense, modality and polarity etc...) and a wordlevel translation system between the languages in focus. The developed system is observed to outperform the popular online translation systems for word-level translation in terms of grammatically correct outputs.	finite-state transducer;machine translation;modality (human–computer interaction)	Hasan Kaya;Gülsen Eryigit	2015		10.18653/v1/W15-4414	direct method;natural language processing;computer science;language industry;linguistics;communication	NLP	-28.47583822420803	-84.21974390713274	142672
87afaae8fbb417b0a6c0376d9e53288880c7017a	danpo-a transcription-based dictionary for danish speech technology		We present a new strategy for the creation of phonetic lexicons. As we argue, lexical resources for speech technology integration should be informed by transcriptions of spontaneous speech. We illustrate our strategy with examples from the dictionary DanPO (Danish Phonetic-Orthographic Dictionary) which is developed at the Center for Computational Modelling of Language (CMOL). For reference corpus we used DanPASS consisting of 57 recordings of task-oriented monologs, transcribed by professional and MAlevel phoneticians using the Danish SAMPA phonetic alphabet. From the transcriptions, dictionaries and concordances were compiled, and these resources were merged with the (prescriptive) phonetic renderings of a standard Danish word dictionary of 87,000 lemmata. As an effect of the “transcription informed” strategy, DanPO is expected to significantly improve the success rate of automatic speech recognizers, as well as the naturalness of artificial voices. Furthermore, we devise an experimental strategy in order to evaluate the dictionary and further improve later versions.	compiler;concordance (publishing);dictionary;finite-state machine;lexicon;medical transcription;orthographic projection;speech technology;spontaneous order;transcription (software)	Peter Rossen Skadhauge;Peter Juel Henrichsen	2005			speech recognition;speech technology;transcription (biology);danish;computer science	NLP	-25.856462409079892	-83.68660532904838	142827
018dbbffedb9e4559a00b94d3f6613e79c5da6fc	class phrase models for language modelling	decoding;dictionaries;classification algorithms;language model;context modeling;speech recognition;mutual information;language modeling;natural languages;linguistics;nomograms	Previous attempts to automatically determine multi-words as the basic unit for language modeling have been successful for extending bigram models [10, 9, 2, 8] to improve the perplexity of the language model and/or the word accuracy of the speech decoder. However, none of these techniques gave improvements over the trigram model so far, except for the rather controlled ATIS task [8]. We therefore propose an algorithm, that minimizes the perplexity improvement of a bigram model directly. The new algorithm is able to reduce the trigram perplexity and also achieves word accuracy improvements in the Verbmobil task. It is the natural counterpart of successful word classi cation algorithms for language modeling [4, 7] that minimize the leaving-one-out bigram perplexity. We also give some details on the usage of class nding techniques and m-gram models, which can be crucial to successful applications of this technique.	algorithm;automatic transmitter identification system (television);bigram;language model;perplexity;trigram;verbmobil	Klaus Ries;Finn Dag Buø;Alexander H. Waibel	1996				NLP	-20.379155851050115	-86.43746966270625	142942
c0ee9d02e9c0c7f45d7698f9da302d1d595534df	tuke-bnews-sk: slovak broadcast news corpus construction and evaluation		This article presents an overview of the existing acoustical corpuses suitable for broadcast news automatic transcription task in the Slovak language. The TUKE-BNews-SK database created in our department was built to support the application development for automatic broadcast news processing and spontaneous speech recognition of the Slovak language. The audio corpus is composed of 479 Slovak TV broadcast news shows from public Slovak television called STV1 or “Jednotka” containing 265 hours of material and 186 hours of clean transcribed speech (4 hours subset extracted for testing purposes). The recordings were manually transcribed using Transcriber tool modified for Slovak annotators and automatic Slovak spell checking. The corpus design, acquisition, annotation scheme and pronunciation transcription is described together with corpus statistics and tools used. Finally the evaluation procedure using automatic speech recognition is presented on the broadcast news and parliamentary speeches test sets.	ski combinator calculus;speech recognition;spell checker;spontaneous order;transcriber;transcription (software)	Matús Pleva;Jozef Juhár	2014			speech recognition;multimedia	NLP	-22.795466715601556	-84.31769717920277	143178
8687e36a640ce13834c3c7c292f19f699406eb1e	evaluation frameworks for speech translation technologies	comparative research	This paper reports on activities carried out under the European project PF-STAR and within the CSTAR consortium, which aim at evaluating speech translation technologies. In PFSTAR, speech translation baselines developed by the partners and off-the-shelf commercial systems will be compared systematically on several language pairs and application scenarios. In CSTAR, evaluation campaigns will be organized, on a regular basis, to compare research baselines developed by the members of the consortium. The first evaluation campaign, which will take place in 2003, will focus on written language translation by exploiting a large phrase-book parallel corpus covering several European and Asiatic languages.	machine translation;parallel text	Marcello Federico	2003			speech translation;speech recognition;natural language processing;written language;computer science;comparative research;artificial intelligence	NLP	-24.373761910414032	-83.59394293593196	143378
803bdfcf3076559a45f47f8ae9b1ce061257a2c3	ubm-based incremental speaker adaptation	broadcast news;broadcasting speaker recognition speech processing audio databases;information science;broadcasting news;real time;speech processing;real time processing;speech;speaker recognition;training data;adaptive algorithm;vector quantization;maximum likelihood linear regression;real time systems speech asia speaker recognition maximum likelihood linear regression information science instruction sets training data broadcasting vector quantization;real time speaker segmentation system;universal background model;broadcasting news incremental speaker adaptation universal background model real time processing training data real time speaker segmentation system;audio databases;broadcasting;incremental speaker adaptation;speaker adaptation;asia;instruction sets;real time systems	This paper addresses a novel algorithm of incremental speaker adaptation (ISA) based on universal background model (UBM) for saving storage and real-time processing. This algorithm can be seen as an extension of traditional speaker adaptation. It consists of two steps, adaptation and combination. It not only considers the speaker’s characteristics in limited training data, but also prohibits over-fitting of the updated model. The incremental adaptation algorithm needs little storage and meets the requirement of real-time processing. In order to evaluate the efficiency and effectivity of the proposed approach, a real-time speaker segmentation system for broadcasting news is built. Experiment results demonstrate that our approach yields real time operation and achieves satisfactory performance.	algorithm;overfitting;real-time clock;real-time computing	Ting-Yao Wu;Lie Lu;Ke Chen;HongJiang Zhang	2003		10.1109/ICME.2003.1221718	speaker recognition;training set;speech recognition;information science;computer science;speech;machine learning;instruction set;pattern recognition;speech processing;broadcasting;vector quantization	Robotics	-19.58400794716174	-88.83526187916577	143722
613c587fbdfae236d8976faa1f30db414334bda6	tessa - the essex syntactic analyser		The program TESSA is a two-pass syntactic analyser based on the parser of Winograd's SHRDLU. We wanted to see how dependent the success of the parsing in SHRDLU was on the restriction of the subject matter. The implementation was based on the description in the book (Winograd 1972), with considerable reference to textbooks of grammar and other linguistic material. The program was then further modified and extended to analyse sentences in a corpus obtained from children's books.	book;coppersmith–winograd algorithm;parsing;shrdlu;subject matter expert turing test;text corpus	Michael Soul	1977	SIGART Newsletter	10.1145/1045283.1045317	natural language processing;speech recognition;computer science;linguistics;programming language	NLP	-27.97289208261448	-81.94669083939473	144018
31780417bbdd9c1b60304d379a8f903449184b02	toward an aposynthesis of topic continuity and intrasentential anaphora	analyse de corpus;pragmatics;computacion informatica;anaphore pronominale;filologias;anaphora resolution;grupo de excelencia;semantics;linguistique appliquee;semantique;methodologie;algorithme;algorithm;structure thematique;linguistica;pragmatique;corpus analysis;anglais;reference;ciencias basicas y experimentales;computational linguistics;grec;methodology;grupo a;pronominal anaphora;linguistique informatique;applied linguistics;algoritmo;thematic structure	The problem of proposing referents for anaphoric expressions has been extensively researched in the literature and significant insights have been gained through the various approaches. However, no single model is capable of handling all the cases. We argue that this is due to a failure of the models to identify two distinct processes. Drawing on current insights and empirical data from various languages we propose an aposynthetic1 model of discourse in which topic continuity, computed across units, and focusing preferences internal to these units are subject to different mechanisms. The observed focusing preferences across the units (i.e., intersententially) are best modeled structurally, along the lines suggested in centering theory. The focusing mechanism within the unit is subject to preferences projected by the semantics of the verbs and the connectives in the unit as suggested in semantic/pragmatic focusing accounts. We show that this distinction not only overcomes important problems in anaphora resolution but also reconciles seemingly contradictory experimental results reported in the literature. We specify a model of anaphora resolution that interleaves the two mechanisms. We test the central hypotheses of the proposed model with an experimental study in English and a corpus-based study in Greek. Aposynthesis is a Greek word that means decomposition, that is, pulling apart the components that constitute what appears to be a uniform entity.	anaphora (linguistics);experiment;expression (computer science);logical connective;scott continuity;text corpus	Eleni Miltsakaki	2002	Computational Linguistics	10.1162/089120102760276009	natural language processing;computer science;computational linguistics;applied linguistics;methodology;semantics;linguistics;pragmatics	NLP	-33.62961859537496	-81.76592910017753	144108
81b84b1d7ad66c2761348f6b7618afed29d62c16	us-based method for speech reception threshold measurement in french		We propose a new method for measuring the threshold of 50% sen tence intelligibility in noisy or multi-source speech comm unication situations (Speech Reception Threshold, SRT). Our SRT-tes t complements those available e.g. for English, German, Dut ch, Swedish and Finnish by a French test method. The approach we take is based on semantically unpredictable sentences (SUS), which can p rinci ally be created for various languages. This way, the proposed met hod nables better cross-language comparisons of intellig ibility tests. As a starting point for the French language, a set of 288 sentence s (24 lists of 12 sentences each) was created. Each of the 24 li sts is optimized for homogeneity in terms of phoneme-distribution as compar ed to average French, and for word occurrence frequency of th e employed monosyllabic keywords as derived from French language data bases. Based on the optimized text material, a speech target sentence database has been recorded with a trained speaker. A test cal ibration was carried out to yield uniform measurement resul ts over the set of target sentences. First intelligibility measurements s how good reliability of the method.	database;division algorithm;intelligibility (philosophy);multi-source	Alexander Raake;Brian F. G. Katz	2006			artificial intelligence;speech reception threshold;natural language processing;speech recognition;computer science	NLP	-22.46009163133775	-83.67798351797687	144121
bfdee2f21397b09090afde3880b54272a9e2ecf7	high-performance low-complexity wordspotting using neural networks	spectra warping neural network wordspotter low complexity wordspotting high performance wordspotting radial basis function networks hidden markov model hmm talker independent switchboard corpus figure of merit training wordspotter parameters fom performance metric voice transformations training data across talker vocal tract length variability;control systems;nist;keyword;sistema hibrido;measurement;talker independent switchboard corpus;neural networks;learning;maximum likelihood;modele markov cache;hidden markov model;speech processing;maximum vraisemblance;wordspotter parameters;figure of merit training;tratamiento palabra;traitement parole;hmm;speech;low complexity;palabra clave;testing;spectra warping;mot cle;low complexity wordspotting;vocal tract length;voice;voz;performance metric;aprendizaje;radial basis function networks;training data;apprentissage;hidden markov models;radial basis function;analyse spectrale;maximum likelihood detection;hybrid system;fom performance metric;acoustic signal detection;speech recognition;across talker vocal tract length variability;analisis espectral;feedforward neural nets;neural networks hidden markov models speech nist training data measurement maximum likelihood detection acoustic signal detection testing control systems;neutral network;voice transformations;learning artificial intelligence;reseau neuronal;spectral analysis;figure of merit;wordspotter;high performance;speech processing feedforward neural nets learning artificial intelligence hidden markov models speech recognition;red neuronal;maxima verosimilitud;neural network wordspotter;high performance wordspotting;neural network;systeme hybride;voix	A high-performance low-complexity neural network wordspotter was developed using radial basis function (RBF) neural networks in a hidden Markov model (HMM) framework. Two new complementary approaches substantially improve performance on the talker-independent Switchboard corpus. Figure of Merit (FOM) training adapts wordspotter parameters to directly improve the FOM performance metric, and voice transformations generate additional training examples by warping the spectra of training data to mimic across-talker vocal tract length variability.	artificial neural network;hidden markov model;markov chain;radial (radio);radial basis function;spatial variability;telephone switchboard;tract (literature)	Eric I. Chang;Richard Lippmann	1997	IEEE Trans. Signal Processing	10.1109/78.650114	training set;radial basis function;figure of merit;speech recognition;nist;computer science;neutral network;speech;artificial intelligence;machine learning;software testing;maximum likelihood;voice;hidden markov model;measurement;hybrid system	ML	-19.84837777833218	-90.11997589932427	144435
e7e7b41fb4accffc52364dd0e2a595d7a2266512	from members to teams to committee-a robust approach to gestural and multimodal recognition	probability;neural nets;robustness pattern recognition uncertainty feature extraction handwriting recognition acoustic noise cepstral analysis testing character recognition decision making;indexing terms;multimodal integration;divide and conquer methods;combination of multiple classifiers;pattern recognition;multiple classifiers gesture recognition multimodal recognition complex pattern recognizer high dimensional input features performance evaluation members to teams to committee modeling uncertainty mtc posterior estimator divide and conquer estimators three tiered architectural structure neural nets handwritten gesture recognition pattern recognition;probability gesture recognition handwritten character recognition divide and conquer methods neural nets;gesture recognition;handwritten character recognition	When building a complex pattern recognizer with high-dimensional input features, a number of selection uncertainties arise. Traditional approaches to resolving these uncertainties typically rely either on the researcher's intuition or performance evaluation on validation data, both of which result in poor generalization and robustness on test data. This paper describes a novel recognition technique called members to teams to committee (MTC), which is designed to reduce modeling uncertainty. In particular, the MTC posterior estimator is based on a coordinated set of divide-and-conquer estimators that derive from a three-tiered architectural structure corresponding to individual members, teams, and the overall committee. Basically, the MTC recognition decision is determined by the whole empirical posterior distribution, rather than a single estimate. This paper describes the application of the MTC technique to handwritten gesture recognition and multimodal system integration and presents a comprehensive analysis of the characteristics and advantages of the MTC approach.	finite-state machine;generalization (psychology);gesture recognition;intuition;multimodal interaction;performance evaluation;system integration;test data;teams	Lizhong Wu;Sharon L. Oviatt;Philip R. Cohen	2002	IEEE transactions on neural networks	10.1109/TNN.2002.1021897	speech recognition;index term;feature;intelligent character recognition;computer science;machine learning;pattern recognition;probability;gesture recognition;sketch recognition;artificial neural network;statistics	Vision	-19.161913512057495	-90.64491813918116	144494
8c2b4ab359b357f4ea34ecbe4177d719cb8d6e4d	using a natural language interface for elementary instruction	natural language interface;natural language processing	If man is ever to realize the fullest potential of computers, it is essential to shift the communication burden from the user to the machine. Although the recognition and interpretation of full English is not within our grasp, recent advances in natural language processing (e.g. ATNs), in computer software (e.g. the symbol manipulation language Spitbol) and in computer hardware (e.g. faster CPUs and less expensive memories) have made it possible to construct simple but useful natural language interfaces.	central processing unit;computer hardware;natural language processing;natural language user interface;spitbol	Alan L. Tharp	1977	SIGART Newsletter	10.1145/1045283.1045337	natural language processing;cache language model;natural language programming;universal networking language;language primitive;question answering;object language;natural language user interface;computer science;linguistics;low-level programming language;natural language;programming language;language technology;high-level programming language	Arch	-31.811092871808153	-84.81157846653187	144842
4dd33a8cfe5364e0d86d95ee047969f9a6cf302a	collection and evaluation of broadcast news data for arabic		This paper focuses on presenting a general methodology for acquiring and automatically segmenting broadcast news data from the web. It was shown that it is possible starting from a relatively small corpus of about 10 hours to segment automatically about 30 hours of data. This step is important because manual segmentation of broadcast news data is generally very tedious and time consuming. In addition to the data collection proposal we show the development of an initial recognition system. We present an automatic procedure for creating vowelizations for Arabic words. This is again important because most available Arabic transcriptions lack vowelization, which is crucial for creating phonetic transcription. The performance of our system is initially 36% error rate.	bit error rate;text corpus;transcription (software)	Mohamed Afify;Ossama Emam	2004			word error rate;natural language processing;arabic;market segmentation;artificial intelligence;data collection;information retrieval;computer science;phonetic transcription;transcription (linguistics);broadcasting;segmentation	NLP	-21.89203408383135	-84.04530708149147	145523
1224d34a3d31b80ec4df903b6885a3672afa1ab1	on the naturalness of software	natural language processing;software engineering;statistical analysis;eclipse built-in completion capability;english;tamil;n-gram model;natural language translation;question-answering;software naturalness;speech recognition;statistical language models;statistical methods;text comprehension;text mining;code completion;code suggestion;language models;n-gram	Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension.  We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations---and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether (a) code can be usefully modeled by statistical language models and (b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very regular, and, in fact, even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area.	artificial intelligence;computation;computational linguistics;eclipse;emoticon;integrated development environment;java;language model;list comprehension;n-gram;natural language;question answering;software engineer;speech recognition;statistical model;text mining	Abram Hindle;Earl T. Barr;Zhendong Su;Mark Gabel;Premkumar T. Devanbu	2012	2012 34th International Conference on Software Engineering (ICSE)	10.1145/2902362	natural language processing;computer science;artificial intelligence;mathematics;programming language;management;algorithm	SE	-25.511146041770385	-81.68157133066423	145591
fa0aa259d12db01f1677d1ad202d9cd455be4529	good spellers write more textism than bad spellers in instant messaging: the case of french		The increased use of digital writing has led to the emergence of a new form of communication between discourse and writing. We elaborated a research protocol to target the processes linked to the use of instant messaging to look for differences in the use of spelling modifications as a function of French students’ spelling levels. The task required students to use Digital Writing in Instant Messaging (DWIM) in a semi-natural situation. Analyses showed that modifications that may be confused with misspellings in traditional writing (i.e. substitutions like “sa” instead of “ça”) occurred more often than those that may not (e.g. reductions/ alterations like “chepa” instead of “je ne sais pas”), regardless of spelling level. These results show no impact of the use of DWIM on the quality of spelling (for good spellers only).	dwim;emergence;instant messaging;semiconductor industry	Tonia Lanchantin;Aurélie Simoës-Perlant;Pierre Largy	2014	PsychNology Journal		natural language processing;speech recognition;computer science;communication	HCI	-28.7516224665024	-83.76988430670241	145733
ac64ca771509fe39c272faac98d06389e7b4c628	sinhalese morphological analysis: a step towards machine processing of sinhalese	natural languages laboratories natural language processing fuel processing industries technology transfer history europe;language translation;input representation methods sinhalese morphological analysis machine processing automatic translation;natural languages;natural languages computational linguistics language translation;morphological analysis;computational linguistics	Initial steps toward the machine processing of Sinhalese, including automatic translation, are discussed. Script input representation methods, morphological analysis, and some characteristics of Sinhalese are considered. >	computation	Susantha Herath;Takashi Ikeda;S. Yokoyama;Hitoshi Isahara;Shun Ishizaki	1989		10.1109/TAI.1989.65308	natural language processing;speech recognition;morphological analysis;computer science;computational linguistics;rule-based machine translation;natural language	ML	-23.16668223889466	-82.31685003675076	146131
828444dfe221ac7ce5c0d1a5d7a7db7d1d78b7ce	speaker diarization with lexical information		This work presents a novel approach to leverage lexical information for speaker diarization. We introduce a speaker diarization system that can directly integrate lexical as well as acoustic information into a speaker clustering process. Thus, we propose an adjacency matrix integration technique to integrate word level speaker turn probabilities with speaker embeddings in a comprehensive way. Our proposed method works without any reference transcript. Words, and word boundary information are provided by an ASR system. We show that our proposed method improves a baseline speaker diarization system solely based on speaker embeddings, achieving a meaningful improvement on the CALLHOME American English Speech dataset.	acoustic cryptanalysis;adjacency matrix;automated system recovery;baseline (configuration management);cluster analysis;speaker diarisation	Tae Jin Park;Kyu J. Han;I. Lane;P. Georgiou	2018	CoRR			NLP	-20.468852535902332	-82.48832824579019	146180
3e9337e03bb6dd016dbc269ad9271c9a9bac57fa	the simulation of three machines which read rows of handwritten arabic numbers	computer aided instruction;computational modeling;ear;logic programming;registers;information processing;mathematical model;error rate;writing;simulation study;ear computational modeling registers computer simulation information processing logic programming computer aided instruction writing character recognition tagging;computer simulation;character recognition;tagging	Three machines have been simulated using an optical scanner and the IBM 704 computer. Each of these simulated machines has read documents containing rows of handwritten Arabic numbers. Sample numbers were produced by at least 20 people for each simulation study. The three machines simulated differ in the control required of the writer during document preparation and in the complexity of the machines. Writing controls were required for the preparation of the first two types of documents. A section of this paper concerns experiments with and a mathematical model of controlled writing. The third simulated machine was applied both to numbers written within preprinted boxes and to numbers written without any guide marks. About one per cent of 2180 of these numbers were misread as the wrong character. This error rate is based on a sequential experiment in which the recognition logic is constructed from all characters not recognized and thus rejected prior to each input character. Numbers, when rejected, cause the programn to identify them from a table. Their structure is then entered into the recognition logic. The rejection rate decreased throughout the experiment. The last rejection rate was about 10 per cent.	simulation	Louis A. Kamentsky	1961	IRE Trans. Electronic Computers	10.1109/TEC.1961.5219238	computer simulation;natural language processing;speech recognition;information processing;word error rate;computer science;theoretical computer science;mathematical model;processor register;logic programming;computational model;writing	ECom	-27.142265509463062	-81.46784672013892	146224
b4173f0a55dbae68672c6a72afc2a00699273a7e	a study on music genre classification based on universal acoustic models	acoustic segment models;latent-semantic indexing;musical genres;hidden markov models;hidden markov model;latent semantic indexing	Classification of musical genres gives a useful measure of similarity and is often the most useful descriptor of a musical piece. Previous techniques to use hidden Markov models (HMMs) for automatic genre classification have used a single HMM to model an entire song or genre. This paper provides a framework to give finer segmentation of HMMs through acoustic segment modeling. Modeling each of these acoustic segments with an HMM builds a timbral dictionary in the same fashion that one would create a phonetic dictionary for speech. A symbolic transcription is created by finding the most likely sequence of symbols. These transcriptions then serve as inputs into an efficient text classifier utilized to provide a solution to the genre classification problem. This paper demonstrates that language-ignorant approaches provide results that are consistent with the current state-of-the-art for the genre classification problem. However, the finer segmentation potentially allows for “musical language”-based syntactic rules to enhance performance.	acoustic cryptanalysis;algorithm;data descriptor;dictionary;document retrieval;hidden markov model;lambda calculus;markov chain;string (computer science);text corpus;transcription (software)	Jeremy Reed;Chin-Hui Lee	2006			machine learning;latent semantic indexing;speech recognition;syntax;transcription (linguistics);hidden markov model;classifier (linguistics);computer science;artificial intelligence;pattern recognition	ML	-21.471141892280826	-82.72336432179127	146477
a01b35d34daa3fe4789778355561f0e8c8cbf077	simplifying design specification for automatic training of robust natural language call router	information retrieval;natural languages;robustness natural languages routing information retrieval filtering error analysis degradation loans and mortgages speech synthesis speech recognition;telecommunication network routing;natural language;call centres natural languages telecommunication network routing speech recognition;speech recognition;speech understanding;discriminative training;topic identification;call centres;expert knowledge;call center design specification simplification automatic training robust natural language call router task specifications optimized system matrix size reduction stop word filtering discriminative training routing matrix parameters performance degradation minimum error classification training speech understanding topic identification information retrieval voice response system speech recognizer	In this paper, we study techniques that allow us to relax some constraints imposed by expert knowledge in task specifications of natural language call router design. We intend to fully automate training of the routing matrix while still maintaining the same level of performance (over 90% accuracy) as that in an optimized system. Two specific issues are investigated: (1) reducing matrix size by removing word pairs and triplets in key term definition while using only single word terms; and (2) increasing matrix size by removing the need for defining stop words and performing stop word filtering. Since simplification of design often implies a degradation of performance, discriminative training of routing matrix parameters becomes an essential procedure. We show in our experiments that the performance degradation caused by relaxing design constraints can be compensated entirely by minimum error classification (MCE) training even with the above two simplifications. We believe the procedure is applicable to algorithms addressing a broad range of speech understanding, topic identification, and information retrieval problems.	algorithm;elegant degradation;experiment;information retrieval;linuxmce;microsoft word for mac;natural language;router (computing);routing;text simplification	Hong-Kwang Jeff Kuo;Chin-Hui Lee	2001		10.1109/ICASSP.2001.940905	natural language processing;speech recognition;computer science;machine learning;natural language	ML	-20.920312091746162	-84.3672331239887	146542
da7c3a823016ac02aaf230017bd92130ca451cd2	dialect classification via text-independent training and testing for arabic, spanish, and chinese	desciframiento;processus gauss;evaluation performance;speaker identification;gaussian mixture;audio signal processing;performance evaluation;ligne de base;decodage;learning;decoding;maximum likelihood;gaussian processes;frame selection;dialect classification;kullbackleibler divergence;acoustics;kullback leibler divergence;evaluacion prestacion;localization;frame selection decoding;speech processing;training;maximum vraisemblance;tratamiento palabra;dialect;traitement parole;maximum likelihood estimation text independent training automatic dialect classification speech recognition speaker identification text independent spontaneous speech gaussian mixture model kullback leibler divergence frame selection decoding;spanish dialects arabic dialects dialect classification frame selection gaussian mixture kullback leibler divergence;speech;acoustic signal processing;testing;localizacion;chino;maximum likelihood estimation;speaker recognition;algorithme;aprendizaje;discriminant analysis;analyse discriminante;algorithm;analisis discriminante;apprentissage;gaussian mixture model;adaptation model;localisation;reconocimiento voz;arabic;speech recognition robustness automatic speech recognition natural languages probes acoustic testing acoustic signal detection maximum likelihood decoding maximum likelihood estimation loudspeakers;automatic dialect classification;traitement signal audio;spanish;signal classification;text independent spontaneous speech;baseline;reconnaissance locuteur;linea de base;classification signal;speech recognition;speech recognition gaussian processes learning artificial intelligence maximum likelihood estimation natural language processing;dialecto;teoria mezcla;text independent training;arabe;traitement signal acoustique;gaussian process;reconnaissance parole;classification automatique;learning artificial intelligence;espagnol;arabic dialects;proceso gauss;automatic classification;chinois;mixture theory;chinese;spanish dialects	Automatic dialect classification has emerged as an important area in the speech research field. Effective dialect classification is useful in developing robust speech systems, such as speech recognition and speaker identification. In this paper, two novel algorithms are proposed to improve dialect classification for text-independent spontaneous speech in Arabic and Spanish languages, along with probe results for Chinese. The problem considers the case where no transcripts but dialect labels are available for training and test data, and speakers are speaking spontaneously, which is defined as text-independent dialect classification. The Gaussian mixture model (GMM) is used as the baseline system for text-independent dialect classification. The major motivation is to suppress confused/distractive regions from the dialect language space and emphasize discriminative/sensitive information of the available dialects. In the training phase, a symmetric version of the Kullback-Leibler divergence is used to find the most discriminative GMM mixtures (KLD-GMM), where the confused acoustic GMM region is suppressed. For testing, the more discriminative frames are detected and used via the location of where the frames are in the GMM mixture feature space, which is termed frame selection decoding (FSD-GMM). The first KLD-GMM and second FSD-GMM techniques, are shown to improve dialect classification performance for three-way dialect tasks. The two algorithms and their combination are evaluated on dialects of Arabic and Spanish corpora. Measurable improvement is achieved in both two cases, over a generalized maximum-likelihood estimation GMM baseline (MLE-GMM).	acoustic cryptanalysis;algorithm;approximation error;baseline (configuration management);discriminative model;factor analysis;feature vector;futures studies;google map maker;ground truth;information sensitivity;kullback–leibler divergence;mixture model;speaker recognition;speech recognition;spontaneous order;test data;text corpus	Yun Lei;John H. L. Hansen	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2045184	natural language processing;speaker recognition;speech recognition;computer science;pattern recognition;gaussian process;speech processing;mathematics;linguistics;maximum likelihood;statistics	NLP	-20.10928463774437	-90.14654035104698	146576
5e0b58be3c0dfaf6334b8552dbeb1d09844fe67e	identifying symptoms and diseases in mednlp japanese materials using chinese resources		In this paper, we describe the Sinica-Yuntech system (TeamID: SinicaNLP) at the NTCIR-10 MedNLP task. Materials of the MedNLP task are in Japanese. However, having only Chinese resources and knowledge, we need to translate these materials into Chinese. Two preprocessing approaches, different in the timing of translation, were taken. One was to translate Japanese sentences in to Chinese ones, and then to perform segmentation and part of speech tagging on these Chinese sentences; the other was to segment and tag parts of speech on Japanese sentences, and then to translate the composite words. After knowing words and their parts of speech, we identified symptoms and diseases by a vocabulary matching approach. The Internet searching results and parts of speech patterns were also utilized to recognize out of vocabulary symptoms. After recognizing the targets in Chinese, a reverse translation was performed in order to label the original Japanese materials. We merged the tags from vocabulary matching, Internet searching and pattern mapping to obtain the performance of our best run: an f-score 53.88 and an accuracy 91.46.	f1 score;part-of-speech tagging;preprocessor;vocabulary	Lun-Wei Ku;Edward T.-H. Chu;Cheng-Wei Sun;Wan-Lun Li	2013				NLP	-23.843858416090438	-80.93847938126805	146601
f164003026f86a46427c564ba413f646240faee7	multimodal speech synthesis	speech synthesis;audio technology components;communications;e-commerce;multimodal speech synthesis;text to speech synthesis;visual face presentation synthesis;visual technology components	"""The main goal of the speech synthesis group in SmartKom was to develop a natural sounding synthetic voice for the avatar """" Smartakus """" that is judged to be agreeable, intelligible, and friendly by the users of the SmartKom system. Two aspects of the SmartKom scenario facilitate the achievement of this goal. First, since speech output is mainly intended for the interaction of Smartakus with the user, most of the output corresponds to dialog turns generated by the language generation module (see Chapter ??). Therefore, most speech output can be generated from linguistic concepts produced by the language generation module ("""" concept-to-speech synthesis """" , CTS) instead of from raw text ("""" text-to-speech synthesis """" , TTS). The advantage of CTS over TTS is that it avoids errors that may be introduced by linguistic analysis in TTS mode. Second, the CTS approach narrows down the SmartKom synthesis domain from a theoretically open domain to a restricted domain, which makes unit selection synthesis a promising alternative to diphone synthesis for the SmartKom application. Multimodality introduces additional requirements for the synthesis module. The visual presence of Smartakus on the screen during speech output requires lip synchronization. Furthermore, Smartakus executes pointing gestures that are related to objects which are also referred to linguistically. These pointing gestures influence the prosodic structure of the utterance and necessitate temporal alignment of the ges-tural and linguistic modes. Another momentous requirement was that the graphical design of Smartakus was given before the voice database was recorded. This entailed that the appropriateness of the speaker's voice for Smartakus could be included as an important factor in the speaker selection process. In developing the synthesis voice for Smartakus, we pursued the following strategy: after the speaker selection process, a diphone voice was developed first. This voice served both as a starting point for implementing a unit selection voice by the same speaker tailored to the typical SmartKom domains, and as the default voice for external open domain applications that require TTS instead of CTS. The diphone voice and the unit selection voice were both evaluated in the progress of the project. This chapter is organized as follows. We focus on the prosody generation in CTS mode in the subsequent section. The speaker selection process is described in Sec"""	automatic sounding;carpal tunnel syndrome;exptime;graphical user interface;multimodal interaction;natural language generation;netware file system;norm (social);requirement;semantic prosody;speech synthesis;synthetic intelligence;dialog	Antje Schweitzer;Norbert Braunschweiler;Grzegorz Dogil;Tanja Klankert;Bernd Möbius;Gregor Möhler;Edmilson Da Silva Morais;Bettina Säuberlich;Matthias Thomae	2006		10.1007/3-540-36678-4_27	speech technology;audio mining;speech recognition;speech corpus;computer science;speech processing;multimedia;speech synthesis	NLP	-25.24406503556463	-86.18435981478095	146627
1b88ce651ff7e66855ec6bc6411c9e0ea8d85ca0	book reviews		The research method applied by the authors, and outlined in introductory remarks, is determined by the assumptions of the Pronominal Approach, which has been conceived by K. van den Eynde and developed in collaboration with CI. BlancheBenveniste. In this linguistic theory it is argued that syntactic structures in which a verbal nucleus is completed with merely pronominal elements function as classifiers for basic syntactic structures as such. More specifically, feature values that can be directly obtained from the observation of constructions of the first type (i.e. pronominal constructions) are said to characterize fully the syntactic properties of the second type of constructions whose lexical complexity often resists direct access to their syntactic features. In the first and second parts of the article, the authors set out to give evidence for the syntactic phenomena that control coordination by et ( 'and') and ma/s ('but'), via the investigation of pronominal constructions. To that end, they introduce a system of feature categories and corresponding feature values, in terms of which ascertainments based on the observation of pronominal constructions can be formalized so as to make implementation possible. The resulting feature system, together with the rules governing the combination of feature values, is shown to reflect most accurately the complexities of the problem under consideration. The final part of the article considers more generally the implications, possibilities and advantages of the approach for the computational treatment of syntactic units with lexical items in Natural Language Processing. 0.1. The pronominal approach: Terminology The pronominal approach provides a specific research method for the study of basic syntactic structures in any natural language. ~ At the heart o f this method lie the fol lowing axioms: 9 the verb is the basic unit for syntactic analysis Computers and Translation 3 (1988), 177 213 01988 by Kluwer Academic Publishers 178 van den EYNDE, BROEDERS, EGGERMONT, VANGILBERGEN 9 pronouns are the basic forms of which the lexical items constitute a particular instantiation. Both these basic assumptions will be spelled out briefly before we apply them to the study of coordination. The verb is considered to be the nucleus of an utterance in that it constructs a number of elements and establishes relations between them. In (1), for instance, (1) mon p~re 6tait 6tonn6 de mes r6suitats the verbal form ~tait ~tonn~ takes two elements : mon pdre and de mes r~sultats. Together, the verb and its constructed elements constitute a verbal construction. The central idea enunciated by the pronominal approach is that the verbal constructions with pronouns only (the so-caned 'pronominal sentences') function as classifiers for verbal constructions. (2) il en est 6tonn6 (3) lui est 6tonn6 de ~a (4) mon p~re est 6tonn6 de mes r6sultats In examples (2), (3) and (4) the pronoun il is related to l u / a n d mon pdre in the same way as en is related to de ~a and de mes r~sultats. The relation between them is called one of proportionality. It differs in two major ways from substitution as conceived in most handbooks of linguistics. 9 the strict linear order is not taken into account 9 whereas there is a possibility of substituting mon p~re by ma mdre, there is no relation o f proportionality between il and ma mdre. The specific proportionality relation between pronoun and lexicon is called lexicalization, for pronouns are considered anterior to the lexicon. The relation o f proportionality between the different pre-lexical (or pro-nominal) items induces a distinction into at least four different categories: z the clitic pronouns (e.g. je, tu, il . . . . ), the non-clitic pronouns (e.g. moi, toi, lui . . . . ) (See NOTE 3), the suspensives (e.g. qui, quoi, quand . . . . ) and the ones we will refer to as paranouns (e.g. quelqu' un, quelque chose, tout, rien, personne, quelqe part, 9176 A paradigm is the set of all pronouns that can appear in a certain position with a certain verb. 3 Thus, for example: THE PRONOMINAL APPROACH 179	central processing unit;john c.s. lui;lexicon;moment of inspiration;natural language processing;parsing;programming paradigm;random access;third-order intercept point;universal instantiation	K. van den Eynde;E. Broeders;C. Eggermont;L. Vangilbergen	1988	Computers and translation	10.1007/BF02055237		NLP	-33.32878164584158	-82.14052770182788	146732
ceb8e549deb2bee6337902773f927af874b1a7d9	implementation and verification of speech database for unit selection speech synthesis		The main aim of this study was to prepare a new speech database for the purpose of unit selection speech synthesis. The object was to design a database with improved parameters compared with the existing database [1], making use of the theses proved in studies [2]-[4]. The quality of the corpus, a selection of the suitable speaker, and the quality of the speech database are all crucially important for the quality of synthesized speech. The considerably larger text corpora used in the study as well as the broader multiple balancing of the database yielded a greater number of varied acoustic units. For the purpose of the recording, one voice talent was selected from among a group of 30 professional speakers. The next stage involved database segmentation. The resultant database was then verified with a prototype speech synthesizer. The quality of the synthetic speech was compared to that of synthetic speech obtained in other Polish unit selection speech synthesis systems. Consequently, the end result proved to be better than the one obtained in the previous study [4]. The database had been supplemented and extended, significantly enhancing the quality of synthesized speech.	acoustic fingerprint;database;multimodal interaction;prototype;resultant;semiconductor industry;speech corpus;speech synthesis;synthetic intelligence;text corpus	Krzysztof Szklanny;Sebastian Koszuta	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F395	voice activity detection;viterbi algorithm;computer science;database;psqm;speech corpus;text corpus;speech recognition;speech coding;linear predictive coding;speech synthesis	DB	-20.503416015497027	-84.24537612248443	147157
1f73e4b4fe19f0991f195c50fc48ebd0d6f5bf5a	mining chat conversations: the next frontier	air force training;message processing;missions;data mining;classification;learning machines;symposia;accuracy;air force research;identification;interactions;artificial intelligence;teams personnel;air force personnel;automation	Analyzing chat traffic has important applications for both the military and the civilian world. This poster will report on an effort to automatically separate chat messages into topic threads.		Sowmya Ramachandran;Randy Jensen;Oscar Bascara;Tamitha Carpenter;Todd Denning;Shaun Sucillon	2011			identification;interaction;simulation;biological classification;computer science;artificial intelligence;automation;machine learning;accuracy and precision;operations research	HCI	-26.79708749316489	-90.08013047622836	147267
625701d5f62503e70c499f4b20ae9c1b287c3a29	improving posterior based confidence measures using enhanced local posteriors	speech signal posterior based confidence measures enhanced local posterior estimates posterior probability asr systems phone posterior probabilities;acoustics;speech;telephone sets estimation theory probability speech processing speech recognition;error analysis;hidden markov models;estimation;hidden markov models acoustic measurements speech estimation acoustics speech recognition error analysis;speech recognition;acoustic measurements	In this paper, we propose a new technique for enhancing posterior probability based confidence measures in ASR systems. We propose to enhanced local posterior estimates used in confidence measurement process, in order to improve the overal confidence score in terms of ability to accept/reject a hypothesis. Posterior based confidence measures are global scores obtained by accumulating local evidences. These local evidences are usually phone posterior probabilities estimated in frame basis from speech signal. Having better (more informative) local evidences can potentially lead to better confidence measures. In [1, 2, 3], a method for enhancing local phone posterior estimates (evidences) has been proposed. This method is based on integrating prior knowledge (such as phone duration, lexical knowledge) and temporal context in the local posterior estimation. We show that using enhanced local posteriors in the confidence measurement process significantly and constantly improves their ability to predict whether a hypothesis (at word or phone level) is correct or incorrect, as compared to using regular local posterior estimates.	automatic system recovery;confidentiality;experiment;hidden markov model;information	Hamed Ketabdar	2010	2010 18th European Signal Processing Conference		speech recognition;computer science;machine learning;pattern recognition	Vision	-19.55237231293967	-89.44439129573118	147404
6d3b21eb0f9dedb496b37cac3adb7ed5a7b7d25e	texting, techspeak, and tweens: the relationship between text messaging and english grammar skills	observational learning;text messaging;textual adaptations;adolescents;low road high road theory of transfer of learning;english grammar	The perpetual use of mobile devices by adolescents has fueled a culture of text messaging, with abbreviations and grammatical shortcuts, thus raising the following question in the minds of parents and teachers: Does increased use of text messaging engender greater reliance on such ‘textual adaptations’ to the point of altering one’s sense of written grammar? A survey (N = 228) was conducted to test the association between text message usage of sixth, seventh and eighth grade students and their scores on an offline, age-appropriate grammar assessment test. Results show broad support for a general negative relationship between the use of techspeak in text messages and scores on a grammar assessment, with implications for Social Cognitive Theory and Low-Road/High-Road Theory of Transfer of Learning. These results indicate that adolescents may learn through observation in communication technologies, and that these learned adaptations may be transferred to standard English through Low-Road transfer of learning. Further mediation analyses suggest that not all forms of textual adaptation are related to grammar assessment score in the same way. ‘Word adaptations’ were found to be negatively related to grammar scores, while ‘structural adaptations’ were found to be non-significant.	mediation (statistics);microsoft word for mac;mobile device;online and offline	Drew P. Cingel;S. Shyam Sundar	2012	New Media & Society	10.1177/1461444812442927	natural language processing;observational learning;speech recognition;computer science;sociology;english grammar;communication;world wide web	HCI	-29.340113885012887	-84.68151390427602	147424
c4f9bef15994205e7a1bd15e204014630a4338b8	the structure of the ord speech corpus of russian everyday communication	information system	The paper presents the structure of the ORD speech corpus of Russian everyday communication, which contains recordings of all spoken episodes recorded during twenty-four hours by a demographically balanced group of people in St. Petersburg. The paper describes the structure of the corpus, consisting of audio files, annotation files and information system and reviews the main communicative episodes presented in the corpus.	information system;speech corpus;text corpus	Tatiana Y. Sherstinova	2009		10.1007/978-3-642-04208-9_37	natural language processing;speech recognition;speech corpus;computer science;linguistics;information system	NLP	-24.86053555069331	-84.2472321536137	147621
d3169649f35e1f70c67e13c35f55517fdc5076d5	how (not) to select your voice corpus: random selection vs. phonologically balanced		This paper compares the effect of two different voice corpus selection methods on the overall quality of unit selection-based text-to-speech (TTS) voices resulting from training on these corpora. The first selectionmethod aims to maximize the coverage of stressed as well as unstressed diphones (phonologically balanced: Phonbal) while the second method simply selects sentences at random (Random). We show that, as expected, the Phonbal method results in better phonetic and phonological coverage for the trainingas well as unseen test sentences. However, we also provide evidence from an objective evaluationand a subjective listening test that the Random method results in an overall better voice quality when only automatic corpus annotation tools (such as forced alignment)are used, and potentially even with manual annotation. This result has general implications for the fast creation of TTS voices.	randomness;speech synthesis;text corpus	Tanya Lambert;Norbert Braunschweiler;Sabine Buchholz	2007			sampling (statistics);artificial intelligence;active listening;computer science;pattern recognition;annotation	NLP	-19.701836612271794	-83.1565880543972	147668
5abfbb62d1a9a3906dc23b086630b5ef998c5ce6	application of vector quantized hidden markov modeling to telephone network based connected digit recognition	maximum mutual information;telephone networks;hidden markov model;speech coding;telephony;texas instruments;hidden markov models;error rate;speech recognition;vector quantisation speech recognition hidden markov models telephone networks telephony speech coding;vector quantizer;hidden markov models telephony speech recognition instruments databases computational efficiency error analysis mutual information size measurement computer architecture;vector quantisation;computational efficiency;performance vector quantized hidden markov modeling telephone network connected digit speech recognition speech technology recognition performance error rate hidden markov models computational efficiency vq hmm maximum mutual information training sender modeling codebook size recognition accuracy model architecture	Connected digit speech recognition in the telephone network is becoming increasingly more important as the demand for speech technology becomes widespread. In the past few years, several highly successful techniques for recognizing spoken connected digit strings have been proposed. Although these techniques have been applied to non-telephone based speech [e.g. Texas Instruments database], they have produced high recognition performance. Further, similar levels of performances have been demonstrated using discrete density and continuous density based hidden Markov models (HMMs). The success of the vector quantized (VQ) modeling approach, in particular, is encouraging and rather important from the viewpoint of computational efficiency. This paper presents a study of connected digit recognition on telephone network based data using VQ HMMs. We investigate several factors affecting the error rate of VQ HMMs-such as maximum mutual information (MMI) training, sender modeling, and codebook size-and measure their contributions to recognition accuracy. The model architecture, number of states and transitions, is also optimized and its contribution to overall performance discussed. >		Eric R. Buhrke;Régis Cardin;Yves Normandin;Mazin G. Rahim;Jay G. Wilpon	1994		10.1109/ICASSP.1994.389344	speech recognition;telephone network;word error rate;computer science;machine learning;speech coding;pattern recognition;markov model;telephony;hidden markov model	AI	-19.6479406242787	-90.73613706254139	147778
0fba2dc2919375990d144f03fa5bdc3dc801b629	arabic language modeling with finite state transducers	arabic inflectional morphology;morphologically rich language;arabic automatic speech recognition;arabic language modeling;word error rate;oov problem;finite state transducers;fewer inflected form;test set word;increased morpheme combination;word form;finite state transducer;arabic language	In morphologically rich languages such as Arabic, the abundance of word forms resulting from increased morpheme combinations is significantly greater than for languages with fewer inflected forms (Kirchhoff et al., 2006). This exacerbates the out-of-vocabulary (OOV) problem. Test set words are more likely to be unknown, limiting the effectiveness of the model. The goal of this study is to use the regularities of Arabic inflectional morphology to reduce the OOV problem in that language. We hope that success in this task will result in a decrease in word error rate in Arabic automatic speech recognition.	acoustic fingerprint;automated system recovery;finite-state transducer;galaxy morphological classification;kirchhoff's theorem;language model;log probability;perplexity;speech recognition;test set;vocabulary;word error rate	Ilana Heintz	2008			natural language processing;finite state transducer;speech recognition;word error rate;computer science;arabic;linguistics	NLP	-20.47070787866749	-80.40057104457554	148072
18eb487aeb834b9bbd03b96fc9d5b34c669e16df	a canadian french emotional speech dataset		Until recently, there was no emotional speech dataset available in Canadian French. This was a limiting factor for research activities not only in Canada, but also elsewhere. This paper introduces the newly released Canadian French Emotional (CaFE) speech dataset and gives details about its design and content. This dataset contains six different sentences, pronounced by six male and six female actors, in six basic emotions plus one neutral emotion. The six basic emotions are acted in two different intensities. The audio is digitally recorded at high-resolution (192 kHz sampling rate, 24 bits per sample). This new dataset is freely available under a Creative Commons license (CC BY-NC-SA 4.0).	color depth;emotion markup language;image resolution;sampling (signal processing)	Philippe Gournay;Olivier Lahaie;Roch Lefebvre	2018		10.1145/3204949.3208121	real-time computing;license;limiting factor;speech recognition;digital recording;emotion classification;computer science;commons	NLP	-28.885565992393502	-88.57399133322716	148106
233d7b9e231264c6cb642ec76050d0597a88e67c	incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems	multimodal conversational system;human machine conversation;acquisition performance;human language processing;automatic word acquisition;human eye;conversational system;new word;domain knowledge;semantic information;word acquisition;eye gaze	One major bottleneck in conversational systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words. To overcome this problem, conversational systems must be able to learn new words automatically during human machine conversation. Motivated by psycholinguistic findings on eye gaze and human language processing, we are developing techniques to incorporate human eye gaze for automatic word acquisition in multimodal conversational systems. This paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition. Our experiment results indicate that eye gaze provides a potential channel for automatically acquiring new words. The use of extra temporal and domain knowledge can significantly improve acquisition performance.	dialog system;experiment;eye tracking;multimodal interaction;protologism;semantic similarity;tracking system	Shaolin Qu;Joyce Yue Chai	2008			natural language processing;speech recognition;eye tracking;computer science;domain knowledge	NLP	-26.06791558461268	-86.85308822954302	148211
f89760c4fc1aadbef441a6e1fe6ce0b9411f1c38	user simulation for spoken dialogue systems: learning and evaluation	simulation model;indexing terms;speech recognition;user model;linear model	We propose the “advanced” n-grams as a new technique for simulating user behaviour in spoken dialogue systems, and we compare it with two methods used in our prior work, i.e. linear feature combination and “normal” n-grams. All methods operate on the intention level and can incorporate speech recognition and understanding errors. In the linear feature combination model user actions (lists of〈 speech act, task 〉 pairs) are selected, based on features of the current dialogue state which encodes the whole history of the dialogue. The user simulation based on “normal” n-grams treats a dialogue as a sequence of lists of 〈 speech act, task 〉 pairs. Here the length of the history considered is restricted by the order of the n-gram. The “advanced” n-grams are a variation of the normal ngrams, where user actions are conditioned not only on speech acts and tasks but also on the current status of the tasks, i.e. whether the information needed by the application (in our case flight booking) has been provided and confirmed by the user. This captures elements of goal-directed user behaviour. All models were trained and evaluated on the C OMMUNICATOR corpus, to which we added annotations for user actions and dialogue context. We then evaluate how closely the synthetic responses resemble the real user responses by comparing the user response generated by each user simulation model in a given dialogue context (taken from the annotated corpus) with the actual user response. We propose the expected accuracy, expected precision, and expected recall evaluation metrics as opposed to standard precision and recall used in prior work. We also discuss why they are more appropriate metrics for evaluating user simulation models compared to their standard counterparts. The advanced n-grams produce higher scores than the normal n-grams for small values of n, which proves their strength when little amount of data is available to train larger ngrams. The linear model produces the best expected accuracy but with respect to expected precision and expected recall it is outperformed by the large n-grams even though it is trained using more information. As a task-based evaluation, we also run each of the user simulation models against a system policy trained on the same corpus. Here the linear feature combination model outperforms the other methods and the advanced n-grams outperform the normal ngrams for all values of n, which again shows their potential. We also calculate the perplexity of the different user models.	dialog system;evaluation function;grams;linear model;n-gram;perplexity;precision and recall;simulation;speech recognition;synthetic intelligence	Kallirroi Georgila;James Henderson;Oliver Lemon	2006			precision and recall;artificial intelligence;simulation modeling;speech recognition;user modeling;perplexity;linear model;pattern recognition;recall;computer science	NLP	-25.965748905339428	-87.63414904885374	148367
b1a18832ea70fad1dea34a7d96beea8bbe3a2273	efficient but soft communication by haiku-like fragmental sentences	haiku upper ontology;meta ontology;mata sentence;functional grammar;secret of beauty;haiku;eloquent mutism;ontologies artificial intelligence digital signal processing chips grammars;poetic sentence;aesthetics;haiku ontology;aesthetic efficiency haiku like fragmental sentences poetic fragmental sentence linguistic message haiku japanese short poetic sentence soft communication language digital signal processing functional grammar season word ontology eloquent mutism meta sentences kernel sentences;seasonal word ontology;ontologies kernel springs rain grammar semantics;kernel sentence;saijiki;meta ontology poetic sentence haiku aesthetics secret of beauty eloquent mutism functional grammar kernel sentence mata sentence seasonal word ontology saijiki haiku ontology haiku upper ontology	Poetic fragmental sentence such as Haiku often behaves freely from grammatical constraint while maintaining almost full linguistic message transmitting power. It is somewhat mysterious language phenomenon that Haiku-like fragmental sentence can dexterously gives full play to their persuasive power on reader's soul. In this paper we take HAIKU, classical Japanese short poetic sentence, as a soft communication language on highly rigid and efficient DSP world. Here DSP stands for Digital Signal Processing. By using the concept of functional grammar and season-word ontology, we try to approach the secret of taciturn efficiency in poetic sentences. The efficiency of HAIKU is often said to exist in ellipses, abbreviation and suggestions; eloquent mutism. Various events and situation that suggests tacitly the deep emotion and feelings of writer are described in a very short and simple sentence which is composed of 5-7-5 letters words or phrases. In this paper we will introduce a Functional Grammar as a device for analyzing the meaning and construction of HAIKU. The Functional Grammar claims that every sentence is composed of two categories: kernel sentence and meta-sentence. We show typical examples of meta-sentences and kernel sentences obtained from HAIKU, and try to reveal the secrets of aesthetic efficiency of ultimately simple sentence, HAIKU. One more important thing, in this paper we will treat English version of HAIKU, as an efficient soft communication language. In the highly digitized and rigidly designed world, we may need Haiku-like soft and simple fragmental communication language.	digital signal processing;digital signal processor;haiku;kernel (operating system);microsoft lumia;self-reference;systemic functional grammar;transmitter	Yoshihiko Nitta	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7252024	natural language processing;computer science;linguistics;communication	NLP	-33.67602139484692	-84.39196308771548	148396
6a8484e5bb5ef40323e6b2cff94a3d6ee3034a61	maximum entropy-based reinforcement learning using a confidence measure in speech recognition for telephone speech	unsupervised learning;confidence metric logarithm;word error rate;optimisation;model parameter adaptation methods;information sources;robust automatic speech recognition;feature compensation;learning;decoding;model adaptation techniques;confidence measure;reinforcement learning;acoustic modeling;model adaptation;learning speech recognition telephony viterbi algorithm decoding adaptation model hidden markov models entropy error analysis optimization methods;viterbi decoding hidden markov models optimisation speech recognition unsupervised learning;model adaptation techniques maximum entropy based reinforcement learning speech recognition telephone speech confidence based reinforcement learning unsupervised compensation limited estimation data two step viterbi decoding correction factor neighboring hmm metrics linear combination information sources acoustic model log likelihood confidence metric logarithm conditional entropy maximization word error rate wer training testing conditions feature compensation model parameter adaptation methods confidence based rl method;telephony;incremental conditional entropy;error analysis;automatic speech recognition;telephone speech;adaptation model;hidden markov models;correction factor;confidence based rl method;viterbi algorithm;adaptive method;two step viterbi decoding;viterbi decoder;wer;conditional entropy;speech recognition;unsupervised compensation;acoustic model log likelihood;neighboring hmm;limited estimation data;entropy;telephone speech confidence measure incremental conditional entropy reinforcement learning robust automatic speech recognition;metrics linear combination;training testing conditions;confidence based reinforcement learning;conditional entropy maximization;viterbi decoding;maximum entropy;maximum entropy based reinforcement learning;optimization methods	In this paper, a novel confidence-based reinforcement learning (RL) scheme to correct observation log-likelihoods and to address the problem of unsupervised compensation with limited estimation data is proposed. A two-step Viterbi decoding is presented which estimates a correction factor for the observation log-likelihoods that makes the recognized and neighboring HMMs more or less likely by using a confidence score. If regions in the output delivered by the recognizer exhibit low confidence scores, the second Viterbi decoding will tend to focus the search on neighboring models. In contrast, if recognized regions exhibit high confidence scores, the second Viterbi decoding will tend to retain the recognition output obtained at the first step. The proposed RL mechanism is modeled as the linear combination of two metrics or information sources: the acoustic model log-likelihood and the logarithm of a confidence metric. A criterion based on incremental conditional entropy maximization to optimize a linear combination of metrics or information sources online is also presented. The method requires only one utterance, as short as 0.7 s, and can lead to significant reductions in word error rate (WER) between 3% and 18%, depending on the task, training-testing conditions, and method used to optimize the proposed RL scheme. In contrast to ordinary feature compensation and model parameter adaptation methods, the confidence-based RL method takes place in the frame log-likelihood domain. Consequently, as shown in the results presented here, it is complementary to feature compensation and to model adaptation techniques.	acoustic cryptanalysis;acoustic model;approximation;conditional entropy;distributed computing;entropy maximization;finite-state machine;mathematical optimization;pattern recognition;polynomial;programming paradigm;reinforcement learning;speech recognition;unsupervised learning;viterbi decoder;word error rate	Carlos Molina;Néstor Becerra Yoma;Fernando Huenupán;Claudio Garretón;Jorge Wuth	2010	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2032618	unsupervised learning;speech recognition;computer science;machine learning;pattern recognition;reinforcement learning;viterbi decoder;statistics	Vision	-19.428064122262807	-91.52960125840814	148402
de54a4c8218076593f01518609511b20e8d293ca	boosting systems for lvcsr.		We employ a variant of the popular Adaboost algorithm to train multiple acoustic models such that the aggregate system exhibits improved performance over the individual recognizers. Each model is trained sequentially on re-weighted versions of the training data. At each iteration, the weights are decreased for the frames that are correctly decoded by the current system. These weights are then multiplied with the frame-level statistics for the decision trees and Gaussian mixture components of the next iteration system. The composite system uses a log-linear combination of HMM state observation likelihoods. We report experimental results on several broadcast news transcription setups which differ in the language being spoken (English and Arabic) and amounts of training data. Additionally, we study the impact of boosting on ML and discriminatively trained acoustic models. Our findings suggest that significant gains can be obtained for small amounts of training data even after feature and model-space discriminative training.	acoustic cryptanalysis;acoustic model;adaboost;aggregate data;algorithm;boosting (machine learning);decision tree;discriminative model;finite-state machine;hidden markov model;iteration;log-linear model;speech analytics;transcription (software)	George Saon;Hagen Soltau	2010			speech recognition;artificial intelligence;boosting (machine learning);pattern recognition;computer science	NLP	-19.20866186462556	-88.32915131231218	148706
5f92de2b4958a4f470885f7ac7d12f21bf4b5dcc	scientific argumentation detection as limited-domain intention recognition.		We describe the task of intention-based text understanding for scientific argumentation. The model of scientific argumentation presented here is based on the recognition of 28 concrete rhetorical moves in text. These moves can in turn be associated with higherlevel intentions. The intentions we aim to model operate in the limited domain of scientific argumentation and justification; it is the limitation of the domain which makes our intentions predictable and enumerable, unlike general intentions. We explain how rhetorical moves relate to higher-level intentions. We also discuss work in progress towards a corpus annotated with limited-domain intentions, and speculate about the design of an automatic recognition system, for which many components already exist today.	text corpus	Simone Teufel	2014			computer vision;communication;social psychology	AI	-33.04485985554655	-80.49094632328612	148877
235470b70233ac7b00c1a71f1c1f50afb69ba0ff	look-ahead techniques for fast beam search	word recognition accuracy fast beam search algorithm large vocabulary continuous speech recognition language model look ahead phoneme look ahead pruning process time synchronous one pass beam search tree organized pronunciation lexicon bigram language model nab 94 task arpa north american business corpus recognition experiments search space size reduction;large vocabulary continuous speech recognition;search space;search algorithm;grammars speech recognition search problems natural languages;look ahead;natural languages;time synchronization;grammars;word recognition;dynamic programming vocabulary speech recognition acoustic beams search methods testing natural languages computational efficiency hidden markov models histograms;speech recognition;search problems;language model	In this paper, we present two efficient look-ahead pruning te chniques in beam search for large vocabulary continuous speec h recognition. Both techniques, the language model look-ahe ad and the phoneme look-ahead, are incorporated into the word conditioned search algorithm using a bigram language model and a lexical prefix tree [5]. The paper present the following nov el contributions: We describe a method for language model (LM) look-ahead pruning which is similar to [1, 9]. We show special technique s to reduce the memory and computational requirements. These techniques are based on a compressed LM look-ahead tree. To compute the LM look-ahead tree probabilites in an efficient way, we present a backward dynamic programming scheme. We present a phoneme look-ahead pruning technique to increase the efficiency of the acoustic pruning. In particular , we refine the acoustic pruning strategy by a 1and 2-phoneme look-ahead, respectively. We report results for both look-ahead pruning methods on the the 20,000-word North American Business (NAB’94) task. As a result, the combination of bigram look-ahead and 1phoneme look-ahead reduces the search space by a factor of 10without loss in recognition accuracy in comparison with the baseline search using a unigram language model look-ahead as described in [2]. The computational costs can be reduced by a factor of5 on a SGI workstation (Indy R4400).	acoustic cryptanalysis;baseline (configuration management);beam search;bigram;computation;dynamic programming;language model;n-gram;r4000;requirement;search algorithm;trie;vocabulary;workstation	Stefan Ortmanns;Andreas Eiden;Hermann Ney;Norbert Coenen	1997	Computer Speech & Language	10.1109/ICASSP.1997.598876	natural language processing;speech recognition;word recognition;computer science;pattern recognition;bigram;natural language;language model;search algorithm	NLP	-21.525303169152984	-87.15304294552286	149431
8cc0808a021c87dc59fa8834e8ad96bea0246d8f	a comparison of ligature and contextual models for hidden markov model based on-line handwriting recognition	word error rate;handwriting recognition;hidden markov model;on line handwriting recognition;vocabularies ligature models contextual models word models on line handwriting recognition writer independent unconstrained handwriting recognition hidden markov models contextual character shape variations ligature variations diacriticals performance complexity pause model word error rate reduction character models mixed style word recognition test sets;hidden markov models context modeling error analysis handwriting recognition robustness shape costs character recognition testing vocabulary;hidden markov models;computational complexity;word recognition;computational complexity hidden markov models handwriting recognition online operation;online operation	This paper addresses the problem of on-line, writerindependent, unconstrained handwriting recognition. Based on hidden Markov models (HMM), we focus on the construction and use of word models which are robust towards contextual character shape variations and variations due to ligatures and diacriticals with the objective of an improved word error rate. We compare the performance and complexity of contextual hidden Markov models with a ‘pause’ model for ligatures. While the common contextual models lead to a word error rate reduction of 12.7%-38% at the cost of almost six times more character models, the pause model improves the word error rate by 15%-25% and adds only a single model to the recognition system. The results for a mixed-style word recognition task on two test sets with vocabularies of 200 (up to 98% correct words) and 20,000 words (up to 88.6% correct words) are given.	handwriting recognition;hidden markov model;markov chain;online and offline;vocabulary;word error rate	Hans J. G. A. Dolfing	1998		10.1109/ICASSP.1998.675454	natural language processing;speech recognition;word recognition;word error rate;computer science;intelligent word recognition;machine learning;pattern recognition;computational complexity theory;hidden markov model	Vision	-19.66533430483646	-90.92935803596421	149544
3d70eda75d466a820967eb1a15059ed3f31b1bce	finite-state approximation of phrase structure grammars	context-free grammar;limited-domain speech recognition task;finite-state approximation;real-time speech recognition;language model;right-linear context-free grammar;finite-state language model;phrase structure grammar;natural language;computes finite-state approximation;certain context-free;regular language;expressive power;real time;speech recognition;context free grammar	Phrase-structure grammars are effective models for important syntactic and semantic aspects of natural languages, but can be computationally too demanding for use as language models in real-time speech recognition. Therefore, finite-state models are used instead, even though they lack expressive power. To reconcile those two alternatives, we designed an algorithm to compute finite-state approximations of context-free grammars and context-free-equivalent augmented phrase-structure grammars. The approximation is exact for certain context-free grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to build finite-state language models for limited-domain speech recognition tasks.	algorithm;approximation;context-free grammar;context-free language;finite-state machine;language model;natural language;real-time clock;regular language;speech recognition	Fernando Pereira;Rebecca N. Wright	1991			natural language processing;context-sensitive grammar;id/lp grammar;tree-adjoining grammar;indexed grammar;noun phrase;l-attributed grammar;phrase structure grammar;pattern recognition;phrase structure rules;definite clause grammar;context-free grammar;generalized phrase structure grammar;combinatory categorial grammar;immediate constituent analysis;c-command;dependency grammar	NLP	-27.8149403820748	-80.71332607399437	149680
f1801693a18ce5931ae2166786a1a9ab8bb9059e	constructing family trees of multilingual speech using gaussian mixture models.	gaussian mixture model	This paper proposes a method for automatically clustering multilingual speech so as to derive language family trees. We consider that the language is the source of information which generates speech feature parameters; the probability or statistical characteristics of this information is modeled by Gaussian mixture models (GMMs); then a distance measure between the GMMs is introduced. Based on this, we construct family trees of multilingual speech which are quite similar to those considered in linguistics.	cluster analysis;family tree;information source;mixture model	Shuichi Itahashi;Shiwei Zhu;Mikio Yamamoto	2005			speech recognition;computer science;machine learning;pattern recognition;mixture model	NLP	-20.17688314126169	-90.4254388251302	150374
210f1143b0177c26e58112b01d871c3ec1b88a32	contextual language learning with capti esl assistant	assistant;tesol;ell;enl;english;language learning;esl	In this paper, we present Capti ESL Assistant, a novel universally accessible web application that facilitates acquisition of English by helping language learners develop their reading and listening skills simultaneously. It features built-in translation and the Word Challenge game that enables users to learn the language in the context. It is also accessible to users with print disabilities.	web application;word lens	Yevgen Borodin;Yury Puzis;Andrii Sovyak;Vikas Ashok;Andrii Melnyk;I. V. Ramakrishnan	2016		10.1145/2899475.2899508	natural language processing;computer science;linguistics;communication	HCI	-28.226493659138495	-84.27784461622427	150500
2cce4e3732e3403b4413fbdf775530777826de73	application of the nok method in sentence modelling	knowledge representation semantics natural languages mathematical model computational modeling;network based knowledge representation node of knowledge text based knowledge representation;natural language processing artificial intelligence knowledge representation;natural human languages nok method sentence modelling knowledge representation artificial intelligence graphical representation text expressed knowledge nodes of knowledge aesop fable	"""Knowledge representation is one of the areas covered by artificial intelligence. One of the methods for graphical representation of text expressed knowledge is the method NOK (Nodes Of Knowledge). NOK method enables transformation of text expressed knowledge into a graphical network of words and group of words. In this paper application of NOK method is presented. This application is based on sentences from an Aesop's Fable in Croatian (""""Golden eggs in the chicken"""") and English (""""The Goose with the golden eggs"""") version. In this way the applicability of this method on two natural human languages is presented, and similarities and differences that are partially conditioned by freedom of translators, and not only by differences in the syntax of the two languages, are observed."""	artificial intelligence;autonomy;computer;graphical user interface;knowledge representation and reasoning	M. Rauker Koch;Mile Pavlic;Alen Jakupovic	2014	2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2014.6859746	natural language processing;knowledge representation and reasoning;computer science;artificial intelligence;machine learning;programming language;commonsense knowledge	AI	-31.37045984490243	-80.47526232181104	150801
98aa7cfc9b1efd016fb6da4e684f027d9fef6f15	statistical language model adaptation: review and perspectives	in vocabulary;ajustamiento modelo;interpolation;interpolacion;statistical model;ajustement modele;automatic recognition;reconocimiento voz;language model adaptation;model matching;modele statistique;speech recognition;modelo estadistico;reconnaissance parole;statistical language model;language model;reconocimiento automatico;reconnaissance automatique	Speech recognition performance is severely affected when the lexical, syntactic, or semantic characteristics of the discourse in the training and recognition tasks differ. The aim of language model adaptation is to exploit specific, albeit limited, knowledge about the recognition task to compensate for this mismatch. More generally, an adaptive language model seeks to maintain an adequate representation of the current task domain under changing conditions involving potential variations in vocabulary, syntax, content, and style. This paper presents an overview of the major approaches proposed to address this issue, and offers some perspectives regarding their comparative merits and associated tradeoffs. 2003 Elsevier B.V. All rights reserved.	language model;lexicon;speech recognition;vocabulary	Jerome R. Bellegarda	2004	Speech Communication	10.1016/j.specom.2003.08.002	natural language processing;speech recognition;interpolation;computer science;linguistics;statistics;language model	AI	-21.94332507818622	-80.41927221289568	151193
2e8d47fcba60cff5cf5ba9aa535192cccfc37db1	topic identification of spoken documents using unsupervised acoustic unit discovery		This paper investigates the application of unsupervised acoustic unit discovery for topic identification (topic ID) of spoken audio documents. The acoustic unit discovery method is based on a non-parametric Bayesian phone-loop model that segments a speech utterance into phone-like categories. The discovered phone-like (acoustic) units are further fed into the conventional topic ID framework. Using multilingual bottleneck features for the acoustic unit discovery, we show that the proposed method outperforms other systems that are based on cross-lingual phoneme recognizer.	acoustic cryptanalysis;finite-state machine;unsupervised learning	Santosh Kesiraju;Raghavendra Pappagari;Lucas Ondel;Lukás Burget;Najim Dehak;Sanjeev Khudanpur;Jan Cernocký;Suryakanth V. Gangashetty	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953257	artificial intelligence;pattern recognition;hidden markov model;data modeling;computer science;bottleneck;utterance;speech recognition;vocabulary	Robotics	-19.517487026106327	-86.46943291149236	151453
f5c81aa8a5c3c41c03b8385a8d6c2d7c434feebd	building class-based language models with contextual statistics	cluster algorithm;maximum mutual information;distance measure;speech processing;context modeling statistics vocabulary clustering algorithms partitioning algorithms computational efficiency mutual information distortion measurement natural languages speech recognition;natural languages;context sensitive grammars;pattern recognition speech processing speech recognition natural languages statistical analysis context sensitive grammars information theory;statistical analysis;pattern recognition;speech recognition;large vocabulary continuous speech recognition class based language models contextual statistics clustering algorithms words minimum discriminative information distance measure bigram language models computational cost mdi algorithm maximum mutual information algorithm efficiency tree building clustering;language model;information theory	In this paper, novel clustering algorithms are proposed by using the contextual statistics of words for classbased language models. The Minimum Discriminative Information (MDI) is used as a distance measure. Three algorithms are implemented to build bigram language models for a vocabulary of 50; 000 words over a corpus of over 200 million words. The computational cost of algorithms and resulting LM perplexity are studied. The comparisons between the MDI algorithm and the Maximum Mutual Information algorithm are also given to demonstrate the e ectiveness and the efciency of the new algorithms. It is shown that the MDI approaches make the tree-building clustering[2] possible with large vocabulary.	algorithm;bigram;cluster analysis;computation;computational complexity theory;language model;medium-dependent interface;multiple document interface;mutual information;n-gram;perplexity;speech recognition;super robot monkey team hyperforce go!;vocabulary	Shuanhu Bai;Haizhou Li;Zhiwei Lin;Baosheng Yuan	1998		10.1109/ICASSP.1998.674395	natural language processing;speech recognition;information theory;computer science;machine learning;pattern recognition;speech processing;cluster analysis;natural language;statistics;language model	ML	-20.179072785439217	-90.60530592577777	151604
1cbe074f891dfcaaa4818587c6283093cd11d68d	a modular and hybrid connectionist system for speaker identification	speaker identification;learning algorithm;sistema hibrido;connectionism;reconocimiento palabra;hidden markov model;conexionismo;systeme modulaire;sistema modular;algorithme apprentissage;architecture reseau;connexionnisme;a priori knowledge;modular system;hybrid system;speech recognition;network architecture;reconnaissance parole;systeme hybride	This paper presents and evaluates a modular/hybrid connectionist system for speaker identification. Modularity has emerged as a powerful technique for reducing the complexity of connectionist systems, and allowing a priori knowledge to be incorporated into their design. Text-independent speaker identification is an inherently complex task where the amount of training data is often limited. It thus provides an ideal domain to test the validity of the modular/hybrid connectionist approach. To achieve such identification, we develop, in this paper, an architecture based upon the cooperation of several connectionist modules, and a Hidden Markov Model module. When tested on a population of 102 speakers extracted from the DARPA-TIMIT database, perfect identification was obtained.	connectionism;extraction;hidden markov model;markov chain;speaker recognition;timit	Younès Bennani	1995	Neural Computation	10.1162/neco.1995.7.4.791	connectionism;a priori and a posteriori;speech recognition;network architecture;computer science;artificial intelligence;machine learning;hidden markov model;hybrid system	AI	-21.328221440363084	-91.74717397112302	152345
e44453cbba1036a5f67963c2f3e08379528a9219	a system for historical documents transcription based on hierarchical classification and dictionary matching		Information contained in historical sources is highly important for the research of historians; yet, extracting it manually from documents written in difficult scripts is often an expensive and time-consuming process. This paper proposes a modular system for transcribing documents written in a challenging script (German Kurrent Schrift). The solution comprises of three main stages: Document Processing, Word Processing and Word Selector, chained together in a linear pipeline. The system is currently under development, with several modules in each stage already implemented and evaluated. The main focus so far has been on the character recognition module, where a hierarchical classifier is proposed. Preliminary evaluations on the character recognition module has yielded ~ 82% overall character recognition rate, and a series of groups of confusable characters, for which an additional identification model is currently investigated. Also, word composition based on a dictionary matching approach using the Levenshtein distance is presented.	coefficient;data dictionary;discrete cosine transform;document layout analysis;document processing;hierarchical classifier;historical document;image processing;levenshtein distance;machine learning;medical transcription;optical character recognition;transcription (software)	Camelia Lemnaru;Andreea Sin-Neamtiu;Mihai-Andrei Veres;Rodica Potolea	2012			data mining;computer science;machine-readable dictionary;speech recognition;transcription (biology);pattern recognition;artificial intelligence	AI	-23.162803854567954	-81.16523999694449	152410
c140084fd7cf292a2d22db0901d88c9950ab1c02	an analysis of frame semantics of continuous processes		Qualitative Process theory provides a formal representation for human-like models of continuous processes. Prior research mapped qualitative process elements onto English language constructions, but did not connect the representations to existing frame semantic resources. Here we identify and classify QP language constituents through their instantiation in FrameNet frames to provide a unified semantics for linguistic and non-linguistic representations of processes. We demonstrate that all core QP relations can map to FN, though larger QP evoking phrasal constructions do exist outside of this mapping. We conclude with a corpus analysis showing that these frames occur in natural text involving a variety of continuous processes.	calculus of constructions;framenet;inferential theory of learning;mental model;semantics (computer science);universal instantiation	Clifton James McFate;Kenneth D. Forbus	2016			natural language processing;semantics;artificial intelligence;frame semantics;process theory;mathematics	AI	-32.49583261264236	-80.75059669453357	152483
0618331d7a50a70ecff145df393163a4205411ad	insights into the dialogue processing of verbmobil	speech-to-speech translation system;mediating scenario;dialogue module;robust component;dialogue processing;correct translation;artificial intelligence	"""We present the dialogue module of the speech-to-speech translation system VERBMOBIL. We follow the approach that the solution to dialogue processing in a mediating scenario can not depend on a single constrained processing tool, but on a combination of several simple, efficient, and robust components. We show how our solution to dialogue processing works when applied to real data, and give some examples where our module contributes to the correct translation from German to English. 1 I n t r o d u c t i o n The imPlemented research prototype of the speechto-speech translation system VEaBMOBIL (Wahlster, 1993; Bub and Schwinn, 1996) consists of more than 40 modules for both speech and linguistic processing. The central storage for dialogue information within the overall system is the dialogue module that exchanges data with 15 of the other modules. Basic notions within VERBMOBIL are t u ~ 8 a n d u t t e r a n c e s . A turn is defined as one contribution of a dialogue participant. Each turn divides into utterances that sometimes resemble clauses as defined in a traditional grammar. However, since we deal exclusively with spoken, unconstrained contributions, utterances are sometimes just pieces of linguistic material. For the dialogue module, the most important dialogue related information extracted for each utterance is the so called dialogue act (Jekat et al., 1995). Some dialogue acts describe solely the illocutionary force, while other more domain specific ones describe additionally aspects of the propositional content of an utterance. Prior to the selection of the dialogue acts, we analyzed dialogues from VERBMOBIL'S corpus of spoken and transliterated scheduling dialogues. More than 500 of them have been annotated with dialogue related information and serve as the empirical foundation of our work. Throughout this paper we will refer to the example dialogue partly shown in figure 1. The translations are as the deep processing line of VERBMOBIL provides them. We also annotated the utterances with the dialogue acts as determined by the semantic evaluation module. ' ' / / ' ' shows where utterance boundaries were determined. We start with a brief introduction to dialogue processing in the VERBMOBIL setting. Section 3 introduces the basic data structures followed by two sections describing some of the tasks which are carried out within the dialogue module. Before the concluding remarks in section 8, we discuss aspects of robustness and compare our approach to other systems. 2 I n t r o d u c t i o n t o D i a l o g u e P r o c e s s i n g i n VERBMOBIL In contrast to many other NL-systems, the VEaBMOBIL system is mediating a dialogue between two persons. No restrictions are put on the locutors, except for the limitation to stick to the approx. 2500 words VERBMOBIL recognizes. Therefore, VERBMOBIL and especially its dialogue component has to follow the dialogue in any direction. In addition, the dialogue module is faced with incomplete and incorrect input, and sometimes even gaps. When designing a component for such a scenario, we have chosen not to use one big constrained processing tool. Instead, we have selected a combination of several simple and efficient approaches, which together form a robust and efficient processing platform. As an effect of the mediating scenario, our module cannot serve as a """"dialogue controller"""" like in man-machine dialogues. The only exception is when"""	approximation;data structure;dialog system;machine translation;nl (complexity);prototype;robustness (computer science);scheduling (computing);speech corpus;verbmobil	Jan Alexandersson;Norbert Reithinger;Elisabeth Maier	1997			natural language processing;speech recognition;computer science;artificial intelligence	NLP	-30.558253016409523	-82.41108069282964	152670
1cac4d45b3981b0bf49c7d5f4aebf76f354b0de9	oasis natural language call steering trial	limiting factor;natural language	A recent trial of natural language call steering on live UK calls to the operator is described along with its results. The characteristics of the problem are described along with the acoustic, language, semantic and dialogue modelling approaches employed. Natural language call steering is found to be viable, with recognition and semantic accuracy the current limiting factors.	acoustic cryptanalysis;current limiting;natural language	Peter J. Durston;Mark Farrell;David Attwater;James Allen;Hong-Kwang Jeff Kuo;Mohamed Afify;Eric Fosler-Lussier;Chin-Hui Lee	2001			simulation;speech recognition;universal networking language;limiting factor;computer science;linguistics;natural language	NLP	-29.292266748858914	-80.6729705050923	152732
30364ed287bad88fbe57d9b7113eef055a932a0e	reflexives and reciprocals in synchronous tree adjoining grammar		An attractive feature of the formalism of synchronous tree adjoining grammar (STAG) is its potential to handle linguistic phenomena whose syntactic and semantic derivations seem to diverge. Recent work has aimed at adapting STAG to capture such cases. Anaphors, including both reflexives and reciprocals, have presented a particular challenge due to the locality constraints imposed by the STAG formalism. Previous attempts to model anaphors in STAG have focused specifically on reflexives and have not expanded to incorporate reciprocals. We show how STAG can not only capture the syntactic distribution and semantic representation of both reflexives and reciprocals, but also do so in a unified way.	anaphora (linguistics);formal system;locality of reference;semantics (computer science);tree-adjoining grammar	Cristina Aggazzotti;Stuart M. Shieber	2017			tree-adjoining grammar;linguistics;natural language processing;mathematics;artificial intelligence	NLP	-33.105142440277184	-81.67257739324052	153036
237deadeaaac917eb8048156bb54812ab3d94839	adaptive post-processing of ocr text via knowledge acquisition	information retrieval;optical character recognition;machine learning;error correction;knowledge acquisition;ocr post processing;adaptive error correction;ocr environment;office automation	Optical Character Recognit ion (OCR) is a convenient and eff ic ient tool for off ice automation and information retrieval, and is becoming more and more important in today's off ice and library environment. Depending on the text to be recognized, and the software and hardware employed, OCR software produces various types of errors in the recognized texts. The error types and their distributions are environment-dependent . In this paper, we first provide a classification for the types of errors that occur. An eff icient approach for the post-processing of recognition errors in OCR text is proposed so that errors can be e f f i ciently detected and corrected with the aid of a computer. The approach also allows for the correction of OCR errors to be partially automated. The major contribution of this approach is the capability of knowledge acquisition for OCR postprocessing which facilitates the eff ic ient correction of OCR errors. Through self-learning, the postprocessor is able to perform better and more accurately as processing proceeds. Experimental results are provided to demonstrate the eff ic iency and the effect iveness of this approach.	comparison of optical character recognition software;information retrieval;knowledge acquisition;video post-processing	Lon-Mu Liu;Yair M. Babad;Wei Sun;Ki-Kan Chan	1991		10.1145/327164.328773	natural language processing;error detection and correction;speech recognition;computer science;machine learning;pattern recognition;optical character recognition	AI	-23.02101777347577	-80.6784316188627	153095
1d6a4bf658ba0d7968b82b4e9057d9bb0883ea50	multi-class model m	cluster algorithm;nist;multiclass model m;decoding;gale arabic to english development;language modeling;prediction algorithms;language translation;automatic evaluation;n gram model;data model;training data;maximum entropy models language modeling machine translation;class based exponential language model;speech recognition system;speech recognition language translation;clustering algorithms;nist multiclass model m class based exponential language model machine translation speech recognition system n gram model gale arabic to english development;speech recognition;maximum entropy model;maximum entropy models;clustering algorithms prediction algorithms data models adaptation models decoding training data speech recognition;adaptation models;language model;machine translation;data models	Model M, a novel class-based exponential language model, has been shown to significantly outperform word n-gram models in state-of-the-art machine translation and speech recognition systems. The model was motivated by the observation that shrinking the sum of the parameter magnitudes in an exponential language model leads to better performance on unseen data. Being a class-based language model, Model M makes use of word classes that are found automatically from training data. In this paper, we extend Model M to allow for different clusterings to be used at different word positions. This is motivated by the fact that words play different roles depending on their position in an n-gram. Experiments on standard NIST and GALE Arabic-to-English development and test sets show improvements in machine translation quality as measured by automatic evaluation metrics.	brown clustering;evaluation function;evaluation of machine translation;experiment;language model;model m keyboard;n-gram;speech recognition;time complexity	Ahmad Emami;Stanley F. Chen	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947608	natural language processing;cache language model;data modeling;training set;speech recognition;nist;prediction;data model;computer science;principle of maximum entropy;machine learning;pattern recognition;cluster analysis;statistics;language model	NLP	-21.730398794496907	-89.91529505567092	153260
d83e179c66de04f9d3a985650fa1f0639ef121ab	portable knowledge sources for machine translation	conventional user-dictionary method;portable knowledge source;word sense;attachment disambignuation;repeated occurrence;mt system;machine translation;knowledge source;identical error;annoying thing	in this paper, we describe the acquisition iuld (Irga-nization of knowledge sources fur machine translation (MT) systems. It has heen liointed out by many users that one of the most annoying things idmtlt MT sys-terns is tim repeated occurrence of identical errors in word sense and attachment dlsambiguation. We show the limitations of a conventional user-dictionary method and explain how our approach solves the prol/lem. 1. Introduction In the last decal% more and more commercia.l machine translation (MT) systems have lmcome available for a wide variety (if languag, e Iiairs. An MT system is a very handy tool: trot one quickly Iinds out thai, it Irlakes tt, e same errors over and over again even if a user dictionary is carefully maintained. There are sew, ral re;mons for such repeated errors. 1. Commercial MT systems are not tmilt in actor dance with a powerful h;xical semantic formalism. The user dictionary alone cannot (llsamlfiguate word senses and phrasal icttitelimei/ts satisNmtorily. 2. MT systems cannot handle the domain and context dei)endency of word sm,se, ph rasal atl, aeh men L an d word selection. 3. In a shared environment, each user has a different nt user dictionary, and must therefore redumhmtly correct the same errors ms all the other users. A powerful lexieal semantic apl)roaeh [s] couhl give more accurate translatiml~ but it might be. too Inuch to ask users to develop their dictionaries within that formalism. Tl, e simple structure of a user dieti(mary also restricts the learning ability of M'r systems during the post-editing process. The second of the almw~ re;kstms tl~ motivated recent exanlple-ba~ed and case-b~med machine translation re.search [9, s, 10]. However, a method for finding the best-matehlng eases hi a cime. base., where cases (or exalnples) are collected from different dmna.ins or contexts~ has not been studied well. Nor is it kllown whether considering the frequency of eases gives a better result. The third reason is rarely <liscussed> hut it is riot desirable sirnply to share a single user dictionary, since the dictionary may become inconsistent by reflecting multil>le users' updates. McRoy [s] discussed word sense disambiguation using multlph; knowledge sources, but her method is still dictionary-b~med. Some of the eoolmerclal systems for human-aided trailslatlm h such as the Translation Manager/2 [~1~ can provide the user with nmre Ilexible access to multilile dictionaries and the Iranslation memory (a repository of pairs of smlrce and target sentences). This …	attachments;dictionary;formal system;handy board;machine translation;postediting;semantics (computer science);word sense;word-sense disambiguation	Koichi Takeda	1994			natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation	NLP	-31.839436397583516	-81.369293109182	153531
2ae0565c9842623ae2201899e43df1417eeaf594	application of confidence measures for dialogue systems through the use of parallel speech recognizers	error rate;error correction;proper names	To assess the correctness of a recognizer output in any instance of a dialogue is a complex task that has been studied thoroughly during the past decade. Its importance relays on the need for robust dialogue systems, capable of dealing with difficulties inherent to human-machine communications: user errors and corrections, speech recognizer errors, error recovery techniques, etc. In this paper, we present a novel approach to the problem of deciding what the user has said. We use confidence measures derived from low level knowledge sources (acoustic and linguistic information) and generated in parallel from several topic-adapted speech recognizers. Each recognizer is aimed to the recognition of a particular topic, and confidence measures are compared through the use of a classifier that lead to a most probable solution. This approach shows to be specially suited for difficult topics, such as proper names or confirmations, which are highly meaningful for error correction tasks. These topics present high error rates when using an application-wide speech recognizer, but recognition correction is greatly enhanced through the use of parallel recognizers. Moreover, the use of topic-adapted recognizers seems to help also in the identification of the user intention and in the detection of outof-application utterances.	acoustic cryptanalysis;correctness (computer science);dialog system;error detection and correction;finite-state machine;importance sampling;relay;speech recognition;speech synthesis;statistical classification	David Pérez-Piñar López;Carmen García-Mateo	2005			error detection and correction;artificial intelligence;word error rate;speech recognition;proper noun;pattern recognition;machine learning;computer science;correctness;classifier (linguistics);rule-based machine translation	NLP	-25.851789359053388	-86.62778773823601	153694
603907b799345a36b424e48d2260aeffac389648	minimum phoneme error based heteroscedastic linear discriminant analysis for speech recognition	minimisation;broadcast news;lattice theory;high dimensionality;hidden markov model;natural languages;heteroscedastic linear discriminant analysis;linear discriminant analysis speech recognition speech analysis hidden markov models maximum likelihood estimation robustness telephony broadcasting control systems analysis of variance;broadcast news minimum phoneme error heteroscedastic linear discriminant analysis speech recognition discriminative feature analysis phoneme error minimization lattice based training discriminative training hidden markov models hmm conversational telephone speech;speech recognition;feature selection;learning artificial intelligence;natural languages learning artificial intelligence minimisation speech recognition lattice theory;conversational telephone speech	We introduce a discriminative feature analysis method that seeks to minimize phoneme errors in lattice-based training frameworks. This technique, referred to as minimum phoneme error heteroscedastic linear discriminant analysis (MPE-HLDA), is shown to be more robust than traditional LDA methods in high dimensional spaces, and easy to incorporate with existing training procedures, such as HLDA-SAT and discriminative training of hidden Markov models (HMMs). Results on conversational telephone speech and broadcast news corpora also show that the recognition accuracy is improved using features selected by MPE-HLDA.	hp multi-programming executive;hidden markov model;linear discriminant analysis;markov chain;speech recognition;text corpus	Bing Zhang;Spyridon Matsoukas	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415266	minimisation;speech recognition;computer science;machine learning;pattern recognition;lattice;natural language;feature selection;hidden markov model	Vision	-19.4163122375778	-91.31059333983592	154570
179f257e498cd0c6f7be778e6be2f28060a245fa	parallel corpus approach for name matching in record linkage	databases;machine translation record linkage crowd sourcing;probability;string similarity methods parallel corpus approach name matching record linkage entity resolution data mining genealogy person records user search query logs crowd sourced training set machine translation problem character level information retrieval evaluation methodology phonetic method;training;data mining;training databases couplings buildings computational modeling probability data mining;crowd sourcing;computational modeling;couplings;string matching data mining language translation parallel processing query processing;buildings;machine translation;record linkage	Record linkage, or entity resolution, is an important area of data mining. Name matching is a key component of systems for record linkage. Alternative spellings of the same name are a common occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name-matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use information retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and recall. Our result can lead to a significant practical impact in entity resolution applications.	blocking (computing);crowdsourcing;data mining;edit distance;information retrieval;jaro–winkler distance;levenshtein distance;linkage (software);machine translation;new york state identification and intelligence system;parallel text;phonetic algorithm;precision and recall;string metric;test set	Jeffrey Sukharev;Leonid Zhukov;Alexandrin Popescul	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.76	natural language processing;record linkage;computer science;machine learning;probability;data mining;database;machine translation;coupling;computational model	DB	-23.551678155612798	-81.96554187400984	154645
e2ae9a97f21beea249fc8e07e07704188734e87f	over-generative finite state transducer n-gram for out-of-vocabulary word recognition	databases;grammar;handwriting recognition;vocabulary grammar databases transducers handwriting recognition decoding character recognition;decoding;vocabulary;transducers;handwritten recognition out of vocabulary modeling finite state transducer;out of vocabulary modeling;finite state transducer;character recognition;handwritten recognition	"""Hybrid statistical grammars both at word and character levels can be used to perform open-vocabulary recognition. This is usually done by allowing the special symbol for unknown-word in the word-level grammar and dynamically replacing it by a (long) n-gramat character-level, as the full transducer does not fit in the memory of most current computers. We present a modification of a finite-state-transducer (fst) n-gram that enables the creation of a static transducer, i.e. when it is not possible to perform on-demand composition. By combining paths in the """"LG"""" transducer (composition of lexicon and n-gram)making it over-generative with respect to the n-grams observed in the corpus, it is possible to reduce the number of actual occurrences of the character-level grammar, the resulting transducer fits the memory of practical machines. We evaluate this model for handwriting recognition using the RIMES and the IAM dabases. We study its effect on the vocabulary size and show that this model is competitive with state-of-the-art solutions."""	computer;fits;finite-state transducer;grams;handwriting recognition;identity management;lexicon;microsoft word for mac;n-gram;stochastic grammar;vocabulary	Ronaldo O. Messina;Christopher Kermorvant	2014	2014 11th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2014.24	natural language processing;finite state transducer;speech recognition;transducer;intelligent character recognition;computer science;intelligent word recognition;grammar;handwriting recognition	NLP	-21.44398112895197	-80.83114648890891	154966
7ca42ab1865c13add83b9ff86beb5b31d3340d98	sample-based automatic dictionary generation for keyword spotting system	keyword spotting system;data driven generation;acoustics;canonical dictionary sample based automatic dictionary generation keyword spotting system data driven generation pronunciation dictionary pronunciations speech samples recognized sequences phoneme confusion network confidence based metric;recognized sequences;speech;confusion network;data mining;confidence metric;dictionaries hidden markov models speech recognition fuzzy systems automatic speech recognition merging personal communication networks acoustic measurements lattices interpolation;sample based automatic dictionary generation;gold;hidden markov models;pronunciations;dictionaries;pronunciation extraction;speech samples;speech recognition;confidence based metric;speech recognition dictionaries;confidence metric data driven pronunciation extraction phoneme confusion network;phoneme confusion network;canonical dictionary;data driven;pronunciation dictionary	In this paper we develop an approach to automatic, data-driven generation of pronunciation dictionaries for keyword spotting(KWS) systems. In practical applications, KWS tasks often have to deal with keywords whose pronunciations can not be found in the dictionary. To solve this problem, we study how to derive pronunciations automatically from speech samples of keywords. Recognized sequences from these samples are used as candidates, and merged to form a phoneme confusion network(PCN) from which the pronunciations are extracted based on a confidence-based metric. Experimental results show that sample-based dictionary reaches similar performance with the canonical dictionary, and the proposed approach is independent of the sample set.	data dictionary;phoneme	Li Lu;Fengpei Ge;Ta Li;Qingwei Zhao;Yonghong Yan	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.506	gold;natural language processing;speech recognition;computer science;speech;pattern recognition;hidden markov model	Robotics	-20.05516689491264	-86.21760195006794	155092
267e7faa0a99bb10f2f32fd5b60759abf54c2dbc	integrating multiple pronunciations during mce-based acoustic model training for large vocabulary speech recognition	speech recognition	In this paper, we report on the implementation of an automatic method for discovering an appropriate pronunciation for each speech utterance of every speaker and integrating this new information into minimum classification error (MCE) based training algorithm. The proposed method allows a lot more flexibility in adapting multiple pronunciations during the existing supervised acoustic model training where the phoneme sequence of a particular word is always fixed irrespective of speaker accents and pronunciation variations. Several large vocabulary recognition results on French SpeechDat-II speech corpus show a consistent string error rate reduction of about 48% and 13% obtained by the proposed integrated method when compared to the MLEtrained and MCE-trained baseline systems.	acoustic cryptanalysis;acoustic model;algorithm;baseline (configuration management);speech corpus;speech recognition;statistical classification;tinymce;vocabulary	Rathinavelu Chengalvarayan	2002			speech recognition;natural language processing;computer science;acoustic model;artificial intelligence;speech corpus;vocabulary	NLP	-19.539484412453298	-86.17085588866415	155637
943b84641db735fb945892dff247728425cb9b18	limited vocabulary natural language dialogue	natural language dialogue	Two-person teams of subjects worked at realistic problem-solving tasks by communicating through a teletypewriter system. One third of the teams had to limit their vocabulary to words on lists of 300 words, one-third were required to use words on lists of 500 words, and one third of the teams worked with no vocabulary restrictions. Each team solved a different problem on each of three successive days. Dependent measures were taken on four classes of variables: (1) time to solve the problem, (2) measures of overt behavior, (3) measures of verbal output, and (4) errors made by subjects who used the restricted vocabularies. The main finding of the experiment was that subjects who worked with the restricted vocabularies interacted and solved problems as successfully as their counterparts who worked with no vocabulary restrictions. The results indicate that, at least for the kinds of problems tested here, it is possible to develop vocabularies of limited size that can be used effectively in man-computer communications.	natural language;vocabulary	Michael J. Kelly;Alphonse Chapanis	1977	International Journal of Man-Machine Studies	10.1016/S0020-7373(77)80015-1	natural language processing;speech recognition;computer science;artificial intelligence	Vision	-26.779278956159622	-86.7633529354941	155978
590be8dbb6b6aedab0a234e82af9fe314bf3f1f6	automatic prosodic break labeling for mandarin chinese speech data	speech synthesis;recursive algorithm;mandarin chinese	For corpus-based speech synthesis, large quantities of labeled speech are required. Manually labeling speech data is quite laborintensive. Therefore, automatic speech labeling is highly desired. Prosodic break detection is one of the tasks for automatic speech labeling. In the paper, we propose an automatic break detection algorithm for mandarin Chinese speech. In this approach, we use energy contour to normalize duration of syllables and use the concept of normalized transition time to represent the time interval between two syllables. A recursive algorithm is then used to select locally longer intervals as pauses. Language specific constraint rules are also used to produce a better judgment. The automatic break labeling results have been proved to be good.	algorithm;database normalization;recursion (computer science);rise time;speech corpus;speech synthesis;super robot monkey team hyperforce go!;syllable;text corpus	Minghui Dong;Kim-Teng Lua	2002			artificial intelligence;speech recognition;mandarin chinese;pattern recognition;automatic speech;recursion (computer science);computer science;speech synthesis	NLP	-20.504407468248264	-82.49560195124513	155988
48a27b5b2ca781ca48cf7102dca2cb901f1ff9b6	building conceptual dictionary for providing common knowledge in the integrated narrative generation system	social and behavioral sciences	We explain the current version of a conceptual dictionary containing two hierarchies of verb concepts and noun concepts to be functioned in our narrative generation system. It is used for operating naturalness or validity of generated events and realizing or adjusting the intentional defamiliarization. Namely, this dictionary is a mechanism to be able to flexibly adjust a variety of generation from realistic narratives to fantastical narratives as well as the foundation for a narrative event and the elements. In the current version, verb concept dictionary has originally defined 5338 case frames and modified 1158 constraints and noun concept dictionary contains 121573 concepts including 5808 intermediate concepts.	dictionary	Kensuke Oishi;Yasunari Kurisawa;Mami Kamada;Itaru Fukuda;Taisuke Akimoto;Takashi Ogata	2012			psychology;natural language processing;narrative criticism;narrative network;philosophy;computer science;artificial intelligence;mathematics;linguistics;sociology;communication;cognitive science	AI	-32.982734074396134	-80.49844342126356	156045
04f746e8e52a19e74ae06675c84edc941144f993	new telephone speech corpora at cslu	language identification;word recognition	 The Center for Spoken Language Understanding (CSLU) collects, annotates and distributes telephone speech data to enable research in spoken language understanding and automatic language identification. This paper gives a brief overview of recent activities in pursuit of this mission. We summarize corpus development activities at CSLU and describe new corpora useful for research on specific tasks: alphabet recognition, numbers recognition, large vocabulary word recognition, and yes/no... 	cslu toolkit;text corpus	Ronald A. Cole;Mike Noel;Terri Lander;T. Durham	1995			speech recognition;word recognition;natural language processing;language identification;computer science;artificial intelligence	NLP	-24.158042584293884	-84.19762806581046	156310
253b72c1e8a932da101698110b1dc95661658e4f	using syntactic and semantic information in a word prediction aid.	semantic information;word prediction	"""An adaptive word prediction program developed several years ago is being extended to include syntactic and semantic information. This expansion is still in progress and there are, as yet, no test results. INTRODUCTION A word prediction program was developed several years ago to allow ~notorically disabled persons using speech synthesis as a prosthesis to increase their typing speed (Hunnicutt, 1986). It has been used for a year and a half now and has been distributed to about fifteen persons, schools or institutions. In the interest of offering Inore appropriate predictions, syntactic and semantic components are being added. THE BASIC PROGRAM AND MODIFICATIONS The basic prediction program is run by simply typing one letter of a word at a time. If a prediction is possible, it is presented to the user after each keystroke. The user may then accept the prediction or type a hrther letter. In order to take advantage of word ranking due to syntactic and semantic factors, the program has been changed to allow five predictions rather than a single one. In the basic program, a word is predicted based on word frequency and recency of use. Typing the first letter of a word results in accessing the most frequent word beginning with that letter from a special small lexicon. When further letters are typed, successive predictions are made from a large lexicon marked with word frequencies and from a lexicon that stores recently typed words. When semantic and syntactic information is considered, frequency and recency will still take precedence for the first two or three choices. The last two or three choices will take recently-used semantic classes into consideration. Syntactic information will be used to rank the five choices according to the likelihood of a particular word class occurring next. A lexicon containing word pairs is consulted when a word is terminated. If found, the word paired with it is automatically predicted without its initial letter being typed. In the new program, it is possible to give up to five choices for the second half of a word pair. The lexicon containing word pairs has therefore been expanded to contain these five choices. The large lexicon previously mentioned contains up to 10.000 w o ~ tls. the most frequent words according to published corpus statistics. Base programs i t 1 Swedish, English, Norwegian and Danish are currently in use. The Swedish lexicolr col~tains word class information for use with the syntactic co~nponent. Word class is cu~retltly being added to the English lexicon as well. A study investigating the positive affect of perfectly predicted word class showed a sniall effect in reduced keystrokes. However, it was noted that the exclusion of words with incorrect word class gave the user more pleasure and less frustration. Semantic categories have been added to the Swedish lexicon for 1,500 words. These categories are taken from the classification of international Blissymbols. (Storr, Reich, & McNaughton, 1982). There are six main categories, fifty subcategories. One main category is """"Living Together."""" There are four divisions in this category: """"Communication,"""" """"Transportation,"""" """"Occupations"""" and """"Recreation."""" Each of these divisions has between three and five subcategories. We hope to be able to report on results of tests regarding both reduced keystrokes and appropriateness of predictions at a later date. Acknowledgments This work has been supported by grants from the Swedish Board for Technical Development."""	basic;event (computing);lexicon;mcgurk effect;microsoft word for mac;norm (social);speech synthesis;swedish board for computing machinery;typing;word lists by frequency;words per minute	Sheri Hunnicutt	1989			natural language processing;semantic similarity;semantic computing;semeval;pattern recognition;word lists by frequency;semantic property;linguistics;semantic technology	NLP	-27.201046302376785	-82.12239538416517	157009
e12bc1aa36fdb218d22a8577c4ab32d111cebe84	speed's dnn approach to romanian speech recognition		This paper presents the main improvements brought recently to the large-vocabulary, continuous speech recognition (LVCSR) system for Romanian language developed by the Speech and Dialogue (SpeeD) research laboratory. While the most important improvement consists in the use of DNN-based acoustic models, instead of the classic HMM-GMM approach, several other aspects are discussed in the paper: a significant increase of the speech training corpus, the use of additional algorithms for feature processing, speaker adaptive training, and discriminative training and, finally, the use of lattice rescoring with significantly expanded language models (n-gram models up to order 5, based on vocabularies of up to 200k words). The ASR experiments were performed with several types of acoustic and language models in different configurations on the standard read and conversational speech corpora created by SpeeD in 2014. The results show that the extension of the training speech corpus leads to a relative word error rate (WER) improvement between 15% and 17%, while the use of DNN-based acoustic models instead of HMM-GMM-based acoustic models leads to a relative WER improvement between 18% and 23%, depending on the nature of the evaluation speech corpus (read or conversational, clean or noisy). The best configuration of the LVCSR system was integrated as a live transcription web application available online on SpeeD laboratory's website at https://speed.pub.ro/live-transcriber-2017.	acoustic cryptanalysis;algorithm;automated system recovery;client–server model;discriminative model;experiment;google map maker;heart rate variability;hidden markov model;kaldi;language model;n-gram;open-source software;period-doubling bifurcation;server (computing);speech analytics;speech corpus;speech recognition;text corpus;transcription (software);vocabulary;web application;word error rate	Alexandru-Lucian Georgescu;Horia Cucu;Corneliu Burileanu	2017	2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)	10.1109/SPED.2017.7990443	web application;voxforge;discriminative model;word error rate;speech corpus;natural language processing;hidden markov model;acoustic model;speech recognition;language model;computer science;artificial intelligence	NLP	-20.65394862230044	-85.62535357142816	157026
19d25223606862bc7b1f927b5f46e3d456a49adf	enriching mandarin speech recognition by incorporating a hierarchical prosody model	character error rate;hierarchical structure;pragmatics;tcc300 corpus;mandarin speech recognition hierarchical prosody model;decoding;hidden markov model;acoustics;baseline system;hmm based system;speech;mandarin speech recognition;error analysis;speech recognition hidden markov models;hidden markov models;tcc300 corpus mandarin speech recognition hierarchical prosody model hmm based system punctuation marks intersyllable break types baseline system;proceedings paper;part of speech;punctuation marks;hidden markov models speech recognition acoustics speech pragmatics decoding error analysis;speech recognition;intersyllable break types;hierarchical prosody model	This paper presents a new probabilistic framework of Mandarin speech recognition by incorporating a sophisticated hierarchical prosody model into the conventional HMM-based system. The prosody model describes the relations of linguistic cues of various levels, break types and prosodic states which represent the prosody hierarchical structure, and prosody-related acoustic features. Aside from producing the recognized word sequences, the system also decodes other information including word's part-of-speech, punctuation marks, inter-syllable break types, and prosodic states of syllables. Experimental results on the TCC300 corpus, which consists of paragraphic utterances, showed that the proposed system significantly outperformed the baseline system. The word and character error rates decreased from 24.4% and 18.1% to 20.7% and 14.4% (or 15.2% and 20.4% relative improvements), respectively.	acoustic cryptanalysis;baseline (configuration management);hidden markov model;semantic prosody;speech recognition;super robot monkey team hyperforce go!;syllable;text corpus	Jyh-Her Yang;Ming-Chieh Liu;Hao-Hsiang Chang;Chen-Yu Chiang;Yih-Ru Wang;Sin-Horng Chen	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947492	natural language processing;speech recognition;part of speech;computer science;speech;hidden markov model	NLP	-19.19223330595977	-84.35033821480832	157633
42cc4e356bba4db42e87c5330dbc952b2b030099	brazilian portuguese speech-driven answering system	brazilian portuguese;customer service;automatic speech recognition;ivr;metric wip brazilian portuguese speech driven answering system call centers customer services human attendants numeric keypad driven automatic service speech recognition computer science department secretary federal university department lectures voice messages recording automatic e mailing;hidden markov models electronic mail internet telephony software irrigation speech recognition hardware;speech recognition;call center;natural language processing;speech recognition educational institutions natural language processing;call center ivr automatic speech recognition	Call centers are being increasingly incorporated into companies and institutions. There are usually two different types of call centers: customer services with human attendants and numeric keypad-driven automatic service. Human attendants have high costs. The use of numeric keypads are not intuitive and increases system's rejection rate. In order to contribute to the reduction of these limitations, this paper proposes an automated answering system with speech recognition for Brazilian Portuguese. As a case study, the system handles phone calls of the Computer Science Department secretary at a federal university. Current version is able to provide automatic call transfer to department's lectures, voice messages recording and automatic e-mailing. Recognition evaluation has been done by means of four different metrics. The metric WIP pointed a speech recognition rate of ~91% for limited vocabulary.	computer science;interactive voice response;question answering;rejection sampling;speech recognition;vocabulary	Artur L. C. Oliveira;Eduardo S. Silva;Hendrik T. Macedo;Leonardo Nogueira Matos	2012	2012 6th Euro American Conference on Telematics and Information Systems (EATIS)	10.1145/2261605.2261647	speech recognition;telecommunications;computer science;artificial intelligence;operating system;database;multimedia;world wide web;speech analytics	NLP	-22.35835890576688	-85.91819057819922	157700
7aaac6f58b865f10a7fa79f035aef44008b26383	the effect of language model probability on pronunciation reduction	probability;speech synthesis;logistic regression;statistical analysis probability speech recognition speech synthesis linguistics;statistical analysis;regression analysis speech recognition speech synthesis logistics natural languages probability distribution collaborative work predictive models frequency;speech recognition;word probability language model probability pronunciation reduction lexical form switchboard corpus linear regression logistic regression contextual factors high unigram probability reverse bigram probability speech recognition speech synthesis;language model;linguistics	We investigate how the probability of a word affects its pronunciation. We examined 5618 tokens of the 10 most frequent (function) words in Switchboard: I, and, the, that, a, you, to, of, it, and in, and 2042 tokens of content words whose lexical form ends in a t or d. Our observations were drawn from the phonetically hand-transcribed subset [1] of the Switchboard corpus [2], enabling us to code each word with its pronunciation and duration. Using linear and logistic regression to control for contextual factors, we show that words which have a high unigram, bigram, or reverse bigram (given the following word) probability are shorter, more likely to have a reduced vowel, and more likely to have a deleted final t or d. These results suggest that pronunciation models in speech recognition and synthesis should take into account word probability given both the previous and following words, for both content and function words.	bigram;language model;logistic regression;n-gram;speech recognition;telephone switchboard	Daniel Jurafsky;Alan Bell;Michelle Gregory;William D. Raymond	2001		10.1109/ICASSP.2001.941036	n-gram;natural language processing;speech recognition;computer science;pattern recognition;probability;bigram;logistic regression;speech synthesis;statistics;language model	NLP	-20.33866973031977	-86.84827216313764	157882
390b32d88555eb15c37ffea29cee658637bab275	automatic music transcription: breaking the glass ceiling	m music;qa75 electronic computers computer science	Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to overcome the limited performance of transcription systems, algorithms have to be tailored to specific use-cases. Semiautomatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects.	algorithm;medical transcription;signal processing;transcription (software)	Emmanouil Benetos;Simon Dixon;Dimitrios Giannoulis;Holger Kirchhoff;Anssi Klapuri	2012			simulation;speech recognition;computer science;world wide web	NLP	-23.20343889501816	-85.73390271147977	158115
3b5ab7f94edb2dfc443dc74171544b48f302f16d	machine translation vs. common language: effects on idea exchange in cross-lingual groups	cross lingual communication;computer mediated communication;idea exchange;machine translation	Diversity among members of international teams can be a valuable source of novel ideas. However, to reap these benefits, groups need to overcome communication barriers that stem from differences in members' native languages. We compare two strategies for overcoming these barriers: the use of English as a common language, and the use of machine translation (MT) tools that allow each person to communicate in his or her own native language. Dyads consisting of one English-speaking American and one native Mandarin-speaking Chinese participant exchanged ideas to perform brainstorming tasks, either through English or using MT. We found that MT helped the non-native English speakers produce ideas but that both native and non-native English speakers viewed MT-mediated messages as less comprehensible than English messages. The findings suggest it can be effective to support cross-lingual communication with asymmetric design, using MT technology to help people produce messages in their native languages, while leaving incoming messages untranslated and leveraging people's second language proficiency for comprehension.	machine translation;super robot monkey team hyperforce go!	Hao-Chuan Wang;Susan R. Fussell;Dan Cosley	2013		10.1145/2441776.2441882	natural language processing;speech recognition;computer science;machine translation;communication;social psychology;world wide web;computer-mediated communication	HCI	-28.24841076272912	-86.55146223881049	158173
5759f98d0f39c52a13ec9b80f738529d6a9eb221	translating common english and chinese verb-noun pairs in technical documents with collocational and bilingual information	computational methods;information systems;conference paper;translation languages;linguistic information;machine translations;feature comparison;technical documents;bi lingual information	We studied a special case for the translation of English verbs in verb-object pairs. Researchers have studied the effects of the linguistic information about the verbs being translated, and many have reported how considering the objects of the verbs will facilitate the quality of translations. In this study, we took an extreme venue – assuming the availability of the Chinese translations of the English objects. We explored the issue with thousands of samples that we extracted from 2011 NTCIR PatentMT workshop. The results indicated that, when the English verbs and objects were known, the information about the object’s Chinese translation could still improve the quality of the verb’s translations but not quite significantly.	venue (sound system)	Yi-Hsuan Chuang;Chao-Lin Liu;Jing-Shin Chang	2011			natural language processing;speech recognition;computer science;linguistics	NLP	-24.670230471016037	-82.56929891612089	158235
ba90d86513c343a2ed31a0c71572aef4b59ec05c	recurrent hmms and cursive handwriting recognition graphs	graph theory;image recognition;handwriting recognition hidden markov models character recognition viterbi algorithm automata image recognition text analysis computational efficiency image segmentation probability;handwriting recognition;language models cursive script recognition hidden markov models;hidden markov model;character segmentation;hmm based word recognition system;cursive script recognition;image recognition graph theory handwriting recognition hidden markov models;cost accounting;hidden markov models;compact representation;viterbi algorithm;feature extraction;cursive handwriting recognition graph;present day;word recognition;computational cost;cities and towns;interpretation algorithm;interpretation algorithm recurrent hidden markov model cursive handwriting recognition graph computational cost hmm based word recognition system lexicon free recognition character segmentation;computational efficiency;recurrent hidden markov model;character recognition;language model;lexicon free recognition;language models	"""Standard cursive handwriting recognition is based on a language model, mostly a lexicon of possible word hypotheses or character n-grams. The result is a list of word alternatives ranked by confidence. Present-day applications use very large language models, leading to high computational costs and reduced accuracy. For a standard HMM-based word recognition system, a new recurrent HMM approach for very fast lexicon-free recognition will be presented. The evaluation of this model creates a """"recognition graph"""", a compact representation of result alternatives of lexicon-free recognition. This structure is formally identical to results of single character segmentation and recognition. Thus it can be directly evaluated by interpretation algorithms following this process, and can even be merged with these results. In addition, the recognition graph is a basis for further evaluation in terms of word recognition. It allows fast evaluation of word hypotheses, easy integration of various language models like n-grams, and the efficient extraction of lexicon-free n-best result alternatives."""	algorithm;computation;error-tolerant design;grams;handwriting recognition;hidden markov model;interpretation (logic);language model;lexicon;n-gram;optical character recognition;recurrent neural network;value (ethics)	Marc-Peter Schambach	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.217	natural language processing;speech recognition;feature extraction;word recognition;intelligent character recognition;viterbi algorithm;computer science;intelligent word recognition;machine learning;pattern recognition;hidden markov model;language model;cost accounting	Vision	-22.809347549893868	-80.44140559032746	158469
669c221d79eaeb2afc6f2b20bf6a12ee3ce1b75c	cellular diagnostic systems using hidden markov models	kullback leibler divergence;position location;hidden markov models;dropped call prediction;dissertation;1xev dv;coverage problem;cellular diagnostics		hidden markov model;markov chain	Maruf Mohammad	2006			speech recognition;computer science;machine learning;markov model;statistics;variable-order markov model	Robotics	-23.75237226967025	-93.20867405069868	158800
adae94cd3e7e5217da527c399a9a0059f7f3fd8f	design of 3-factor strict local language in ethological data mining of bengalese finch's and white-rumped munia's song		The aim of the paper is to extract and compare the behavioral units of Bengalese Finch and White-Rumped Munia's songs and represent them by a scanner which is a counter automaton of sub-regular language. Scanner allows us to recognize the bird songs at a fast rate. We have used ethological data mining and automata-based approach for finding the rules governing the structure of bird songs. We have demonstrated the difference in the results obtained for both the birds. The results indicate that it is possible to represent these songs by using 3-factor strictly local language.	counter automaton;data mining;finch;regular language	Pulkit Mathur;Vishwajeet Dagar;Ajay Kumar	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393131	counter automaton;data mining;white-rumped munia;local language;spectrogram;finch;data modeling;computer science;grammar	ML	-31.323528044161872	-86.84702786723021	159003
47e2324df4e51520e1f5d2b35a7c0a5c67d1699f	experiments on unsupervised statistical parametric speech synthesis	unsupervised learning;speech synthesis;unsupervised learning decision trees hidden markov models speech synthesis;speech accuracy hidden markov models decision trees speech synthesis degradation buildings;hidden markov models;decision trees unsupervised statistical parametric speech synthesis web based voicefonts linguistic properties acoustic properties speech corpus speech quality transcript accuracy training dataset phone accuracy mos quality voice degradation hmm based voices;web based voicefonts voice degradation unsupervised method hmm based speech synthesis automatic speech transcription;decision trees	In order to build web-based voicefonts, an unsupervised method is needed to automate the extraction of acoustic and linguistic properties of speech. This paper addresses the impact of automatic speech transcription on statistical parametric speech synthesis based on a single speaker's 100 hour speech corpus, focusing particularly on two factors of affecting speech quality: transcript accuracy and size of training dataset. Experimental results indicate that for an unsupervised method to achieve fair (MOS 3) voice quality, 1.5 hours of speech are necessary for phone accuracy over 80% and 3.5 hours necessary for phone accuracy down to 65%. Improvement in MOS quality turns out not to be significant when more than 4 hours of speech are used. The usage of automatic transcripts certainly leads to voice degradation. One of the mechanisms behind this is that transcript errors cause mismatches between speech segments and phone labels that significantly distort the structures of decision trees in resultant HMM-based voices.	acoustic cryptanalysis;decision tree;distortion;elegant degradation;hidden markov model;resultant;speech corpus;speech recognition;speech synthesis;transcription (software);unsupervised learning;web application	Jinfu Ni;Yoshinori Shiga;Hisashi Kawai;Hideki Kashioka	2012	2012 8th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2012.6423518	voice activity detection;natural language processing;unsupervised learning;speech recognition;speech corpus;computer science;machine learning;decision tree;pattern recognition;acoustic model;speech synthesis;hidden markov model;speech analytics	NLP	-19.85377206047988	-83.9553519637317	159131
dfc07fdcb46fd3aefde3c8749fa00a507a051e73	cross-linguistic cognitive modeling of verbal morphology acquisition	cognitive linguistics;cognitive modeling;language acquisition;inflectional morphology;english inflectional system;spanish inflectional system	How children acquire and process inflectional morphology is still an open question. Despite the fact that English past tense acquisition has been studied and modeled in depth, the current approaches do not account for many of the errors made by humans. Moreover, not much work has been done with highly inflected languages, like Spanish. However, the modeling of any linguistic phenomenon in different languages is very important in order to understand the general cognitive processes underlying each particular phenomenon. This paper presents an ACT-R dual-mechanism model that accomplishes the task of acquiring verbal morphology systems from one of the simplest systems (the English one) to one of the most complex systems (the Spanish one), by using a double analogy process of stem and suffix. The model proposed was able to match all types of errors that developing children make (from a sample of them), both in English and Spanish. The models for both languages used very similar parameters. The introduced approach not only shows how children could acquire a highly inflected morphology system in terms of dual-mechanism theories but, given its cross-linguistic character, also sheds light on the possible general processes involved in the acquisition and processing of inflectional morphology.	act-r;cognitive model;complex systems;galaxy morphological classification;mathematical morphology;theory	Jesus Oliva;Jose Ignacio Serrano;M. Dolores del Castillo;Ángel Iglesias	2017	Cognitive Computation	10.1007/s12559-017-9454-8	natural language processing;computer science;communication	NLP	-32.75139801441315	-80.57940956691301	159163
5448ad49f94ee91af35b8546401a6518d9cff4fd	assigning parts-of-speech to words from their orthography using a connectionist model	part of speech	The Orthographie surface structure of Swedish words has been us~d ~or predicting Pru:ts-of-speech information using a connecttomst approach. Th1s technique can be used to aid syntactic processing within a text-to-speech system. The error back-propagation technique has been used for the connectionis! learning. A corpus of the 10 000 most frequent Swedish words have been used for training and testing the system. The results indicate that around 80% of the words can be correctly classified by using the last part of each word. The system is compared to a rule based system that makes the same sort of predictions from word endings. Both systems give comparable results for the lexicon used. INTRODUCTION ~ifferent a_spects of the syntactic structure of speech have an unportant mfluence on the way it is produced. This is especially ~e of prosodi~ realizations, e.g., pause insertions and p~ase-fmal length~nmg: J1;. full sy;11tactic parsing normally rebes on an extenstve dtchonary w1th parts-of-speech information. In the tex~-tl:~-speech system developed at our department [1,2], th1s ts at present not feasible. However, comple~e parse~ ar~ not always required. Local clause and phra;;e mfonnatton 1s expected to contribute essentially to the qual1ty ?f t.he s~e~h output. An alternative approach is to use only a lun1ted dict10nary and to predict parts-of-speech from surf~ce .word structure. In the following we have used a connecttomst approach for making these predictions. This rnethod will also be compared to a rule based system that was designed for the same task.	backpropagation;connectionism;lexicon;parsing;rule-based system;software propagation;speech synthesis;speeded up robust features;text corpus	Kjell Elenius;Rolf Carlson	1989			orthography;part of speech;speech recognition;connectionism;natural language processing;trace (psycholinguistics);computer science;artificial intelligence	NLP	-26.174189470406716	-80.73784544116434	159263
52418e8b4af9ba59f682a10498e13b555ad9ef46	bilingual dictionary based sentence alignment for chinese english bitext	dynamic programming;lenguaje natural;alignement;programacion dinamica;bilingualism;langage naturel;dynamic program;chino;bilinguisme;dictionnaire;anglais;bilinguismo;word alignment;natural language;dictionaries;programmation dynamique;alineamiento;english;chinois;chinese;ingles;diccionario;natural language processing;alignment	"""Bitext is a rather """"hot"""" issue among current natural language processing and automatic sentence alignment is the first step towards bitext processing. Following the shift form previous knowledge-poor approach (pure length-based) to current some-what knowledge-rich approach, this paper suggests a bilingual dictionary based sentence alignment method for Chinese English bitext and realizes it through dynamic programming. Experiment on HIT bitext shows that this method has achieved an accuracy as high as 95%, and therefore is worthy of further exploring."""		Tiejun Zhao;Muyun Yang;Liping Qian;Gaolin Fang	2000		10.1007/3-540-40063-X_33	natural language processing;speech recognition;computer science;english;dynamic programming;linguistics;natural language;chinese	NLP	-23.694865266601735	-80.24507431055655	159362
2620ab41d9185fb413505f54b90c5984ea1d348c	automatic evaluation for a palpable measure of a speech translation system's capability.	automatic evaluation;speech translation	The main goal of this paper is to propose automatic schemes for the translation paired comparison method. This method was proposed to precisely evaluate a speech translation system's capability. Furthermore, the method gives an objective evaluation result, i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability. However, this method requires tremendous evaluation costs. Accordingly, automatization of this method is an important subject for study. In the proposed method, currently available automatic evaluation methods are applied to automate the translation paired comparison method. In the experiments, several automatic evaluation methods (BLEU, NIST, DPbased method) are applied. The experimental results of these automatic measures show a good correlation with evaluation results of the translation paired comparison method.	bleu;boo;experiment;machine translation;nist (metric);test set;usability	Keiji Yasuda;Fumiaki Sugaya;Toshiyuki Takezawa;Seiichi Yamamoto;Masuzo Yanagida	2003			natural language processing;speech recognition;computer science	NLP	-24.22211466880158	-82.8658237805026	159445
c9a8f8a90d04c75c088037e7ddc8d11f1eb00b46	model-free pomdp optimisation of tutoring systems with echo-state networks		Intelligent Tutoring Systems (ITSs) are now recognised as an interesting alternative for providing learning opportunities in various domains. The Reinforcement Learning (RL) approach has been shown reliable for finding efficient teaching strategies. However, similarly to other human-machine interaction systems such as spoken dialogue systems, ITSs suffer from a partial knowledge of the interlocutor’s intentions. In the dialogue case, engineering work can infer a precise state of the user by taking into account the uncertainty provided by the spoken understanding language module. A model-free approach based on RL and Echo State Newtorks (ESNs), which retrieves similar information, is proposed here for tutoring.	algorithm;dialog system;echo state network;experiment;human–computer interaction;language module;mathematical optimization;partially observable markov decision process;reinforcement learning	Lucie Daubigney;Matthieu Geist;Olivier Pietquin	2013			natural language processing;computer science;artificial intelligence;communication	AI	-27.460872848489362	-86.97447270048214	159527
560e31d17e93a099bd8e6a786dd734e2438441e5	text modification for bulgarian sign language users		The paper discusses the main issues regarding the reading skills and comprehension proficiency in written Bulgarian of people with communication difficulties, and deaf people, in particular. We consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modification of Bulgarian texts intended to facilitate comprehension by deaf people, to assist education, etc. In order to demonstrate the benefits of such a system and to evaluate its performance, we have carried out a study among a group of deaf people who use Bulgarian Sign Language (BulSL) as their primary language (primary BulSL users), which compares the comprehensibility of original texts and their modified versions. The results shows a considerable improvement in readability when using modified texts, but at the same time demonstrates that the level of comprehension is still low, and that a complex set of modifications will have to be implemented to attain satisfactory results.	list comprehension;rule-based system;vocabulary	Slavina Lozanova;Ivelina Stoyanova;Svetlozara Leseva;Svetla Koeva;Boian Savtchev	2013			natural language processing;computer science;linguistics;communication	HCI	-28.43014701290582	-83.75404858770547	159763
7ca3635e43d78ba5a9d02fced713fda493c8d1cd	on the exploitation of hidden markov models and linear dynamic models in a hybrid decoder architecture for continuous speech recognition	hidden markov model;dynamic model;continuous speech recognition	Linear dynamic models (LDMs) have been shown to be a viable alternative to hidden MARKOV models (HMMs) on smallvocabulary recognition tasks, such as phone classification. In this paper we investigate various statistical model combination approaches for a hybrid HMM-LDM recognizer, resulting in a phone classification performance that outperforms the best individual classifier. Further, we report on continuous speech recognition experiments on the AURORA4 corpus, where the model combination is carried out on wordgraph rescoring. While the hybrid system improves the HMM system in the case of monophone HMMs, the performance of the triphone HMM model could not be improved by monophone LDMs, asking for the need to introduce context-dependency also in the LDM model inventory.	experiment;finite-state machine;hidden markov model;hybrid system;markov chain;speech recognition;statistical model;triphone	Volker Leutnant;Reinhold Häb-Umbach	2010			maximum-entropy markov model;speech recognition;viterbi algorithm;computer science;machine learning;pattern recognition;markov model;hidden markov model;variable-order markov model	NLP	-19.214694855490148	-88.53288366627378	160127
09a7c4664cdcc5d7a3644b73711ba0a843377c5d	arabic handwritten words recognition based on a planar hidden markov model	hidden markov model	Off-line recognition of handwritten words is a difficult task due to the high variability and uncertainty of human writing. The majority of the recent systems are constrained by the size of the lexicon to deal with and the number of writers. In this paper, we propose an approach for multi-writers Arabic handwritten words recognition. The developed method uses multiple sources of information at the description and the classification levels. A hybrid planar Markovien modelling permitting to follow the horizontal and vertical variations of the writing has been adopted. This modelling is based on different levels of segmentation: horizontal, natural and vertical. The process of segmentation conducts to the decomposition of the writing in a limited set of elementary entities, with simplified morphologies specific to every horizontal band. The choice of different type of primitives is then imposed in order to assure an efficient description. Different architectures of modelling proved also to be indispensable. The classification is finally achieved using a Planar Hidden Markov Model.	delimiter;elementary particle;entity;hidden markov model;hough transform;lexicon;markov chain;spatial variability	Sameh Masmoudi Touj;Najoua Essoukri Ben Amara;Hamid Amiri	2005	Int. Arab J. Inf. Technol.		artificial intelligence;arabic;machine learning;hidden markov model;computer science;horizontal and vertical;intelligent word recognition;pattern recognition;planar;lexicon	AI	-27.328567213115914	-80.56955181591212	160177
9528fa09fbd918618dbd1bac72fe8c24f5574400	iris: a chat-oriented dialogue system based on the vector space model	system demonstration paper;example-based dialogue system;additional strategy;vector model space framework;system adaptation;vector space model framework;dialogue sample;informal response interactive system;chat capability;chat-oriented dialogue system	This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed.	dialog system;dialog tree;interactivity;user (computing);vocabulary	Rafael E. Banchs;Haizhou Li	2012			simulation;artificial intelligence	NLP	-27.43631647904276	-85.78256563961207	160571
798741709333d60974156c1d033cdc8805911fc7	dynamic bayesian networks for multi-band automatic speech recognition	traitement automatique de la parole;speech processing;time frequency;linguistique appliquee;methode;algorithme;modelisation;probabilistic model;algorithm;automatic speech recognition;continuous speech recognition;dynamic bayesian network;reconnaissance de la parole;acoustic phonetics;modele probabiliste;speech recognition;linguistique mathematique;computational linguistics;phonetique acoustique;chaine de markov;experimentation;linguistique informatique;frequence fondamentale;method;reseaux bayesiens;theorie de bayes;fundamental frequency;inference;mathematical linguistics;applied linguistics;algoritmo;bayesian networks	This paper presents a new approach to multi-band automatic speech recognition which has the advantage to overcome many limitations of classical muti-band systems. The principle of this new approach is to build a speech model in the time–frequency domain using the formalism of dynamic Bayesian networks. In contrast to classical multi-band modeling, this formalism leads to a probabilistic speech model which allows communications between the different sub-bands and, consequently, no recombination step is required in recognition. We develop efficient learning and decoding algorithms both for isolated and continuous speech recognition. We present illustrative experiments on isolated and connected digit recognition tasks. These experiments show that the this new approach is very promising in the field of noisy speech recognition. 2003 Elsevier Science Ltd. All rights reserved.	algorithm;background process;benchmark (computing);cross-correlation;dynamic bayesian network;ergodicity;experiment;preemption (computing);semantics (computer science);speech recognition;time–frequency representation;tree decomposition	Khalid Daoudi;Dominique Fohr;Christophe Antoine	2003	Computer Speech & Language	10.1016/S0885-2308(03)00011-1	natural language processing;speaker recognition;statistical model;method;speech recognition;time–frequency analysis;computer science;computational linguistics;applied linguistics;bayesian network;speech processing;linguistics;fundamental frequency;dynamic bayesian network	AI	-21.07860229323546	-92.51929264387877	160610
06641c887f2711ab07291dd50e491f091fc039f3	validation of spoken language resources: an overview of basic aspects	application development;spoken corpus;corpus oral;qualite;language resources spoken language resources;linguistique de corpus;linguistic resources;language resources;article letter to editor;it value;quality assessment;quality;ressources linguistiques;quality assessment validation evaluation;corpus linguistics;evaluation;computational linguistics;linguistique informatique;assessment	Spoken language resources (SLRs) are essential for both research and application development. In this article we clarify the concept of SLR validation. We define validation and how it differs from evaluation. Further, relevant principles of SLR validation are outlined. We argue that the best way to validate SLRs is to implement validation throughout SLR production and have it carried out by an external and experienced institute. We address which tasks should be carried out by the validation institute, and which not. Further, we list the basic issues that vali­ dation criteria for SLR should address. A standard validation protocol is shown, illustrating how validation can prove its value throughout the production phase in terms of pre-validation, full validation and pre-release validation.	cross-validation (statistics);data validation;digital single-lens reflex camera;scalable linear recording;verification and validation	Henk van den Heuvel;Dorota J. Iskra;Eric Sanders;Folkert de Vriend	2008	Language Resources and Evaluation	10.1007/s10579-007-9049-1	natural language processing;speech recognition;computer science;evaluation;computational linguistics;data validation;corpus linguistics;linguistics;rapid application development;educational assessment	NLP	-25.182789013123692	-83.41153407367915	161128
b38c119201288ce8ab6664321d27964807bab524	improving performance of transfer-driven machine translation with extra-linguistic informatioon from context, situation and environment		This paper describes an improvement in the performance of Transfer-Driven Machine Translation ( T D M T ) by the use of extralinguistic information. In evaluating what constitutes natural speech, particular attention was paid to word usage that depended on the extra-linguistic information from the context, situation, environment and so on. We discuss what types of extra-linguistic information a spoken-language translation system requires to create naturally communicative dialogs. We then propose a method of improving the precision of translation by utilizing this extralinguistic information. Preliminary experimentation showing performance improvements in T D M T makes us believe that the proposed scheme can improve the performance of dialog M T .	machine translation;natural language;dialog	Hideki Mima;Osamu Furuse;Hitoshi Iida	1997				NLP	-26.072881854643768	-86.47752519591684	161169
42d3464986ddd614ba4ae2732364055446b80ce6	development of a stochastic dialog manager driven by semantics		We present an approach for the development of a dialog manager based on stochastic models for the representation of the dialogue structure and strategy. This dialog manager processes semantic representations and, when it is integrated with our understanding and answer generation modules, it performs natural language dialogs. It has been applied to a Spanish dialogue system which answers telephone queries about train timetables.	dialog manager;dialog system;natural language;schedule;stochastic process	Francisco Torres;Emilio Sanchis Arnal;Encarna Segarra	2003			natural language processing;semantic interpretation;computer science;artificial intelligence;dialog system	AI	-27.300627237644548	-85.62387697217176	161661
84a9612672bdefe17f46364b8db2f5bd306d541f	using a natural-artificial hybrid language for database access	artificial language part;japanese-sml hybrid language processing;database access;natural language part;chosen language;free form;natural-artificial hybrid language;data model;kana japanese;compact database system;database system;natural language	In this paper we propose a naturalartificial hybrid language for database access. The global construction of a sentence in this language is highly schematic, but allows expressions in the chosen language such as Japanese or English. Its artificial language part, SML, is closely related to our newly introduced data model, called scaled lattice. Adopting Japanese as its natural language part, we implemented a Japanese-SML hybrid language processing system for our compact database system SCLAMS, whose database consists of scaled lattices. The main features of this implementation are (i) a small lexicon and limited grammar, and (2) an almost free form in writing Kana Japanese.	data model;database;lexicon;natural language;schematic	Teruaki Aizawa;Nobuko Hatada	1980			natural language processing;language identification;cache language model;first-generation programming language;data definition language;natural language programming;speech recognition;universal networking language;language primitive;data manipulation language;object language;specification language;data control language;data model;computer science;linguistics;low-level programming language;modeling language;natural language;programming language;database schema;high-level programming language;database design;context-sensitive language	NLP	-30.634133891570592	-80.60849855927702	161693
54507d3acf8290ed9b95a1b3c201b474a40c788a	a speech corpus for modeling language acquisition: caregiver	native speaker;modeling language;language acquisition	A multi-lingual speech corpus used for modeling language acquisition called CAREGIVER has been designed and recorded within the framework of the EU funded Acquisition of Communication and Recognition Skills (ACORNS) project. The paper describes the motivation behind the corpus and its design by relying on current knowledge regarding infant language acquisition. Instead of recording infants and children, the voices of their primary and secondary caregivers were captured in both infant-directed and adultdirected speech modes over four languages in a read speech manner. The challenges and methods applied to obtain similar prompts in terms of complexity and semantics across different languages, as well as the normalized recording procedures employed at different locations, is covered. The corpus contains nearly 66000 utterance based audio files spoken over a two-year period by 17 male and 17 female native speakers of Dutch, English, Finnish, and Swedish. An orthographical transcription is available for every utterance. Also, time-aligned word and phone annotations for many of the sub-corpora also exist. The CAREGIVER corpus will be published via ELRA.	complexity;medical transcription;modeling language;speech corpus;speech synthesis;text corpus;transcription (software)	Toomas Altosaar;Louis ten Bosch;Guillaume Aimetti;Christos Koniaris;Kris Demuynck;Henk van den Heuvel	2010				NLP	-22.82544699506906	-84.48330576014212	161725
d8dad7045e697f1a9dd31d55c01f0bdacf118b19	computing implicit entities and events with getaruns		In this paper we will focus on the notion of “implicit” or lexically unexpressed linguistic elements that are nonetheless necessary for a complete semantic interpretation of a text. We referred to “entities” and “events” because the recovery of the implicit material may affect all the modules of a system for semantic processing, from the grammatically guided components to the inferential and reasoning ones. Reference to the system GETARUNS offers one possible implementation of the algorithms and procedures needed to cope with the problem and allows to deal with all the spectrum of phenomena. The paper will address at first the following three types of “implicit” entities and events: the grammatical ones, as suggested by a linguistic theories like LFG or similar generative theories; the semantic ones suggested in the FrameNet project, i.e. CNI, DNI, INI; the pragmatic ones: here we will present a theory and an implementation for the recovery of implicit entities and events of (non-) standard implicatures. In particular we will show how the use of commonsense knowledge may fruitfully contribute in finding relevant implied meanings. Last Implicit Entity only touched on, though for lack of space, by the paper is the Subject of Point of View which is computed by Semantic Informational Structure and contributes the intended entity from whose point of view is expressed a given subjective statement.	accessibility;algorithm;anaphora (linguistics);commonsense knowledge (artificial intelligence);dialed number identification service;entity;experiment;framenet;inferential programming;lexical functional grammar;netware;open mind common sense;point of view (computer hardware company);semantic interpretation;semantic network;software repository;theory;wordnet	Rodolfo Delmonte	2009			generative grammar;natural language processing;semantic interpretation;semantic memory;commonsense knowledge;artificial intelligence;framenet;computer science	NLP	-33.30610493000637	-80.45785927321229	162166
9f32e7f655caf956952bcdb272efc289915be872	comparison between two models of language for the automatic phonetic labeling of an undocumented language of the south-asia: the case of mo piu		This paper aims at assessing the automatic labeling of an undocumented, unknown, unwritten and under-resourced language (Mo Piu) of the North Vietnam, by an expert phonetician. In the previous stage of the work, 7 sets of languages were chosen among Mandarin, Vietnamese, Khmer, English, French, to compete in order to select the best models of languages to be used for the phonetic labeling of Mo Piu isolated words. Two sets of languages (1° Mandarin + French, 2° Vietnamese + French) which got the best scores showed an additional distribution of their results. Our aim is now to study this distribution more precisely and more extensively, in order to statistically select the best models of languages and among them, the best sets of phonetic units which minimize the wrong phonetic automatic labeling.	select (sql);super robot monkey team hyperforce go!;undocumented feature	Geneviève Caelen-Haumont;Sam Sethserey	2012			linguistics	NLP	-21.311806426722992	-80.94778833324524	162188
7ff4963765db66d0adad18df23a5b0f289526a37	dinasti: dialogues with a negotiating appointment setting interface		This paper describes the DINASTI (DIalogues with a Negotiating Appointment SeTting Interface) corpus, which is composed of 1734 dialogues with the French spoken dialogue system NASTIA (Negotiating Appointment SeTting InterfAce). NASTIA is a reinforcement learning-based system. The DINASTI corpus was collected while the system was following a uniform policy. Each entry of the corpus is a system-user exchange annotated with 120 automatically computable features.The corpus contains a total of 21587 entries, with 385 testers. Each tester performed at most five scenario-based interactions with NASTIA. The dialogues last an average of 10.82 dialogue turns, with 4.45 reinforcement learning decisions. The testers filled an evaluation questionnaire after each dialogue. The questionnaire includes three questions to measure task completion. In addition, it comprises 7 Likert-scaled items evaluating several aspects of the interaction, a numerical overall evaluation on a scale of 1 to 10, and a free text entry. Answers to this questionnaire are provided with DINASTI. This corpus is meant for research on reinforcement learning modelling for dialogue management.	computable function;dialog system;interaction;numerical analysis;reinforcement learning;spoken dialog systems;text corpus	Layla El Asri;Romain Laroche;Olivier Pietquin	2014			human–computer interaction;multimedia	NLP	-27.389596202687116	-86.85891154561781	162335
fc980a431529a689e96c55418bf0a211785f2701	language exercise generation: emulating cambridge open cloze	преподавание английского языка;английский язык как иностранный;автоматическое создание упражнений;регулировка сложности упражнений;тесты open cloze;adjusting difficulty;изучение иностранных языков с помощью компьютера;fill in the blanks;english as a foreign language efl;english proficiency testing;автоматическая обработка естественного языка;natural language processing;computer assisted language learning call;автоматическая обработка текста	This manuscript presents an approach to the automatic generation of open cloze exercises based on arbitrary English text. The exercise format is similar to the open cloze test used in Cambridge English certificate exams FCE, CAE, CPE. The presented method also makes it possible to adjust the difficulty of the resulting exercises to better suit specific proficiency levels. Three experiments were conducted to evaluate the usefulness of the machine-generated exercises, compare them with authentic Cambridge English tests and study the difficulty-setting capabilities. The experiments showed that the generation method used was quite effective. With some customization, the method can be applied to generating similar exercises for other languages.	emulator	Alexey Malafeev	2014	IJCSSA	10.4018/IJCSSA.2014070102	natural language processing;speech recognition;computer science;programming language	NLP	-27.94652524524975	-82.90370294835279	162466
5b6ba6a7d517dfeb915378c4368347adcfdb1833	improving pronunciation by analogy for text-to-speech applications		This paper extends previous work on pronunciation by analogy (PbA) in several directions. PbA is a data-driven method for converting letters to sound, with potential application to next-generation text-to-speech systems. We experiment with a range of methods for matching letter patterns in input words to those in the system dictionary when building a pronunciation lattice. We give preliminary consideration to deriving lexical stress for input words. Common errors are analysed: these mostly involve vowel letters and phonemes. An output is not necessarily guaranteed in PbA { the so-called silence problem. We report on a simple but e ective strategy for silence avoidance. Finally, we introduce the idea of using di erent strategies in combination to improve performance.	dictionary;speech synthesis;words (unix)	Robert I. Damper;Yannick Marchand	1998			natural language processing;analogy;computer science;speech synthesis;pronunciation;artificial intelligence	NLP	-21.23769539867733	-83.50191944036654	163434
dfba0365a30713cb0537931bfa05772080d05466	hidden markov models for two-dimensional data		Hidden Markov models are well-known methods for image processing. They are used in many areas where 1D data are processed. In the case of 2D data, there appear some problems with application HMM. There are some solutions, but they convert input observation from 2D to 1D, or create parallel pseudo 2D HMM, which is set of 1D HMMs in fact. This paper describes authentic 2D HMM with two-dimensional input data, and its application for pattern recognition in image processing.	ergodicity;hidden markov model;image processing;markov chain;pattern recognition	Janusz Bobulski	2013		10.1007/978-3-319-00969-8_14	forward algorithm;markov chain;maximum-entropy markov model;markov kernel;variable-order bayesian network;partially observable markov decision process;markov property;viterbi algorithm;causal markov condition;hidden semi-markov model;markov blanket;markov renewal process;markov process;markov model;hidden markov model;variable-order markov model	ML	-20.630623800900732	-93.82596537577062	163691
345e06b69baad57eee637e4d0fac49c694956dcc	moby dick meets geocr: lexical considerations in wordrecognition	lexical content;word occurrence intersection;moby dick;paper technology;optical character recognition;linguistics optical character recognition;recognition speed;word recognition accuracy moby dick geocr high speed lexically driven ocr lexical content lexical structure lexical processing word recognition engine word recognition performance lexicon specificity recognition speed word occurrence intersection;lexicon specificity;geocr;lexical structure;word recognition accuracy;optical character recognition software;word recognition engine;engines;shape;lexical processing;dictionaries;milling machines;word recognition;word recognition performance;text recognition;frequency;shape optical character recognition software engines character recognition text recognition dictionaries paper technology milling machines laboratories frequency;character recognition;high speed;high speed lexically driven ocr;linguistics	The author has previously (Proc. Int. Conf. on Doc. Anal. and Recognition, Montreal, pp. 723-728, 1995) described a high-speed, lexically driven OCR called GEOCR (Good Enough Optical Character Recognition). This paper expands on that work by describing the effects of lexical content, structure and processing on the performance of GEOCR as a word recognition engine, describing the recognition of a particular text, Moby Dick. Word recognition performance is shown to be enhanced by the application of an appropriate lexicon. Recognition speed is essentially independent of the details of lexical content, provided that the intersection of the occurrences of words in the document and the lexicon is high. Word recognition accuracy is dependent on both the intersection and specificity of the lexicon.		A. Lawrence Spitz	1997		10.1109/ICDAR.1997.619845	natural language processing;speech recognition;word recognition;shape;computer science;intelligent word recognition;frequency;optical character recognition	NLP	-23.748644620279375	-86.16620099465376	163792
6375c11a1e1f8518c99e6210f50c02b17990ad93	ontomusic: from scores to expressive music performances	knowledge based system;music performance;ontologies;applications;knowledge based systems;knowledge base	The literal performance of the symbols contained in a traditional score is not enough to produce expressive music. Human interpreters use musical knowledge that is not explicitly represented in it. This paper presents a knowledge-based approach to introduce expressiveness to the performance of a score by calculating dynamics and tempo envelopes of the piece, combining implicit musical knowledge with the explicit one contained in the score.	literal (mathematical logic);performance	Pere Ferrera;Josep Puyol-Gruart	2005			natural language processing;speech recognition;computer science;knowledge-based systems;procedural knowledge;communication	AI	-31.414333697104567	-80.31628238121806	164135
49aadf64efe9e124492d882caa56d1412dfb046a	utterance-based selective training for the automatic creation of task-dependent acoustic models	modelizacion;metodo estadistico;tecnologia electronica telecomunicaciones;acoustic modeling;statistical method;sufficient statistics;selective training;modelisation;automatic recognition;reconocimiento voz;methode statistique;estimacion parametro;speech recognition;reconnaissance parole;parameter estimation;estimation parametre;tecnologias;grupo a;modeling;task dependency;reconocimiento automatico;reconnaissance automatique;development costs	To obtain a robust acoustic model for a certain speech recognition task, a large amount of speech data is necessary. However, the preparation of speech data including recording and transcription is very costly and time-consuming. Although there are attempts to build generic acoustic models which are portable among different applications, speech recognition performance is typically task-dependent. This paper introduces a method for automatically building task-dependent acoustic models based on selective training. Instead of setting up a new database, only a small amount of task-specific development data needs to be collected. Based on the likelihood of the target model parameters given this development data, utterances which are acoustically close to the development data are selected from existing speech data resources. Since there are too many possibilities for selecting a data subset from a larger database in general, a heuristic has to be employed. The proposed algorithm deletes single utterances temporarily or alternates between successive deletion and addition of multiple utterances. In order to make selective training computationally practical, model retraining and likelihood calculation need to be fast. It is shown, that the model likelihood can be calculated fast and easily based on sufficient statistics without the need for explicit reconstruction of model parameters. The algorithm is applied to obtain an infant- and elderly-dependent acoustic model with only very few development data available. There is an improvement in word accuracy of up to 9% in comparison to conventional EM training without selection. Furthermore, the approach was also better than MLLR and MAP adaptation with the development data.	acoustic cryptanalysis	Tobias Cincarek;Tomoki Toda;Hiroshi Saruwatari;Kiyohiro Shikano	2006	IEICE Transactions	10.1093/ietisy/e89-d.3.962	speech recognition;systems modeling;sufficient statistic;computer science;artificial intelligence;acoustic model;estimation theory;statistics	Vision	-20.190713848026984	-91.72003678845206	164419
4409c000a9a9ae3ecd6c0cad5100fe9c8f1abc9f	special issue on noisy text analytics	noisy text analytics;special issue	Noisy unstructured text data are ubiquitous in real-world communications. Text produced by processing signals intended for human interpretation, such as printed and handwritten documents, spontaneous speech, and cameracaptured scene images, are prime examples. Application of Automatic Speech Recognition (ASR) systems on telephonic conversations between call center agents and customers often see 30–40% word error rates. Optical character recognition (OCR) error rates for hardcopy documents can range widely from 2–3% for clean inputs to 50% or higher depending on the quality of the page image, the complexity of the layout, and aspects of the typography. Unconstrained handwriting recognition is still considered to be largely an open problem. Recognition errors are not the sole source of noise; natural language and its creative usage can cause problems for computational techniques. Electronic text taken directly from the Internet (emails, message boards, newsgroups, blogs, wikis, chat logs, and web pages), contact centers (customer complaints, emails, call transcriptions, message summaries), and mobile phones (text messages) is often very noisy and challenging to process. Spelling errors, abbreviations,	blog;email;handwriting recognition;image noise;mobile phone;natural language;noisy text analytics;optical character recognition;printing;software bug;speech recognition;spontaneous order;text corpus;text mining;web page;wiki	Daniel P. Lopresti;Shourya Roy;Klaus U. Schulz;L. Venkata Subramaniam	2011	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-011-0162-8	machine learning;optical character recognition;the internet;speech recognition;artificial intelligence;natural language;web page;noisy text analytics;handwriting recognition;transcription (linguistics);spelling;computer science	ML	-24.87684279029976	-82.17229638652853	164550
32bcc1ccba0765eca188cf2167a09694be90c9c2	topic and speaker identification via large vocabulary continuous speech recognition	novel approach;switchboard corpus;telephone conversation;speaker identification;theoretical framework;complementary problem;message identification system;large vocabulary continuous speech	"""In this paper we exhibit a novel approach to the problems of topic and speaker identification that makes use of a large vocabulary continuous speech recognizer. We present a theoretical framework which formulates the two tasks as complementary problems, and describe the symmetric way in which we have implemented their solution. Results of trials of the message identification systems using the Switchboard corpus of telephone conversations are reported. 1. I N T R O D U C T I O N The task of topic identification is to select from a set of possibilities the topic that is most likely to represent the subject matter covered by a sample of speech. Similarly, speaker identification requires selecting from a list of possibilities the speaker most likely to have produced the speech. In this paper, we present a novel approach to the problems of topic and speaker identification which uses a large vocabulary continuous speech recognizer as a preprocessor of the speech messages. The motivation for developing improved message identification systems derives in part from the increasing reliance on audio databases such as arise from voice mail, for example, and the consequent need to extract information from them. Technology that is capable of searching such a database of recorded speech and classifying material by subject matter or by speaker would have substantial value, much as text-based information retrieval technology has for textual corpora. Several approaches to the problems of topic and speaker identification have already appeared in the literature. For example, an approach to topic identification using wordspotting is described in [1] and approaches to the speaker identification problem are reported in [2] and [3]. Dragon Systems' approach to the message identification tasks depends crucially on the existence of a large vocabulary continuous speech recognition system. We view the tasks of topic and speaker identification as complementary problems: for topic identification, the speaker is irrelevant and only the subject matter is of interest; for speaker identification, the reverse is true. For efficiency of computation, in either case we first use a speakerindependent topic-independent recognizer to transcribe the speech messages. The resulting output is then scored using topic-sensitive or speaker-sensitive models. This approach to the problem of message identification is based on the belief that the contextual information used in a full-scale recognition is invaluable in extracting reliable data from difficult speech channels. For example, unlike standard approaches to topic identification through spotting a small collection of topic-specific words, the approach via continuous speech recognition should more reliably detect keywords because of the acoustic and language model context available to the recognizer. Moreover, with large vocabulary recognition, the list of keywords is no longer limited to a small set of highly topic-specific (but generally infrequent) words, and instead can grow to include much (or even all) of the recognition vocabulary. The use of contextual information makes the message systems sufficiently robust that they are able to operate even with vocabulary sizes and noise environments that would make speech recognition extremely difficult for other applications. To test our message identification systems, we have been using the """"Switchboard"""" corpus of recorded telephone messages [4] collected by Texas Instruments and now available through the Linguistic Data Consortium. This collection of roughly 2500 messages includes conversations involving several hundred speakers. People who volunteered to participate in this program were prompted with a subject to discuss (chosen from a set that they had previously specified as acceptable) and were expected to talk for at least five minutes. We report results of topic identification tests involving messages on ten different topics using four and a half minutes of speech and speaker identification tests involving 24 speakers with test intervals containing as little as 10 seconds of speech. In the next section, we describe the theoretical framework on which our message identification systems are based and discuss the dual nature of the two problems. We then describe how this theory is implemented in the current message processing systems. Preliminary tests of"""	acoustic cryptanalysis;computation;context-sensitive grammar;database;dragon naturallyspeaking;finite-state machine;full scale;information retrieval;language model;linguistic data consortium;preprocessor;regular expression;relevance;speaker recognition;speech recognition;subject matter expert turing test;telephone switchboard;text corpus;text-based (computing);vocabulary	Barbara Peskin;Larry Gillick;Yoshiko Ito;Stephen Lowe;Robert Roth;Francesco Scattone;James K. Baker;Janet M. Baker;John S. Bridle;Melvyn J. Hunt;Jeremy Orloff	1993			natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science	NLP	-22.141668368909315	-85.04093080320813	164968
04abf21803efe02f3d08a429cb71343a5b8e8fd3	computing relative redundancy to measure grammatical constraint in speech recognition tasks.	q measurement;uncertainty;particle measurements;efficient algorithm;vocabulary;natural languages;statistical properties;speech recognition entropy q measurement natural languages laboratories particle measurements vocabulary uncertainty markov processes;speech recognition;entropy;markov processes	In this paper we present new and computationally efficient algorithms for computing some statistical properties of finite languages. In particular, the relative redundancy which measures grammatical constraint, is computed for several speech recognition task languages which have appeared in the literature.		Man Mohan Sondhi;Stephen E. Levinson	1978		10.1109/ICASSP.1978.1170406	natural language processing;entropy;speech recognition;uncertainty;computer science;machine learning;pattern recognition;mathematics;markov process;natural language;statistics	NLP	-21.917043941663515	-91.81185002562685	165015
90469a7cf79de6d820a8f7a476bb838c86dc3095	investigating scalability in hierarchical language identification system		State-of-the-art language identification (LID) systems are not easily scalable to accommodate new languages. Specifically, as the number of target languages grows the error rate of these LID systems increases rapidly. This paper addresses such a challenge by adopting a hierarchical language identification (HLID) framework. We demonstrate the superior scalability of the HLID framework. In particular, HLID only requires the training of relevant nodes in a hierarchical structure instead of re-training the entire tree. Experiments conducted on a dataset that combined languages from the NIST LRE 2007, 2009, 2011 and 2015 databases show that as the number of target languages grows from 28 to 42, the performance of a single level (non-hierarchical) system deteriorates by around 11% while that of the hierarchical system only deteriorates by about 3.4% in terms of Cavg. Finally, experiments also suggest that SVM based systems are more scalable than GPLDA based systems.	database;experiment;language identification;scalability	Saad Irtza;Vidhyasaharan Sethu;Eliathamby Ambikairajah;Haizhou Li	2017			natural language processing;speech recognition;language identification;scalability;computer science;artificial intelligence	NLP	-22.066083389051236	-87.02517835332056	165037
3645c5bdccd1d1ec1ed6379dd63bc98a7946a616	anaphora and discourse structure	grammaire lexicale;adverbial construction;traitement automatique des langues naturelles;computacion informatica;construction adverbiale;filologias;grupo de excelencia;semantic interpretation;circonstancielle;discourse structure;anaphora;linguistica;grammaire formelle;interpretation semantique;compositionnalite;ciencias basicas y experimentales;compositionality;semantique logique;syntactic relation;formal grammar;linguistique mathematique;adverbial clause;lexical grammar;logical semantics;relation syntaxique;anaphore;grupo a;structure discursive;natural language processing;inference;mathematical linguistics	We argue in this article that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure. This allows a simpler discourse structure to provide scaffolding for compositional semantics and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure. We conclude by sketching out a lexicalized grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution, and inference.	anaphora (linguistics);categorial grammar;discourse relation;logical connective	Bonnie L. Webber;Matthew Stone;Aravind K. Joshi;Alistair Knott	2003	Computational Linguistics	10.1162/089120103322753347	natural language processing;semantic interpretation;computer science;linguistics;formal grammar;principle of compositionality;lexical grammar	NLP	-33.013154218712636	-81.34679157671151	165283
fd91959566b2a91d387305dd396db33cf0ff4e7e	using an ordinal ranking rule to find the top-performing gaussian mixture models for language recognition	reduced model testing time top performing gaussian mixture model language recognition worst performing model best performing model calculated dispersion measures multiple dispersion measurement ordinal ranking rule;gaussian processes;rank aggregation gaussian mixture models language recognition ordinal ranking rules;speech recognition gaussian processes;weight measurement indexes;speech recognition	In previous work [1], we developed a method for finding the top-performing Gaussian mixture models for the language recognition. This method orders the models from best-performing to worst-performing using calculated dispersion measures. Multiple dispersion measurements are used to produce multiple rankings of the models, which are combined to produce a ranking from which the top-performing models can be extracted. This method has reduced model testing time, since researchers can determine the top-performing models without evaluating the entire population of models. In this paper we demonstrate the ability of our ranking rule to find the top-performing models for different data sets and performance measures. The performance of our ranking rule is also compared to existing ordinal ranking rules: Kohler [2], Arrow & Raynaud [2], Borda [3], and Copeland [3].	enhanced entity–relationship model;language identification;mixture model;ordinal data	DeAnna Bailey;M. A. Kohler;Arlene A. Cole-Rhodes	2013	2013 47th Annual Conference on Information Sciences and Systems (CISS)	10.1109/CISS.2013.6552299	computer science;machine learning;pattern recognition;gaussian process;mathematics;ordinal data;ranking svm;statistics	ML	-22.238532614840906	-89.54868030281689	165560
47daf55b2bdabf6b6e63c54d3cee60ad24dfa85e	dialogueview - an annotation tool for dialogue		This paper describes DialogueView, a tool for annotating dialogues with utterance boundaries, speech repairs, speech act tags, and discourse segments. The tool provides several views of the data, including a word view that is timealigned with the audio signal, and an utterance view that shows the dialogue as if it were a script for a play. The utterance view abstracts away from lower level details that are coded in the word view. This allows the annotator to have a simpler view of the dialogue when coding speech act tags and discourse structure, but still have access to the details when needed.	higher-order function;level structure;principle of abstraction	Peter A. Heeman;Fan Yang;Susan E. Strayer	2002			natural language processing;speech recognition;computer science;communication	NLP	-28.601780490211965	-80.26479372690686	165624
57fb3cde4231d4ed58ef71b06cd3a2c320bd20fe	scope disambiguation as a tagging task		In this paper we present a pragmatic account of scope alternation involving universal quantifiers in a lexicalist framework based on CCG and DRT. This account can derive the desired reading for 96% of all cases of scope interaction involving universal quantification mediated by prepositions in a real corpus. We show how this account allows for recasting scope resolution as a simple token classification task, providing a simpler handle for statistical approaches to scope resolution than previous accounts.	combinatory categorial grammar;scope resolution operator;universal quantification;word-sense disambiguation	Kilian Evang;Johan Bos	2013			universal quantification;machine learning;alternation (linguistics);computer science;artificial intelligence	NLP	-33.54578386742266	-81.47899095620656	165673
2c63b5a592d28be2220d8b62f395c6c5ab1907c8	collecting an american sign language corpus through the participation of native signers	fuller social inclusion;motion-capture studio configuration;native signer;understandable asl animation;deaf people;asl motion-capture corpus;non-scripted asl passage;asl linguistics;deaf participant;native asl signer;american sign language corpus;linguistic annotation	Animations of American Sign Language (ASL) can make more information, websites, and services accessible for the significant number of deaf people in the United States with lower levels of written language literacy – ultimately leading to fuller social inclusion for these users. We are collecting and analyzing an ASL motion-capture corpus of multi-sentential discourse to seek computational models of various aspects of ASL linguistics to enable us to produce more accurate and understandable ASL animations. In this paper, we will describe our motion-capture studio configuration, our data collection procedure, and the linguistic annotations being added by our research team of native ASL signers. This paper will identify the most effective prompts we have developed for collecting non-scripted ASL passages in which signers use particular linguistic constructions that we wish to study. This paper also describes the educational outreach and social inclusion aspects of our project – the participation of many deaf participants, researchers, and students.	computational model;construction grammar;motion capture	Pengfei Lu;Matt Huenerfauth	2011		10.1007/978-3-642-21657-2_9	natural language processing;computer science;linguistics;communication	NLP	-28.12111702890901	-83.84084487417616	166207
3949ef493a46b282b436c6710ecdf4b078c8e249	towards the question: why has speaking rate such an impact on speech recognition performance?	speech recognition	It has repeatedly been shown, mostly in terms of WER, that the rate of speech significantly affects speech recognition accuracy. However, the question how is not yet satisfactorily answered. In this paper we scrutinized in which way already modeling accuracy is influenced by the rate of speech. We observed the existence of a rather direct (negative) correlation between the local speech rate (LSR) and the local average HMM score (LAS). This correlation can already be found for utterances in the training database, i.e. utterances that actually were used for the parameter estimation of the acoustic phonetic models. By introducing confidence measures based on likelihood distance we verified that statistical modeling with respect to speech rate seems most accurate in slow speech segments and deteriorates already at average speaking rates. We further found that the correlation is little, yet observable, for the static features and increases with the frame range of delta(delta) features reaching up to . The correlation persists regardless of simple monophone models or context dependent triphones. The LSR-LAS dependency can be used to predict LSR on independent test data directly from the acoustic HMM scores. In addition, LAS can be used as an indicator to assess the performance gain of rate dependent HMM models, which seems small (for fast speech) in comparison to the overall score degradation.	acoustic cryptanalysis;elegant degradation;estimation theory;hidden markov model;observable;speech recognition;statistical model;test data;triphone;word error rate	Robert Faltlhauser;Günther Ruske;Matthias Thomae	2002			speaker recognition;speech recognition;computer science	NLP	-20.07907712455529	-88.61812584026661	166858
38331fd9165f1f8b3dbf1b7633f1d23b6a5879e9	evaluation of voice stress analysis technology	air force research laboratory;application software;speech processing;speech analysis;pharmaceutical technology;national institute of justice;stress analysis;law enforcement;stress measurement;speech recognition;speech analysis law enforcement speech recognition stress measurement laboratories military computing pharmaceutical technology speech processing application software costs;military computing	The Air Force Research Laboratory (AFRL) has been tasked by the National Institute of Justice to investigate voice stress analysis (VSA) technology and evaluate its effectiveness for both military and law enforcement applications. This technology has been marketed as commercially available in computer-based form, and marketed as being capable of measuring stress and in some systems deception. This technology is reported as being easier to use, less invasive, and less constrained in their operation than standard polygraph technology. This study has found that VSA technology can identify stress better than chance with performance approaching that of current polygraph systems. However, it is not a technology that is mature enough to be used in a court of law. We also found that experience and training improves the accuracy over less trained individuals. Lastly, we explored how this technology may become an effective interrogation tool, when combined with polygraph technology.	chart;experience;ground truth;sensor;stress–strain analysis;voice stress analysis;waveform	Clifford S. Hopkins;Roy J. Ratley;Daniel S. Benincasa;John J. Grieco	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.254	application software;speech recognition;computer science;artificial intelligence;software engineering;speech processing;stress–strain analysis;management;law;world wide web;computer security	HCI	-26.634761465478373	-89.90450597988547	166873
1717a3adf399bb70ef2916a5b163caf8e8b9882b	reconnaissance d'écriture manuscrite par des techniques markoviennes : une approche bidimensionnelle et générique		We present an approach of the problem of handwriting recognition using hidden Markov random fields and based on a trully bidimensional analysis of the handwriting. The main innovative aspect of this approah is the combination of a windowed analysis of the image, a Markovian modelisation and an implementation of the 2D dynamic programming algorithm that achieves a fast and optimal decoding of Markov fields. Another feature of this study is the development methodology that focuses on a systematic evaluation of the algorithms and parameters. These algorithms are partly taken from techniques of the domain of speech processing and are very generic. This approach is validated on two different applications corresponding to standard public databases. The application of this generic algorithm to a handwritten digits recognition task achieved results similar to the ones of state-of-the-art methods. The application to a handwritten words recognition task showed that this approach can be extended to more complex tasks in a natural way. This work showed that the proposed approach is valid and appears as a candidate standard method for solving various tasks in computer vision. It paves the way for further developments in handwriting recognition and other important enhancements are expected with the use of other principles commonly used in speech and language processing problems. Other tasks such as image segmentation could also benefit from the robustness and the learning ability of our approach.	algorithm;computer vision;database;dynamic programming;generic programming;handwriting recognition;image segmentation;markov chain;markov random field;speech processing;triple des;window function	Sylvain Chevalier	2004			performance art;philosophy	Vision	-21.557580645270804	-92.88849919298569	167016
e752fdd828f80dd3e3daddb81adf21da81ed33c3	towards automatic learning in lvcsr: rapid development of a persian broadcast transcription system.	indexing terms;speech recognition	We present a new method for automatic learning and refining of pronunciations for large vocabulary continuous speech recognition which starts from a small amount of transcribed data and uses automatic transcription techniques for additional untranscribed speech data. The recognition performance of speech recognition systems usually depends on the available amount and quality of the transcribed training data. The creation of such data is a costly and tedious process and the approach presented here allows training with small amounts of annotated data. The model parameters of a statistical joint-multigram grapheme-to-phoneme converter are iteratively estimated using small amounts of manual and relatively larger amounts of automatic transcriptions and thus the system improves itself in an unsupervised manner. Using the new approach, we create a Persian broadcast transcription system from less than five hours of transcribed speech and 52 hours of untranscribed audio data.	broadcast domain;machine learning;speech analytics;speech recognition;transcription (software);unsupervised learning;vocabulary	Christian Gollan;Hermann Ney	2008			natural language processing;speech recognition;index term;computer science;pattern recognition	NLP	-20.669880653300837	-85.73663390394711	167329
787b37843e0629f936a4b9fe7803321ad12fbacc	a corpus and phonetic dictionary for tunisian arabic speech recognition		In this paper we describe an effort to create a corpus and phonetic dictionary for Tunisian Arabic Automatic Speech Recognition (ASR). The corpus, named TARIC (Tunisian Arabic Railway Interaction Corpus) has a collection of audio recordings and transcriptions from dialogues in the Tunisian Railway Transport Network. The phonetic (or pronunciation) dictionary is an important ASR component that serves as an intermediary between acoustic models and language models in ASR systems. The method proposed in this paper, to automatically generate a phonetic dictionary, is rule based. For that reason, we define a set of pronunciation rules and a lexicon of exceptions. To determine the performance of our phonetic rules, we chose to evaluate our pronunciation dictionary on two types of corpora. The word error rate of word grapheme-to-phoneme mapping is around 9%.	acoustic cryptanalysis;acoustic model;dictionary;language model;lexicon;speech corpus;speech recognition;text corpus;word error rate	Abir Masmoudi;Mariem Ellouze;Yannick Estève;Lamia Hadrich Belguith;Nizar Habash	2014			natural language processing;speech recognition;speech corpus;corpus linguistics;linguistics	NLP	-22.30184815252227	-83.78017361845282	167334
eef707913feb9c5a323e7e68310c80b1068a517f	autoling: an automated linguistic fieldworker	linguistic fieldworker;query language;major analytic component;live informant;linguistically unsophisticated informant;extended algol;automated linguistic fieldworker;morphological analysis;bi-lingual transformation;context sensitive phrase structure;machine translation program;machine translation	A system intended to act as a linguistic fieldworker via teletype 'interaction with a linguistically unsophisticated informant has been designed and ls being progra,~med in extende~GOL for the Burroughs5500 and8500 computers. The system consists of the three major analytic components ; a program for performing morphological analyses and deriving a segmentation algorithm for sentences in any language ; a syntactic learning program that formulates context free and context sensitive phrase structure rules (monolingual learning component to be added later) ; and a machine translation program that learns to translate in both directions between the query language (English) and the language of the llve informant via bi-lingual transformations. The informant may be viewed as a fourth component, and is assumed to be able to read and write English in standard graphemics, and to be able to read and writeh~on-English language in a phonemic notation.	algorithm;computer;machine translation;phrase structure rules;query language	Sheldon Klein	1967			natural language processing;speech recognition;computer science;linguistics;machine translation;programming language;query language	NLP	-29.945877422151444	-81.81867461737	167341
1624b5b56afa9f90409253a054e18d8a04f1c3e4	assessing the retrieval effectiveness of a speech : retrieval system by simulating recognition errors	information retrieval;speech retrieval;indexation;speech recognition;content based retrieval;test collection	We show how the recognition performance of a speech recognition component in a speech retrieval system affects the retrieval effectiveness. A speech retrieval system facilitates content-based retrieval of speech documents, i.e. audio recordings containing spoken text. The speech retrieval process receives queries from users and for every query it ranks the speech documents in decreasing order of their probabilities that they are relevant to the query. The speech recognition component is an impor tant part of a speech retrieval system, since it detects the occurrences of indexing features in the documents. Because the recognition of indexing features in continuous speech is error prone, the question arises how much an error prone recognition of indexing features affects the retrieval effectiveness. As an answer to this question and main contribution of this paper we simulated the recognition of indexing features in speech documents on s tandard information retrieval test collections and show the resulting retrieval accuracies. 1. I n t r o d u c t i o n We show how the recognition performance of a speech recognition component in a speech retrieval system affects the retrieval effectiveness. A speech retrieval system facilitates content-based retrieval of speech documents, i.e. audio recordings containing spoken text [5]. The speech retrieval process receives queries from users and for every query it ranks the speech documents in decreasing order of their probabilities that they are relevant to the query. These probabilities are derived from the occurrences of indexing features tha t were identified in the speech documents by a speech recognition component [4]. Because the recognition of indexing features in continuous speech is error prone, the question arises how much an error prone recognition of indexing features affects the retrieval effectiveness. The indexing features used in our speech retrieval system are phonetically motivated subword units having an intermediate specificity. The general pat tern of an indexing feature is a maximum sequence of consonants enclosed by two maximum sequences of vowels at both ends. We call these indexing features VCV-features where C stands for the maximum sequence of consonants and V stands for the maximum sequence of vowels. As an example, the word INTERNATIONAL contains the VCV-features INTE, ERNA, ATIO, and IONA. The indexing vocabulary is defined to be the set of those VCV-featutes ~,i whose inverse document frequencies idf(~oi) are between a lower bound i d f m i , and an upper bound idf , no , such that the indexing features are neither 370 very specific nor very broad. The lower bound guarantees the suitabili ty for indexing and the upper bound guarantees the trainability, i.e. there are enough examples to train the 8MMs. Experiments on s tandard information retrieval test collections showed that when using an appropriate subset of only 1000 VCV-features we can achieve a retrieval effectiveness that is comparable to s tandard weighted retrieval which is based on a much larger indexing vocabulary [4]. In addition to the VCV-features, we have also studied indexing vocabularies that have been extended by CVand VC-features at the word boundaries The recognition of speech documents is carried out with standard speech recognition technology, i.e. a wordspotter [6], [11], [14] locates the occurrences of indexing features in documents. For each document in the collection we create a description vector based on the number of occurrences of each feature and use a conventional retrieval function [12] to est imate the similarity between a document and a query description. Our indexing features consisting of VCV-features can be identified in both text and speech documents. As a consequence, the document collection may contain a mixture of text and speech documents. Furthermore, the query may also be entered as either text or speech. The controlled indexing vocabulary consisting of selected VCV-features has the advantage that the document description can be computed before the query evaluation. In particular, an access structure (e.g. an inverted file) can be constructed to allow fast query evaluation. Another impor tant advantage of our indexing vocabulary for both text and speech retrieval is that speech retrieval can simulated by using text collections as described in the subsequent sections. Information Retrieval on audio documents has been investigated very little. A wordspott ing system for voice indexing was developed by Wilcox and Bush [14] and an information retrieval system that classifies speech messages was presented by Rose, Chang, and Lippmann [10]. Recently a project for video mail retrieval using voice was proposed by Olivetti research Limited, Cambridge University Engineering Department, and Cambridge University Computer Laboratory [7]. The effects of recognition errors on on the retrieval effectiveness has been studied in the context of OCR based Information Retrieval [1]. These results are not directly comparable because speech retrieval performance may be considerably affected by false alarms in contrast to OCR-based retrieval where false alarms can be ignored because the occur infrequently. MEDLARS: average precision of the reference method: 0.534 (100%) ,.dr, f a 0 90% 0.539 (101%) 80% 0.530 (99%) 70% 0.487 (91%) 60% 0.481 (90%) 50% 0.431 (81%) 40% 0.421 (79%) 10 0.464 (87%) 0.444 (83%) 0.406 (76%) 0.400 (75%)	access structure;archive;cognitive dimensions of notations;document;experiment;information retrieval;inverted index;medline;optical character recognition;part-of-speech tagging;sensitivity and specificity;speech recognition;substring;video email;vocabulary	Peter Schäuble;Ulrike Glavitsch	1994			natural language processing;visual word;speech recognition;computer science;information retrieval	Web+IR	-22.290489162241663	-83.47968706616662	168316
488233501fde6e0ab4438fef62be80b5cfc4f30c	modular logic grammars	shift operator;noun phrase;semantic interpretation;logical form	This report describes a logic grammar formalism, Modular Logic Grammars, exhibiting a high degree of modularity between syntax and semantics. There is a syntax rule compiler (compiling into Prolog) which takes care of the building of analysis structures and the interface to a clearly separated semantic interpretation component dealing with scoping and the construction of logical forms. The whole system can work in either a one-pass mode or a two-pass mode. [n the one-pass mode, logical forms are built directly during parsing through interleaved calls to semantics, added automatically by the rule compiler. [n the two-pass mode, syntactic analysis trees are built automatically in the first pass, and then given to the (one-pass) semantic component. The grammar formalism includes two devices which cause the automatically built syntactic structures to differ from derivation trees in two ways: [I) There is a shift operator, for dealing with left-embedding constructions such as English possessive noun phrases while using rightrezursive rules (which are appropriate for Prolog parsing). (2) There is a distinction in the syntactic formalism between strong non-terminals and weak non-terminals, which is important for distinguishing major levels of grammar.	care-of address;compiler;formal grammar;logic programming;parsing;prolog;scope (computer science);semantic interpretation;semantics (computer science)	Michael C. McCord	1985		10.3115/981210.981223	natural language processing;semantic interpretation;noun phrase;logical form;computer science;s-attributed grammar;linguistics;shift operator;programming language	NLP	-30.767488172491714	-82.0547827212086	168368
4c634ed4042dbbad6fee8c616cfdf0aa3eccfc45	issues in automatic transcription of historical audio data.		This work deals with some interesting issues that arose when the ITC-irst broadcast news transcription system was applied to transcribe the audio track of historical documentary films. Due to an evident acoustic and linguistic mismatch between the broadcast news and the new application domain, the initial word error rate was of 46.4%. By exploiting a limited amount of manually annotated training data, adaptation of all components of the transcription system was performed, namely the audio partitioner, the acoustic model, and the language model. This permitted to achieve a word error rate of 30%, which makes automatic transcription of documentary films effective for information retrieval applications.	acoustic cryptanalysis;acoustic model;application domain;information retrieval;language model;transcription (software);word error rate	Fabio Brugnara;Mauro Cettolo;Marcello Federico;Diego Giuliani	2002			speech recognition;data mining;database	NLP	-21.63908360282773	-84.1529490546396	168448
4433a3adc6dd2fc5108bfeddf6e361313d51efa8	scaling smoothed language models	bayes rule;acoustic modeling;system performance;continuous speech recognition;probability distribution;experimental evaluation;language model	In Continuous Speech Recognition (CSR) systems a Language Model (LM) is required to represent the syntactic constraints of the language. Then a smoothing technique needs to be applied to avoid null LM probabilities. Each smoothing technique leads to a different LM probability distribution. Test set perplexity is usually used to evaluate smoothing techniques but the relationship with acoustic models is not taken into account. In fact, it is well-known that to obtain optimum CSR performances a scaling exponential parameter must be applied over LMs in the Bayes’ rule. This scaling factor implies a new redistribution of smoothed LM probabilities. The shape of the final probability distribution is due to both the smoothing technique used when designing the language model and the scaling factor required to get the optimum system performance when integrating the LM into the CSR system. The main object of this work is to study the relationship between the two factors, which result in dependent effects. Experimental evaluation is carried out over two Spanish speech application tasks. Classical smoothing techniques representing very different degrees of smoothing are compared. A new proposal, Delimited discounting, is also considered. The results of the experiments showed a strong dependence between the amount of smoothing given by the smoothing technique and the way that the LM probabilities need to be scaled to get the best system performance, which is perplexity independent in many cases. This relationship is not independent of the task and available training data.	acoustic cryptanalysis;delimited continuation;experiment;image scaling;language model;n-gram;performance;perplexity;smoothing;speech recognition;test set;time complexity	Amparo Varona;M. Inés Torres	2005	I. J. Speech Technology	10.1007/s10772-006-9047-5	probability distribution;additive smoothing;speech recognition;computer science;machine learning;bayes' theorem;statistics;smoothing;language model	NLP	-21.999397168798943	-89.22468078003585	168484
1abca09e7f3019a86f76ef2ec798ce7ef3b64d71	multilingual acoustic models for speech recognition in low-resource devices	multilingual speech recognition;acoustic devices;spoken language identification;gas insulated transmission lines;application software;language independent recognizer;pervasive computing;real time;language independent recognizer multilingual speech recognition common phone alphabet spoken language identification;acoustic modeling;speech processing;natural languages;automatic speech recognition;multilingual acoustic models;access to information;speech resource devices;speech recognition linguistics speech processing;mobile handsets;language identification;speech recognition;ubiquitous computing;winches;acoustic devices speech recognition natural languages automatic speech recognition gas insulated transmission lines winches ubiquitous computing europe application software mobile handsets;europe;language independent system;common phone alphabet;language independent system multilingual acoustic models speech resource devices ubiquitous computing pervasive computing multilingual speech recognition;ubiquitous computing environment;linguistics	Multilingual access to information and services is a key requirement in any pervasive or ubiquitous computing environment. In this paper we review the design of a common alphabet for up to fifteen languages and describe its application to multilingual speech recognition in low-resource devices in real-time. We give an overview of the special requirements for acoustic modeling in such environments and present initial results of a technique that aims on a more efficient discrimination between languages in training while keeping low memory footprint. We also report the usefulness of a multilingual recognizer as a language-independent system to bootstrap a new language.	acoustic cryptanalysis;acoustic model;finite-state machine;freedom of information laws by country;language-independent specification;memory footprint;pervasive informatics;real-time clock;requirement;speech recognition;ubiquitous computing	Enrique Gil Garcia;Erhan Mengusoglu;Eric Janke	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367236	natural language processing;language identification;application software;speech recognition;computer science;winch;natural language;ubiquitous computing	Robotics	-22.94362892711746	-86.63395110394782	168588
2576199e52b6c1c5fd4c3605093be1dc778beb0a	classification and postprocessing of documents using an error-correcting parser	error correcting parsing algorithm;recognition accuracy;error correction bank data processing cheque processing document image processing image classification grammars;image classification;optical character recognition software costs character recognition automata error correction;bank data processing;execution speed;cheque processing;optical character recognition software;automata;grammars;postprocessing task;error correction;automatic check processing;document image processing;synthetic data;document classification;synthetic data document classification error correcting parsing algorithm postprocessing task automatic check processing recognition accuracy execution speed;character recognition	In this paper an error-correcting parsing algorithm and its application to a postprocessing task in the context of automatic check processing is described. The proposed method has shown very good results in terms of recognition accuracy and execution speed on both real and synthetic data.	algorithm;categorization;error detection and correction;parsing;synthetic data	Horst Bunke;R. Liviero	1995		10.1109/ICDAR.1995.598981	natural language processing;contextual image classification;error detection and correction;speech recognition;computer science;pattern recognition;automaton;statistics;synthetic data	NLP	-22.91943873377227	-80.43842944905644	168742
65f4e8a490bccceb6991891d48601427371c9235	the waxholm application database	log files;computer and information science;spoken dialogue system;data och informationsvetenskap	This paper describes an application database collected in Wizard-of-Oz experiments in a spoken dialogue system, WAXHOLM. The system provides information on boat traffic in the Stockholm archipelago. The database consists of utterance-length speech files, their corresponding transcriptions, and log files of the dialogue sessions. In addition to the spontaneous dialogue speech, the material also comprise recordings of phonetically balanced reference sentences uttered by all 66 subjects. In the paper the recording procedure is described as well as some characteristics of the speech data and the dialogue.	data logger;dialog system;experiment;spoken dialog systems;spontaneous order	J. Bertenstam;Mats Blomberg;Rolf Carlson;Kjell Elenius;Björn Granström;Joakim Gustafson;Sheri Hunnicutt;J. Hogberg;Roger Lindell;Lennart Neovius;Lennart Nord;Antonio de Serpa-Leitao;N. Strom	1995			natural language processing;speech recognition;computer science;data mining	NLP	-24.758957295123334	-84.39430254603255	168869
b67d9227e63e4a6c5e5166df8eb92cabe4968f17	automatic generation of visual scenarios for spoken corpora acquisition		The paper describes a system, in JAVA, for written and visual scenario generation to collect speech corpora in the framework of a Tourism Information System. Methods and experimental results are also presented for evaluating the degree of understanding of the proposed scenarios. The corpus generated from visual scenarios appears to be much richer than the one generated from textual descriptions.	information system;java;text corpus	Demetrio Aiello;Cristina Delogu;Renato De Mori;Andrea Di Carlo;Marina Nisi;Silvia Tummeacciu	1998			speech recognition;computer science	NLP	-29.462276887361458	-80.75589367760026	169006
1452ce2d68af95c860aad220bc093671d07d597d	phonetic alignment: speech synthesis-based vs. viterbi-based	speech synthesis;hidden markov model;large speech corpora;speech segmentation;hidden markov models;speech recognition;continuous density hidden markov model;hybrid hmm ann systems;artificial neural network	In this paper we compare two different methods for automatically phonetically labeling a continuous speech data-base, as usually required for designing a speech recognition or speech synthesis system. The first method is based on temporal alignment of speech on a synthetic speech pattern; the second method uses either a continuous density hidden Markov models (HMM) or a hybrid HMM/ANN (artificial neural network) system in forced alignment mode. Both systems have been evaluated on read utterances not part of the training set of the HMM systems, and compared to manual segmentation. This study outlines the advantages and drawbacks of both methods. The speech synthetic system has the great advantage that no training stage (hence no large labeled database) is needed, while HMM Systems easily handle multiple phonetic transcriptions (phonetic lattice). We deduce a method for the automatic creation of large phonetically labeled speech databases, based on using the synthetic speech segmentation tool to bootstrap the training process of either a HMM or a hybrid HMM/ANN system. The importance of such segmentation tools is a key point for the development of improved multilingual speech synthesis and recognition systems.	speech synthesis	Fabrice Malfrère;Olivier Deroo;Thierry Dutoit;Christophe Ris	2003	Speech Communication	10.1016/S0167-6393(02)00131-0	natural language processing;speech recognition;computer science;pattern recognition;speech segmentation;hidden markov model	NLP	-19.856472711422416	-85.95312155075221	169023
091ac31f3c1ffc19e0a562ebf8cda309c8e1395d	the human as a constrained optinal editor	constrained optimization;availability;information retrieval;text processing;programming profession predictive models productivity feedback text processing human factors man machine systems systems engineering and theory availability information retrieval;systems engineering and theory;feedback;human factors;programming profession;predictive models;productivity;man machine systems	A model of human performance on intraline text editing, the modification of a line of text, is presented. This model is based on an optimal minimum keystroke solution to the text modification plus a description of those keystrokes that are not easily predicted: feedback, command selection idiosyncrasies, and errors. To better match the human, who is assumed to employ an optimal sequence of editing commands, the optimal solution-was constrained to use the same types of command each human had been observed to use. The model's constraints and descriptions of keystrokes both give rise to an increased understanding of text editing and recommendations for editor design improvements.	event (computing);feedback;human reliability;text editor	John M. Hammer;Willian Bill Rouse	1982	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1982.4308911	natural language processing;availability;mathematical optimization;constrained optimization;productivity;computer science;artificial intelligence;machine learning;data mining;feedback;predictive modelling	HCI	-27.11977585573372	-88.72443393550599	169223
e1b66142994fb04736a230962b15a11ee0c30c6d	creation and analysis of a corpus of text rich indian tv videos	television;motion pictures;indian tv video;video signal processing;optical character recognition;video ocr;text analysis;indian tv video analysis video ocr corpus indian tv video;videos tv context optical character recognition software text recognition motion pictures statistical analysis;optical character recognition software;video signal processing optical character recognition statistical analysis television text analysis ubiquitous computing;statistical analysis;corpus;video ocr system corpus analysis corpus creation text rich indian tv video context extraction text recognition video optical character recognition vocr multiligual country rf cable multilingual text region localization modern digital tv signal text script indian tv context recognition video corpus construction text rich indian tv shows video frame statistical analysis lower case character video frame;indian tv video analysis;ubiquitous computing;tv;text recognition;context;videos	A lot of research is now going on to extract the context of the show to provide additional information related to the TV show. One major method to extract the context from TV is to recognize the texts from the videos which is also known as video Optical Character Recognition (VOCR). The problem of VOCR from the TV shows of a multiligual country like India is more difficult. In India still more than 90\% TV viewers are using RF Cable as input to TV and nearly 90% channels have multilingual texts in the TV shows. Thus the video quality is poor in compare to the modern digital TV signals as well as different text scripts are present in a single video frame. These made the problem of Indian TV context recognition more challenging. So this paper is concerned about the construction of a video corpus of text rich Indian TV shows. The proposed database contains more than 100 videos each of nearly 10 min duration containing text in the video frame. A statistical analysis of the corpus is also presented in the paper which can be used to identify the genre of TV show. The analysis also revealed that distribution of numerals, special characters, uppercase and lower case character can be used to classify a news video frame. This corpus is useful for a wide variety of research problems namely, (i) localization of the text regions from a video frame, (ii) recognition of texts from a video frame, (iii) extraction of context from video, and (iv) performance evaluation of a video OCR system.	computer monitor;emoticon;frame language;heuristic;image resolution;logo;maxima and minima;optical character recognition;peak signal-to-noise ratio;performance evaluation;radio frequency;text corpus	Tanushyam Chattopadhyay;Soumik Sengupta;Aniruddha Sinha;Nisha Rampuria	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.174	video compression picture types;speech recognition;computer science;multimedia;television;ubiquitous computing	Vision	-24.43636170068568	-80.75532338642768	169352
d9149401d5385a9f230f6c34c5b880c2259765a4	towards a novel digital household account book	household account book;user centered design;receipt analysis;automatic receipt capturing	We introduce the concept of a novel digital household account book which lessens the burden of manually entering single items. In this paper, we present the results of two studies. We first conducted an online questionnaire with 142 participants to assess requirements. One of the lessons learned supports our initial notion to enhance digital household account books with automatic receipt capturing for increasing the acceptance rate. Subsequently, we analyzed a corpus of 117 German receipts in a technical study to learn about their structure and content. The results from these two studies form the basis for the realization of the concept.	book;requirement;text corpus	Frederic Kerber;Pascal Lessel;Maximilian Altmeyer;Annika Kaltenhauser;Christian Neurohr;Antonio Krüger	2014		10.1145/2559206.2581288	user-centered design;simulation;human–computer interaction;computer science;management	HCI	-29.52427801087346	-84.09846585105534	169394
3e38ee44648588e866acf8b76cc8fd5aac271769	automatic detection and correction of syntax-based prosody annotation errors	speech synthesis;acoustic signal processing;speech synthesis acoustic signal processing hidden markov models natural language processing;corpus prosody speech synthesis annotation;hidden markov models;french corpus syntax based prosody annotation error automatic detection syntax based prosody annotation error correction unit selection hmm based speech synthesis speech corpora annotation phonemes suprasegmental characteristics automatic syntactic analysis acoustic information labeling error reduction syntax driven prosody labeling acoustic features syntax based labels;acoustics labeling niobium speech feature extraction syntactics decision trees;natural language processing	Both unit-selection and HMM-based speech synthesis require large annotated speech corpora. To generate more natural speech, considering the prosodic nature of each phoneme of the corpus is crucial. Generally, phonemes are assigned labels which should reflect their suprasegmental characteristics. Labels often result from an automatic syntactic analysis, without checking the acoustic realization of the phoneme in the corpus. This leads to numerous errors because syntax and prosody do not always coincide. This paper proposes a method to reduce the amount of labeling errors, using acoustic information. It is applicable as a post-process to any syntax-driven prosody labeling. Acoustic features are considered, to check the syntax-based labels and suggest potential modifications. The proposed technique has the advantage of not requiring a manually prosody-labelled corpus. The evaluation on a corpus in French shows that more than 75% of the errors detected by the method are effective errors which must be corrected.	acoustic cryptanalysis;hidden markov model;natural language;parsing;semantic prosody;speech synthesis;text corpus	Sandrine Brognaux;Thomas Drugman;Richard Beaufort	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424259	natural language processing;speech recognition;speech corpus;computer science;linguistics;speech synthesis;hidden markov model	NLP	-19.767140193498438	-83.24459470229634	169521
239d3d3a6af2c815b913b632e981876724d65c1f	a formal design of an arabic text formatter for microcomputers	minimisation;regle transition;minimization;automata estado acabado;formatage;scanneur;minimizacion;automate etat fini;scanner;formataje;formatting;arabic;transition rule;barredor;finite automaton;arabe;edicion texto;text editing;edition texte	Alamnct--This work presents the formal design of an Arabic Text Formatter (ATF), that may help many users in the Arab world. The design procedure is based on an automata approach to describe and recognize the Arabic characters. Thus a scanner is constructed in order to deal with the input stream and to recognize the words being written as well as to produce output tokens. Actually the form of an Arabic letter depends on its position in word. Therefore, the character generator should produce the correct shape to match the rules of writing the Arabic words. Here, we employ a keyboard that has a unique form for each character, however, the justification and the proper choice of a symbol format is carried out by the designed scanner. For the scanner, the regular expressions, the nondeterministic finite automata and the deterministic finite automata are given. The system commands and their corresponding actions are also pointed out.	automata theory;computer keyboard;deterministic finite automaton;finite-state machine;microcomputer;nondeterministic finite automaton;regular expression;stream (computing)	Mohammed Zaki;Al. H. Albarhamtoshy	1987	Comput. Lang.	10.1016/0096-0551(87)90004-X	minimisation;speech recognition;computer science;artificial intelligence;arabic;finite-state machine;programming language;algorithm	Logic	-27.511442196664245	-81.24536448051431	169535
6f36110a6cb86816b171e276c9c411a4b8225c43	optical font recognition using conditional random field	page layout;conditional random fields;optical font recognition;document design	Automated publishing systems require large databases containing document page layout templates. Most of these layout templates are created manually. A lower cost alternative is to extract document page layouts from existing documents. In order to extract the layout from a scanned document image, it is necessary to perform Optical Font Recognition (OFR) since the font is an important element in layout design. In this paper, we use the Conditional Random Field (CRF) model to perform OFR. First, we extract typographical features of the text. Then, we train the probabilistic model using a log-linear parameterization of CRF. The advantage of using CRF is that it does not assume that the typographical features are independent of each other. We demonstrate the effectiveness of this approach on a set of 616 fonts.	conditional random field;database;log-linear model;optical character recognition;statistical model	Aziza Satkhozhina;Ildus Ahmadullin;Jan P. Allebach	2013		10.1145/2494266.2494307	page layout;speech recognition;computer science;document layout analysis;pattern recognition;data mining;conditional random field	Web+IR	-23.44082475638674	-81.0746338531805	169811
ba909c144abcf9b389e185de0f20e43cb19079bb	machine learning and fuzzy logic techniques for personalized tutoring of foreign languages		Intelligent computer-assisted language learning employs artificial intelligence techniques to create a more personalized and adaptive environment for language learning. Towards this direction, this paper presents an intelligent tutoring system for learning English and French concepts. The system incorporates a novel model for error diagnosis using machine learning. This model employs two algorithmic techniques and specifically Approximate String Matching and String Meaning Similarity in order to diagnose spelling mistakes, mistakes in the use of tenses, mistakes in the use of auxiliary verbs and mistakes originating from confusion in the simultaneous tutoring of languages. The model for error diagnosis is used by the fuzzy logic model which takes as input the results of the first or the knowledge dependencies existing among the different domain concepts of the learning material and decides dynamically about the learning content that is suitable to be delivered to the learner each time.	fuzzy logic;machine learning	Christos Troussas;Konstantina Chrysafiadi;Maria Virvou	2018		10.1007/978-3-319-93846-2_67	confusion;artificial intelligence;machine learning;personalization;fuzzy logic;language acquisition;approximate string matching;spelling;computer science;foreign language;intelligent tutoring system	NLP	-27.430209422103694	-83.54862407891875	169896
937f4bef19666bd886674d8c6e6652ff835698e1	utterance selection for optimizing intelligibility of tts voices trained on asr data		This paper describes experiments in training HMM-based text-to-speech (TTS) voices on data collected for Automatic Speech Recognition (ASR) training. We compare a number of filtering techniques designed to identify the best utterances from a noisy, multi-speaker corpus for training voices, to exclude speech containing noise and to include speech close in nature to more traditionally-collected TTS corpora. We also evaluate the use of automatic speech recognizers for intelligibility assessment in comparison with crowdsourcing methods. While the goal of this work is to develop natural-sounding and intelligible TTS voices in Low Resource Languages (LRLs) rapidly and easily, without the expense of recording data specifically for this purpose, we focus on English initially to identify the best filtering techniques and evaluation methods. We find that, when a large amount of data is available, selecting from the corpus based on criteria such as standard deviation of f0, fast speaking rate, and hypo-articulation produces the most intelligible voices.	automatic sounding;automatic system recovery;biconnected component;crowdsourcing;experiment;finite-state machine;hidden markov model;intelligibility (philosophy);netware file system;optimizing compiler;speech recognition;speech synthesis;station hypo;text corpus	Erica Cooper;Xinyue Wang;Alison Chang;Yocheved Levitan;Julia Hirschberg	2017			speech recognition;natural language processing;pattern recognition;artificial intelligence;computer science;utterance;intelligibility (communication)	NLP	-21.019030553091696	-84.24038706064923	170048
3794a69bd3cd8930d291ec51ff773a0f68e65168	using normalized compression distance for classifying file fragments	classification algorithm;computer forensics;data compression;distance measure;approximation algorithms;compression algorithms;training;k nearest neighbor algorithm;n valued classification normalized compression distance file fragment classification k nearest neighbor algorithm;software engineering;accuracy;n valued classification;app iden;pattern classification computer forensics data compression learning artificial intelligence;file fragment classification;pattern classification;normalized compression distance;for dmin;k nearest neighbor;programvaruteknik;learning artificial intelligence;app iden for dmin;security;classification algorithms machine learning algorithms compression algorithms shape measurement concatenated codes availability security machine learning noise shaping forensics;graphics;noise	We have applied the generalized and universal distance measure NCD--Normalized Compression Distance--to the problem of determining the types of file fragments via example. A corpus of files that can be redistributed to other researchers in the field was developed and the NCD algorithm using k-nearest-neighbor as a classification algorithm was applied to a random selection of file fragments. The experiment covered circa 2000 fragments from 17 different file types. While the overall accuracy of the n-valued classification only improved the prior probability of the class from approximately 6% to circa 50% overall, the classifier reached accuracies of 85%--100% for the most successful file types.	circa;k-nearest neighbors algorithm;network computing devices;text corpus	Stefan Axelsson	2010	2010 International Conference on Availability, Reliability and Security	10.1109/ARES.2010.100	computer science;machine learning;pattern recognition;data mining	ML	-22.248817853662015	-90.03989622418732	170144
2100ed95a8276563a410e8d06606415ecaeb4b17	keyword generation by native speaker is quick and useful in conversation between native and non-native speaker		Real-time keywords potentially demonstrate positive effects when they are provided in cross-cultural communication. Previously real-time keywords generated by a speaker during talking were investigated, and it was found it contributes to build mutual understanding and knowledge. However the use of keywords was not clarified in respect of when and how was typed in real-time communication. In this paper we analyze the data sets of multiple media files and typed materials collected through the experiment, and investigate the use of keyword in a collaborative dialogue. Consequently it was found that speaking was earlier and typing was 2.6 seconds behind of relevant speech, which met the requirement that real-time transcript was beneficial when its shown within 5 seconds. Besides, noun phrases that were typed as keywords were categorized referring to utterance. It was consequently found that 55% of all keywords were expansion and replacement of NS message when speakers got explicit interlocutors acceptance. Lastly conversation analysis was conducted to articulate how keywords are expressed in a conversation. The data showed that speakers uttered multiple episodes when speakers utilized keywords, but briefly uttered in standard manner without keywords.	categorization;experiment;message authentication code;nearest neighbor search;real-time clock;real-time locating system;real-time transcription	Hiromi Hanawa;Xiaoyu Song;Tomoo Inoue	2017	2017 IEEE 21st International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2017.8066685	computer science;first language;conversation;noun phrase;typing;artificial neural network;utterance;conversation analysis;natural language processing;speech recognition;artificial intelligence	Visualization	-25.941543968144575	-85.91137437824003	170186
52b725b98d247041b7b7d188038dfea5a0c7a726	lightly supervised acoustic model training for imprecisely and asynchronously transcribed speech	text analysis acoustic signal processing audio signal processing learning artificial intelligence natural language processing speech recognition;audio signal processing;text analysis;acoustic signal processing;cross domain adaptation large vocabulary continuous speech recognition lightly supervised training acoustic modeling;lightly supervised across domain acoustic model retraining asynchronously transcribed speech imprecisely transcribed speech audio material automated hungarian parliamentary speech transcription lightly supervised across domain acoustic model adaptation low resource broadcast news model process bootstrapping automatic parliamentary training speech recognition dynamic text alignment based data selection task specific acoustic model edited official transcriptions unaligned speech data unseen target speech real time recognition word accuracy difference;speech recognition;learning artificial intelligence;acoustics training speech speech recognition data models filtering adaptation models;natural language processing	In a variety of speech recognition tasks a large amount of approximate transcription is available for the audio material, but is not directly applicable for acoustic model training. Whereas roughly time-synchronous closed-captions or proper audiobook texts are already used in lightly supervised techniques, the utilization of more imperfect and at the same time completely unaligned transcriptions is not self-evident. In this paper we describe our experiments aiming at automated transcription of Hungarian parliamentary speeches. Essentially, a lightly supervised across-domain acoustic model adaptation/retraining is performed. A low-resource broadcast news model is used to bootstrap the process. Relying on automatic recognition of parliamentary training speech and on dynamic text alignment based data selection, a new, task-specific acoustic model is built. For the adaptation to the parliamentary domain, only edited official transcriptions and unaligned speech data are used, without any additional human annotation effort. The adapted acoustic model is applied on unseen target speech in real-time recognition. The word accuracy difference between the automatic and the human powered, official transcription is only 5% (as compared to the exact reference text).	acoustic cryptanalysis;acoustic model;approximation algorithm;audiobook;booting;discriminative model;experiment;language model;medical transcription;numerical analysis;perplexity;real-time locating system;speech recognition;supervised learning;test set;transcription (software)	Péter Mihajlik;András Balog	2013	2013 7th Conference on Speech Technology and Human - Computer Dialogue (SpeD)	10.1109/SpeD.2013.6682653	voice activity detection;voxforge;natural language processing;audio mining;speech recognition;speech corpus;computer science;speech coding;speech processing;acoustic model;communication;speech analytics	NLP	-20.8318102967253	-85.6807375100673	170248
0fd9a5d0d007c0bfa895362ddcacbbfc5448cfe1	principles for the design of cooperative spoken human-machine dialogue	human factors;interactive systems;natural language interfaces;natural languages;speech processing;danish dialogue system;controlled user testing;cooperative spoken human machine dialogue design;dialogue component;dialogue evaluation;evaluation tools;human human dialogue theory;systematic dialogue development	This paper presents a consolidated set of 24 principles of cooperative spoken human-machine dialogue which are based on the development and controlled user testing of the dialogue component of the Danish dialogue system as well as on comparison with human-human dialogue theory. Potentially, the principles could be used as effective and systematic dialogue development and evaluation tools both during early design and in later phases of dialogue evaluation.	dialog system;usability testing	Niels Ole Bernsen;Hans Dybkjær;Laila Dybkjær	1996			natural language processing;speech recognition;computer science;communication	NLP	-25.430247275475413	-85.84074294913445	170270
e8d02c21f0310142eb955c366ea119af2653fb03	manuscore: music notation-based computer assisted composition		ManuScore is a music notation-based, interactive music composition application, backed by a cognitively-inspired music learning and generation system. In this paper we outline its various functions, describe an applied composition study using the software, and give results from a study of listener evaluation of the music composed during the composition study. The listener study was conducted at a chamber music concert featuring a mixed programme of human-composed, machine-composed, and computerassisted works.	dynamic music	James B. Maxwell;Arne Eigenfeldt;Philippe Pasquier	2012			software;multimedia;chamber music;composition (visual arts);musical notation;musical composition;computer science	HCI	-30.062328104764518	-88.62546210967355	170368
533a0f61ab774b0a3316f8e02d894bc013b66c9c	automatic transcription of lithuanian text using dictionary	grapheme to phoneme transcription;speech recognition	There is presented a technique of transcribing Lithuanian text into phonemes for speech recognition. Text-phoneme transformation has been made by formal rules and the dictionary. Formal rules were designed to set the relationship between segments of the text and units of formalized speech sounds – phonemes, dictionary – to correct transcription and specify stress mark and position. Proposed the automatic transcription technique was tested by comparing its results with manually obtained ones. The experiment has shown that less than 6% of transcribed words have not matched.	automatic control;dictionary;medical transcription;speech recognition;transcription (software)	Mantas Skripkauskas;Laimutis Telksnys	2006	Informatica, Lith. Acad. Sci.		natural language processing;speech recognition;computer science;transcription;acoustic model	NLP	-21.433389999607737	-82.57682187469094	170998
02c2d34f3aed5b2731d62a9f9005ca67a466512e	comparing the utility of state features in spoken dialogue using reinforcement learning	best action;best feature;system action;dialogue success;tutoring system;recent work;dialogue system;user state;reinforcement learning;previous work;state feature	Recent work in designing spoken dialogue systems has focused on using Reinforcement Learning to automatically learn the best action for a system to take at any point in the dialogue to maximize dialogue success. While policy development is very important, choosing the best features to model the user state is equally important since it impacts the actions a system should make. In this paper, we compare the relative utility of adding three features to a model of user state in the domain of a spoken dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action.	dialog system;feedback;reinforcement learning	Joel R. Tetreault;Diane J. Litman	2006			natural language processing;computer science;artificial intelligence;machine learning;reinforcement learning	NLP	-27.198729182810645	-86.9031351999794	171108
21411e6b0b4bad6037d112d60875c61ef234ba2f	leveraging multiple query logs to improve language models for spoken query recognition	databases;speech interfaces;positive feedback loop;positive feedback;query processing;spoken query recognition;perforation;speech recognition query processing;training;language modeling;voice search;natural languages;speech interface;indexing terms;multiple query logs;training data;mobile search;automatic speech recognition;accuracy;feedback loop;positive feedback loop multiple query logs spoken query recognition voice search system speech interface multiple data sources asr language models mobile search web search;business;asr language models;mobile communication;natural languages speech recognition training data robustness web search databases automatic speech recognition predictive models feedback loop text recognition;multiple data sources;speech recognition;robustness;web search;predictive models;text recognition;click data language modeling voice search query log;query logs;voice search system;language model;query log;data models;click data	A voice search system requires a speech interface that can correctly recognize spoken queries uttered by users. The recognition performance strongly relies on a robust language model. In this work, we present the use of multiple data sources, with the focus on query logs, in improving ASR language models for a voice search application. Our contributions are three folds: (1) the use of text queries from web search and mobile search in language modeling; (2) the use of web click data to predict query forms from business listing forms; and (3) the use of voice query logs in creating a positive feedback loop. Experiments show that by leveraging these resources, we can achieve recognition performance comparable to, or even better than, that of a previously deploy system where a large amount of spoken query transcripts are used in language modeling.	clickstream;language model;positive feedback;web search engine	Xiao Li;Patrick Nguyen;Geoffrey Zweig;Dan Bohus	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960433	natural language processing;sargable;query optimization;query expansion;web query classification;speech recognition;positive feedback;data control language;computer science;rdf query language;web search query;query language;language model	Visualization	-22.777308988478417	-87.42109317178438	171525
c91ebd73ae9b7d413fdf4b6861a1be66d471d505	intelligent fuzzy spelling evaluator for e-learning systems	electronic learning;inadvertent error;evaluation methods;scoring;spelling;e learning;fuzzy membership;artificial intelligence;evaluation;student reaction;fuzzy automata;error patterns	Evaluating Learners’ Response in an e-Learning environment has been the topic of current research in areas of Human Computer Interaction, e-Learning, Education Technology and even Natural Language Processing. The current paper presents a twofold strategy to evaluate single word response of a learner in an e-Learning environment. The response of the learner to be evaluated would consist of errors committed due to lack of knowledge and also out of inadvertent mistakes committed while typing the answers. The proposed system benevolently considers such errors and still marks the learner partially. The feature incorporated in this work adds the human element to the mechanised system of evaluation and assessment in an e-Learning environment.	human computer;human–computer interaction;interpreter (computing);natural language processing;software bug;thresholding (image processing)	Udit Kr. Chakraborty;Debanjan Konar;Samir Roy;Sankhayan Choudhury	2014	Education and Information Technologies	10.1007/s10639-014-9314-z	speech recognition;computer science;artificial intelligence;evaluation;machine learning;world wide web	HCI	-27.268343078276683	-83.2676214618923	171620
513522e3fc4ad58eb27d15f508bdd75cd3f402ad	automatic segmentation and summarization of meeting speech	coherent segment;meeting segment;important word;automatic segmentation;extractive summarisation;compression component;important utterance;information extraction module;recorded meeting;meeting speech;meeting transcript;downstream information retrieval	AMI Meeting Facilitator is a system that performs topic segmentation and extractive summarisation. It consists of three components: (1) a segmenter that divides a meeting into a number of locally coherent segments, (2) a summarizer that selects the most important utterances from the meeting transcripts. and (3) a compression component that removes the less important words from each utterance based on the degree of compression the user speci ed. The goal of the AMI Meeting Facilitator is two-fold: rst, we want to provide su cient visual aids for users to interpret what is going on in a recorded meeting; second, we want to support the development of downstream information retrieval and information extraction modules with the information about the topics and summaries in meeting segments.	coherence (physics);downstream (software development);information extraction;information retrieval;text segmentation	Gabriel Murray;Pei-yun Hsueh;Simon Tucker;Jonathan Kilgour;Jean Carletta;Johanna D. Moore;Steve Renals	2007			natural language processing;speech recognition;multimedia	NLP	-23.242157005076965	-83.74318689380446	171771
6175222c11357d95eb819b6b686726f9b9488bd1	joint discriminative learning of acoustic and language models on decoding graphs	graph theory;hidden markov models acoustics joints optimization training speech recognition decoding;decoding;natural languages;hidden markov models;pattern classification;speech recognition decoding graph theory hidden markov models learning artificial intelligence natural languages parameter estimation pattern classification;speech recognition;joint discriminative learning parameter estimation approach rm1 resource management timit speech corpora trigram language models context dependent hidden markov models weighted finite state transducers minimum classification error criterion joint optimization framework suboptimal recognition performance speech recognition decoding graphs language models acoustic models;parameter estimation;learning artificial intelligence	In traditional models of speech recognition, acoustic and language models are treated in independence and usually estimated separately, which may yield a suboptimal recognition performance. In this paper, we propose a joint optimization framework for learning the parameters of acoustic and language models using minimum classification error criterion. The joint optimization is performed in terms of a decoding graph constructed using weighted finite-state transducers based on context-dependent hidden Markov models and trigram language models. To emphasize the effectiveness of the proposed framework, two speech corpora, TIMIT and Resource Management (RM1), are incorporated in the conducted experiments. The preliminary experiments show that the proposed approach can achieve significant reduction in phone, word and sentence error rates on both TIMIT and RM1 when compared with conventional parameter estimation approaches.	acoustic cryptanalysis;context-sensitive language;estimation theory;experiment;hidden markov model;iso/iec 11404;language model;markov chain;mathematical optimization;speech recognition;statistical classification;timit;text corpus;transducer;trigram;word error rate	Abdelaziz A. Abdelhamid;Waleed H. Abdulla	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694237	speech recognition;computer science;machine learning;pattern recognition	NLP	-19.22355767830794	-91.27833129932115	171847
2a029f49255919bd77b874436868ad24867850f3	probabilistic parse scoring based on prosodic phrasing	rule-based algorithm;prosodic phrase boundary;parse score;prosodic phrasing;prosodic phrase structure;decision tree;alternative scoring criterion;previous work;prosodic pattern;probabilistic parse;rule-based synthesis algorithm;scoring algorithm;rule based	The relative size and location of prosodic phrase boundaries provides an important cue for resolving syntactic ambiguity. In previous work, we have introduced an analysis/synthesis formalism for scoring parses in terms of the similarity between prosodic patterns recognized from a given utterance and synthesized for the hypothesized parse. This paper describes a new approach to the synthesis problem, as well as an alternative scoring criterion. Specifically, a decision tree is designed to predict prosodic phrase structure for a given syntactic parse, and the tree is used to compute a parse score, which now is the probability of the recognized break sequence. Unlike the rule-based synthesis algorithm used in the previous work, the decision tree can be automatically trained and can therefore be designed specifically for different speaking styles or task domains. In experiments with a corpus of ambiguous sentences spoken by FM radio a n n o u n c e r s , we have achieved disambiguation performance similar to the rule-based algorithm, which is close to the performance of human subjects in perceptual experiments using the scoring algorithm with hand labeled breaks.	algorithm;decision tree;depth perception;experiment;logic programming;parsing;phrase structure rules;semantics (computer science);text corpus;word-sense disambiguation	N. M. Veilleuz;Mari Ostendorf	1992			rule-based system;natural language processing;speech recognition;computer science;decision tree;pattern recognition;linguistics	AI	-19.80718250059964	-81.51928629082649	171892
d563a2c6a855d8b66813a905333e69b86a84ffe2	large-scale processing, indexing and search system for czech audio-visual cultural heritage archives	audio visual systems;document handling;history;spoken documents large scale processing indexing search system czech audio visual cultural heritage archives oral history segment czechoslovakia;information retrieval systems audio visual systems document handling history indexing;indexing;speech acoustics speech recognition servers databases training vocabulary;information retrieval systems	This paper describes a complex system developed for processing, indexing and accessing data collected in large audio and audio-visual archives that make an important part of Czech cultural heritage. Recently, the system is being applied to the Czech Radio archive, namely to its oral history segment with more than 200.000 individual recordings covering almost ninety years of broadcasting in the Czech Republic and former Czechoslovakia. The ultimate goals are a) to transcribe a significant portion of the archive - with the support of speech, speaker and language recognition technology, b) index the transcriptions, and c) make the audio and text files fully searchable. So far, the system has processed and indexed over 75.000 spoken documents. Most of them come from the last two decades, but the recent demo collection includes also a series of presidential speeches since 1934. The full coverage of the archive should be available by the end of 2014.	archive;complex system;emoticon	Jan Nouza;Karel Blavka;Jindrich Zdánský;Petr Cerva;Jan Silovský;Marek Bohac;Josef Chaloupka;Michaela Kucharová;Ladislav Seps	2012	2012 IEEE 14th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2012.6343465	natural language processing;search engine indexing;audio mining;speech recognition;computer science;information retrieval	ML	-23.352422826387055	-84.08440070256606	171978
25b190d9509c34160b0d60a129aa56463859a629	maximum entropy modeling for speech recognition	bayesian framework;word error rate;word error rate maximum entropy modeling speech recognition maxent models natural language processing language modeling acoustic modeling maximum entropy direct models probability word sequence asynchronous features overlapping features maximum entropy markov model phone level decoding memm;maximum entropy methods;maximum entropy principle;generic model;hidden markov model;statistical independence;acoustic modeling;speech processing;natural languages;statistical model;markov model;entropy speech recognition hidden markov models natural languages computer science natural language processing bayesian methods probability decoding error analysis;speech recognition;error statistics;maximum entropy model;markov processes;natural language processing;maximum entropy;discriminative model;language model;error statistics maximum entropy methods speech recognition natural languages markov processes speech processing	Summary form only given. Maximum entropy (maxent) models have become very popular in natural language processing. We begin with a basic introduction of the maximum entropy principle, cover the popular algorithms for training maxent models, and describe how maxent models have been used in language modeling and (more recently) acoustic modeling for speech recognition. Some comparisons with other discriminative modeling methods is made. A substantial amount of time is devoted to the details of a new framework for acoustic modeling using maximum entropy direct models, including practical issues of implementation and usage. Traditional statistical models for speech recognition have all been based on a Bayesian framework using generative models such as hidden Markov models (HMM). The new framework is based on maximum entropy direct modeling, where the probability of a state or word sequence given an observation sequence is computed directly from the model. In contrast to HMM, features can be asynchronous and overlapping, and need not be statistically independent. This model therefore allows for the potential combination of many different types of features. Results from a specific kind of direct model, the maximum entropy Markov model (MEMM) are presented. Even with conventional acoustic features, the approach already shows promising results for phone level decoding. The MEMM significantly outperforms traditional HMM in word error rate when used as stand-alone acoustic models. Combining the MEMM scores with HMM and language model scores shows modest improvements over the best HMM speech recognizer. We give a sense of some exciting possibilities for future research in using maximum entropy models for acoustic modeling.	acoustic cryptanalysis;acoustic model;algorithm;finite-state machine;hidden markov model;language model;markov chain;maximum entropy spectral estimation;maximum-entropy markov model;natural language processing;principle of maximum entropy;speech recognition;statistical model;word error rate	Hong-Kwang Jeff Kuo	2004	2004 International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2004.1409569	natural language processing;maximum-entropy markov model;speech recognition;computer science;principle of maximum entropy;machine learning;pattern recognition;speech processing;hidden markov model;statistics;language model	NLP	-20.83758153202043	-90.63435090736665	172028
23d7d48846628c054a1ccda6e38469f5f19e16f9	multilingual voice creation toolkit for the mary tts platform	mandarin chinese;speech synthesis;graphic user interface	This paper describes an open source voice creation toolkit that supports the creation of unit selection and HMM-based voices, for the MARY (Modular Architecture for Research on speech Synthesis) TTS platform. We aim to provide the tools and generic reusable runtime system modules so that people interested in supporting a new language and creating new voices for MARY TTS can do so. The toolkit has been successfully applied to the creation of British English, Turkish, Telugu and Mandarin Chinese language components and voices. These languages are now supported by MARY TTS as well as German and US English. The toolkit can be easily employed to create voices in the languages already supported by MARY TTS. The voice creation toolkit is mainly intended to be used by research groups on speech technology throughout the world, notably those who do not have their own pre-existing technology yet. We try to provide them with a reusable technology that lowers the entrance barrier for them, making it easier to get started. The toolkit is developed in Java and includes intuitive Graphical User Interface (GUI) for most of the common tasks in the creation of a synthetic	graphical user interface;hidden markov model;java;netware file system;open-source software;runtime system;speech synthesis;speech technology;super robot monkey team hyperforce go!;synthetic data	Sathish Pammi;Marcela Charfuelan;Marc Schröder	2010			speech recognition;natural language processing;runtime system;telugu;artificial intelligence;speech technology;british english;constructed language;computer science;java;speech synthesis;graphical user interface	HCI	-23.33859116181089	-85.09162582554488	172736
93720b9abea5f503e8270e02510b130b57b14ea4	user-defined expected error rate in ocr postprocessing by means of automatic threshold estimation	reliability index;probability;handwriting recognition;image segmentation;optical character recognition user defined expected error rate ocr postprocessing automatic threshold estimation language model probability reliability index;helium;optical character recognition;training;biological system modeling;error analysis;optical character recognition software;estimation;ocr postprocessing;indexation;probability image segmentation optical character recognition;error analysis optical character recognition software estimation training helium handwriting recognition biological system modeling;error rate;user defined expected error rate;language model;automatic threshold estimation	In this work, a method for the automatic estimation of a threshold that allows the user of an OCR system to define an expected error rate is presented. When the OCR output is post-processed using a language model, a probability, a reliability index (or a “transformation cost”) is usually obtained, reflecting the likelihood (or its inverse) that the string of OCR hypotheses belongs to the model. Using a threshold on this index (or cost) to reject the less reliable hypotheses, a variable level of expected accuracy can be imposed on the output. It is much more convenient for the user the ability to “fix” at an acceptable level the expected error rate instead of having to deal with an arbitrary threshold. Of course, the result will always be high reject rates for difficult tasks and lower reject rates for easier tasks.	approximation algorithm;inverted index;language model;optical character recognition;rejection sampling;test set	Jose Ramón Navarro-Cerdán;Joaquim Arlandis;Juan Carlos Pérez-Cortes;Rafael Llobet	2010	2010 12th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2010.126	estimation;speech recognition;word error rate;computer science;machine learning;pattern recognition;probability;handwriting recognition;image segmentation;helium;optical character recognition;language model	Vision	-21.713810029508913	-90.697918075087	172864
2fc66483b8c31881aa43279b88eb9957841a6eee	interfaces for speech recognition systems: the impact of vocabulary constraints and syntax on performance	human interaction;speech recognition	An experiment was conducted to investigate the effects of vocabulary constraints and syntax on human interactions with a speech interactive system. Three dialogue styles for a telephone banking application, all using constrained vocabularies, were compared: yes/no, menu and query prompts. These styles differ both in the degree of vocabulary constraint, and in how that constraint i s communicated to the user. It was found that although i t involved more dialogue steps the yes/no interaction style was the most effective in terms of both task completion rates and performance time. The query strategy was least preferred by users.	interaction;interactivity;speech recognition;vocabulary	Kate S. Hone;David Golightly	1998			speech recognition;telephone banking;speech analytics;syntax;natural language processing;computer science;information technology;artificial intelligence;vocabulary	HCI	-26.16993872090605	-86.23378655486154	173089
923d03151b1c97205d60f42ea86e15792c6c32f9	coordination in minimalist grammars: excorporation and across the board (head) movement		This paper describes how coordination has been integrated into a broad coverage statistical Minimalist Grammar parser currently under development, and presents a unified analysis for a number of coordinate (and related) constructions sometimes considered problematic for transformational syntax; these include across-theboard (ATB) head and phrasal movements, argument cluster coordination, right node raising and parasitic gaps. To accommodate all these structures, a number of novel extensions are introduced into the formalism, including a mechanism for excorporation which enables ATB head movement; this supplements a variant of Kobele’s (2008) mechanism for ATB phrasal movement. The weak expressive power of the formalism is shown to be unaffected by these extensions.	expressive power (computer science);formal system;minimalism (computing);minimalist grammar;semantics (computer science)	John Torr;Edward P. Stabler	2016			programming language;rule-based machine translation;mathematics	NLP	-33.24864527894368	-82.03950285859013	173112
6fecb330b4564c493f3fc4210ce8204ada7a9031	toward realtime transcription of broadcast news.	broadcast news	In this paper, we describe our recent work in fast automatic transcription of broadcast news programming from radio and television. Given our state-of-the-art BBN BYBLOS primary system [1] running at 230 times real time (230xRT) we show that eliminating and approximating many computationally expensive components speeds up the system by a factor of more than 20 without significant loss in recognition accuracy. This is accomplished without retraining or changing the base-line system structure. The components of the primary system which are refined include segmentation, adaptation, decoding, cross-word rescoring with adaptation, and system combination.	analysis of algorithms;decoding methods;image segmentation;real-time computing;real-time transcription;transcription (software)	Jason Davenport;Long Nguyen;Spyridon Matsoukas;Richard M. Schwartz;John Makhoul	1999			broadcast journalism;multimedia;internet privacy;world wide web	NLP	-21.224347052593913	-85.57780737458567	173162
57f186f64fb5c256732a96f04063f0f4dde075ef	on the estimation of error-correcting parameters	maximum likelihood;stochastic processes error correction string matching parameter estimation formal languages maximum likelihood sequence estimation pattern classification;formal languages;stochastic processes;minimum distance;context free grammar;error correction;pattern classification;pattern recognition;maximum likelihood sequence estimation;pattern recognition error correcting parameter estimation pattern string divergences language regular grammar context free grammar ec parsers minimum distance parsers stochastic parsers maximum likelihood rule string classes stochastic models;parameter estimation;string matching;estimation error parameter estimation equations stochastic processes state estimation frequency contracts robustness cost function yield estimation	Error-Correcting (EC) techniques allow for coping with divergences in pattern strings with regard to their “standard” form as represented by the language L accepted by a regular or context-free grammar. There are two main types of EC parsers: minimum-distance and stochastic. The latter apply the maximum likelihood rule: classification into the classes of the strings in L that have the greatest probability given the strings representing unknown patterns. Stochastic models are important in pattern recognition if good esti mations for their parameters are provided. The problem of parameter estimation has been well studied for stochastic grammars, but this is not the case of EC parameters. This work is aimed at providing solutions to adequately solve it.	context-free language;error detection and correction;estimation theory;parsing;pattern recognition;stochastic grammar;stochastic process	Juan-Carlos Amengual;Enrique Vidal	2000		10.1109/ICPR.2000.906215	stochastic process;formal language;error detection and correction;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;context-free grammar;maximum likelihood sequence estimation;estimation theory;statistics;string searching algorithm	AI	-21.27430915600502	-92.18021949758513	173666
bf5a48750a7ca73074383c6ab05799ae1d3a69ef	sql translator using artificial neural networks	multiple users sql translator artificial neural networks natural language database requests frontend information system natural language interface nli everyday english requests database reporting tool sql command neural net based converter translator recurrent neural network paradigm neural net approach adaptive behaviour query recognition domain dependent training set commonly used words dos file;natural language interfaces;sql;language translation;natural languages;query languages;neural net;recurrent network;adaptive behaviour;word processing language translation natural language interfaces natural languages sql query languages recurrent neural nets;natural language;natural language interface;recurrent neural nets;recurrent neural network;information system;artificial neural networks natural languages databases neural networks recurrent neural networks dictionaries data structures pattern matching information systems australia;word processing;artificial neural network	The paper presents a novel approach to convert natural language database requests into SQL. The SQL translator acts as a frontend to any information system and provides a natural language interface (NLI) to the end user. The translator is designed to understand everyday English requests and invoke appropriate database reporting tool for a valid query. Each valid query is mapped onto an appropriate SQL command through a neural net based converter/translator. A special class of recurrent neural network paradigm suggested by M.I. Jordan (1988) has been successfully used for this purpose. The query is read one word at a time and clamped at the input of the recurrent network. After recognising the valid sequences of words the output of the network stabilises and indicates the category of the SQL command. The biggest advantage of using a neural net approach is the trainable and adaptive behaviour of the translator. The translator can be trained to recognise queries in other languages and domain by designing a domain dependent training set and incorporating a dictionary of related words. The dictionary of commonly used words and synonyms required for this purpose is an ordinary DOS file that can be shared by multiple users in a network.	artificial neural network;sql	Neeraj Prakash;K. Garg;Y. C. Chopra	1996		10.1109/ANZIIS.1996.573892	natural language processing;sql injection;stored procedure;computer science;artificial intelligence;query by example;machine learning;data mining;database;natural language;programming language;null;artificial neural network	Robotics	-23.410136178812067	-87.82618445893863	173700
42a9c575acb53fac332993087c1e1dbcc8161ccd	an all-subtrees approach to unsupervised parsing	wall street journal;frequency estimation;maximum likelihood estimate;binary tree	"""We investigate generalizations of the allsubtrees """"DOP"""" approach to unsupervised parsing. Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. We report state-of the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that n unsupervised parsing model beats a widely used supervised model (a treebank PCFG)."""	binary tree;coding tree unit;parse tree;parsing;stochastic context-free grammar;the wall street journal;tree (data structure);treebank;unsupervised learning	Rens Bod	2006			binary tree;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;maximum likelihood sequence estimation;statistics	NLP	-21.173684927097657	-89.6481745349336	173703
4e8c7f77a3421e54e478d64c8088e40bf9a4beb5	discourse structure for multi-speaker spontaneous spoken dialogs: incorporating heuristics into stochastic rtns	human database dialog;speech understanding system;corpus probabilities;degradation;human computer interaction;stochastic processes natural languages predictive models application software speech recognition laser sintering problem solving computer science speech analysis level control;laser sintering;application software;rule based system;speech processing;rule based;speech analysis;stochastic parameters;level control;utterance corpus;natural languages;discourse structure;corrections;clarifications;grammars;stochastic processes;multi speaker discourse;topic changes;stochastic rtn;speech recognition;predictive models;stochastic utterance predictions;digressions;heuristics;knowledge based systems speech processing natural languages grammars stochastic processes;computer science;spontaneous speech;speech understanding system multi speaker spontaneous spoken dialogs heuristics discourse structure stochastic rtn spoken language applications spontaneous speech digressions clarifications corrections topic changes multi speaker discourse grammars language models stochastic parameters speech recognition human database dialog utterance corpus corpus probabilities rule based system language model stochastic utterance predictions;knowledge based systems;language model;multi speaker spontaneous spoken dialogs;spoken language applications;problem solving;language models	In real spoken language applications, w here speakers interact sp<?n­ taneously. there is much seemmg unpredictability that makes recognttion difficult. Multi-speaker spontaneous dialog where two speakers interact verbally to cooperatively solve a mutual, shared problem is more varied than human-computer mteractions. Spontaneous speech is not well structured, e�biting mid-utterance corrections and restarts in utterances. Discourse contains digressions, clarifications, corrections and topic changes. But, multi-speaker discourse is even more varied, with initiattve effects, speakers interacting, planning and res(Xlnding. This makes it extremely difficult to develop grammars and language models with adequate coverage and reliable stochastic parameters. Perplexity in­ creases and recognition de&rades considerably vis-a-vis human-database dialo g. Iu spite of all tlilS, multi-speaker dialogs are structured and predictable when the discourse is appropriately modelled. We have developed heuristics to model spontaneous speech and multi-speaker dialogs [1,2]. lhe underlying heuristics have been evaIuated and shown to adequately and accurately predict discourse phenomena, as evaluated on a lO,iXXl+ utterance corpt!s. Generally, the 'heuristics for com[lutin£ discourse structure and denving constraints from it are rule based. We have taken the rules and used them to develop a set of stochastic RTNs that capture both the rules and cOipus probabilities. The resulting l!lnguagc model can � II;sed predictivery to dyn�mically generate stochas­ IIc utterance predlcllons or can be mcorporated mto any recognition/understanding system where a single prior state is maintained.	heuristic (computer science);interaction;language model;norm (social);perplexity;recursive transition network;spontaneous order;stochastic modelling (insurance);dialog	Sheryl R. Young	1995		10.1109/ICASSP.1995.479393	natural language processing;application software;speech recognition;degradation;computer science;heuristics;predictive modelling;natural language;selective laser sintering;language model	NLP	-21.758150398530088	-86.82994750548417	174093
6e3c66ecc123523fe4218cb0bcc010a4685eaa26	an omni-font gurmukhi to shahmukhi transliteration system		This paper describes a font independent Gurmukhi-to-Shahmukhi transliteration system. Even though Unicode is gaining popularity, but still there is lot of material in Punjabi, which is available in ASCII based fonts. A problem with ASCII fonts for Punjabi is there is no standardisation of mapping of Punjabi characters and a Gurmukhi character may be internally mapped to different keys in different Punjabi fonts. In fact there are more than 40 mapping tables in use for the commonly used Punjabi fonts. Thus there is an urgent need to convert the ASCII fonts to standard mapping, such as Unicode without any loss of information. Already many such font converters have been developed and are available online. But one limitation is that, all these systems need manual intervention in which the user has to know the name of source font. In the first stage, we have proposed a statistical model for automatic font detection and conversion into Unicode. Our system supports around 225 popular Gurmukhi font encodings. The ASCII to Unicode conversion accuracy of the system is 99.73% at word level with TOP1 font detection. The second stage is conversion of Gurmukhi to Shahmukhi at high accuracy. The proposed Gurmukhi to Shahmukhi transliteration system can transliterate any Gurmukhi text to Shahmukhi at more than 98.6% accuracy at word level.	analog-to-digital converter;statistical model;unicode	Gurpreet Singh Lehal;Tejinder Singh Saini;Savleen Kaur Chowdhary	2012			font;pattern recognition;artificial intelligence;unicode;programming language;computer science;transliteration;ascii	Web+IR	-26.350190892762956	-81.88663238839449	174445
fa868b674a873ea425638accec1381186fe31d27	quality-adaptive spoken dialogue initiative selection and implications on reward modelling		Adapting Spoken Dialogue Systems to the user is supposed to result in more efficient and successful dialogues. In this work, we present an evaluation of a quality-adaptive strategy with a user simulator adapting the dialogue initiative dynamically during the ongoing interaction and show that it outperforms conventional non-adaptive strategies and a random strategy. Furthermore, we indicate a correlation between Interaction Quality and dialogue completion rate, task success rate, and average dialogue length. Finally, we analyze the correlation between task success and interaction quality in more detail identifying the usefulness of interaction quality for modelling the reward of reinforcement learning strategy optimization.	dialog system;failure;information retrieval;logic programming;mathematical optimization;reinforcement learning;simulation;user experience	Stefan Ultes;Matthias Kraus;Alexander Schmitt;Wolfgang Minker	2015			simulation;computer science;knowledge management;communication	NLP	-27.642887747429196	-87.11950393112038	174871
3b89136cbc1682ce2037acc1067de2bf0b44472a	flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output	mixed initiative;confidence measure;speech recognition;dialogue manager	We present a method to realize exible mixedinitiative dialogue, in which the system can make e ective con rmation and guidance using concept-level con dence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors. We de ne two concept-level CMs, which are on contentwords and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations. Less con dent interpretations are given to conrmation process. The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user's intention and generates system-initiative guidances even when successful interpretation is not obtained.	dialog system;finite-state machine;naruto shippuden: clash of ninja revolution 3;parsing;rejection sampling;speech recognition	Kazunori Komatani;Tatsuya Kawahara	2000		10.3115/990820.990888	natural language processing;speech recognition;computer science	NLP	-26.522526661294958	-85.45273555747951	175009
49db1b07c71b25677d5192b97cd2680fa167357d	csg-tag: constraint based synchronous grammar tree annotation system	databases;grammar;grammar rules;extensible markup language;cybernetics;knowledge based system;tree annotation system;language translation;syntactics grammar xml machine learning cybernetics knowledge based systems databases;trees mathematics;grammars;semi automatic annotation process;syntactic structure acquisition;tct csg tag constraint based synchronous grammar tree annotation system syntactic structure acquisition machine translation semi automatic annotation process extensible markup language format xml grammar rules monolingual skeletal bracketing syntactic tree translation corresponding tree structures;machine learning;syntactics;automatic annotation;xml;translation corresponding tree;xml grammars language translation trees mathematics;tct;constraint based synchronous grammar tree annotation system;monolingual skeletal bracketing syntactic tree;knowledge based systems;constraint synchronous grammar;csg tag;machine translation;translation corresponding tree structures;machine translation tree annotation system constraint synchronous grammar translation corresponding tree;extensible markup language format	The construction of grammars and the acquisition of syntactic structures from corpora are always considered as a time consuming task. Moreover, according to the purpose of the application, different standards have to be defined. In Machine Translation (MT), the situation is even more complicated since it covers two languages. In this paper, CSG-Tag, a Constraint based Synchronous Grammar (CSG) Tree Annotation System is proposed. This system provides a semi-automatic annotation process in the creation of syntactic structure of the source sentence linked with the corresponding target sentential patterns. All learned information are stored in Extensible Markup Language (XML) format and can be converted into grammar rules in application to MT. Moreover, the system has a function to import monolingual skeletal bracketing syntactic tree and Translation Corresponding Tree (TCT) structures in the creation of CSG rules.	constructive solid geometry;formal system;internet;machine translation;markup language;operating system;semiconductor industry;text corpus;the coroner's toolkit;tree structure;xml	Fai Wong;Francisco Oliveira;Sam Chao;Fan Sun	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6017024	natural language processing;synchronous context-free grammar;xml;speech recognition;computer science;artificial intelligence;knowledge-based systems;regular tree grammar;programming language	NLP	-30.605547609550374	-80.55892978869323	175288
5886394f2ef08fc3e3041ece35f28c5bf9b5f84c	evaluation of the impact of corpus phonetic alignment on the hmm-based speech synthesis quality	phonetic labeling;french speech synthesis;phonetic alignment;mos;hmm based speech synthesis;tts;hts;subjective evaluation	This study investigates the impact of phonetization and phonetic segmentation of training corpora on the quality of HMM-based TTS synthesis. HMM-TTS requires phonetic symbols aligned to the speech corpus in order to train the models used for synthesis. Phonetic annotation is a complex task, since pronunciation usually differs from spelling, as well as differing among regional accents. In this paper, the infrastructure of a French TTS system is presented. A corpus whose phonetic label occurrences were systematically modified number of schwas and liaisons and label boundaries were displaced, was used to train several systems, one for each condition. A perceptual evaluation of the influence of labeling accuracy on synthetic speech quality was conducted. Despite the degree of annotation changes, the synthetic speech quality of the five best systems remained close to that of the reference system, built upon the corpus whose labels were manually corrected.	hidden markov model;international conference on acoustics, speech, and signal processing;markov chain;proceedings of the ieee;semantic prosody;speech recognition;speech synthesis;statistical model;text corpus	Marc Evrard;Albert Rilliard;Christophe d'Alessandro	2015		10.1007/978-3-319-25789-1_7	natural language processing;speech recognition;speech corpus;computer science;phonetic search technology;communication	NLP	-20.507842025944427	-83.53062259778692	175607
0c2e6b4eb58c290771bef80f1f821a63dcac2b0d	unsupervised dialogue act induction using gaussian mixtures		This paper introduces a new unsupervised approach for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the dialogue as Hidden Markov model with emission probabilities estimated by Gaussian mixtures. We use Gibbs sampling for posterior inference. We present the results on the standard Switchboard-DAMSL corpus. Our algorithm achieves promising results compared with strong supervised baselines and outperforms other unsupervised algo-	algorithm;gibbs sampling;hidden markov model;markov chain;sampling (signal processing);telephone switchboard;unsupervised learning	Tomas Brychcin;Pavel Král	2017			speech recognition;computer science;machine learning;pattern recognition	NLP	-19.790679692856358	-88.91749461955425	175625
1d178425a22ba859636766f7a5ac3511f2b40edd	using prosodic features in language models for meetings	bayesian framework;speech technology;word error rate;bayesian language model;latent variable;conference paper;automatic speech recognition;detection algorithm;speech recognition;cross validation;language model	Prosody has been actively studied as an important knowledge source for speech recognition and understanding. In this paper, we are concerned with the question of exploiting prosody for language models to aid automatic speech recognition in the context of meetings. Using an automatic syllable detection algorithm, the syllable-based prosodic features are extracted to form the prosodic representation for each word. Two modeling approaches are then investigated. One is based on a factored language model, which directly uses the prosodic representation and treats it as a ‘word’. Instead of direct association, the second approach provides a richer probabilistic structure within a hierarchical Bayesian framework by introducing an intermediate latent variable to represent similar prosodic patterns shared by groups of words. Fourfold cross-validation experiments on the ICSI Meeting Corpus show that exploiting prosody for language modeling can significantly reduce the perplexity, and also have marginal reductions in word error rate.	algorithm;bayesian network;cross-validation (statistics);experiment;factored language model;latent variable;marginal model;perplexity;semantic prosody;speech recognition;syllable;word error rate	Songfang Huang;Steve Renals	2007		10.1007/978-3-540-78155-4_17	latent variable;natural language processing;speech technology;speech recognition;factored language model;word error rate;computer science;pattern recognition;cross-validation;language model	NLP	-19.422091595553695	-88.89847887296492	175908
b6d5e50a5c67bb0b3163b3d13b8aa3239cae82d9	explicit duration modelling in hmm/ann hybrids	transition state;duration model;modelizacion;duracion;lenguaje natural;finnois;modelo markov oculto;word error rate;hongrois;modelo markov;modele markov cache;hungarian;transition probability;etude experimentale;hidden markov model;acoustics;speech processing;loi gamma;finlandes;modelo hibrido;langage naturel;finnish;tratamiento palabra;traitement parole;intelligence artificielle;modele hybride;duration;hybrid model;modelisation;ley gama;markov model;reconocimiento voz;hungaro;estado transitorio;natural language;probabilidad transicion;methode moyenne;speech recognition;gamma distribution;artificial intelligence;inteligencia artificial;reconnaissance parole;modele markov;reseau neuronal;modeling;estudio experimental;acoustique;averaging method;probabilite transition;red neuronal;etat transition;metodo medio;state transition;duree;acustica;neural network	In some languages like Finnish or Hungarian phone duration is a very important distinctive acoustic cue. The conventional HMM speech recognition framework, however, is known to poorly model the duration information. In this paper we compare different duration models within the framework of HMM/ANN hybrids. The tests are performed with two different hybrid models, the conventional one and the “averaging hybrid” recently proposed. Independent of the model configuration, we report that the usual exponential duration model has no detectable advantage over using no duration model at all. Similarly, applying the same fixed value for all state transition probabilities, as is usual with HMM/ANN systems, is found to have no influence on the performance. However, the practical trick of imposing a minimum duration on the phones turns out to be very useful. The key part of the paper is the introduction of the gamma distribution duration model, which proves clearly superior to the exponential one, yielding a 12-20% relative improvement in the word error rate, thus justifying the use of sophisticated duration models in speech recognition.	acoustic cryptanalysis;averaging argument;experiment;hidden markov model;hoc (programming language);markov chain;molecular dynamics;speech recognition;state transition table;time complexity;word error rate	László Tóth;András Kocsor	2005		10.1007/11551874_40	gamma distribution;markov chain;speech recognition;systems modeling;word error rate;computer science;artificial intelligence;duration;speech processing;transition state;markov model;natural language;artificial neural network;hidden markov model;statistics	AI	-21.080251794237853	-91.18058468861196	175916
6ae280861a93ca9ec8b62a735f0b0ead9ad34190	ergodic hidden markov models and polygrams for language modeling	statistical approach;unsupervised learning;probability hidden markov models speech recognition unsupervised learning grammars computational linguistics natural languages;hidden markov models probability history frequency estimation natural languages speech recognition smoothing methods robustness testing databases;probability;hidden markov model;natural languages;baum welch;grammars;hidden markov models;speech recognition;sentence probability language modeling speech recognition ergodic discrete density hidden markov models bigrams word categories statistical approach unsupervised learning procedure baum welch algorithm model conditional probability perplexity training corpus polygrams trigram grammars smoothing techniques n grams interpolation atis corpus word recognizer german database n best paradigm;computational linguistics;conditional probability;language model	In this paper we present two new techniques for language modeling in speech recognition. The rst technique is based on ergodic discrete density Hidden Markov Models (HMM) which can be applied to bigrams based on word categories. This statistical approach of the so-called Markov bigrams enables an eecient unsupervised learning procedure for the bigram probabilities with the well-known Baum-Welch algorithm. Furthermore, maximizing the model-conditional probability is equivalent to minimizing the perplexity of the training corpus. The second technique is based on poly-grams which are an extension of the bigram (n = 2) or trigram (n = 3) grammars to any possible value of n. According to the smoothing techniques for bigram or trigram models, the probabilities of the n-grams in the polygram model are interpolated using the relative frequencies of all n 0-grams with n 0 n. Both techniques were evaluated on the ATIS corpus by computing the test set perplexity. Furthermore we integrated the Markov bigrams as well as the polygrams into our word recognizer for continuous speech. Experimental results on a German database are discussed using the N-best paradigm to reorder the generated word sequences according to the sentence probability of the language model.	automatic transmitter identification system (television);baum–welch algorithm;bigram;database;ergodic theory;ergodicity;finite-state machine;grams;hidden markov model;interpolation;language model;markov chain;n-gram;perplexity;programming paradigm;smoothing;speech recognition;test set;trigram;unsupervised learning;welch's method;whole earth 'lectronic link	Thomas Kuhn;Heinrich Niemann;Ernst Günter Schukat-Talamazzini	1994		10.1109/ICASSP.1994.389282	n-gram;natural language processing;maximum-entropy markov model;speech recognition;conditional probability;computer science;baum–welch algorithm;machine learning;perplexity;pattern recognition;probability;bigram;markov model;natural language;hidden markov model;statistics;variable-order markov model	NLP	-20.939878882842823	-89.80837189314514	176098
131896b7c121d8c532bcbfc06e64e5be47e51b50	corpus and voices for catalan speech synthesis	speech synthesis	In this paper we describe the design and production of a Catalan database for building synthetic voices. Two speakers have recorded 10 hours of speech each one. The speaker selection and the corpus d esign aim to provide resources for high quality synthesis. In fact, as a side effect, in the speaker selection proccess we have produced 1 0 databases of 1 hour each one which allows producing medium quality speech synthesis. The resources have been used to build voices for the Festival TTS. Both the original recordings and the Festival databases are freely available for research and for commercial use .	database;display resolution;electronic signatures in global and national commerce act;speech synthesis	Antonio Bonafonte;Jordi Adell;Ignasi Esquerra;Silvia Gallego;Asunción Moreno;Javier Pérez	2008			natural language processing;artificial intelligence;speech recognition;speech corpus;catalan;computer science;speech synthesis	NLP	-22.948902889233803	-84.46241014802987	176104
809399faf4a1b43ff1785ac874246d3c539290f8	historical change in language using monte carlo techniques		A system has been programmed in JOVIAL to serve as a vehicle for testing hypotheses about language change through time. A basic requirement of the system is that models must be formulated within the framework of Sapir's concept of drift and Bloomfield's definition of a speech community. Outside these restrictions, an experimenters selection of hypotheses is free. The system, which can be viewed as performing Monte Carlo simulations of group, language change, has been successfully tested in several computer runs using an extremely simple model of linguistic interaction. (The system, and any model tested within its framework, are separate entities. Accordingly, the use of a trivial model to check out the operation of the system does not depreciate its ability to handle models of vast complexity.) The initial test population consisted of fifteen adults and five children, each represented by a phrase-structure generation-recognition grammar. The grammars and the frequency parameters associated with their individual rules were not necessarily identical. During the course of a run some individuals died and others were born. Newborn children acquired the language of the community. The units of interaction consisted of conversations that were produced by the grammars of speakers and parsed by the grammars of auditors. The linguistic structure of a conversation determined changes in the auditor's grammar. Decisions in the system were made with random numbers on the basis of weighted frequency parameters. To insure control of free variables before undertaking experiments with factors causing change, the goal of the initial experiment was to obtain a condition of linguistic stability and essentially identical results for the population as a whole from several computer runs which differed only in the choice of random numbers referred to in decision-making processes. Such results were obtained; even though the fate of individual members of the speech community varied widely in the different trials, the mean values of the frequency of the grammatical rules in the total population were very similar at identical time periods in each run, for a simulated span of twenty-five years and the structure equilibrium state.	entity;experiment;free variables and bound variables;jovial;monte carlo method;parsing;random number generation;simulation	Sheldon Klein	1966	Mech. Translat. & Comp. Linguistics		monte carlo method in statistical physics	NLP	-31.014894874947004	-83.27108430027701	177086
feeb4671a8081c980789ec6e1fbbebcba50846fa	multi-lingual speaker-independent voice user interface for mobile devices	mobile device;speech synthesis;vocabulary;telephone sets;natural languages;speech based user interfaces;mobile phone;voice commands;feedback;user interfaces natural languages speech recognition speech synthesis mobile handsets vocabulary manufacturing telephone sets feedback acoustic signal detection;speech based user interfaces mobile handsets speech recognition speech synthesis;nokia s60 mobile phones;speaker independent;voice commands multilingual speaker independent voice user interface mobile devices nokia s60 mobile phones speech recognition speech synthesis system speaker independent name dialing;speaker independent name dialing;speech synthesis system;manufacturing;mobile handsets;acoustic signal detection;speech recognition;multilingual speaker independent voice user interface;user interfaces;mobile devices;voice user interface;embedded device	This paper presents a multi-lingual speaker-independent voice user interface (UI) that has been implemented for Nokia S60 mobile phones. The paper concentrates on discussing the specific approach used for achieving a multi-lingual and configurable speech recognition and speech synthesis system. The main applications are speaker-independent name dialing and voice commands. The novelty of the applications is that the user does not need to train the voice dialing system but the application reads the user's phonebook and generates the required voice tags automatically. The speaker-independent voice dialing has already been introduced in regions where the language diversity is not so great. The system presented in this paper is the first of its kind to support both speech recognition and speech synthesis in more than 40 languages in embedded devices with strict memory and performance requirements	embedded system;mobile phone;requirement;s60 (software platform);speech recognition;speech synthesis;voice command device;voice user interface	Juha Iso-Sipilä;Marko Moberg;Olli Viikki	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660212	voice activity detection;speech recognition;computer science;voice tag;mobile device;multimedia;speech synthesis	Robotics	-23.242342916167505	-86.94773789172605	177221
03edd8efa97bc5a5f878278e7cf1ebf2d4c1070a	rapid training of acoustic models using graphics processing unit		Robust and accurate speech recognition systems can only be realized with adequately trained acoustic models. For common languages, state-of-the-art systems are now trained on thousands of hours of speech data. Even with a large cluster of machines the entire training process can take many weeks. To overcome this development bottleneck we propose a new framework for rapid training of acoustic models using highly parallel graphics processing units (GPUs). In this paper we focus on Viterbi training and describe the optimizations required for effective throughput on GPU processors. Using a single NVIDIA GTX580 GPU our proposed approach is shown to be 51x faster than a sequential CPU implementation, enabling a moderately sized acoustic model to be trained on 1000 hours of speech data in just over 9 hours. Moreover, we show that our implementation on a two-GPU system can perform 67% faster than a standard parallel reference implementation on a high-end 32-core Xeon server. Our GPU-based training platform empowers research groups to rapidly evaluate new ideas and build accurate and robust acoustic models on very large training corpora.	acoustic cryptanalysis;acoustic model;central processing unit;computer graphics;geforce 500 series;graphics processing unit;reference implementation;server (computing);speech recognition;text corpus;throughput	Senaka Buthpitiya;Ian R. Lane;Jike Chong	2011			graphics processing unit;speech recognition;computer science;computer hardware	ML	-22.040967173216615	-88.06927408974508	178089
e8f753208fc354fa9aeb3fa9c6acb3d45e7eac7b	definite description lexical choice: taking speaker's personality into account		In Natural Language Generation (NLG), Referring Expression Generation (REG) lexical choice is the subtask that provides words to express a given input meaning representation. Since lexical choices made in real language use tend to vary greatly across speakers, computational models of lexicalisation have long addressed the issue of human variation in the REG field as well. However, studies of this kind will often rely on large collections of pre-recorded linguistic examples produced by every single speaker of interest, and on every domain under consideration, to obtain meaning-to-text mappings from which the lexicalisation model is built. As a result, speaker-dependent lexicalisation may be impractical when suitable annotated corpora are not available. As an alternative to corpus-based approaches of this kind, this paper argues that differences across human speakers may be at least partially influenced by personality, and presents a personality-dependent lexical choice model for REG that is, to the best of our knowledge, the first of its kind. Preliminary results show that our personality-dependent approach outperforms a standard lexicalisation model (i.e., based on meaning-to-text mappings alone), and that the use of personality information may be a viable alternative to strategies that rely on corpus knowledge.	choice modelling;computation;computational model;lexical choice;natural language generation;referring expression generation;text corpus	Alexander Lan;Ivandré Paraboni	2018			artificial intelligence;natural language processing;definite description;lexical choice;personality;computer science	NLP	-32.06527254235439	-81.29260972730185	178267
2e0f5b35129b89804eb1ad4ea51e9b29f5753b61	assessing the post-editing effort for automatic and semi-automatic translations of dvd subtitles		With the increasing demand for fast and accurate audiovisual translation, subtitlers are starting to consider the use of translation technologies to support their work. An important issue that arises from the use of such technologies is measuring how much effort needs to be put in by the subtitler in post-editing (semi-)automatic translations. In this paper we present an objective way of measuring post-editing effort in terms of time. In experiments with English-Portuguese subtitles, we measure the post-editing effort of texts translated using machine translation and translation memory systems. We also contrast this effort against that of translating the texts without any tools. Results show that post-editing is on average 40% faster than translating subtitles from scratch. With our best system, more than 69% of the translations require little or no postediting.	experiment;machine translation;postediting;semiconductor industry;translation memory	Sheila C. M. de Sousa;Wilker Aziz;Lucia Specia	2011			natural language processing;simulation;speech recognition;computer science;multimedia	HCI	-24.328929676761312	-81.16621715403248	178358
b0759859af9d89211fe23b6a885cd06ba8101a3a	modified polyphone decision tree specialization for porting multilingual grapheme based asr systems to new languages	asr systems;decision tree;multilingual acoustic model;acoustic modeling;rapid porting of asr systems;acoustic signal processing;multilingual asr;automatic speech recognition;grapheme based acoustic models;modified polyphone decision tree;cost efficiency;porting multilingual grapheme;speech recognition;decision trees automatic speech recognition natural languages dictionaries switches costs acceleration cultural differences speech recognition vocabulary;multilingual grapheme models;multilingual acoustic model automatic speech recognition modified polyphone decision tree porting multilingual grapheme asr systems pronunciation dictionary multilingual grapheme models;multilingual asr automatic speech recognition grapheme based acoustic models rapid porting of asr systems;decision trees;speech recognition acoustic signal processing decision trees;digital divide;pronunciation dictionary	Automatic speech recognition (ASR) systems have been developed only for a very limited number of the estimated 7,000 languages in the world. In order to avoid the evolvement of a digital divide between languages for which ASR systems exist and those without one, it is necessary to be able to rapidly create ASR systems for new languages in a cost efficient way. Grapheme based systems, which eliminate the costly need for a pronunciation dictionary, have been shown to work for a variety of languages. They are thus destined for porting ASR systems to new languages. This paper studies the use of multilingual grapheme based models for rapidly bootstrapping acoustic models in new languages. The cross language performance of a standard, multilingual (ML) acoustic model on a new language is improved by introducing a new, modified version of polyphone decision tree specialization that improves the performance of the ML models by up to 15.5% relative.	acoustic cryptanalysis;acoustic model;automated system recovery;cost efficiency;decision tree;dictionary;partial template specialization;speech recognition	Sebastian Stüker	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518593	natural language processing;speech recognition;computer science;decision tree	Robotics	-21.767361124541075	-85.84439746110341	178363
6329f02353db4d8e7fc65058d36aaa05c82f153a	semantic annotations for conversational speech: from speech transcriptions to predicate argument structures	argument structure;semantic annotation;dialog act;speech processing;automatic generation;speech processing interactive systems learning artificial intelligence;machine learning;part of speech tagging;speech man machine systems humans contracts machine learning training data tagging taxonomy statistics natural languages;learning artificial intelligence;interactive systems;constituent chunking level conversational speech speech transcriptions predicate argument structures advanced dialog systems machine learning part of speech tagging	In this paper, we describe the semantic content, which can be automatically generated, for the design of advanced dialog systems. Since the latter will be based on machine learning approaches, we created training data by annotating a corpus with the needed content. Given a sentence of our transcribed corpus, domain concepts and other linguistic levels ranging from basic ones, i.e. part-of-speech tagging and constituent chunking level, to more advanced ones, i.e. syntactic and predicate argument structure (PAS) levels are annotated. In particular, the proposed PAS and taxonomy of dialog acts appear to be promising for the design of more complex dialog systems. Statistics about our semantic annotation are reported.	argument map;dialog system;machine learning;part-of-speech tagging;shallow parsing	Arianna Bisazza;Marco Dinarelli;Silvia Quarteroni;Sara Tonelli;Alessandro Moschitti;Giuseppe Riccardi	2008	2008 IEEE Spoken Language Technology Workshop	10.1109/SLT.2008.4777841	natural language processing;speech recognition;speech corpus;computer science;machine learning;speech processing;dialog system;linguistics	NLP	-21.117778593367092	-85.01041074479721	178518
0ba00f9835803ab7511f96cf0f3225489edad0e6	circsim-tutor: an intelligent tutoring system using natural language dialogue	misspelled word;syntactic construction;lexical item;blood pressure;sentence fragment;dialogue-based intelligent tutoring system;circsim-tutor version;natural language;cardiovascular physiology;natural language dialogue	CIRCSIM-Tutor version 2, a dialogue-based intelligent tutoring system (ITS), is nearly five years old. It conducts a conversation with a student to help the student learn to solve a class of problems in cardiovascular physiology dealing with the regulation of blood pressure. It uses natural language for both input and output, and can handle a variety of syntactic constructions and lexical items, including sentence fragments and misspelled words.	cognitive tutor;input/output;natural language	Martha W. Evens;Ru-Charn Chang;Yoon Hee Lee;Leemseop Shim;Chong-Woo Woo;Yuemei Zhang	1997			natural language processing;computer science;linguistics;communication	NLP	-27.16048713658967	-82.28737567339255	178528
128a9f09f95c8f7d6d84c5fe2d106041191ea9a9	the american english sala-ii data collection		We discuss the collection of the American English SALA-II speech corpus. We focus on how we designed the prompt sheets to ensure maximum variability and on our strategy for recruiting the required 4000 speakers. We also present results on the effectiveness of the phonetically rich sentence. This paper should benefit others who are interested in using this corpus, or who are planning to collect a speech corpus with a large number of speakers.	heart rate variability;mobile phone;speech corpus;weatherstar	Peter A. Heeman	2004			speech recognition;american english;data collection;artificial intelligence;natural language processing;speech corpus;computer science;sentence	NLP	-22.296080878115486	-84.47968034818304	178687
1c94e619f11c95c54fdfd7a93d16eb2a10661909	the production of code-mixed discourse	language membership;model condition;postpositional language;code-switching grammar;lexical borrowing;production model;insertional code-switching;code-mixed discourse;equivalence constraint;code-switching acconting;code-switched sentence production	"""We propose a comprehensive theory of codemixed discourse, encompassing equivalencepoint and insertional code-switching, palindromic constructions and lexical borrowing. The starting point is a production model of code-switching accounting for empirical observations about switch-point distribution (the equivalence constraint), well-formedness of monolingual fragments, conservation of constituent structure and lack of constraint between successive switch points, without invoking any """"code-switching grammar"""". Codeswitched sentence production makes alternate reference to two virtual monolingual sentences, one in each language, and is based on conservative conditions on language labeling of constituents, together with a constraint against real-time """"look-ahead"""" from one code-switch to the next. Selective weakening of model conditions can produce (i) the type of palindromic (or portmanteau) construction occasionally occurring e.g., in switches between prepositional and postpositional languages, (ii) the switching by """"insertion"""" of very specific kinds of constituent reported e.g., for French noun phrases in switching with Arabic and, most important, (iii) lexical borrowing. Borrowing can create ambiguity as to language membership of sentence items, but the model predicts where this can be resolved, and the confirmation of these predictions, based on empirical studies of inflectional morphology, validates key aspects of the"""	galaxy morphological classification;network switch;real-time transcription;turing completeness	David Sankoff	1998			natural language processing;noun phrase;computer science;linguistics;empirical research	NLP	-32.53452779818419	-82.02171984277565	178765
a2b7cb6d0309d10ed63bb5153be99f61eaffb73f	xenophones: an investigation of phone set expansion in swedish and implications for speech recognition and speech synthesis	swedish;speech synthesis;reconocimiento palabra;automatic speech recognition asr;phonetic expansion;specification language;experimental result;second language acquisition sla;second language acquisition;automatic speech recognition;conversion texte parole;engineering and technology;automatic recognition;teknik och teknologier;phonology;resultado experimental;text to speech;sueco;speech recognition;fonologia;multi linguality;sintesis palabra;phonologie;reconnaissance parole;resultat experimental;multilinguisme;suedois;multilingualism;text to speech conversion tts;synthese parole;reconocimiento automatico;multilinguismo;reconnaissance automatique;xenophones	"""In recent years, both automatic speech recognition (ASR) and text-to-speech (TTS) conversion systems have attained quality levels that allow inclusion in everyday applications. One remaining problem to be solved in both these types of applications is that alleged phone inventories of specific languages are commonly expanded with phones from other languages, a problem that becomes the more acute in an increasingly internationalized world where multilingual automatic speech-based services are a desideratum. This paper investigates the nature of phone set expansion in Swedish. The status of these phones is discussed, and since such added phones do not have a phonemic (or allophonic) function, the term 'xenophones' is suggested. The analysis is based on a production study involving 491 subjects, and the observed xenophonic expansion is described in terms of three categories along the """" awareness """" and the """" fidelity """" dimensions. The results show that very few subjects resort to full rephonematization and that xenophonic expansion is the rule, although there is an uneven distribution depending on particular phones, spanning from phones produced by most subjects, to phones produced by almost no subjects. Of the possible explanatory factors analyzed—regional background, gender, age and educational level—the latter is by far the most important. sie keine phonematische (oder allophonische) Funktion haben, wird der Terminus """" Xenophon """" vorgeschlagen. Die Analyse gründet sich auf eine Produktionsstudie mit 491 Informanten, und der beobachtete Xenophonausbau wird in drei Kategorien entlang der Dimensionen """" Bewusstheit """" und """" Getreue """" beschrieben. Die Ergebnisse zeigen, dass sehr wenige Informanten auf vollständige Rephonematisierung zurückgreifen und dass Xenophonausbau die Regel ist, aber die Verbreitung der einzelnen Phone ist ungleichmäßig und erstreckt sich von solchen, die die meisten bis zu solchen, die fast keine Sprecher produzieren. Von den möglichen Erklärungsfaktoren, die analysiert wurden – regionale Herkunft, Geschlecht, Alter und Bildungsgrad, ist der letztere bei weitem der wichtigste. 4 Resumé Au cours de ces dernières années, les systèmes de reconnaissance automatique de la parole ainsi que les systèmes de synthèse de la parole à partir du texte ont atteint une qualité leur permettant d'être intégrés à des applications quotidiennes. Un problème demeure cependant quant à ces applications, à savoir, l'expansion fréquente des inventaires de phones d'une langue spécifique à des phones issus d'autres langues, problème d'autant plus important dans un monde où les services automatisés de la parole multilingues sont un souhait. Cet article examine la nature de l'expansion des phones …"""	eine and zwei;file spanning;inventory;linear algebra;sie (file format);speech recognition;speech synthesis	Robert Eklund;Anders Lindström	2001	Speech Communication	10.1016/S0167-6393(00)00097-2	natural language processing;speech technology;speech recognition;specification language;second-language acquisition;computer science;linguistics;speech synthesis	Security	-29.847617846275888	-87.04073926183734	178921
805c60a22534012c04207767586c8f1ca2e52051	bootstrapping text-to-speech for speech processing in languages without an orthography	speech data models speech recognition decoding synthesizers speech processing;speech synthesis;speech processing;synthetic voices german language english language automatic word segmentation automatic speech recognition speech data text to speech bootstrapping speech processing speech synthesis technology well defined writing system single speaker database;languages without an orthography speech synthesis synthesis without text;speech recognition;bootstrapping;natural language processing;speech synthesis bootstrapping natural language processing speech processing speech recognition	Speech synthesis technology has reached the stage where given a well-designed corpus of audio and accurate transcription an at least understandable synthesizer can be built without necessarily resorting to new innovations. However many languages do not have a well-defined writing system but such languages could still greatly benefit from speech systems. In this paper we consider the case where we have a (potentially large) single speaker database but have no transcriptions and no standardized way to write transcriptions. To address this scenario we propose a method that allows us to bootstrap synthetic voices purely from speech data. We use a novel combination of automatic speech recognition and automatic word segmentation for the bootstrapping. Our experimental results on speech corpora in two languages, English and German, show that synthetic voices that are built using this method are close to understandable. Our method is language-independent and can thus be used to build synthetic voices from a speech corpus in any new language.	language-independent specification;speech corpus;speech processing;speech recognition;speech synthesis;synthetic intelligence;text corpus;text segmentation;transcription (software)	Sunayana Sitaram;Sukhada Palkar;Yun-Nung Chen;Alok Parlikar;Alan W. Black	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639221	voice activity detection;voxforge;natural language processing;speech technology;speech production;cued speech;audio mining;speech recognition;speech corpus;computer science;speech;motor theory of speech perception;speech processing;acoustic model;chinese speech synthesis;speech segmentation;speech synthesis;bootstrapping;speech analytics	NLP	-20.54700826953421	-84.54660038058262	179075
43b3f6a9eb1c8dd2152a1963a01dd00b86b7dab2	dialogue management in vector-based call routing	n-gram term;last case;appropriate destination;center handling hundred;human operator;dialogue management;financial service;vector-based call routing;hand-routed call;routing behavior;disambiguation question;call router	"""This paper describes a domain independent, automatically trained call router which directs customer calls based on their response to an open-ended """"How may ldirect your call?"""" query. Routing behavior is trained from a corpus of transcribed and handorouted calls and then carried out using vector-based information retrieval techniques. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent. Our approach is domain independent and the training process is fully automatic. Evaluations over a financial services call center handling hundreds of activities with dozens of destinations demonstrate a substantial improvement on existing systems by correctly routing 93.8% of the calls after punting 10.2% of the calls to a human operator."""	centrality;dialog system;information retrieval;n-gram;nonlinear gameplay;question answering;rejection sampling;relevance;router (computing);routing;test set;word-sense disambiguation	Jennifer Chu-Carroll;Bob Carpenter	1998			call volume;call management;financial services;computer science;data mining;call setup success rate;call control;system dynamics;world wide web	OS	-22.820117014310853	-85.53873016982105	179498
f91df09bf2dc7abfe1d2d3e87a0777cf873a2b2e	interface model for accessible forums for blind, deaf and non-disabled people		The objective of this article is to present an interface model that enables the integration of deaf, blind and non-disabled people on internet forums, considering their different linguistic features. Therefore we designed a theoretical model that uses Sign Language (LS) Sign Writing (SW), text and speech. The shortage of automatic SL translators makes it difficult to implement the interface and its evaluation, undermining researches in the area of digital accessibility.	accessibility;digital rights management;sl (complexity);shattered world;theory	Carla da Silva Flor;Ronnie Fagundes de Brito;Douglas Kaminski;Vânia Ribas Ulbricht;Tarcísio Vanzin	2013		10.1007/978-3-642-39473-7_55	human–computer interaction;multimedia	HCI	-28.678487487267414	-84.0113673869396	179613
050ee3c3949ee4a0747bbac8758368e583bbbc27	spelling correction using context	system filter;alternative correction target;semantic check;phrase context;spelling correction system;dialogue context;intelligent tutor;spelling corrector;sentence context;natural language dialogue	This paper describes a spelling correction system that functions as part of an intelligent tutor that carries on a natural language dialogue with its users. The process that searches the lexicon is adaptive as is the system filter, to speed up the process. The basis of our approach is the interaction between the parser and the spelling corrector. Alternative correction targets are fed back to the parser, which does a series of syntactic and semantic checks, based on the dialogue context, the sentence context, and the phrase context.	algorithm;lexicon;natural language;parsing;spell checker	Mohammad Ali Elmi;Martha W. Evens	1998			natural language processing;speech recognition;computer science;linguistics	NLP	-27.041320128186555	-83.39294024276063	179762
f66709cd08194ceb56e6a602d9d163d6358fc9aa	enabling technology for multilingual natural language generation: the kpml development environment	development environment;natural language generation	Natural language generation is now moving away from research prototypes into more practical applications. Generation functionality is also being asked to play a more signi cant role in established applications such as machine translation. In both cases, multilingual generation techniques have much to o er. However, the take-up of multilingual generation is being restricted by a critical lack both of large-scale linguistic resources suited to the generation task and of appropriate development environments. This paper describes kpml, a multilingual development environment that o ers one possible solution to these problems. Kpml aims to provide generation projects with standardized, broad-coverage, reusable resources and a basic engine for using such resources for generation. A variety of focused debugging aids ensure e cient maintenance, while supporting multilingual work such as contrastive language development and automatic merging of independently developed resources. Kpml is based on a new, generic approach to multilinguality in resource description that extends signi cantly beyond previous approaches. The system has already been used in a number of large generation projects and is freely available to the generation community.	debugging;documentation;drafter;economy of second life;fabio paternò;gist;hartley (unit);home page;input/output;machine translation;michael j. fischer;natural language generation;open road tolling;overhead (computing);usability;www	John A. Bateman	1997	Natural Language Engineering	10.1017/S1351324997001514	natural language processing;speech recognition;computer science;development environment	NLP	-28.990955240175882	-84.43183915226705	180405
7c658ad1613496ee4aba59ef4d0bb18fdfcf065a	sqel: a multilingual and multifunctional dialogue system	information need	Within the EC-funded project SQEL, the German EVAR spoken dialogue system has been extended with respect to multilinguality and multifunctionality. The current demonstrator can handle four di erent languages and domains: German, Slovak, and Czech (and their national train connections), and Slovenian (European ights). The SQEL demonstrator can also access databases on the WWW, which enables users without an internet connection to meet their information needs by just using the phone. The system starts up with a German opening phrase and the user is free to use any of the implemented languages. A multilingual word recognizer implicitly identi es the language, which is then associated with the appropriate domain and database. For the remainder of the dialogue, the corresponding monolingual recognizer is used instead. Experiments to date have shown that the multilingual and the (respective) monolingual recognizers attain comparable word accuracy rates, although the former is less e cient. The existence of language-independent task parameters, such as goal and source location, has meant that porting the system to a new language involves mainly the development of lexica and grammars (apart from the word recognizers) and not an extensive restructuring of the interpretation process within the Dialogue Manager. The latter is su ciently exible to switch between the di erent domains and languages. 1. The EVAR Dialogue System The spoken dialogue system EVAR has been connected to the German public telephone network since 1994 to answer enquiries on German InterCity train connections [3, 2]. One of the ambitions regarding EVAR has been to render it multifunctional. The application should be generalised to cover not just train connections, but also other means of transport, as well as hotel and holiday reservations. The rst step towards this direction has been the development of the SQEL demonstrator, which covers multiple languages and domains. In Section 2, the multilingual recognizer and the multifunctional dialogue manager are Erkennen Verstehen Antworten R uckfragen (Recognize, Understand, Reply, Ask back) described, including preliminary results with the former. Then in Section 3, the connection of the system to the World-Wide-Web is explained. 2. Multilinguality and Multifunctionality The multifunctionality of the EVAR system was tested in the framework of the EC-funded Copernicus project COP1634 SQEL (Spoken Queries in European Languages) [6]. The goal was partly to enhance the functionality of the system with regards to a number of domains, namely ight and train information. The main aim, however, was to achieve multilinguality for EVAR, that is the system should be capable of operating across the German, Slovak, Slovenian, and Czech languages. The core of this research has been the development of a multilingual word recognizer (Section 2.1) and the extension of the already exible Dialogue Manager of EVAR (Section 2.2), giving rise to the SQEL demonstrator. 2.1. Speech Recognition One of the major tasks of a multilingual dialogue system is the recognition of the user utterances. Inside the SQEL system, this is done by a multilingual Speech Recognizer (SR). One method to perform multilingual speech recognition is to run all existing recognizers in parallel and choose the most probable word chain. To reduce the computational load, a single recognizer was built instead that contains the words from all languages in its dictionary. The basis for our multilingual SR is a series of monolingual SRs. Semi-continuous HMMs are used for acoustic modelling and bigrams for linguistic modelling. The monolingual recognizers are trained in the ISADORA environment which uses polyphones with maximum context as subword units [5]. The development of the multilingual SR involved the following steps: 1. The number of codebook density functions was increased to re ect the language-dependent codebooks. In the case of two languages, for example, with a codebook of 256 density functions for each, the multilingual recognizer would have 512 density functions. 5th International Conference on Spoken Language Processing (ICSLP 98) Sydney, Australia November 30 -December 4, 1998 ISCA Archive http://www.isca-speech.org/archive 2. Special weight coe cients were added to the HMM output density functions to re ect the increased number of available density functions. The new weight coe cients were set to zero, so that every density function belonging to di erent languages bears no e ect on the output probability of the HMM. 3. A special bigram model was constructed which consists of the monolingual bigrams and does not allow any transitions between languages, as shown in Equation 1. P (wordlanguagei jwordlanguagej ) = 0 for i 6= j (1) 4. A special silence category was established for language-speci c silence models, which allows transition to and from every language, so that the language can be switched by means of inserting pauses. In order to re ect the quality of the acoustic models for the di erent languages, an additional a priori value was introduced for each language. In theory, there will only be word chains in the spoken language after a few seconds, using the standard beam search in forward decoding. The e ect is that the number of words inside the active vocabulary will be the same as when using the respective monolingual recognizers. Experimental Results Our approach to multilingual speech recognition has been evaluated with the four languages of the SQEL project; German, Slovenian, Slovak and Czech. Because of the special silence category used, the recognized word chain can contain words from di erent languages. In order to assess the accuracy, the language of the word chain is determined on the basis of the number of words in each language, selecting the one with the most words. All words found in other languages are deleted from the recognized word chain, each one counting as a deletion error. In the context of a dialogue system, only the rst user utterance will be processed by the multilingual SR. The language identi ed at that point will be adopted for the whole of the remaining dialogue, which involves the use of a monolingual SR. As shown in Table 1, the monolingual SRs are still superior to the multilingual SR, because of the instances of language identi cation failure salient in the latter. These failure instances occur especially within short sentences, as the time available for a robust discrimination between languages is insu cient in these cases (Table 2). In evaluating the monoand the multilingual SRs on utterances with more than 5 words, there are only slight di erences in the corresponding word accuracy rates, but the language identi cation rates are signi cantly higher. The Real Time Factor (RTF) for the multilingual system is more than two times higher than for monolingual recognizers with the language already established. However, the multilingual system is nearly twice as fast as using 4 monolingual recognizers in parallel. The reason for this is that, at the beginning, all possible languages are inside the beam and Recognition Rates R Recognizer (Word Accuracy) T SloveSlovak Czech German F nian Mono 88% 1 Slovenian (90%) Mono 88% 1 Slovak (88%) Mono 84% 1.3 Czech (83%) Mono 90% 1.2 German (91%) Multi 83% 86% 84% 84% 2.5 (87%) (85%) (83%) (86%) Table 1: Recognition rates and Real Time Factor (RTF) using monolingual and multilingual speech recognizers on all sentences of the SQEL test corpus; the recognition rates for sentences longer than 5 words are shown in brackets. Test Set SloveSlovak Czech German	acoustic cryptanalysis;archive;beam search;bigram;codebook;database;dialog system;dictionary;entropic value at risk;finite-state machine;hidden markov model;information needs;international symposium on computer architecture;internet access;language-independent specification;lexicon;multi-function printer;speech recognition;spoken dialog systems;substring;test set;vocabulary;www;world wide web	Maria Aretoulaki;Stefan Harbeck;Florian Gallwitz;Elmar Nöth;Heinrich Niemann;Jozef Ivanecký;Ivo Ipsic;Nikola Pavesic;Václav Matoušek	1998			speech recognition;information needs;computer science	NLP	-22.05231954098071	-85.55249262361639	180412
47a700d129c9a155ac15671ec4b608c7aed8560d	duration normalization and hypothesis combination for improved spontaneous speech recognition	hidden markov model;word error rate	When phone segmentations are known a priori, normalizing the duration of each phone has been shown to be effective in overcoming weaknesses in duration modeling of Hidden Markov Models (HMMs). While we have observed potential relative reductions in word error rate (WER) of up to 34.6% with oracle segmentation information, it has been difficult to achieve significant improvement in WER with segmentation boundaries that are estimated blindly. In this paper, we present simple variants of our duration normalization algorithm, which make use of blindly-estimated segmentation boundaries to produce different recognition hypotheses for a given utterance. These hypotheses can then be combined for significant improvements in WER. With oracle segmentations, WER reductions of up to 38.5% are possible. With automaticallyderived segmentations, this approach has achieved a reduction of WER of 3.9% for the Broadcast News corpus, 6.2% for the spontaneous register of the MULT_REG corpus, and 7.7% for a spontaneous corpus of connected Spanish digits collected by Telefónica Investigación y Desarrollo.	algorithm;database normalization;hidden markov model;markov chain;oracle nosql db;speech recognition;spontaneous order;word error rate	Jon P. Nedel;Richard M. Stern	2003			word error rate;phone;speech recognition;oracle;artificial intelligence;hidden markov model;computer science;utterance;pattern recognition;normalization (statistics)	NLP	-19.129878652703177	-86.46493622976162	180614
104bca087863214853e935f1b59e101e354d23cc	zero alignment of verb arguments in a parallel treebank		This paper analyses several points of interlingual dependency mismatch on the material of a parallel Czech-English dependency treebank. Particularly, the points of alignment mismatch between the valency frame arguments of the corresponding verbs are observed and described. The attention is drawn to the question whether such mismatches stem from the inherent semantic properties of the individual languages, or from the character of the used linguistic theory. Comments are made on the possible shifts in meaning. The authors use the findings to make predictions about possible machine translation implementation of the data.	comment (computer programming);data structure alignment;machine translation;natural language;predicate (mathematical logic);relocation (computing);treebank;unification (computer science)	Jana Sindlerová;Eva Fucíková;Zdenka Uresová	2015			natural language processing;linguistics;programming language	NLP	-32.79772903335059	-82.00234443423977	180748
5b813e871a96aae48b11633f83dd49badf6c591f	a hybrid hmm/autoregressive time-delay neural network automatic speech recognition system	dynamic acoustic features hybrid hmm autoregressive time delay neural network automatic speech recognition system asr systems complex phonetic features germination stress backpropagation algorithm ar tdnn based experts post processors system complex language particularities static acoustic features;speech recognition autoregressive processes backpropagation hidden markov models neural nets;hidden markov models abstracts delay effects gold speech	This paper describes a new hybrid approach which aims to significantly improve the performance of Automatic Speech Recognition (ASR) systems when they are confronted with complex phonetic features such as germination, stress or relevant lengthening of vowels. The underlying idea of this approach consists of dividing the global task of recognition into simple and well-defined sub-tasks and using hearing/perception-based cues. The sub-tasks are assigned to a set of suitable Time-Delay Neural Networks using an autoregressive version of the backpropagation algorithm (AR-TDNN). When they are incorporated in the hybrid structure, the AR-TDNN-based experts act as post-processors of a HMM-based system which thus acquires the ability to overcome failures due to complex language particularities. Results of experiments using either static or dynamic acoustic features show that the proposed HMM/AR-TDNN system outperforms that of the HMM-based system.	acoustic cryptanalysis;algorithm;artificial neural network;automated system recovery;autoregressive model;backpropagation;baseline (configuration management);central processing unit;experiment;hidden markov model;hybrid system;performance;point of view (computer hardware company);speech recognition;time delay neural network	Sid-Ahmed Selouani;Douglas D. O'Shaughnessy	2002	2002 11th European Signal Processing Conference		speech recognition;computer science;pattern recognition;time delay neural network;communication	NLP	-19.182533076817148	-87.05641185289474	180999
6184affcc42bc75b0fad60ad03f329a9fede8e3d	optimizing the number of states, training iterations and gaussians in an hmm-based handwritten word recognizer	gaussian mixture;handwriting recognition;hidden markovmodel hmm;hidden markov model;optical character recognition;training strategy;state number optimization training iteration optimization hmm based handwritten word recognizer off line handwriting recognition hidden markov models training algorithms baum welsh procedure transition optimization hmm architecture output distributions optimization strategies hmm classifier continuous feature values gaussian mixtures;baum welch;hidden markov models;system design;feature extraction;state number optimization;handwritten word recognition;gaussian distribution handwriting recognition handwritten character recognition hidden markov models feature extraction optical character recognition;gaussian processes hidden markov models handwriting recognition gaussian distribution vocabulary maximum likelihood estimation computer science electronic mail character recognition speech recognition;gaussian distribution;handwritten character recognition;training algorithm	"""In off-line handwriting recognition, classifiers based on hidden Markov models (HMMs) have become very popular. However, while there exist well-established training algorithms , such as the Baum-Welsh procedure, which optimize the transition and output probabilities of a given HMM architecture , the architecture itself, and in particular the number of states, must be chosen """" by hand """". Also the number of training iterations and the output distributions need to be defined by the system designer. In this paper we examine some optimization strategies for an HMM classifier that works with continuous feature values and uses the Baum-Welch training algorithm. The free parameters of the optimization procedure introduced in this paper are the number of states of a model, the number of training iterations, and the number of Gaussian mixtures for each state. The proposed optimization strategies are evaluated in the context of a handwritten word recognition task."""	baum–welch algorithm;existential quantification;finite-state machine;handwriting recognition;hidden markov model;iteration;lempel–ziv–welch;markov chain;mathematical optimization;online and offline;optimizing compiler;systems design;welch's method	Simon Günter;Horst Bunke	2003		10.1109/ICDAR.2003.1227710	normal distribution;speech recognition;feature extraction;computer science;baum–welch algorithm;machine learning;pattern recognition;optical character recognition;hidden markov model;systems design	ML	-19.52385411935729	-91.92681777726817	181458
713e7479051fd023d75fba5d55b993f923d052be	asr error detection in a conversational spoken language translation system	word boundary detection automatic speech recognition asr error detection conversational speech translation smt confidence estimation;baseline error detector conversational spoken language translation system automatic speech recognition error detection statistical machine translation asr decoder hypothesized word sequence information streams error robust cslt system smt confidence named entity detection ned automated word boundary detector acoustic prosodic features false alarm rate detection rate improvement;language translation;statistical analysis;speech recognition;statistical analysis language translation speech recognition;feature extraction speech decoding detectors training acoustics speech recognition	Detection of automatic speech recognition (ASR) errors is crucial to preventing their further propagation through statistical machine translation (SMT) in conversational spoken language translation (CSLT) systems. In this paper, we venture beyond traditional features obtained from the ASR decoder and hypothesized word sequence, and explore additional information streams provided by an error-robust CSLT system, including SMT confidence estimates and posteriors from named entity detection (NED). Another significant novelty of this work is the use of an automated word boundary detector based on acoustic-prosodic features to verify the existence of ASR-hypothesized word boundaries, which further improves ASR error detection. Offline evaluation on a test set designed to invoke ASR errors showed that at 10% false alarm rate, the proposed features provide 2.8% absolute (4.2% relative) improvement in detection rate over a state-of-the-art baseline error detector that uses a rich set of features traditionally employed in the existing literature.	acoustic cryptanalysis;automated system recovery;automatic system recovery;baseline (configuration management);error detection and correction;named entity;online and offline;software propagation;speech recognition;statistical machine translation;test set	Wei Chen;Sankaranarayanan Ananthakrishnan;Rohit Kumar;Rohit Prasad;Premkumar Natarajan	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639104	natural language processing;speech recognition;word error rate;computer science;pattern recognition;mathematics;translation;statistics	NLP	-19.74675953429456	-82.02437711172965	181522
562f4f3320e51c7a36c4e2e303a4255bc05b81e2	robust speech recognition based on a bayesian prediction approach	bayesian prediction;gaussian noise;robust speech recognition;minimax decision;gaussian mixture;prediction bayes;gaussian processes;reconocimiento palabra;ambient noise;bayes methods;estudio comparativo;decision a posteriori;ruido gaussiano;additive noise;gender difference;ruido aditivo;prior distribution;bruit additif;bayes prediction;awgn;awgn speech recognition bayes methods hidden markov models prediction theory gaussian processes signal classification;speech recognition bayesian methods testing hidden markov models noise robustness viterbi algorithm additive white noise uncertainty predictive models decoding;decision minimax;indexing terms;test;classification;experimental result;ensayo;etude comparative;essai;gaussian white noise;automatic recognition;bruit ambiant;hidden markov models;prediction theory;speaker independent;ruido ambiente;robustesse;bruit gaussien;signal classification;viterbi decoder;comparative study;resultado experimental;speech recognition;ruido blanco;robustness;decodage viterbi;reconnaissance parole;resultat experimental;bruit blanc;continuous density hidden markov model;mismatch conditions robust speech recognition bayesian prediction training conditions testing conditions test data prior distribution constrained uniform distribution cdhmm pretrained gaussian mixture continuous density hidden markov models mean vectors uncertainty model compensation technique bayesian predictive density robust decision strategy viterbi bayesian predictive classification viterbi decoding algorithm speaker independent recognition experiments isolated digits ti connected digit strings automatic speech recognition additive gaussian white noise additive ambient noise gender difference experimental results;article;clasificacion;viterbi decoding;white noise;reconocimiento automatico;desciframiento viterbi;reconnaissance automatique;uniform distribution;robustez	In this paper, we study a category of robust speech recognition problem in which mismatches exist between training and testing conditions, and no accurate knowledge of the mismatch mechanism is available. The only available information is the test data along with a set of pretrained Gaussian mixture continuous density hidden Markov models (CDHMM’s). We investigate the problem from the viewpoint of Bayesian prediction. A simple prior distribution, namely constrained uniform distribution, is adopted to characterize the uncertainty of the mean vectors of the CDHMM’s. Two methods, namely a model compensation technique based on Bayesian predictive density and a robust decision strategy called ViterbiBayesian predictive classification are studied. The proposed methods are compared with the conventional Viterbi decoding algorithm in speaker-independent recognition experiments on isolated digits and TI connected digit strings (TIDIGITS), where the mismatches between training and testing conditions are caused by: 1) additive Gaussian white noise, 2) each of 25 types of actual additive ambient noises, and 3) gender difference. The experimental results show that the adopted prior distribution and the proposed techniques help to improve the performance robustness under the examined mismatch conditions.	additive white gaussian noise;bayesian network;decision theory;experiment;hidden markov model;markov chain;speech recognition;test data;utility functions on indivisible goods;viterbi algorithm;white noise	Hui Jiang;Keikichi Hirose;Qiang Huo	1999	IEEE Trans. Speech and Audio Processing	10.1109/89.771309	speech recognition;computer science;machine learning;pattern recognition;mathematics;white noise;viterbi decoder;hidden markov model;statistics	ML	-19.89414192971477	-91.70051622773447	181601
ffb7a1a40b4ea5f1244107ce3be82737d5de4e75	experiments on reducing footprint of unit selection tts system	katedra kybernetiky;kybernetika;informacni a řidici systemy;automaticke řizeni;uměla inteligence;publications experiments on reducing footprint of unit selection tts system;publikace experiments on reducing footprint of unit selection tts system	The quality of speech produced by modern TTS systems utilizing the unit selection approach is very high. However, the system demands are enormous. The storage requirements are directly proportional to the size of speech unit inventory from which the units are selected during the synthesis process. This paper presents the analysis and reduction experiments performed on two large speech corpora employed by a unit selection TTS system for the Czech language. A procedure for exclusion of utterances from the default speech corpus based on statistics of the usage of particular speech units was proposed. The exclusion of whole utterances from the corpus was preferred over the exclusion of individual speech units in order to preserve the fundamental feature of the unit selection method – selection of possibly longest sequences of speech units. Experiments were performed for several reduction levels. Resulting synthetic speech was evaluated by a proposed statistics based on the concatenation points density. Moreover, the speech quality was evaluated in listening tests. All reduced versions of TTS system were evaluated as similar or slightly worse than the baseline system.	experiment;netware file system	Zdenek Hanzlícek;Jindrich Matousek;Daniel Tihelka	2013		10.1007/978-3-642-40585-3_32	artificial intelligence;natural language processing;speech recognition;computer science;speech corpus;concatenation;slightly worse;czech;footprint;active listening;speech synthesis	NLP	-20.16206377434935	-83.6261659639076	181659
d4b95f770288393aaa7df9bc13413dc8bfdeee17	the automatic speech recognition engine espere : experiments on telephone speech	hmm;automatic speech recognition;telephone speech;reconnaissance de la parole;speech recognition;parole telephonique	This paper presents our automatic speech recognition engine ESPERE and several results obtained from experiments on telephone speech. ESPERE (Engine for SPEech REcognition) is a HMM-based toolbox for speech recognition allowing the user to choose the modeled unit (word, phone, triphone), define the topology of every Hidden Markov Model, train the models with the Baum-Welch algorithm and evaluate the recognition accuracy on speech databases. To validate the ESPERE toolbox, we have conducted tests on real world data: the recognition of a three-digit code to access a call center. We have investigated the influence of some parameters and some preprocessing algorithms. Finally, combining the best parameters, the recognition score reaches 96.4% at the word level and 92.1% at the sentence level.		Dominique Fohr;Odile Mella;Christophe Antoine	2000			voice activity detection;voxforge;natural language processing;speaker recognition;audio mining;speech recognition;computer science;acoustic model;hidden markov model;speech analytics	NLP	-19.705107655598624	-86.00432449252725	181715
18ed6a8c945ec32ba349d62e6b8d269beeaba2ae	intelligent input software of tibetan	latin transcribing;intelligent input;or phrases;latin coding;tibetan;root coding;high frequency	This paper exhibits an original study and design of an intelligent input method of Tibetan based on phrases. A system is developed to input Tibetan phrases through only one-step transcribing using Tibetan-Latin transcribing method. This study successfully solved the challenge of consistent, accurate, and convenient Tibetan inputting using Latin transcribing of syllables and words. The system has newly formed words or phrases, and high frequency word first and dynamic adjustment ability, so it can reach an intelligent level. Experiments demonstrated that the system's input speed improves by a large margin.		Wang Wei-lan	2007	Computer Standards & Interfaces	10.1016/j.csi.2006.11.001	speech recognition;telecommunications;high frequency	HCI	-26.507722392747983	-83.09188337498865	181771
9cce0dbeffd47941ed9cf381a21c58bd77952620	the bbn single-phonetic-tree fast-match algorithm.		In this paper we present a very fast and accurate fast-match algorithm which, when followed by a regular beam search restricted within only the subset of words selected by the fastmatch, can speed up the recognition process by at least two orders of magnitude in comparison to a typical single-pass speech recognizer utilizing the Viterbi (or Beam) search algorithm. This is a novel fast-match algorithm that has two important properties: high-accuracy recognition and run-time proportional to only the cube root of the vocabulary size.	beam search;finite-state machine;search algorithm;speech recognition;speedup;vocabulary	Long Nguyen;Richard M. Schwartz	1998			speech recognition;pattern recognition;artificial intelligence;computer science	AI	-21.618833118454788	-87.86340256293126	182225
79838f1724d0616accccfaa778a22740b0d320b1	spoken language systems	command and control	The objective of this project is to develop a realtime spoken language system capable of understanding and responding to spoken English commands and queries for interactive humanmachine applications, such as battle management, command and control, and training of personnel on complex tasks. The system will also include a capability to adapt to new speakers and a capability to detect when a user says a new word, and to allow the user to add the word to the system.	protologism	John Makhoul	1989			natural language processing;speech technology;speaker recognition;cued speech;speech recognition;speech corpus;computer science;speech;communication;speech analytics	NLP	-24.55299383401717	-85.72347551004408	182607
128b49bb01218f00a68de7f0ea849b992da0dbcb	an analysis of hmm-based prediction of articulatory movements	modelizacion;evaluation performance;performance evaluation;coeficiente correlacion;maximum likelihood;hidden markov model;feature modeling;evaluacion prestacion;electromagnetisme;modele markov variable cachee;erreur quadratique moyenne;maximum vraisemblance;text input;articulatory features;probabilistic approach;articulatory feature;carta de datos;algorithme;modelisation;algorithm;hidden markov models;mean square error;enfoque probabilista;approche probabiliste;mappage;parameter generation;signal classification;classification signal;signal acoustique;context dependent;mapping;electromagnetism;acoustic signal;root mean square;electromagnetismo;error medio cuadratico;classification automatique;correlation coefficient;automatic classification;modeling;clasificacion automatica;coefficient correlation;maxima verosimilitud;senal acustica;algoritmo	This paper presents an investigation into predicting the movement of a speaker’s mouth from text input using hidden Markov models (HMM). A corpus of human articulatory movements, recorded by electromagnetic articulography (EMA), is used to train HMMs. To predict articulatory movements for input text, a suitable model sequence is selected and a maximum-likelihood parameter generation (MLPG) algorithm is used to generate output articulatory trajectories. Unified acoustic-articulatory HMMs are introduced to integrate acoustic features when an acoustic signal is also provided with the input text. Several aspects of this method are analyzed in this paper, including the effectiveness of context-dependent modeling, the role of supplementary acoustic input, and the appropriateness of certain model structures for the unified acoustic-articulatory models. When text is the sole input, we find that fully context-dependent models significantly outperform monophone and quinphone models, achieving an average root mean square (RMS) error of 1.945 mm and an average correlation coefficient of 0.600. When both text and acoustic features are given as input to the system, the difference between the performance of quinphone models and fully context-dependent models is no longer significant. The best performance overall is achieved using unified acoustic-articulatory quinphone HMMs with separate clustering of acoustic and articulatory model parameters, a synchronous-state sequence, and a dependent-feature model structure, with an RMS error of 0.900 mm and a correlation coefficient of 0.855 on average. Finally, we also apply the same quinphone HMMs to the acoustic-articulatory, or inversion, mapping problem, where only acoustic input is available. An average root mean square (RMS) error of 1.076 mm and an average correlation coefficient of 0.812 are achieved. Taken together, our results demonstrate how text and acoustic inputs both contribute to the prediction of articulatory movements in the method used. 2010 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;acoustic fingerprint;algorithm;cluster analysis;coefficient;context-sensitive language;experiment;feature model;finite-state machine;hidden markov model;iterative method;junichi iijima;markov chain;mathematical optimization;mean squared error;netware file system;norm (social);text corpus;viterbi algorithm	Zhen-Hua Ling;Korin Richmond;Junichi Yamagishi	2010	Speech Communication	10.1016/j.specom.2010.06.006	speech recognition;systems modeling;root mean square;electromagnetism;computer science;machine learning;context-dependent memory;pattern recognition;mathematics;mean squared error;maximum likelihood;hidden markov model;statistics	NLP	-20.531980301520186	-90.24064327950308	183372
3acd2eb6cbfa37b514a8ccc2ef0ead6f8bdc9e33	an approach to automatic language identification based on language-dependent phone recognition	duration model;information sources;neural networks;oglts database;decoding;neural nets;modeling automatic language identification language dependent phone recognition language dependent recognizers database information sources backward bigram language models forward bigram context dependent duration models hidden markov models neural network oglts database system performance correct rate utterances experiments;hidden markov model;hidden markov models natural languages context modeling decoding power system modeling neural networks spatial databases viterbi algorithm speech;language dependent recognizers;speech processing;correct rate;database;utterances;speech;natural languages;system performance;backward bigram;grammars;hidden markov models;viterbi algorithm;forward bigram;spatial databases;context dependent duration models;experiments;language identification;speech recognition;context dependent;power system modeling;modeling;context modeling;automatic language identification;language model;neural network;language models;language dependent phone recognition;neural nets speech recognition speech processing hidden markov models natural languages grammars	An approach to language identification (LID) based on language-dependent phone recognition is presented. A variety of features and their combinations extracted by language-dependent recognizers were evaluated based on the same database. Two novel information sources for LID were introduced: (1) forward and backward bigram based language models, and (2) context-dependent duration models. An LID system using hidden Markov models and neural network was developed. The system was trained and evaluated using the OGLTS database. For a six-language task, the system performance (correct rate) for 45-second long utterances and 10-second long utterances reached 91-96% and 81-08% respectively. The experiments demonstrated the importance of detailed modeling and the method by which these information sources are combined.	language identification	Yonghong Yan;Etienne Barnard	1995		10.1109/ICASSP.1995.479743	natural language processing;language identification;speech recognition;systems modeling;viterbi algorithm;computer science;speech;context-dependent memory;pattern recognition;context model;natural language;artificial neural network	NLP	-20.30517531738852	-87.15609505271574	183733
8f2a176367dd6d096890da5d6653264516b0329d	articulation and acoustics of /i/ in preboundary position in french	electro magnetic;intonational phrase;accentual phrase	HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Articulation and acoustics of /i/ in pre-boundary position in French Marija Tabain, Pascal Perrier	acoustic cryptanalysis;archive;biconnected component;comefrom;hal;jun wang (scientist);linear algebra;modernpascal	Marija Tabain;Pascal Perrier	2005	J. Phonetics	10.1016/j.wocn.2004.04.003	speech recognition;acoustics;electromagnetism;computer science;linguistics	Vision	-30.175100661019368	-86.76192706562479	184332
b7a3c83427c4c562fe85e042d5139ed02043c7ee	adaptive tutorial dialogue systems using deep nlp techniques	generation technique;dialogue history;tutorial dialogue system;dialogue management;deep natural language processing;deep nlp technique;adaptive tutorial dialogue system;student performance;tentative answer;different domain;natural language processing	We present tutorial dialogue systems in two different domains that demonstrate the use of dialogue management and deep natural language processing techniques. Generation techniques are used to produce natural sounding feedback adapted to student performance and the dialogue history, and context is used to interpret tentative answers phrased as questions.	automatic sounding;dialog system;feedback;natural language processing	Myroslava O. Dzikovska;Charles B. Callaway;Elaine Farrow;Manuel Marques-Pita;Colin Matheson;Johanna D. Moore	2007			natural language processing;computer science;artificial intelligence	NLP	-29.79606625453382	-80.39506287991745	184619
fbdfd93a11766701e8398647000a7340e279781b	implementation of an extended recognition network for mispronunciation detection and diagnosis in computer-assisted pronunciation training		This paper presents recent extensions to our ongoing effort in developing speech recognition for automatic mispronuncia tion detection and diagnosis in the interlanguage of Chinese lea rners of English. We have developed a set of context-sensitive pho nological rules based on cross-language (Cantonese versus En glish) analysis which has also been validated against commo n mispronunciations observed from the learners interlangua ge. These rules are represented as finite state transducers whic h can generate an extended recognition network (ERN) based on arbitrary canonical pronunciations. The ERN includes not onl y standard English pronunciations but also common mispronun ciations of learners. Recognition with the ERN enables the spe ch recognizer to phonetically transcribe the learner’s input speech. This transcription can be compared with the canonical pronu nciations to identify the location(s) and type(s) of phoneti c d fferences, thus facilitating mispronunciation detection a nd diagnoses. We have developed a prototype implementation known as the CHELSEA system and have validated the approach based on a new, annotated test set of 600 utterances recorded from 1 00 Cantonese learners of English. The approach achieves a fals e rejection rate (i.e. system identifies a phone as incorrect w hen it is actually correctly pronounced) of 13.6%; as well as a fa lse acceptance rate (i.e. system identifies a phone as correct wh n it is actually mispronounced) of 44.7%. Among the detected errors, the system can correctly diagnose 54.8% of the mispr onunciations.	commo;context-sensitive grammar;finite-state machine;finite-state transducer;prototype;rejection sampling;speech recognition;test set;transcription (software)	Alissa M. Harrison;Wai Kit Lo;Xiaojun Qian;Helen M. Meng	2009			natural language processing;artificial intelligence;speech recognition;computer science;phone;standard english;test set;phonological rule;pronunciation;interlanguage;pattern recognition	NLP	-19.53352827195909	-85.02271306559007	184751
2d64077ecc905e78a11764c8a64c5ea038eaacfe	a telephone number inquiry system with dialog structure	knowledge based system telephone number inquiry system dialog structure interactive system telephone speech recognition word sequence probability key feature matching;probability;maximum likelihood;telephony speech recognition natural languages spatial databases hidden markov models mel frequency cepstral coefficient cepstrum cities and towns information retrieval rail transportation;feature matching;feature extraction telephony speech recognition interactive systems knowledge based systems probability pattern matching;telephony;pattern matching;feature extraction;speech recognition;interactive systems;knowledge based systems;knowledge base	A Telephone Number Inquiry System (TNIS) answers caller the phone number he/she wants to know. Traditional system requires the caller to know the full name of the party [1] [7]. If the caller forgets the name, the system fails to retrieve correct information for the caller. In this paper, we propose a novel TNIS with dialog structure that can let caller use a more flexible method while inquiring, i.e., the caller may interact with our system to inquire the phone number by providing just the working, researching area, the surname, or the title, etc. Our system takes the telephone speech as input, after generating the word sequence, it performs a maximum likelihood key-feature matching with knowledge base. If necessary information is not derived, interactive dialog manager is activated to resolve the caller’s requirement. The experimental results show that our novel approach can make the system more natural.	dialog manager;knowledge base;telephone number	Hsien-Chang Wang;Jhing-Fa Wang	1998		10.1109/ICASSP.1998.674400	knowledge base;speech recognition;feature extraction;computer science;machine learning;pattern matching;pattern recognition;probability;dialog system;maximum likelihood;telephony;statistics	AI	-22.084621565846085	-86.3820958218629	184939
a1ecdb2f2cba1ef19bce11748471de96522136a2	dynamic context generation for natural language understanding: a multifaceted knowledge approach	belief networks;bayesian network;natural languages;indexing terms;grammars;natural language understanding;neural net;natural language;associative memory;semantic associations dynamic context generation natural language understanding multifaceted knowledge knowledge representation context dependent model text understanding parsing bayesian network;context dependent;natural languages natural language processing humans glass associate members neural networks associative memory bayesian methods context modeling;knowledge representation;belief networks natural languages grammars knowledge representation;shallow parsing	We describe a comprehensive framework for text understanding, based on the representation of context. It is designed to serve as a representation of semantics for the full range of interpretive and inferential needs of general natural language processing. Its most distinctive feature is its uniform representation of the various simple and independent linguistic sources that play a role in determining meaning: lexical associations, syntactic restrictions, case-role expectations, and most importantly, contextual effects. Compositional syntactic structure from a shallow parsing is represented in a neural net-based associative memory, where it then interacts through a Bayesian network with semantic associations and the context or “gist” of the passage carried forward from preceding sentences. Experiments with more than 2000 sentences in different languages are included.	artificial neural network;bayesian network;content-addressable memory;gist;inferential programming;natural language processing;natural language understanding;shallow parsing	Samuel W. K. Chan;James Franklin	2003	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/TSMCA.2003.811129	natural language processing;language identification;knowledge representation and reasoning;natural language programming;multinet;computer science;machine learning;natural language;artificial neural network	AI	-29.098813254155495	-80.49512399680818	185018
e33cd89ef83cab569384ddc401988fdae4016f67	context-sensitive language modeling for large sets of proper nouns in multimodal dialogue systems	dialogue system;noun;recognition accuracy;in focus subset context sensitive language modeling proper nouns multimodal dialogue systems recognition accuracy metropolitan region;metropolitan region;indexing terms;context modeling natural languages web pages computer science artificial intelligence laboratories error analysis training data robustness cities and towns;context sensitive languages;context sensitive language modeling;error rate;in focus subset;speech recognition;speech recognition context sensitive languages interactive systems;multimodal dialogue systems;proper nouns;interactive systems;language model	We explore several language modeling strategies for increasing the recognition accuracy among large sets of proper nouns in a map- based multimodal dialogue system which provides restaurant information. In particular, we evaluate several mechanisms for exploiting dialogue context, the two most promising of which involve a semi- static metropolitan-region based large set of proper nouns competing with a smaller, in-focus subset. We show that these techniques decrease word, concept, and proper noun error rates under several training conditions. We also present a technique to generalize sparse training data through derived templates to improve language model robustness.	context-sensitive language;dialog system;dialog tree;knowledge-based configuration;language model;multimodal interaction;sparse matrix;synthetic intelligence	Alexander Gruenstein;Stephanie Seneff	2006	2006 IEEE Spoken Language Technology Workshop	10.1109/SLT.2006.326834	natural language processing;noun;speech recognition;index term;word error rate;computer science;machine learning;proper noun;linguistics;language model	NLP	-20.44387964609336	-86.7910553949118	185054
cb2d2f850a2066be76aa27d1c4a0faf214ee9bd7	the interactive systems labs view4you video indexing system		The recognition of broadcast news is a challenging problem in speech recognition. To achieve the long-term goal of robust, real-time news transcription, several problems have to be overcome, e.g. the variety of acoustic conditions and the unlimited vocabulary. Recently, a number of sites have been working on content-addressable multi-media information sources. In the presented paper, we focus on extending this work towards a multi-lingual environment, where queries and multimedia documents may appear in multiple languages. In cooperation with the Informedia project at CMU [4], we attempt to provide cross-lingual access to German and Serbo-Croatian newscasts. 1. THE VIEW4YOU SYSTEM In the View4You system, German and Serbocroatian public newscasts are recorded daily using standard consumer electronics equipment. The newscasts are automatically segmented and an index is created for each of the segments by means of automatic speech recognition. The user can query the system in natural language. The system returns a list of segments which is sorted by relevance with respect to the user query. By selecting a segment, the user can watch the corresponding part of the news show on his or her computer screen. In this work, we give an overview over the three main parts of the View4You system, namely the segmenter, the speech recognizer, and the information retrieval engine.	acoustic cryptanalysis;computer monitor;finite-state machine;information retrieval;natural language;real-time transcription;relevance;speech recognition;transcription (software);vocabulary	Thomas Kemp;Petra Geutner;Michael Schmidt;Borislav Tomaz;Manfred Weber;Martin Westphal;Alexander H. Waibel	1998			artificial intelligence;computer vision;regular polygon;search engine indexing;pattern recognition;video processing;computer science;planar	ML	-22.716426031603334	-84.19666968506738	185085
6bcec9319544fb2913ccb13ccedcbc1013fc1e41	extracting semantic roles from a model of eventualities	semantic role	"""The notion of semantic roles is usually attributed to Fillmore [8], however its history can be traced back through TesniSre [16] to Panini. Following this tradition, many researchers recognize their usefulness in the description of language even if they do not agree on their significance [7]. However, a weak or strong commitment to this notion does not elude the fact that it proves to be very difficult to settle on a finite set of labels along with their formal definitions. The d i lemma resulting from this challenge is well known: to require a univocal identification by each role results in an increase in their number while to abstract their semantic content gives rise to an inconsistent set. If a finite set is possible, one has to find a proper balance between these two extremes. As a result, every flavor of roles have been used from time to time in linguistics (e.g., GB, in the spirit of Fillmore, HPSG, in the line of situation semantics), and also in AI [10, see also 4]. Between the total refusal to use those labels (as in GPSG) and the acceptance of individual roles (as in HPSG) there is a wide range of proposals on what consti tute a good set o f L(inguistic)-Roles [7] and, as a consequence, on the way to differentiate between them and define them. Most of the definitions have been based on the referential properties that can be associated with each role bearer (e.g. an AGENT is a volitional animate entity). Even if this approach is necessary at one time or another, this kind of definition inevitably leads to either the """"let 's create another role"""" or the """"let 's abstract its definition"""" syndromes. Properties are not always of the static kind though. Sometimes, dynamic properties are also used (e.g. an AGENT is the perceived instigator of the action). Since one of the desired characteristic of a roles system is the power to discriminate events [5] (another """"desired"""" property being to offer an easier selection of grammatical functions), the recognition of semantic roles should be linked to the interpretation of the event, that is to their dynamic properties. In a study on locative verbs in French, Boons [3] has convincingly shown the importance of taking into account aspectual criteria in the description of a process, suggesting that GOAL and SOURCE roles should be reinvestigated in the light of those criteria. It is our hypothesis that proliferation of roles is a natural phenomenon caused by the specialized properties required by the interpretation of a predicate within a specific semantic field: to overlook these properties yields the over-generalization already mentionned. The best way to approach the expansion/contraction dilemma is to search for the minimal relations required for a dynamic interpretation of events (in terms of their aspectual criteria and through an identification of all the participants in i0. Our first step toward this abstraction was to consider each par t ic ipant ( individuals or properties) either as a localized entity (a token) or a location (a place), and to determine its role in the realization of the process expressed by the predicate. The model exhibits some common points with a localist approach [1,11] since it recognizes (in an abstract sense) the importance of spatio-temporal """"regions"""" in the process of individuation of events [14]. To express the change of localization (again in an abstract sense), the notion of transitions is used. The entire construction is inspired by Petri net theory [15]: a set S of places, a set T of transitions, a flow relation F: (S x T) ~ (T x S) and markers are the categories used to define the structure of a process (and as a consequence of the events composing it). For example, the dynamic representation of Max embarque la caisse sur le cargo [3J/Max embarks the crate on the cargo boat can be analyzed in two steps. First there is a transition from an initial state IS where the crate is not on the cargo boat to a final state FS where the crate is on the cargo boat. The final state can be expressed by the static passive, la caisse est embarqude sur le cargo~the crate was embarked on the cargo boat, and is schematized in (2). One of the argument (cargo boat) is used as a localization while the other argument is used as a localized entity (crate), the THEME according to Gruber [9]. The initial state can be expressed (in this case) by the negation of the final state and is schematized in (1). The realization of the entire process is then represented by the firing of the net which can be illustrated by the snapshots (1) and (2). 1. Is:t~ir-~O:Fs 2. IS:O---[---(~):Fs To integrate the participation of """"Max"""" in the model, we recognize the importance of"""	agent;generalized phrase structure grammar;gigabyte;head-driven phrase structure grammar;linear algebra;petri net;snapshot (computer storage);tom gruber	Sylvie Ratté	1991			semantic field;machine learning;discrete mathematics;dilemma;artificial intelligence;petri net;negation;predicate (grammar);semantic role labeling;computer science;finite set;abstraction	NLP	-33.593988443375494	-81.6651514761725	185152
607a6f331c3f31d1c6ca4b04fa73e820ba27d4a5	recent enhancements in cu vocal for chinese tts-enabled applications.	unit selection;application program interface;general intelligence;electronic book;text to speech	CU VOCAL is a Cantonese text-to-speech (TTS) engine. We use a syllable-based concatenative synthesis approach to generate intelligible and natural synthesized speech [1]. This paper describes several recent enhancements in CU VOCAL. First, we have augmented the syllable unit selection strategy with a positional feature. This feature specifies the relative location of a syllable in a sentence and serves to improve the quality of Cantonese tone realization. Second, we have developed the CU VOCAL SAPI engine, a version of the synthesizer that eases integration with applications using SAPI (Speech Application Programming Interface). We demonstrate the use of CU VOCAL SAPI in an electronic book (e-book) reader. Third, we have made an initial attempt to use the CU VOCAL SAPI engine in Web content authored with Speech Application Language Tags (SALT). The use of SALT tags can ease the task of invoking Cantonese TTS service on webpages.	application programming interface;concatenative synthesis;e-book;ietf language tag;microsoft speech api;netware file system;speech application language tags;speech synthesis;syllable;tip (unix utility);web content	Helen M. Meng;Yuk-Chi Li;Tien Ying Fung;Man Cheuk Ho;Chi-Kin Keung;Tin Hang Lo;Wai Kit Lo;Pak-Chung Ching	2003			natural language processing;speech recognition;application programming interface;computer science;artificial intelligence;g factor;speech synthesis	NLP	-20.35299060664617	-83.30139525679175	185171
92b10ffb51f2a1a55ac1a0eb83cc40a603ecc918	the 2016 signal separation evaluation campaign		In this paper, we report the results of the 2016 communitybased Signal Separation Evaluation Campaign (SiSEC 2016). This edition comprises four tasks. Three focus on the separation of speech and music audio recordings, while one concerns biomedical signals. We summarize these tasks and the performance of the submitted systems, as well as provide a small discussion concerning future trends of SiSEC.		Antoine Liutkus;Fabian-Robert Stöter;Zafar Rafii;Daichi Kitamura;Bertrand Rivet;Nobutaka Ito;Nobutaka Ono;Julie Fontecave Jallon	2017		10.1007/978-3-319-53547-0_31		OS	-24.47620931394363	-84.30664445287536	185378
a0900b229da3570c4d3f3edb7ab28e90012ebad1	lexicon, meaning relations, and semantic networks.		This paper proposes a fresh formulation of conceptually grounded meaning relations by way of construction of certain well-defined relations over the lexicon of a natural language. These relations are constrained by the logical structures of linguistic meanings across sentence and discourse contexts. One of the biggest advantages of such meaning relations is that they are not defined over, or do not ride on, the syntactic structure of a given language. Nor do they turn on compositional relations for the computation of meaning values. This helps in the formulation of meaning relations to be defined on the symbolic elements of a lexicon on the one hand, and to be extracted from the surface structure of linguistic constructions on the other. This has consequences not merely for the nature of lexical meaning but also for the construction of a kind of (shallow) semantic networks that can be used for semantic processing in natural language understanding or machine translation systems that are driven by a kind of shallow processing of linguistic meanings. Thus, this paper aims to show the usefulness of a kind of conceptually based characterization of linguistic meaning for its relevance to computational language processing.	computation;construction grammar;lexicon;machine translation;natural language understanding;relevance;semantic network	Prakash Chandra Mondal	2018			natural language processing;lexicon;semantic network;artificial intelligence;computer science	NLP	-32.89446518468633	-81.1042523479938	185719
f17da3d29b7bc1d4152fd947c57ef9ccc19132c6	word error rate minimization using an integrated confidence measure	cable television;modelizacion;word error rate;news;tecnologia electronica telecomunicaciones;document audiovisuel;metodo entropia maxima;support vector machines;taux erreur;japonais;confidence measure;implementation;speech processing;maquina vector soporte;tratamiento palabra;traitement parole;posterior probability;modelisation;machine vecteur support;reconocimiento voz;documento audiovisual;probabilite a posteriori;signal classification;probabilidad a posteriori;audiovisual document;teledistribution;classification signal;error rate;speech recognition;noticias;reconnaissance parole;support vector machine;n best rescoring;classification automatique;word error rate minimization;tecnologias;implementacion;grupo a;methode entropie maximum;automatic classification;indice error;actualites;modeling;clasificacion automatica;method of maximum entropy;japones;teledistribucion;maximum entropy;japanese	This paper describes a new criterion of speech recognition using an integrated confidence measure for minimization of the word error rate (WER). Conventional criteria for WER minimization obtain an expected WER of a sentence hypothesis merely by comparing it with other hypotheses in an n-best list. The proposed criterion estimates the expected WER by using an integrated confidence measure with word posterior probabilities for a given acoustic input. The integrated confidence measure, which is implemented as a classifier based on maximum entropy (ME) modeling, is used to get probabilities reflecting whether the word hypotheses are correct or incorrect. The classifier comprises a variety of confidence measures and can deal with a temporal sequence of them in order to attain a more reliable confidence. Our proposed criterion achieved a WER of 7.5% and a 2.6% improvement relative to conventional n-best rescoring methods in transcribing Japanese broadcast news under noisy field and spontaneous speech conditions.	acoustic cryptanalysis;principle of maximum entropy;speech recognition;spontaneous order;word error rate	Akio Kobayashi;Kazuo Onoe;Shoei Sato;Toru Imai	2005	IEICE Transactions	10.1093/ietisy/e90-d.5.835	support vector machine;speech recognition;word error rate;computer science;artificial intelligence;machine learning;pattern recognition;speech processing;statistics	ML	-21.045136693354664	-91.00856517748984	186183
fb51f16a0e3963b649d9950c0a58ff077ec1ecff	template based techniques for automatic segmentation of tts unit database	segmental k means unit database segmentation tts template based segmentation one pass dp;hidden markov models speech databases context heuristic algorithms speech coding context modeling;unit database segmentation;text to speech systems template based techniques tts unit database automatic segmentation unit selection based tts template based forced alignment segmentation one pass dynamic programming framework dp framework multitemplate representation modified k means algorithm mkm algorithm context independent templates context dependent templates segmental k means algorithm phone class mkm modeling embedded reestimation procedure hmm based modeling hmm based segmentation timit database phonetic segmentation phonetic labeling ground truth template based segmentation algorithms syllabic indian language tts spectral distortions sd time aligned speech utterances high resolution segmentation;tts;speech synthesis audio databases distortion dynamic programming hidden markov models natural language processing signal representation signal resolution;one pass dp;segmental k means;template based segmentation	We address the problem of automatic segmentation of the unit database in unit-selection based TTS and propose template based forced alignment segmentation in the one-pass dynamic programming (DP) framework with several variants: i) multi-template representation derived by modified K-means (MKM) algorithm, ii) context-independent and context-dependent templates for reduced multi-template representation, iii) segmental K-means algorithm with MKM modeling of phone classes, as a template-based equivalent of the conventional embedded re-estimation procedure for HMM based modeling and segmentation, that is typical for deriving unit-databases for TTS (e.g. EHMM in Festival). We first benchmark the performance of the proposed segmentation framework on TIMIT database for phonetic segmentation given the availability of phonetic labeling ground truth in TIMIT. We then apply the proposed template based segmentation algorithms for syllabic Indian language TTS, and benchmark the proposed segmentation using objective measures based on spectral distortions (SD) obtained on time-aligned speech utterances and compare it with other recent segmentation approaches, namely the group-delay (GD) based semiautomatic method, Hybrid method, EHMM, HMM and SKM-HMM and show that the proposed template based approaches offer comparable and better spectral distortions, validating their ability to provide accurate high-resolution segmentation of the unit-database.	algorithm;benchmark (computing);context-sensitive language;database;distortion;dynamic programming;embedded system;ground truth;group delay and phase delay;hidden markov model;image resolution;k-means clustering;log-spectral distance;mathematical knowledge management;netware file system;stellar classification;timit	S. Adithya;Sunil Rao;C. Mahima;S. Vishnu;Mythri Thippareddy;V. Ramasubramanian	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472750	natural language processing;speech recognition;computer science;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	-19.482981660794245	-85.94146680662789	186201
18ccba1065c6d434cd01c5dd7e4a297d83c95b42	generating grammar exercises	generating grammar;grammar exercise;specific learning goal;simple sentence;distinct class;language acquisition;generation technique;real life sentence	Grammar exercises for language learning fall into two distinct classes: those that are based on “real life sentences” extracted from existing documents or from the web; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used. In this paper, we introduce a framework (called GramEx) which permits generating the second type of grammar exercises. Using generation techniques, we show that a grammar can be used to semi-automatically generate grammar exercises which target a specific learning goal; are made of short, simple sentences; and whose vocabulary is restricted to that used in a given textbook.	modifier key;parse tree;parsing;real life;semiconductor industry;vocabulary	Laura Perez-Beltrachini;Claire Gardent;Germán Kruszewski	2012			grammar systems theory;natural language processing;link grammar;speech recognition;operator-precedence grammar;regular grammar;computer science;affix grammar;regular tree grammar;phrase structure rules;emergent grammar;linguistics;relational grammar;attribute grammar;unrestricted grammar;lexical grammar	NLP	-27.604326119482955	-82.28907033820174	186355
03032828d61d45c6cbb2a47bf85f0dfd578d052f	a markov-chain monte-carlo approach to musical audio segmentation	prior probability distribution;bayesian framework;audio segmentation;histograms;markov chain monte carlo approach;audio signal processing;hidden markov models probability distribution noise reduction data mining histograms educational institutions labeling audio recording bayesian methods music;bayes methods;automatic segmentation;bayesian methods;audio recording;data mining;statistical distributions;hidden markov models;markov chain monte carlo;noise reduction;probability distribution;bayesian framework markov chain monte carlo approach musical audio segmentation prior probability distribution;musical audio segmentation;markov processes;statistical distributions audio signal processing bayes methods markov processes monte carlo methods music;music;monte carlo methods;labeling	This paper describes a method for automatically segmenting and labelling sections in recordings of musical audio. We incorporate the user's expectations for segment duration as an explicit prior probability distribution in a Bayesian framework, and demonstrate experimentally that this method can produce accurate labelled segmentations for popular music	bayesian network;experiment;markov chain monte carlo	Christophe Rhodes;Michael A. Casey;Samer A. Abdallah;Mark B. Sandler	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1661396	probability distribution;speech recognition;computer science;pattern recognition;mathematics;hidden markov model;statistics	Robotics	-20.627270294763697	-93.00271774842926	186573
a57d9e52329dd420f9395c8f6287654517e45a7c	an analysis of ill-formed input in natural language queries to document retrieval systems	natural language;document retrieval	The most common errors were missing conjunc­ tions; 18.31 per cent of the queries lacked a needed con­ junction. In almost all of these, the English conjunction and was needed to connect a list of words in a scries. The next most serious problem was punctuation errors; these were cither incorrect, missing or misplaced punctuation. Most involved the comma or quotation mark combined with other punctuation markj. The queries containing punctuation errors totaled 6.81 per cent. The third most common type of ill-formed input was spelling errors; 2.34 per cent of the queries had a spelling error. Since some of the spelling errors had been corrected when the data was entered for the computer analysis, it is almost certain Uiat the original queries contained more spelling errors. We did not find any co-occurrence violations such as subjccl/vcrb disagreement or use of ellipsis. Slightly less than one third of the queries examined contained some form of error. This rate is comparable to that found in an earlier study (Eastman and McLean, 1981). However, at least one earlier study found almost no instances of ill-formed input (Fineman, 1983).	angela mclean (biologist);document retrieval;natural language;total loss	Charlene W. Young;Caroline M. Eastman;Robert L. Oakman	1990	Inf. Process. Manage.	10.1016/0306-4573(91)90002-4	natural language processing;document retrieval;speech recognition;computer science;natural language;information retrieval	Web+IR	-26.455453793548674	-80.38993868981296	186755
3abbf34abbb8075ea3c0773587fd8df605a93e2c	automatic recognition of spontaneous speech dialogues.	spontaneous speech	Some approaches for coping with the problem of recognition of spontaneous speech dialogues are presented. Starting from a HMM-based system, developed for dictation tasks, some modi cations are introduced at the acoustic level and in the language model. Acoustic model parameters are modi ed to account for speaking rate variations, and speci c models of extra-linguistic phenomena are de ned and added to the language model. Di erent acoustic models are managed by a single recognizer through their integration into a multi-model search space. Experiments and evaluations were conducted on a spontaneous dialogue corpus collected at our laboratory.	acoustic cryptanalysis;acoustic model;finite-state machine;hidden markov model;language model;spontaneous order	Mauro Cettolo;Daniele Falavigna	1998			computer science	NLP	-19.218828353229917	-86.90668384505759	186878
b1a9b751614067458e0d7b0acb578b9910d6035b	morpheme-based and factored language modeling for amharic speech recognition	lattice rescoring;amharic;word recognition;morpheme based language modeling;speech recognition;factored language modeling;language model	This paper presents the application of morpheme-based and factored language models in an Amharic speech recognition task. Since the use of morphemes in both acoustic and language models often results in performance degradation due to a higher acoustic confusability and since it is problematic to use factored language models in standard word decoders, we applied the models in a lattice rescoring framework. Lattices of 100 best alternatives for each test sentence of the 5k development test set have been generated using a baseline speech recognizer with a word-based backoff bigram language model. The lattices have then been rescored by means of various morpheme-based and factored language models. A slight improvement in word recognition accuracy has been observed with morpheme-based language models while factored language models led to notable improvements in word recognition accuracy.	language model;speech recognition	Martha Yifiru Tachbelie;Solomon Teferra Abate;Wolfgang Menzel	2009		10.1007/978-3-642-20095-3_8	natural language processing;cache language model;speech recognition;factored language model;computer science;linguistics;language model	NLP	-19.79440367198142	-84.99857002065042	187115
7a9860acacf9eb870f066c3e55f5a4504992312d	annotation of heterogeneous multimedia content using automatic speech recognition	broadcast news;robust speech recognition;automatic speech recognition;test collection	This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.	algorithm;archive;grams;hypertext transfer protocol;n-gram;noise reduction;real life;speech recognition;test data;test set;word error rate	Marijn Huijbregts;Roeland Ordelman;Franciska de Jong	2007		10.1007/978-3-540-77051-0_8	voice activity detection;natural language processing;audio mining;speech recognition;speech corpus;computer science;multimedia;speech analytics	NLP	-21.93428487310145	-84.17141491285301	187319
87aebd8b6322c34b36711db8bd6939a693ef96f1	contribution à l'étude et à la reconnaissance automatique de la parole en arabe standard. (contribution at study and automatic recognition of speech in standard arabic)			linear algebra	Mahieddine Djoudi	1991				NLP	-29.837310512797835	-86.72871890895331	187362
119ecf444a9d01616be69504f115cb5c92b2e536	the thoughtful elephant: strategies for spoken dialog systems	databases;lenguaje natural;natural language interfaces;information resources;interfase usuario;human interaction;natural language interfaces speech based user interfaces speech recognition interactive systems vocabulary;architecture systeme;history;automatic switchboards;reconocimiento palabra;user interface;application specific knowledge;spoken dialog system;confidence measure;spoken dialog systems;vocabulary;langage naturel;tratamiento lenguaje;natural languages;vocabulary history large scale systems robustness information resources delay databases system testing natural languages glass;indexing terms;natural language understanding spoken dialog systems travel domain automatic switchboards large scale directory assistance human interaction vocabulary database constraints sentence hypothesis dialog history application specific knowledge;glass;speech based user interfaces;systeme conversationnel;large scale;natural language understanding;sentence hypothesis;language processing;interactive system;elephants;natural language;traitement langage;consistency checking;sistema conversacional;travel domain;large scale directory assistance;speech recognition;system testing;arquitectura sistema;robustness;interface utilisateur;reconnaissance parole;database constraints;system architecture;dialog history;interactive systems;large scale systems	In this paper we present technology used in spoken dialog systems for applications of a wide range. They include tasks from the travel domain and automatic switchboards as well as large scale directory assistance. The overall goal in developing spoken dialog systems is to allow for a natural and flexible dialog flow similar to human–human interaction. This imposes the challenging task to recognize and interpret user input, where he/she is allowed to choose from an unrestricted vocabulary and an infinite set of possible formulations. We therefore put emphasis on strategies that make the system more robust while still maintaining a high level of naturalness and flexibility. In view of this paradigm, we found that two fundamental principles characterize many of the proposed methods: 1) to consider available sources of information as early as possible, and 2) to keep alternative hypotheses and delay the decision for a single option as long as possible. We describe how our system architecture caters to incorporating application specific knowledge, including, for example, database constraints, in the determination of the best sentence hypothesis for a user turn. On the next higher level, we use the dialog history to assess the plausibility of a sentence hypothesis by applying consistency checks with information items from previous user turns. In particular, we demonstrate how combination decisions over several turns can be exploited to boost the recognition performance of the system. The dialog manager can also use information on the dialog flow to dynamically modify and tune the system for the specific dialog situations. An important means to increase the “intelligence” of a spoken dialog system is to use confidence measures. We propose methods to obtain confidence measures for semantic items, whole sentences and even full N-best lists and give examples for the benefits obtained from their application. Experiences from field tests with our systems are summarized that have been found crucial for the system acceptance.	dialog manager;dialog system;directory service;eventual consistency;finite-state machine;high-level programming language;human–robot interaction;natural language understanding;plausibility structure;programming paradigm;relational database;schedule;speech recognition;spoken dialog systems;systems architecture;telephone switchboard;unrestricted grammar;usability;vocabulary	Bernd Souvignier;Andreas Kellner;Bernhard Rüber;Hauke Schramm;Frank Seide	2000	IEEE Trans. Speech and Audio Processing	10.1109/89.817453	natural language processing;speech recognition;computer science;machine learning;natural language;systems architecture	NLP	-26.495010509413756	-86.1714318868976	187905
8390372ab37581a902f328e38af8c71aa3b5c43e	a study on invariance of  $f$-divergence and its application to speech recognition	traitement signal;processus gauss;statistical distributions speech recognition information processing information theory information science stochastic processes kernel pattern recognition performance analysis hidden markov models;evaluation performance;kernel;gaussian mixture;performance evaluation;maximum likelihood;gaussian processes;information science;signal audio;japonais;hidden markov model;evaluacion prestacion;acoustic modeling;speech processing;audio signal;modele markov variable cachee;maximum vraisemblance;tratamiento palabra;traitement parole;invarianza;structural representation;methode acoustique;probabilistic approach;maximum likelihood decomposition;maximum likelihood estimation;vocal;invertible transformation;invariance;japanese vowel utterances f divergence speech recognition invertible transformation gaussian mixture invariant structural representation maximum likelihood decomposition;statistical distributions;acoustic method;hidden markov models;reconocimiento voz;stochastic processes;enfoque probabilista;approche probabiliste;signal processing;metodo acustico;information processing;performance analysis;pattern recognition;signal acoustique;speech recognition;voyelle;f divergence;japanese vowel utterances;acoustic signal;reconnaissance forme;gaussian process;reconnaissance parole;reconocimiento patron;proceso gauss;invariance to transformation;vowel;procesamiento senal;speech recognition gaussian processes maximum likelihood estimation;japones;maxima verosimilitud;senal acustica;senal audio;information theory;structural representation f divergence invariance to transformation speech recognition;invariant structural representation;japanese	Identifying features invariant to certain transformations is a fundamental problem in the fields of signal processing and pattern recognition. This correspondence explores a family of measures called f-divergences that are invariant to invertible transformations, and studies their application to speech recognition. We provide novel proofs for the sufficiency and necessity of the invariance of f-divergence. Several techniques to calculate or approximate f-divergences in general cases and for special distributions such as Gaussian and Gaussian mixture are reviewed. We show how to construct an invariant structural representation from sequence data through maximum likelihood decomposition, and prove the invariance of this decomposition. We demonstrate an application of this invariant representation to recognizing connected Japanese vowel utterances. In addition, we propose several techniques to improve the recognition performance. The experimental results show that the invariant structure achieves better performance than hidden Markov models, a widely used technique for acoustic modeling of speech sounds.	acoustic cryptanalysis;approximation algorithm;experiment;gaussian (software);hidden markov model;kullback–leibler divergence;markov chain;pattern recognition;performance;signal processing;speech recognition	Yu Qiao	2010	IEEE Transactions on Signal Processing	10.1109/TSP.2010.2047340	speech recognition;computer science;machine learning;pattern recognition;gaussian process;mathematics;maximum likelihood;hidden markov model;statistics	ML	-20.82052553607517	-92.37869584918666	188137
1c16e6c0ae177b32054b54d3a0cade5f2bfed962	from tone to pitch in sepedi	presentation;tone languages;southern bantu;pitch contours;sepedi	In this talk, I will describe the development of a Malay-English bidirectional speech translation system in the Institute for Infocomm Research, Singapore, as part of the Asian Speech Translation Advanced Research Consortium. I will introduce the basic components and the linguistic resources, in particular, large vocabulary continuous speech recognition, speech synthesis, and machine translation concerning Malay language. I will also discuss the network-based system architecture that supports the real-time speech translation service. Biography Dr Haizhou Li is the Principal Scientist of Human Language Technology at the Institute for Infocomm Research in Singapore. His research interests include automatic speech recognition and machine translation. Dr Li taught in the University of Hong Kong and South China University of Technology (1988-1994). He was a Visiting Professor at CRIN/INRIA in France (1994-1995). He was a Research Manager in Apple-ISS Research Centre (1996-1998), a Research Director in Lernout & Hauspie Asia Pacific (1999-2001), and a Vice President in InfoTalk Corp. Ltd (2001-2003), responsible for Asian language products. In 2009, he was named one of the two Nokia Professors by Nokia Fundation. Dr Li now serves as an Associate Editor of IEEE Transactions on Audio, Speech and Language Processing. He is an elected Board Member of the International Speech Communication Association (ISCA, 2009-2013), an Executive Board Member of the Asian Federation of Natural Language Processing (AFNLP, 20062010).	afnlp;consortium;international symposium on computer architecture;language technology;machine translation;natural language processing;nokia 105 / nokia 105 dual sim;pitch (music);real-time transcription;speech recognition;speech synthesis;systems architecture;vocabulary	Etienne Barnard;Sabine Zerbian	2010			speech recognition;engineering;linguistics;communication	NLP	-24.50380499836922	-85.09560759741747	188389
a024af68b847b0f2bdf6a17d994ac4159a021e22	vowel sound disambiguation for intelligible korean speech synthesis	conference paper	For speech synthesis systems that transform text materials into voice data, correctness and naturalness are the crucial measures of performance, the latter gaining more emphasis recently. In order to make synthesized voices natural, we must take into account pronunciation-related linguistic phenomena such as homograph, among others. The syntax certainly provides an important clue to disambiguating such homographs, but the relatively free word order in the Korean language makes it hard to utilize such information. In this paper, we describe a computational generation of contextually appropriate vowel lengths for the words in Korean by utilizing a higher level of linguistic information in a Combinatory Categorial Grammar framework. We consider parts-ofspeech information, the possibility of conjunction with a suffix, case information, unconjugated adjectives, numerals, numerical adjectives with related nouns, and the relationship between a noun and its predicate as syntactic and semantic clues for vowel sound disambiguation. The results are expressed in Speech Synthesis Markup Language (SSML) for a target system neutral application. The proposed system with correctly predicted vowel sound can be used not only as an educational tool, but also as a plug-in for enhancing the intelligibility of a general purpose Text-to-Speech (TTS) system.	combinatory categorial grammar;correctness (computer science);intelligibility (philosophy);numerical analysis;plug-in (computing);speech synthesis markup language;word-sense disambiguation	Ho-Joon Lee;Jong C. Park	2005			speech recognition;acoustics;linguistics	NLP	-28.04070607007749	-81.50082406401717	188491
8b9c95b25f791c99bbcb1850c3ea7692782c1721	the ixm2 parallel associative processor for ai	semantic memory;transputer systems;real time;speech processing;semantic network;language translation;natural languages parallel machines associative processing content addressable storage speech processing language translation transputer systems;natural languages;artificial intelligent;artificial intelligence associative memory natural languages terminology laboratories speech processing writing dictionaries computer science robustness;associative processing;speech translation;robust performance;associative memory;interpreting telephony ixm2 parallel associative processor artifical intelligence speech to speech translation semantic memory system machine netl semantic network machine massively parallel simd machine associative memories robust performance transputers language translation tasks social implications real time communication;parallel machines;content addressable storage	Describes the IXM2 associative processor and its main application in speech-to-speech translation. The IXM2 is a semantic memory system machine that began as a faithful implementation of the NETL semantic network machine and grew into a massively parallel SIMD machine that has demonstrated the power of large associative memories. Such processors can support robust performance in speech applications. In fact, the IXM2 with 73 transputers has outperformed a Cray in some language-translation tasks. We selected speech-to-speech translation as our main application because it is one of the grand challenges of massively parallel artificial intelligence. The social implications of successful automatic translation are enormous-e.g. people who speak different languages could communicate in real time by using interpreting telephony.<<ETX>>	artificial intelligence;central processing unit;grand challenges;machine translation;real-time computing;simd;semantic network	Tetsuya Higuchi;Ken'ichi Handa;Naoto Takahashi;Tatsumi Furuya;Hitoshi Iida;Eiichiro Sumita;Kozo Oi;Hiroaki Kitano	1994	Computer	10.1109/2.330048	natural language processing;semantic memory;computer science;theoretical computer science;speech processing;natural language;semantic network;programming language	AI	-31.60031280188319	-84.86376082774125	188903
373b2c765355cae4c85383bb5028fb8a629d1ae9	statistical variable length markov chains for the parameterization of stochastic user models from sparse data	user modelling;model performance;statistical method;stochastic processes context probability information processing ergonomics statistical analysis predictive models algorithm design and analysis autocorrelation labeling;sparse data;markov processes;cognitive engineering;user modelling markov processes;statistical variable length markov chains stochastic user models sparse data;user model;markov chain	This paper presents an algorithm for the parameterization of variable length Markov chains (VLMC). The disadvantages of the conventional algorithms especially in the application field of stochastic use models are discussed. The algorithm proposed in this paper eliminates these disadvantages by the usage of statistical methods to decide which states are accepted for the model. The benefit of this procedure is two-fold. First, the resulting models perform better in respect to prediction quality even they contain fewer states. Second, each state gets a more significant meaning. This makes the algorithm suitable for analyzing user-traces. An example for this application is described.	algorithm;markov chain;sparse matrix;tracing (software)	Carsten Winkelholz;Christopher M. Schlick	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399898	markov decision process;econometrics;markov chain;maximum-entropy markov model;markov kernel;user modeling;sparse matrix;markov property;computer science;continuous-time markov chain;machine learning;markov renewal process;markov algorithm;markov process;markov model;hidden markov model;cognitive ergonomics;statistics;variable-order markov model	Robotics	-22.607367411626086	-92.33653544237102	189210
efff31beb7a534423b813affa7705da7df12bf47	automatic quality assessment for speech translation using joint asr and mt features		This paper addresses automatic quality assessment of spoken language translation (SLT). This relatively new task is defined and formalized as a sequence labeling problem where each word in the SLT hypothesis is tagged as good or bad according to a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality, or both (combined ASR+MT). This research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing: ASR output, verbatim transcript, text translation, speech translation and post-edition of translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR feature can bring interesting complementary information. Our robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios.	automated system recovery;experiment;machine translation;sequence labeling;speech recognition;transcription (software)	Ngoc-Tien Le;Benjamin Lecouteux;Laurent Besacier	2016	CoRR		natural language processing;speech recognition;computer science	NLP	-20.649672420090745	-82.586153575556	189213
4969118812153a00adcd5265ad27f30c5585778a	prosody generation by means of a syntactic approach and its application in a text to speech system		An algorithm for modeling and generating prosody from a written text is described in this paper. Among the several speech processing areas which could benefit of this algorithm, in this paper we have dealt with text to speech synthesis (TTS). An experimental evaluation of the algorithm has been carried out and it has been shown that the naturalness of the produced speech has greatly improved.	algorithm;semantic prosody;speech processing;speech synthesis;syntax (logic)	Enzo Mumolo;Massimo Teia	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		natural language processing;speech recognition;speech corpus;computer science;linguistics;chinese speech synthesis	NLP	-19.609942408772024	-83.49369050265021	189415
29e0082f23194970d87d5cf5df4294c5b2760577	augmented role filling capabilities for semantic interpretation of spoken language	argument structure;spoken language systems;semantic interpretation;semantic processing;natural language;speech recognition	"""This paper describes recent work on the Unisys ATIS Spoken Language System, and reports benchmark results on natural language, spoken language, and speech recognition. We describe enhancements to the system's semantic processing for handling non.transparent argument structure and enhancements to the system's pragmatic processing of material in art. swers displayed to the user. We found that the system's score on the natural language benchmark test decreased from ~8~o to 36~ without these enhancements. We also report results for three spoken language systems, Unisys natural language coupled with MIT-Summit speech recognition, Unisys natural language coupled wish MIT-Lincoln Labs speech recognition and Unisys natural language coupled with BBN speech recognition. Speech recognition results are reported on the results of the Unisys natural language selecting a candidate from the MITSummit N-best (N=16). I N T R O D U C T I O N Improving the performance of spoken language systems requires addressing issues along several fxonts, including basic improvements in na tura l language processing and speech recognition as well as issues of integrat ion of these components in spoken language systems. In this paper we report the results of our recent work in each of these areas. * One major area of work has been in the the semantic and pragmat ic components of the Unisys na tura l language processins system. The work in semantics enhances the robustness of semantic processing by allowing parses which do not directly express the argument structure expected by semantics to nevertheless be processed in a rule-governed way. In the area of pragmatics we have extended our techniques for bringing mater ia l displayed to the user into the dialog context to handle several addi t ional classes of references to mater ia l in the display. • This work was supported by DARPA contract N000014-89C0171, administered by the Office of Naval Research. We are gratefuI to Victor Zue of MIT, Doug Paul of MIT Lincoln Laboratories and John Mak.houl of BBN for making output from their speech recognition systems available to us. We also wish to thank Tim Finln, Rich Fritzson, Don McKay, Jim Meldlnger, and Jan Pastor of Unisys and Lynette Hirschnmn of MIT for their contributions to this work. In the area of integrat ion of speech and natura l language, we report on an experiment with three spoken language systems, coupling the same Unisys na tura l language system to three different speech recognisers as shown in Figure 1. ~"""" :.,~:::::::.~::~:::::::~::::::::~,,-::::~,.:: l~.'.-'.;,-',.~i.~~.,.~,.,,...,...,,~,~,.,.i-.,..-,,~:~il ::+ : P t i q 0 t Y x: .."""" Figu re 1: U n l s y s n a t u r a l l a n g u a g e + m u l t i p l e s p e e c h r e c o g n l s e r s We believe this is a very promising technique for evaluating the components of spoken language systems. Using this technique we can make a very straightforward compe~ison of the performance of the recognizers in a spoken language context. Furthermore, this technique also allows us to make a fine-gralned comparision of the interact ion between speech and natura l language in the three systems by looking at such questions as the relative propor t ion of speech recognizer outputs that fail to parse, fall to receive a semantic analysis and so on. Finally, we repor t on speech recognition results obtained by filtering the N-best (N=16) from MIT-Summlt through the Unisys na tura l language system. We note tha t there was a higher error rate for context-dependent speech as compared to context-independent speech (54.6% compared to 45.8~) and suggest two hypotheses which may account for this difference."""	ambient occlusion;automatic transmitter identification system (television);benchmark (computing);context-sensitive language;emoticon;finite-state machine;interaction;invention of the integrated circuit;ll parser;multiprocessing;nl (complexity);natural language;natural language understanding;numerical aperture;parsing;semantic interpretation;speech recognition;summit;victor zue;dialog	Lewis M. Norton;Marcia C. Linebarger;Deborah A. Dahl;Nghi Nguyen	1991			natural language processing;language identification;semantic interpretation;semantic computing;multinet;speech recognition;universal networking language;semantic memory;computer science;semantic compression;linguistics;natural language	NLP	-29.523632525650957	-82.54110115733326	189779
911e19ea5130cabb88f39f405a3d9d3191a5fcce	improving the transcription of academic lectures for information retrieval	adaptation models encyclopedias electronic publishing internet accuracy crawlers;web sites educational technology further education information retrieval;information retrieval;web sites;educational technology;further education;wikipedia academic lecture transcription information retrieval university lectures lecture capture systems video data audio data automatic transcription academic disciplines standard language models ranked word correct rate search ability word recognition	Recording university lectures through lecture capture systems is increasingly common, generating large amounts of audio and video data. Transcribing recordings greatly enhances their usefulness by making them easy to search. However, the number of recordings accumulates rapidly, rendering manual transcription impractical. Automatic transcription, on the other hand, suffers from low levels of accuracy, partly due to the special language of academic disciplines, which standard language models do not cover. This paper looks into the use of Wikipedia to dynamically adapt language models for scholarly speech. We propose Ranked Word Correct Rate as a new metric better aligned with the goals of improving transcript search ability and specialist word recognition. The study shows that, while overall transcription accuracy may remain low, targeted language modelling can substantially improve search ability, an important goal in its own right.	archive;cmu pronouncing dictionary;human-readable medium;information retrieval;language model;lecture recording;medical transcription;microsoft word for mac;reference model;text corpus;transcription (software);wikipedia;word error rate	Audrey Mbogho;Stephen Marquard	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.177	educational technology;further education;computer science;artificial intelligence;machine learning;multimedia;world wide web;information retrieval	NLP	-24.38481887201811	-81.21936732352232	189836
d90bba2b6ba2f28f59e37c19a2bda50cdbc1dd12	application of isolated word recognition to a voice controlled repertory dialer system	control systems;speech synthesis;real time;speech analysis;vocabulary;telephony;word recognition;speech recognition;system testing;robustness;speech recognition control systems speech analysis telephony vocabulary system testing speech synthesis microcomputers real time systems robustness;high speed;microcomputers;real time systems	In this paper we describe a speaker trained, voice controlled, repertory dialer system. The main elements of the system include: 1. A real-time speech analyzer that detects the presence of speech on the input line, and analyzes the speech to give features appropriate for a word recognizer. 2. An isolated word recognizer that decides which of a set of words was spoken. 3. A voice response system to provide spoken commands to the user to guide the use of the repertory dialer system. 4. A dialer (simulated) to outpulse the desired telephone number. The repertory dialer system is implemented on a minicomputer with a high speed array processor performing the real-time operations. The vocabulary for the system consists of 7 command words, 10 digits, and any number of names up to some specified maximum Recognition is performed on one or more subsets of the vocabulary, depending on fine state of the system. To train the system the user is requested to speak each of the vocabulary words twice to provide reference templates for the system. Following training, the system can dial the telephone number corresponding to any name in the repertory, or it can dial a 4 digit telephone extension spoken as an isolated string of digits. The system was tested extensively by 6 talkers (3 male, 3 female - 3 of whom were naive and 3 experienced users) over a three week period. A total of 4620 words were spoken and during the course of the test there were no recognition errors. A request for a repeat of a spoken word occurred about 2% of the time. These tests demonstrate the reliability and robustness of this voice repertory dialer system.		Lawrence R. Rabiner;Jay G. Wilpon;Aaron E. Rosenberg	1980		10.1109/ICASSP.1980.1171066	voice activity detection;natural language processing;speech recognition;word recognition;computer science;microcomputer;telephony;speech synthesis;system testing;robustness	Vision	-22.331423389623154	-85.7457858804216	190038
e38e0fcf5522453e1ae6e7744e2652624c752136	effective prediction of errors by non-native speakers using decision tree for speech recognition-based call system	automatic speech recognition;speech recognition;language model;decision tree	CALL (Computer Assisted Language Learning) systems using ASR (Automatic Speech Recognition) for second language learning have received increasing interest recently. However, it still remains a challenge to achieve high speech recognition performance, including accurate detection of erroneous utterances by non-native speakers. Conventionally, possible error patterns, based on linguistic knowledge, are added to the lexicon and language model, or the ASR grammar network. However, this approach easily falls in the trade-off of coverage of errors and the increase of perplexity. To solve the problem, we propose a method based on a decision tree to learn effective prediction of errors made by non-native speakers. An experimental evaluation with a number of foreign students learning Japanese shows that the proposed method can effectively generate an ASR grammar network, given a target sentence, to achieve both better coverage of errors and smaller perplexity, resulting in significant improvement in ASR accuracy. key words: speech recognition, CALL, grammar network, decision tree	automated system recovery;decision tree;language model;lexicon;perplexity;speech recognition	Hongcui Wang;Tatsuya Kawahara	2009	IEICE Transactions		natural language processing;speech recognition;computer science;artificial intelligence;speech processing;accuracy and precision;language model	NLP	-20.2741132412306	-84.67143087452858	190359
4902805fe1e2f292f6beed7593154e686d7f6dc2	a novel connectionist system for unconstrained handwriting recognition	databases;modelizacion;distributed system;lenguaje natural;modelo markov oculto;unconstrained handwriting databases unconstrained handwriting text recognition connectionist system overlapping character segmentation language modeling hidden markov models recurrent neural network;unconstrained handwriting databases;text;systeme reparti;handwriting recognition;image segmentation;modelo markov;modele markov cache;caracter manuscrito;hidden markov model;base donnee tres grande;analisis forma;lexicon;manuscript character;language modeling;langage naturel;recurrent neural nets handwriting recognition handwritten character recognition hidden markov models image segmentation;performance index;speech;size measurement;online handwriting recognition;intelligence artificielle;texte;segmentation;long terme;classification;offline handwriting recognition;long term;unconstrained handwriting text recognition;offline handwriting;connectionist system;modelisation;hidden markov model handwriting recognition online handwriting offline handwriting connectionist temporal classification bidirectional long short term memory recurrent neural networks;reconnaissance ecriture;markov model;sistema repartido;reconnaissance caractere;hidden markov models;reconocimiento voz;largo plazo;unconstrained handwriting recognition;online handwriting;natural language;robustesse;handwriting recognition hidden markov models character recognition text recognition speech recurrent neural networks labeling databases robustness size measurement;court terme;palabra;word recognition;bidirectional long short term memory;speech recognition;artificial intelligence;robustness;word;pattern analysis;reseau neuronal recurrent;long range;inteligencia artificial;recurrent neural nets;recurrent neural networks;reconnaissance parole;recurrent neural network;very large databases;modele markov;lexico;reseau neuronal;text recognition;texto;modeling;caractere manuscrit;character recognition;clasificacion;red neuronal	Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.	algorithm;artificial neural network;biological neural networks;connectionism;database;experiment;exploit (computer security);finite-state machine;handwriting recognition;hidden markov model;interdependence;language model;lexicon;markov chain;online and offline;personality character;preprocessor;recurrent neural network;sequence labeling;anatomical layer	Alex Graves;Marcus Liwicki;Sara Fernández;Roman Bertolami;Horst Bunke;Jürgen Schmidhuber	2009	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2008.137	speech recognition;intelligent character recognition;computer science;recurrent neural network;machine learning;pattern recognition;hidden markov model	Vision	-22.016922633773188	-90.74307545590477	190656
0a5e152109177ee2bcc40d430a98fbbb2786f9b0	automatic bass line transcription from streaming polyphonic audio	databases;variable order markov model;causal algorithm automatic bass line transcription streaming polyphonic audio polyphonic music multiple fo estimator musicological models acoustic models variable order markov model viterbi decoding;front end;audio systems;streaming media hidden markov models viterbi algorithm decoding music feature extraction databases audio recording multiple signal classification frequency estimation;decoding;hidden markov model;causal algorithm;acoustic modeling;frequency estimation;audio recording;multiple fo estimator;audio coding;markov model;multiple signal classification;hidden markov models;viterbi decoding music modeling hidden markov models audio systems;streaming media;viterbi algorithm;feature extraction;streaming polyphonic audio;viterbi decoder;musicological models;polyphonic music;markov processes;acoustic models;viterbi decoding audio coding markov processes music;modeling;music;viterbi decoding;automatic bass line transcription	This paper proposes a method for the automatic transcription of the bass line in polyphonic music. The method uses a multiple-FO estimator as a front-end and this is followed by acoustic and musicological models. The acoustic modeling consists of separate models for bass notes and rests. The musicological model estimates the key and determines probabilities for the transitions between notes using a conventional bigram or a variable-order Markov model. The transcription is obtained with Viterbi decoding through the note and rest models. In addition, a causal algorithm is presented which allows transcription of streaming audio. The method was evaluated using 87 minutes of music from the RWC Popular Music Database. Recall and precision rates of 64% and 60%, respectively, were achieved for discrete note events.	acoustic cryptanalysis;acoustic model;beneath a steel sky;bigram;causal filter;line level;markov chain;medical transcription;precision and recall;sound card;streaming media;transcription (software);variable-order markov model;viterbi algorithm	Matti Ryynänen;Anssi Klapuri	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367350	speech recognition;computer science;pattern recognition;viterbi decoder;hidden markov model;statistics	Robotics	-20.043310625274803	-88.01290137396472	190769
57e3bbaa414fea829eb46fd02e13a3a6962e7af5	user study of the bayesian update of dialogue state approach to dialogue management.	user evaluation;partially observed markov decision process;bayesian updating;user study;simulation experiment;dialogue manager	This paper presents the results of a comparative user evaluation of various approaches to dialogue management. The major contribution is a comparison of traditional systems against a system that uses a Bayesian Update of Dialogue State approach. This approach is based on the Partially Observable Markov Decision Process (POMDP), which has previously been shown to give improved robustness in simulation experiments. Results from this paper show that the benefits demonstrated in simulation experiments are also obtained when testing a live system with real users.	dialog system;experiment;markov chain;partially observable markov decision process;simulation	Blaise Thomson;Milica Gasic;Simon Keizer;François Mairesse;Jost Schatzmann;Kai Yu;Steve J. Young	2008			simulation;computer science;knowledge management;data mining;bayesian inference;statistics	AI	-27.748780348231328	-87.22434959794151	190884
331e7fc177702061ad6d1f1d70d7aad843b00d21	an improved template-based approach to spoken language translation	part of speech;speech recognition	In this paper, we describe an improved template-based approach to Chinese-to-English Spoken Language Translation (SLT) and present experimental results. The improved template-based translation approach uses flexible expression format to describe the template condition. The condition of a template may consist of keywords, parts-of-speech and also semantic features, so the input may be matched with a template from shallow level to deep level. In the condition of a template, the distance between two fixed keywords is stretchable, thus some needless words in the input utterances may be skipped in matching operation. And also the translation results of the same template are alterable. The proper results are finally generated according to the specific context. That is, the relation between a template and translated utterance is one-to-n (where, n is an integer and n≥1). The experiments were performed with input of both text transcription and results of speech recognition. The preliminary experimental results have proven the approach is practical.	experiment;machine translation;speech recognition;transcription (software)	Chengqing Zong;Taiyi Huang;Bo Xu	2000			part of speech;machine translation;cued speech;speech recognition;spoken language;speech technology;speech corpus;natural language processing;computer science;utterance;artificial intelligence;speech production	NLP	-21.55012079181357	-82.31789578477476	190905
cfaa9578009acdb6f6cf72dc6c52e714e4ab3ee6	parallel fast likelihood computation for lvcsr using mixture decomposition	speech recognition;indexing terms;decision tree;parallel processing;multi core processor	This paper describes a simple and robust method for improving the runtime of likelihood computation on multi-core processors without degrading system accuracy. The method improves runtime by parallelizing likelihood computations on a multi-core processor. Mixtures are decomposed among the cores and each core computes the likelihood of the mixture allocated to it. We study two approaches to mixture decomposition – Chunk based and Decision-tree based. When applied to RWTH TC-STAR EPPS English LVCSR system on an Intel Core2 Quad processor with varying pruning-beam width settings, the method resulted in a 54% to 70% improvement in the likelihood computation runtime, and a 18% to 59% improvement in the overall runtime.	central processing unit;computation;decision tree;multi-core processor;parallel computing;speech analytics	Naveen Parihar;Ralf Schlüter;David Rybach;Eric A. Hansen	2009			artificial intelligence;speech recognition;decision tree;pattern recognition;computation;multi-core processor;rotation;computer science;hook;molding (process);parallel processing	HPC	-20.16495191918968	-93.28522224050019	191061
f6c89b5a556d56f7f04f5f7ddadb2065758588aa	exploiting linguistic features in lexical steganography: design and proof-of-concept implementation	watermarking;steganography cryptography robustness writing watermarking frequency statistics pixel white spaces image converters;image converters;white spaces;proof of concept;steganography;cryptography;pixel;statistics;writing;robustness;frequency	This paper develops a linguistically robust encryption, LUNABEL, which converts a message into semantically innocuous text. Drawing upon linguistic criteria, LUNABEL uses word replacement, with substitution classes based on traditional word replacement features (syntactic categories and sub-categories), as well as features under-exploited in earlier works: semantic criteria, graphotactic structure, inflectional class and frequency statistics. The original message is further hidden through the use of cover texts — within these, LUNABEL retains all function words and targets specific classes of content words for replacement, creating text which preserves the syntactic structure and semantic context of the original cover text. LUNABEL takes advantage of cover text styles which are not expected to be necessarily comprehensible to the general public, making any semantic anomalies more opaque. This line of work has the promise of creating encrypted texts which are less detectable than earlier steganographic efforts.	cryptosystem;encryption;steganography	Vineeta Chand;C. Orhan Orgun	2006	Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)	10.1109/HICSS.2006.175	speech recognition;computer science;cryptography;artificial intelligence;theoretical computer science;frequency;white spaces;database;steganography;writing;proof of concept;world wide web;pixel;statistics;robustness	NLP	-31.90048111057167	-88.32721054088196	191592
ca044a5b8f53802e75fbbb855430e2a3f5d880a3	a parallel processing keyword recogniser for police national computer enquiries	parallel processing		parallel computing	Fergus R. McInnes;J. A. Elliott;N. W. Ramsey;Mark E. Forsyth;Andrew M. Sutherland;Mervyn A. Jack	1993			speech recognition;natural language processing;artificial intelligence;computer science;parallel processing	Theory	-30.79817604344627	-84.49202472020991	191613
371fa2eba7662d602a25202937c888256da97b55	fast two-level viterbi search algorithm for unconstrained handwriting recognition	handwriting recognition;hidden markov model;search algorithm;handwritten word recognition	This paper describes a fast two-level Viterbi search algorithm for recognizing handwritten words as a sequence of characters concatenated according to a lexicon. The algorithm is based on hidden Markov model (HMM) representations of characters and it breaks up the computation of word likelihood scores into two levels: state level and character level. This enables the reuse of likelihood scores of characters to decode all words in the lexicon, avoiding repeated computation of state sequences. Experimental results with an 85,000-word vocabulary indicate that the computational cost of an off-line handwritten word recognition system may be reduced by more than a factor of 20 while not introducing search errors.	algorithmic efficiency;computation;concatenation;handwriting recognition;hidden markov model;lexicon;markov chain;online and offline;search algorithm;viterbi algorithm;vocabulary	Alessandro L. Koerich;Robert Sabourin;Ching Y. Suen	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745418	speech recognition;viterbi algorithm;computer science;intelligent word recognition;machine learning;pattern recognition;handwriting recognition;hidden markov model;iterative viterbi decoding;search algorithm	Robotics	-21.27834588148934	-88.04563906865707	191697
c6f6ae5012f6edda58bcb1b972a3884eec6919c9	retrospective analysis of clinical performance of an estonian speech recognition system for radiology: effects of different acoustic and language models	automatic speech recognition;estonian language;radiology;spontaneous dictation;word error rate	The aim of this study was to analyze retrospectively the influence of different acoustic and language models in order to determine the most important effects to the clinical performance of an Estonian language-based non-commercial radiology-oriented automatic speech recognition (ASR) system. An ASR system was developed for Estonian language in radiology domain by utilizing open-source software components (Kaldi toolkit, Thrax). The ASR system was trained with the real radiology text reports and dictations collected during development phases. The final version of the ASR system was tested by 11 radiologists who dictated 219 reports in total, in spontaneous manner in a real clinical environment. The audio files collected in the final phase were used to measure the performance of different versions of the ASR system retrospectively. ASR system versions were evaluated by word error rate (WER) for each speaker and modality and by WER difference for the first and the last version of the ASR system. Total average WER for the final version throughout all material was improved from 18.4% of the first version (v1) to 5.8% of the last (v8) version which corresponds to relative improvement of 68.5%. WER improvement was strongly related to modality and radiologist. In summary, the performance of the final ASR system version was close to optimal, delivering similar results to all modalities and being independent on user, the complexity of the radiology reports, user experience, and speech characteristics.	acoustic cryptanalysis;audio media;automated system recovery;automatic speech recognition;component-based software engineering;kaldi;language model;modality (human–computer interaction);open-source software;radiology;spontaneous order;user experience;version;word error rate	Andrus Paats;Tanel Alumäe;Einar Meister;Ivo Fridolin	2018		10.1007/s10278-018-0085-8	modalities;speech characteristics;radiology;component-based software engineering;user experience design;word error rate;language model;speech recognition;estonian;computer science	NLP	-22.716714358932244	-85.04478515605831	191722
2b103137c6fd33a1fc2bbdab830f4792b7950348	improving speech synthesis quality for voices created from an audiobook database		This paper describes an approach to improving synthesized speech quality for voices created by using an audiobook database. The data consist of a large amount of read speech by one speaker, which we matched with the corresponding book texts. The main problems with such a database are the following. First, the recordings were made at different times under different acoustic conditions, and the speaker reads the text with a variety of intonations and accents, which leads to very high voice parameter variability. Second, automatic techniques for sound file labeling make more errors due to the large variability of the database, especially as there can be mismatches between the text and the corresponding sound files. These problems dramatically affect speech synthesis quality, so a robust method for solving them is vital for voices created using audiobooks. The approach described in the paper is based on statistical models of voice parameters and special algorithms of speech element concatenation and modification. Listening tests show that it strongly improves synthesized speech quality.	audiobook;speech synthesis	Pavel Chistikov;Dmitriy Zakharov;Andrey Talanov	2014		10.1007/978-3-319-11581-8_34	natural language processing;speech recognition;multimedia	Logic	-20.72302866857192	-83.92642663427507	191876
b5f5b0a615b998c33ff1da26d3dfbae66830231e	phrase-based language models for speech recognition.	wall street journal;natural language;speech recognition;language model	Including phrases in the vocabulary list can improve ngram language models used in speech recognition. In this paper, we report results of automatic extraction of phrases from the training text using frequency, likelihood, and correlation criteria. We show how a language model built from a vocabulary that includes useful phrases can systematically improve language model perplexity in a natural language call-routing task and the 20K-Nov92 Wall Street Journal evaluation. We also discuss the impact of such phrase-based language models on recognition word error rate.	language model;n-gram;natural language;perplexity;routing;speech recognition;the wall street journal;vocabulary;word error rate	Hong-Kwang Jeff Kuo;Wolfgang Reichl	1999			natural language processing;language identification;telegraphic speech;cued speech;speech recognition;factored language model;speech corpus;speech;computational linguistics;language transfer;linguistics;natural language;speech synthesis	NLP	-20.85110246940673	-82.68668570090188	191969
479ddf31ac7e3d0cb9b9ada8bdd5bdbbee781e13	adaptive statistical utterance phonetization for french	lattices;training;speech;isolated word corpus adaptive statistical utterance phonetization method french uncontextualized constituent word concatenate pronunciation speaker accent weighted finite state transducer conditional random field spoken utterance corpus;hidden markov models;context lattices training context modeling hidden markov models speech adaptation models;statistical analysis speaker recognition speech processing;adaptation models;context modeling;weighted finite state transducers utterance phonetization pronunciation variant modelling phoneme lattices conditional random fields;context	Traditional utterance phonetization methods concatenate pronunciations of uncontextualized constituent words. This approach is too weak for some languages, like French, where transitions between words imply pronunciation modifications. Moreover, it makes it difficult to consider global pronunciation strategies, for instance to model a specific speaker or a specific accent. To overcome these problems, this paper presents a new original phonetization approach for French to generate pronunciation variants of utterances. This approach offers a statistical and highly adaptive framework by relying on conditional random fields and weighted finite state transducers. The approach is evaluated on a corpus of isolated words and a corpus of spoken utterances.	concatenation;conditional random field;finite-state transducer;text corpus	Gwénolé Lecorvé;Damien Lolive	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178895	natural language processing;speech recognition;computer science;speech;lattice;mathematics;context model;hidden markov model	NLP	-19.532605029158955	-86.011940702377	192043
d4a2cbde9540f4c25111b5763c22309fb6ff2714	dealing with incompleteness of linguistic knowledge in language translation - transfer and generation stage of mu machine translation project		Therefore the linguistic contents of machine translation system always fluctuate, and make gradual progress. The system should be designed to allow such constant change and improvements. This paper explains the details of the transfer and generation stages of Japanese-to-English system of the machine translation project by the Japanese Government, with the emphasis on the ideas to deal with the incompleteness of linguistic knowledge for machine translation.	computer scientist;fault tolerance;machine translation	Makoto Nagao;Toyoaki Nishida;Jun'ichi Tsujii	1984				NLP	-29.71842037460518	-81.65725675407779	192167
8518a2b26a6aa2a6d976f261079db08d4b02e20b	unit selection using k-nearest neighbor search for concatenative speech synthesis	unit selection;speech synthesis;concatenative speech synthesis;text to speech;synthesis unit selection;k nearest neighbor;nearest neighbor search;high performance	We propose a new approach to rapidly identifying adequate synthesis units in extremely large speech corpora. Our aim is to develop a concatenative speech synthesis system with high performance (both speech quality and throughput) for various practical applications. Utilizing very large speech corpora allows more natural sounding synthesized speech to be created; the downside is an increase in the time taken to locate the synthesis units needed. The key to overcoming this problem is introducing state-of-the art database retrieval technologies. The first selection step, based on simple hash search, tabulates all synthesis unit candidates. The second step selects N best candidates using nearest neighbor search, a typical database retrieval technique. Finally, the best sequence of synthesis units is determined by Viterbi search. A runtime measurement test and subjective experiment are carried out. Their results confirm that the proposed approach reduces the runtime by about 40% compared to using only hash search with no degradation in the quality of synthesized speech for a 15 hour corpus.	automatic sounding;elegant degradation;experiment;hash table;k-nearest neighbors algorithm;nearest neighbor search;speech synthesis;text corpus;throughput;viterbi algorithm	Hideyuki Mizuno;Satoshi Takahashi	2009		10.1145/1667780.1667858	natural language processing;speech recognition;computer science;pattern recognition	AI	-20.598287749843976	-84.0598361969858	192186
0eb2b5da79814b7327e53c53f9773b49497fa9de	decision-tree based error correction for statistical phrase break prediction in korean	new phrase break prediction;error correction;phrase break;morpheme sequence;probabilistic predictor;probabilistic method;decision tree-based post error;phrase break predictor;decision tree;statistical phrase break prediction;probabilistic approach	In this paper, we present a new phrase break prediction architecture that integrates probabilistic approach with decision-tree based error correction. The probabilistic method alone usually su ers from performance degradation due to inherent data sparseness problems and it only covers a limited range of contextual information. Moreover, the module can not utilize the selective morpheme tag and relative distance to the other phrase breaks. The decision-tree based error correction was tightly integrated to overcome these limitations. The initially phrase break tagged morpheme sequence is corrected with the error correcting decision tree which was induced by C4.5 from the correctly tagged corpus with the output of the probabilistic predictor. The decision tree-based post error correction provided improved results even with the phrase break predictor that has poor initial performance. Moreover, the system can be exibly tuned to new corpus without massive retraining.	c4.5 algorithm;decision tree;elegant degradation;error detection and correction;feature vector;kerrison predictor;netware file system;neural coding;synergy;text corpus	Byeongchang Kim;Gary Geunbae Lee	2000			natural language processing;error detection and correction;speech recognition;computer science;probabilistic method;machine learning;decision tree;pattern recognition	NLP	-19.541224746570453	-80.92754870814989	192248
0ad340e9e3bba93b777658e2aa0cd521f3a2927d	concatenative speech synthesis for amharic using unit selection method	sentence prosody;unit selection;epenthesis;concatenative synthesis;gemination;amharic tts	"""In this paper we propose algorithms and methods that address critical issues in developing a general Amharic text-to-speech synthesizer. Converting grapheme to phoneme in Amharic is a very challenging task because of the two necessary and yet orthographically unrepresented components of the language -- epenthesis and gemination. Modeling prosodic features of various speaking styles is also the other challenging task in developing Amharic TTS. Making use of orthographic property of verbs in their perfect form, this work introduces set of rules that can be used to locate phones that need to be stressed and algorithm that inserts epenthetic vowel wherever necessary. This paper also presents methods to represent intonations that vary according to punctuation marks (""""?"""" and """"#"""") and phoneme location. In addition to these issues we also introduce transliteration rule for numbers and phone labeling scheme that captures allophonic variations in regard to gemination, epenthesis, and declarative versus judgment request intonations. The system is evaluated for naturalness and intelligibility by ten fluent speakers of the language and experimental results are reported."""	algorithm;intelligibility (philosophy);netware file system;orthographic projection;phoneme;speech synthesis	Eyob B. Kasie;Yaregal Assabie	2012		10.1145/2457276.2457282	natural language processing;speech recognition;computer science;gemination	NLP	-20.716092031287392	-82.09943512757683	192619
09cd2f27d3ffe58770c8cf5bc46d4b4891eabb9e	zero pronoun resolution in japanese discourse based on centering theory		Recently there have been a number of works that model the zero pronoun resolution with the concept called`center.' However, the usefulness of the previous centering frameworks has not fully evaluated with naturally occurring discourses. Furthermore, the previous centering theory has handled only the phenomena in successive simple sentences and has not adequately addressed the way to handle complex sentences that are prevalent in naturally occurring discourses. In this paper, we present a method to handle complex sentences with the centering theory and describe our framework that identies the antecedents of zero pronouns in naturally occurring Japanese discourses. We also present the evaluation of our framework with real discourses .	anaphora (linguistics)	Manabu Okumura;Tamura Kouji	1996			linguistics;algorithm	NLP	-32.964050955147286	-81.82335278342522	192805
2476a9d904ab205d557d006e2728a37ea5827649	minimalist parsing of subjects displaced from embedded clauses in free word order languages	word order	In Sayeed and Szpakowicz (2004), we proposed a parser inspired by some aspects of the Minimalist Program. This incremental parser was designed specifically to handle discontinuous constituency phenomena for NPs in Latin. We take a look at the application of this parser to a specific kind of apparent island violation in Latin involving the extraction of constituents, including subjects, from tensed embedded clauses. We make use of ideas about the left periphery from Rizzi (1997) to modify our parser in order to handle apparently violated subject islands and similar phenomena.	displacement mapping;embedded system;essence;gigabyte;government and binding theory;minimalism (computing);minimalist program;parsing;position-independent code;tree (data structure)	Asad B. Sayeed	2005		10.3115/1628960.1628979	word order;natural language processing;parser combinator;computer science;glr parser;linguistics;programming language;recursive descent parser;ll parser;top-down parsing	NLP	-29.981613503206376	-82.44931213972598	192847
32c9d0e4d29f9bbed08f30c20b91e56f960fbd70	efficient and robust language modeling in an automatic children's reading tutor system	children acoustic model robust language modeling automatic children reading tutor system children language learning n gram language models current story sentence reading sentences parallel garbage model;automatic reading tutor;acoustic modeling;children s speech recognition;reading miscues;asr;reading miscues automatic reading tutor children s speech recognition language model asr;on the fly;speech recognition;text detection;language learning;speech recognition natural language processing;tutoring system;natural language processing;language model;robustness subspace constraints acoustic signal detection automatic speech recognition predictive models natural languages acoustic testing automatic testing system testing humans	"""Recently, there has been a rapidly increasing interest in using ASR for children's language learning. An automatic reading tutor system built with ASR technologies can track children's oral reading against story texts, detect reading miscues, and measure the level of reading fluency. They may even diagnose the nature of the miscues and provide feedback to improve reading skills. In such tasks, N-gram language models (LM) may be trained from the whole story text, or may be generated based on current story sentence with heuristic probabilities for both regular words in the sentence and explicitly predicted reading miscues. The disadvantages of those methods are either they require a relatively large text and are time-consuming, or a large-sized LM and complex processing are needed to accommodate all possible words in reading stories as well as in reading miscues. This paper proposes an efficient and robust LM which can be easily built on-the-fly with current reading sentences. With an additional parallel """"garbage"""" model, the LM can also deal effectively with a wide range of reading miscues. Our experiments in a standard children's reading task show that the new LM reaches the state-of-the-art performance in detecting reading miscues with a fast speed while only a relatively simple children's acoustic model of speech was used."""	acoustic cryptanalysis;acoustic model;automated system recovery;experiment;feedback;heuristic;language model;n-gram;sensor	Xiaolong Li;Yun-Cheng Ju;Li Deng;Alex Acero	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367196	language acquisition;natural language processing;speech recognition;computer science;reading;language model	NLP	-20.416352684442423	-84.86538701528684	193090
12ec5ed5b8520ec924b26e4c202e00cc6da94a99	modeling pronunciation variation with context-dependent articulatory feature decision trees	context dependent;decision tree	We consider the problem of predicting the surface pronunciations of a word in conversational speech, using a model of pronunciation variation based on articulatory features. We build context-dependent decision trees for both phone-based and feature-based models, and compare their perplexities on conversational data from the Switchboard Transcription Project. We find that a fully-factored model, with separate decision trees for each articulatory feature, does not perform well, but a feature-based model using a smaller number of “feature bundles” outperforms both the fully-factored model and a phonebased model. The articulatory feature-based decision trees are also much more robust to reductions in training data. We also analyze the usefulness of various context variables.	context-sensitive language;decision tree;medical transcription;perplexity;telephone switchboard	Samuel R. Bowman;Karen Livescu	2010			phone;speech recognition;artificial intelligence;pattern recognition;decision tree;computer science;training set;pronunciation	ML	-19.206538563778864	-86.20989511910956	193307
c6def76406a6190bcbf97819a6195f91c69d469f	the syntactic prediction with token automata: application to handias system		This paper presents a .nite-state machine to compute the probability of a word appearance when one knows the left syntactic context. We memorize the token number of words in a dictionary and the token number of syntactic categories on .nite-state automata. We compute a word probability with these numbers. If we have not predicted the awaited word, we take into account the .rst letter for a new prediction, and so on. This system has been implemented on a prototype software for disabled communication aid, called HandiAS and it is a part of the Research project CNHL of the LI, the Computer Laboratory of the Tours University. c © 2001 Elsevier Science B.V. All rights reserved.	automata theory;automaton;dictionary;finite-state machine;prototype	Denis Maurel;Brigitte Le Pévédic;Olivier Rousseau	1998		10.1007/3-540-48057-9_9	natural language processing;speech recognition;computer science;artificial intelligence	AI	-26.92844762982269	-82.02646522785285	193533
77307ba29cb2aa79a595f33cace3c22036097778	an empirical evaluation on hit-or3c database	databases;online and offline handwriting recognition;handwriting recognition;databases character recognition accuracy handwriting recognition vectors training feature extraction;database management systems;training;chinese character;natural languages;accuracy;chinese document hit or3c online and offline handwriting recognition chinese character;vectors;feature extraction;natural languages database management systems handwriting recognition handwritten character recognition;low average recognition rate handwriting chinese character hit or3c database handwriting recognition method;hit or3c;character recognition;handwritten character recognition;chinese document	Recently, we have proposed a handwriting Chinese character database HIT-OR3C. Though it has been introduced in detail, to date, it has not been evaluated by any handwriting recognition method. To help the researchers use this database for algorithm evaluation, we propose the structure of HIT-OR3C database. Moreover, we evaluate the OR3C database with a series of experiments using state-of-the-art handwriting recognizer. These experiment results on the different subsets can be a benchmark for the researchers who will use the database. The low average recognition rate confirms that the HIT-OR3C database is challenging.	algorithm;benchmark (computing);experiment;finite-state machine;handwriting recognition;hit (internet);neural coding;online and offline;optical character recognition	Shusen Zhou;Qingcai Chen;Xiaolong Wang;Xinyi Guo;Hui Li	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.232	natural language processing;speech recognition;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;handwriting recognition;natural language;database design	Vision	-23.21023734604839	-88.88708651808015	193944
69033e6663a1900e6ef4db2a2224e67e783329a0	hifi-av: an audio-visual corpus for spoken language human-machine dialogue research in spanish	telecomunicaciones;audio visual	In this paper, we describe a new multi-purpose audio-visual database on the context of speech interfaces for controlling household electronic devices. The database comprises speech and video recordings of 19 speakers interacting with a HIFI audio box by means of a spoken dialogue system. Dialogue management is based on Bayesian Networks and the system is provided with contextual information handling strategies. Each speaker was requested to fulfil different sets of specific goals following predefined scenarios, according to both different complexity levels and degrees of freedom or initiative allowed to the user. Due to a careful design and its size, the recorded database allows comprehensive studies on speech recognition, speech understanding, dialogue modeling and management, microphone array based speech processing, and both speech and video-based acoustic source localisation. The database has been labelled for quality and efficiency studies on dialogue performance. The whole database has been validated through both objective and subjective tests.	av-test;acoustic cryptanalysis;bayesian network;complexity;dialog system;information processing;interaction;microphone;multi-purpose viewer;speech processing;speech recognition;spoken dialog systems	Fernando Fernández-Martínez;Juan Manuel Lucas;Roberto Barra-Chicote;Javier Ferreiros;Javier Macías Guarasa	2010			natural language processing;audio mining;speech recognition;speech corpus;computer science;speech analytics	NLP	-26.766525956967318	-85.5715641986575	194779
627ea7aac0b93330d6797440db174f3a8de3987d	multimodal analyses enabling artificial agents in human-machine interaction	ronald;bock	This paper describes a recently created multimodal biometric corpus of spontaneous casual spoken interaction recorded at Trinity College Dublin, the University of Dublin, in Ireland, and currently being made available for wider dissemination. The paper focusses on the use of this corpus for training or learning about the needs and limitations of an interactive spoken dialogue interface for human-machine communication. Since the corpus is still very new and only recently released, the paper does not present research findings based on an analysis of the content but instead suggests methods and goals for annotating the material so that future researchers can use it to design more sensitive interfaces for speech synthesis in spoken dialogue systems. The paper is an extended version of an invited talk at the MA3HMI workshop.	biometrics;dialog system;intelligent agent;multimodal interaction;speech synthesis;spontaneous order;trinity	Ronald Böck;Francesca Bonin;N. Campbell;Ronald Poppe	2014		10.1007/978-3-319-15557-9	machine learning;human–machine system;artificial intelligence;computer science	NLP	-24.335605243269733	-84.24212635798735	195002
b0272c6862a391a7fe5061a84daa035e9e72e857	sinod - slovenian non-native speech database		This paper presents the SINOD database, which is the first Slovenian non-native speech database. It will be used to improve the performance of large vocabulary continuous speech recogniser for non-native speakers. The main quality impact is expected for acoustic models and recogniser’s vocabulary. The SINOD database is designed as supplement to the Slovenian BNSI Broadcast News database. The same BN recommendations were used for both databases. Two interviews with non-native Slovenian speakers were incorporated in the set. Both non-native speakers were female, whereas the journalist was Slovenian native male speaker. The transcription approach applied in the production phase is presented. Different statistics and analyses of database are given in the paper.	acoustic cryptanalysis;acoustic model;non-native speech database;transcription (software);vocabulary	Andrej Zgank;Darinka Verdonik;Alexandra Zögling Markus;Zdravko Kacic	2006			natural language processing;speech recognition;artificial intelligence;computer science;non-native speech database	DB	-22.141064318643444	-84.77231330790225	195633
1e3df77cec0b4f7f84309bb214d0bcd201b6c35e	balancing efficiency and coverage in human-robot dialogue collection		We describe a multi-phased Wizard-of-Oz approach to collecting human-robot dialogue in a collaborative search and navigation task. The data is being used to train an initial automated robot dialogue system to support collaborative exploration tasks. In the first phase, a wizard freely typed robot utterances to human participants. For the second phase, this data was used to design a GUI that includes buttons for the most common communications, and templates for communications with varying parameters. Comparison of the data gathered in these phases show that the GUI enabled a faster pace of dialogue while still maintaining high coverage of suitable responses, enabling more efficient targeted data collection, and improvements in natural language understanding using GUI-collected data. As a promising first step towards interactive learning, this work shows that our approach enables the collection of useful training data for navigation-	dialog system;experiment;graphical user interface;human–robot interaction;natural language understanding;one-shot learning;robot;simulation;wizard (software)	Matthew Marge;Claire Bonial;Stephanie M. Lukin;Cory Juwuan Hayes;Ashley Foots;Ron Artstein;Cassidy Henry;Kimberly A. Pollard;Carla Gordon;Felix Gervits;Anton Leuski;Susan G. Hill;Clare R. Voss;David R. Traum	2018	CoRR		simulation;data collection;engineering;natural language understanding;robot;training set;wizard;interactive learning;human–robot interaction	NLP	-27.80197151288833	-86.54458985682692	196182
d7504c11eec859cccf2318f6237b294051878241	a tutorial on pronunciation modeling for large vocabulary speech recognition	vocabulaire;phonetique;linguistique;speech synthesis;vocabulary;educational software program;vocabulario;didacticiel;automatic speech recognition;linguistica;automatic recognition;phonology;reconocimiento voz;palabra;fonetica;speech recognition;fonologia;word;sintesis palabra;phonologie;phonetics;programa didactico;reconnaissance parole;synthese parole;reconocimiento automatico;mot;reconnaissance automatique;linguistics	Automatic speech recognition (ASR) research has progressed from the recognition of read speech to the recognition of spontaneous conversational speech in the past decade, prompting some in the field to re-evaluate ASR pronunciation models and their role of capturing the increased phonetic variability within unscripted speech. Two basic approaches for modeling pronunciation variation have emerged: encoding linguistic knowledge to pre-specify possible alternative pronunciations of words and deriving alternatives directly from a pronunciation corpus. This tutorial is intended to ground the reader in the basic linguistic concepts in phonetics and phonology that guide both of these techniques and to outline several pronunciation modeling strategies that have been employed through the years. The chapter will conclude with a summary of some promising recent research directions.	speech recognition;vocabulary	Eric Fosler-Lussier	2000		10.1007/978-3-540-45115-0_3	natural language processing;phonetics;speech recognition;computer science;word;speech synthesis;phonology	NLP	-24.203456700892097	-81.89931132857136	196617
0f8b81cd605c0ac3c1a1b95f96dc0fe117b4fad2	speaker verification over the telephone	eficacia sistema;metodo estadistico;phonetique;speaker identification;metodologia;modelo markov;learning;forme onde;reconocimiento palabra;etude experimentale;hidden markov model;analisis cuantitativo;speech processing;authentication;performance systeme;tratamiento palabra;traitement parole;statistical method;posterior probability;red telefonica;system performance;methodologie;text dependent and text independent speaker verification;experimental result;authentification;speaker verification;speaker recognition;aprendizaje;apprentissage;gaussian mixture model;automatic recognition;markov model;autenticacion;hidden markov models;forma onda;receiver operating characteristic curves;analyse quantitative;methode statistique;probabilite a posteriori;equal error rate;probabilidad a posteriori;resultado experimental;reconnaissance locuteur;fonetica;quantitative analysis;speech recognition;phonetics;waveform;reconnaissance parole;modele markov;spontaneous speech;telephone network;methodology;metodo roc;resultat experimental;reseau telephonique;methode roc;text dependent and text independent speaker verification and speaker identification;estudio experimental;reconocimiento automatico;reconnaissance automatique	The aim of the research reported in this paper was to assess the capability of state-of-the-art methods for speaker verification in order to determine if high enough performance le vels could be obtained to support the development of telecom applications. This experimental study quantified speaker recognition performance out of the context of any specific applic ation, as a function of factors more-or-less acknowledged to affect the accura cy. Some issues investigated are: the speaker model (Gaussian mixture models are compared with phonebased models), the influence of the amount and content of training and test data on performance; performance degradation due to model ageing and how can this be counteracted by using adaptation techniques; achievable performance levels using text-dependent a nd textindependent recognition modes. In particular the effect of linguistic content on perfor mance is shown for both read and spontaneous speech. These and other factors were addresse d using a large corpus of read and spontaneous speech (over 2000 hours collected from 100 target speakers and 1000 impostors) in French designed and recorded for the purpose of thi s study. On this data, the lowest equal error rate is 1% for the text-dependent mode whe n 2 trials are allowed per attempt and with a minimum of 1.5s of speech per tria l.	acoustic cryptanalysis;acoustic model;computer performance;cylinder-head-sector;elegant degradation;experiment;heart rate variability;mixture model;numerical aperture;speaker recognition;spontaneous order;systems design;test data;text corpus;uncontrolled format string	Lori Lamel;Jean-Luc Gauvain	2000	Speech Communication	10.1016/S0167-6393(99)00075-8	speaker recognition;speech recognition;computer science;artificial intelligence;authentication;hidden markov model	NLP	-20.804501553406848	-90.52465112413056	196949
62e8aa57505cf982c8f2f546b02d79309e713a04	discriminative codebook design using multiple vector quantization in hmm-based speech recognizers	metodo estadistico;error reduction;maximum mutual information;signal estimation;modelo markov;reconocimiento palabra;acoustic modeling;speech processing;tratamiento palabra;traitement parole;statistical method;acoustic signal processing;maximum likelihood estimation;markov model;cuantificacion vectorial;hidden markov models;design method;vector quantization;methode statistique;estimacion senal;vector quantization hidden markov models speech recognition design methodology mutual information power system modeling error analysis probability distribution signal processing probability density function;speech recognition;maximum likelihood estimation speech recognition hidden markov models vector quantisation acoustic signal processing;vector quantizer;reconnaissance parole;modele markov;vector quantisation;estimation signal;acoustic modeling tool multiple vector quantization hmm based speech recognizers discriminative codebook design speech recognition vq codebook recognition unit mvqhmm model discrete hmm model recognition dynamics input sequence information vq codebook design method modified maximum mutual information estimation error reductions discriminative design;quantification vectorielle	Recent research on multiple vector quantization (MVQ) has shown the suitability of such technique for speech recognition. Basically, MVQ proposes the use of one separated VQ codebook for each recognition unit. Thus, a MVQHMM model is composed of a VQ codebook and a discrete HMM model. This technique allows the incorporation in the recognition dynamics of the input sequence information wasted by discrete HMM models in the VQ process. The use of distinct codebooks also allows to train them in a discriminative manner. In this paper, we propose a new VQ codebook design method for MVQ-based systems, obtained from a modified maximum mutual information estimation. This method provides meaningful error reductions and is performed independently from the estimation of the discrete HMM part of the MVQ model. The results show that the proposed discriminative design turns the MVQHMM technique into a powerful acoustic modeling tool in comparison with other classical methods as discrete or semicontiiuous HMM’s.	acoustic cryptanalysis;codebook;finite-state machine;hidden markov model;mutual information;speech recognition;vector quantization	Antonio M. Peinado;José C. Segura;Antonio J. Rubio;Pedro García-Teodoro;José L. Pérez-Córdoba	1996	IEEE Trans. Speech and Audio Processing	10.1109/89.486058	speech recognition;design methods;computer science;machine learning;pattern recognition;speech processing;mathematics;maximum likelihood;markov model;linde–buzo–gray algorithm;vector quantization;hidden markov model	Robotics	-20.19617710425951	-91.95191479809566	197400
5b0be780fad7cf7b67fe78511917a45ff96571a2	machine translation method using super-function for mobile terminal	mobile handsets language translation context free grammars natural language interfaces;natural language interfaces;context free grammars;language translation;directional graph super function based machine translation mobile terminal mobile telephone mobile communication syntactic analysis semantic analysis translation precision;telephony mobile communication natural languages control systems pattern analysis dictionaries;mobile communication;mobile handsets;mobile terminal;machine translation;semantic analysis	The mobile telephone is the representation of mobile communication. In this paper we proposed a new machine translation method called Super-Function Based Machine Translation (SFBMT) to try to use for mobile terminal. SFBMT uses Super Function(SF) to translate the sentence without thorough syntactic and semantic analysis. And it can translate the sentence fast and high correct. Our experiment shows that this method can get 70% translation precision using the sentences that learned for extracting Super-Function, And 40% translation precision using the sentences that not learned for extracting Super-Function.		Taihao Li;Fuji Ren	2003		10.1109/ICSMC.2003.1244656	computer-assisted translation;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;mobile telephony;computer science;machine translation;rule-based machine translation;context-free grammar;machine translation software usability	Robotics	-24.724077479302796	-80.42363317019442	197451
dcb56302c5026373367e146a7c22343ed5f9fec9	n-best-based unsupervised speaker adaptation for speech recognition	variabilite individuelle;adaptation au locuteur;performance evaluation;hidden markov model;telephone;performance;methode;markov model;speaker independent;reconnaissance de la parole;error rate;speech recognition;evaluation;erreurs;computational linguistics;modele de markov;speaker adaptation;linguistique informatique;method;assessment	Abstract   This paper proposes an instantaneous speaker adaptation method that uses N-best decoding for continuous mixture-density hidden-Markov-model-based speech-recognition systems. This method is effective even for speakers whose decoding using speaker-independent (SI) models are error-prone and for whom speaker adaptation techniques are truly needed. In addition, smoothed estimation and utterance verification are introduced into this method. The smoothed estimation is based on the likelihood values for adapted models of word sequences obtained by N-best decoding and improves the performance of error-prone speakers, and the utterance verification technique reduces the amount of calculation required. Performance evaluation using connected-digit (four-digit strings) recognition experiments performed over actual telephone lines showed a reduction of 36·4% in the error rates of speakers whose decoding using SI models are error-prone.	pc speaker;speech recognition	Tomoko Matsui;Sadaoki Furui	1998	Computer Speech & Language	10.1006/csla.1997.0036	natural language processing;speaker recognition;method;speech recognition;performance;word error rate;computer science;evaluation;computational linguistics;markov model;hidden markov model;educational assessment	NLP	-20.794381159008545	-90.42411210165733	197482
0b4ecccab57dca5107b9a08a4b582ad0f3cc4e8a	an investigation of the impact of speech transcript errors on hmm voices		Toward automatic creation of web-based voice fonts at low cost, automatic speech transcription technology is used to obtain the linguistic features for building HMM-based voices from audio web contents. This paper presents an investigation of the influences of erroneous transcripts on such voices. We simulate varied speech transcript errors by using a large vocabulary automatic speech recognizer (LVASR) to dictate thousands of Japanese utterances from two speakers (a male and a female). A set of experiments is conducted on dozens of HMM voices built upon both dictated and correct transcripts. The results indicate a significant impact of speech transcript errors on the voices. One direct impact is increasing the number of leaf nodes of the decision trees associated with both state duration and F0 but decreasing that with cepstrum in comparison with the reference voices by correct transcripts. The HMM voice quality in mean opinion scores (MOS) is closely related to the word and phone accuracy of speech transcriptions. To achieve fair voice quality with limited training samples, for example, the word and phone accuracy must be higher than 50% and 80%, respectively.	cepstrum;decision tree;experiment;finite-state machine;hidden markov model;simulation;speech recognition;transcription (software);tree (data structure);vocabulary;voice font;web application	Jinfu Ni;Hisashi Kawai	2010			hidden markov model;speech recognition;computer science	NLP	-19.727985591949842	-83.65470635601085	197630
63dcd0dfe52629ca2f7cf7d96897631b7873d868	hidden markov models for labeled sequences	forward backward;hidden markov model;incremental em method hidden markov model class hmm chmm labeled sequences maximum likelihood method optimal recognition forward backward procedure;pattern recognition;hidden markov models speech sequences probability mutual information proteins decoding statistics bayesian methods vocabulary;maximum likelihood method	A hidden Markov model for labeled observations, called a CHMM, is introduced and a maximum likelihood method is developed for estimating the parameters of the model. Instead of training it to model the statistics o f the training sequences it is trained to optimize recognition, It resembles MMI training, but is more general, and has MMI as a special case. The standard forwardbackward procedure for estimating the model cannot be generalized directly, but an “incremental EM” method is proposed.	hidden markov model;markov chain	Anders Krogh	1994		10.1109/ICPR.1994.576891	forward algorithm;markov chain;maximum-entropy markov model;markov property;computer science;machine learning;hidden semi-markov model;pattern recognition;mathematics;maximum likelihood;markov model;hidden markov model;statistics;variable-order markov model	ML	-19.80007170674623	-92.81570739726901	197793
1cda4b412feb884262104c456c28ee5c0e9de79a	text, speech and dialogue		Where have we been and where are we going? Three types of answers will be discussed: consistent progress, oscillations and discontinuities. Moore’s Law provides a convincing demonstration of consistent progress, when it applies. Speech recognition error rates are declining by 10× per decade; speech coding rates are declining by 2× per decade. Unfortunately, fields do not always move in consistent directions. Empiricism dominated the field in the 1950s, and was revived again in the 1990s. Oscillations between Empiricism and Rationalism may be inevitable, with the next revival of Rationalism coming in the 2010s, assuming a 40-year cycle. Discontinuities are a third logical possibility. From time to time, there will be fundamental changes that invalidate fundamental assumptions. As petabytes become a commodity (in the 2010s), old apps like data entry (dictation) will be replaced with new priorities like data consumption (search).	algorithm;ana (programming language);bitext word alignment;complexity of constraint satisfaction;computation;computational linguistics;lexicon;logical possibility;moore's law;parallel text;petabyte;regular expression;sensor;speech coding;speech recognition;tsd	J. Siekmann;Petr Sojka;Ivan Kopecek;Karel Pala Eds	2004		10.1007/b100511		NLP	-29.657152060627507	-83.31821440500666	198178
87118e1d098e196bdb3bb98281ac0dd6181de4b1	an analogical parser for restricted domains	current development;grammar input;language model;uniform grammatical representation;grammatical constraint;analogical parser;parser output;restricted domain	1. T H E P R O B L E M W I T H P A R S E R S A parser is a device that provides a description of the syntactic phrases that make up a sentence. For a speech understanding task such as ATIS, the parser has two roles. First, it should provide a description of the phrases in a sentence so these phrases can be interpreted by a subsequent semantic processor. The second function is to provide a language model a model of the likelihood of a sentence to constrain the speech recognition task. It is unfortunately the case that existing parsers developed for text fulfill neither of these roles very well. It is useful to begin by reviewing some of the reasons for this failure. We can describe the situation in terms of three general problems that parsers face: the Lexicality Problem, the Tail Problem, and the Interpolation Problem. The Lexicality Problem The most familiar way to think of a parser is as a device that provides a description of a sentence given some grammar. Consider for example a context free grammar, where nonterminal categories are rewritten as terminals or nonterminals, and terminals are rewritten as words. There typically is no way to express the constraints among individual words. Yet it is clear that much of our knowledge of language has to do with what words go together.[2] Merely knowing the grammatical rules of the language is not enough to predict which words can go together. So for example, general English grammatical rules admit premodification of a noun by another noun or by an adjective. It is possible to describe broad semantic constraints on such modification; so for example, early morning is a case of a time-adjective modifying a time-period, and morning flight is a time-period modifying an event. Already we are have an explosion of categories in the grammar, since we are talking not about nouns and adjectives, but about a fairly detailed subclassification of semantic types of nouns and adjectives. But the problem is worse than this. As Table 1 shows, even this rough characterization of semantic constraints on modification is insufficient, since the adjective-noun combination early night does not occur. This dependency of syntactic combinability on particular lexicM items is repeated across the grammar and lexicon. The lexicality problem has two aspects. One is representing the information and the other is acquiring it. There has recently been increasing work on both aspects of the problem. The approach described in this paper is but one of many possible approaches, designed with an emphasis on facilitating efficient parsing.	automatic transmitter identification system (television);context-free grammar;interpolation;language model;lexicon;parser;speech recognition;terminal and nonterminal symbols	Donald Hindle	1992			natural language processing;parser combinator;compiler-compiler;lalr parser;canonical lr parser;ll grammar;parsing expression grammar;operator-precedence grammar;top-down parsing language;computer science;parsing;glr parser;linguistics;programming language;attribute grammar;recursive descent parser;ll parser;top-down parsing;lr parser;language model;simple lr parser	NLP	-30.176584529036443	-82.7623271495308	198597
6dfeee677895e840567a7e9b322c8d815b26e4ba	creating user defined new vocabularies for voice dialing		This paper introduces a new approach for generation of phonetic transcriptions for voice dialing applications. where on-line construction of user vocabularies is mandatory. The proposed method allows adaptive selection of new transcriptions requiring much less speech utterances for system training than other approaches. The new approach is compared to other classical approaches showing a clear improvement on performance and efficiency.	online and offline;vocabulary;voice command device	Jose Maria Elvira;Juan Carlos Torrecilla;Francisco Javier Caminero Gil	1997			speech recognition;transcription (linguistics);computer science	Graphics	-20.38472235321632	-84.2299457166248	199533
27252820e2ce0f406aa25e6fbc70f096a397e768	a hybrid approach to supervised machine learning for algorithmic melody composition		English: In this work we present an algorithm for composing monophonic melodies similar in style to those of a given, phrase annotated, sample of melodies. For implementation, a hybrid approach incorporating parametric Markov models of higher order and a contour concept of phrases is used. This work is based on the master thesis of Thayabaran Kathiresan (2015). An online listening test conducted shows that enhancing a pure Markov model with musically relevant context, like count and planed melody contour, improves the result significantly. German: In dieser Arbeit wird ein Algorithmus präsentiert, der einstimmige Melodien komponiert. Diese sind stilistisch ähnlich zu Beispielmelodien, deren Phrasengrenzen zuvor annotiert wurden. Dazu wird ein Hybridansatz verfolgt, der parametrische Markow-Modelle höherer Ordnung, sowie ein Konturenkonzept implementiert. Diese Arbeit basiert auf der Masterarbeit von Thayabaran Kathiresan (2015). Ein Onlineumfrage hat ergeben, dass das erweitern eines reinen Markow-Modells mit musikalisch relevantem Kontext, wie Zählzeit und geplanter Melodiekontur, die Ergebnisse signifikant verbessert.	algorithm;internet explorer;machine learning;markov chain;markov model;supervised learning	Rouven Bauer	2016	CoRR		natural language processing;speech recognition;computer science;artificial intelligence;machine learning	NLP	-20.82903573412191	-80.44539796712002	199672
028a53bf067a8f54e3a3dd059d970d189ccf330a	jhu/apl at trec 2002: experiments in filtering and arabic retrieval	jhu	For ranked retrieval, we relied on a statistical language model to compute query/document similarity values. Hiemstra and de Vries describe such a linguistically motivated probabilistic model and explain how it relates to both the Boolean and vector space models [4]. The model has also been cast as a rudimentary Hidden Markov Model [13]. Although the model does not explicitly incorporate inverse document frequency, it does favor documents that contain more of the rare query terms. The similarity measure can be computed as	apl;adaptive filter;algorithm;boolean algebra;content-control software;cross-language information retrieval;dictionary;document;experiment;financial times;hidden markov model;hoc (programming language);language model;lexicon;markov chain;relevance feedback;routing;similarity measure;standard translation;statistical model;stemming;support vector machine;tf–idf	Paul McNamee;Christine D. Piatko;James Mayfield	2002			natural language processing;speech recognition;computer science;linguistics	Web+IR	-22.36494437908221	-80.7508625402838	199909
