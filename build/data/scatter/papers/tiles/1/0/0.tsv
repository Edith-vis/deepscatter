id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
497ba2bae32ab6cf56981dcbd776eb434f4c3a84	a scalable approach for diffserv multicasting	multicast communication;computer engineering;differentiated services diffserv;group communication;electrical and computer engineering;internet;telecommunication network routing;adaptive multicast trees diffserv multicasting group communications qos aware applications internet multicasting differentiated services group size network size quality of service lan wan extension headers packet size scalable multicasting stateless core routers bandwidth savings incremental deployment tunneling;group size;telecommunication network routing internet multicast communication quality of service;quality of service;diffserv networks bandwidth quality of service educational institutions web and internet services resource management computer networks laboratories ip networks acceleration	The phenomenal growths of group communications and QoS-aware applications over the Internet have respectively accelerated the development of two key technologies, namely, multicasting and Differentiated Services (DiffServ). Although both are complementary technologies, the integration of the two technologies is a nontrivial task due to architectural conflicts between multicasting and DiffServ. In this paper, we propose an approach for providing multicast support across a DiffServ domain that is scalable in terms of group size, network size, and number of groups. We analyze our approach in a detailed manner for feasibility, adaptiveness, and deployment considerations.	differentiated services;encode;encapsulation (networking);multicast;network packet;quality of service;scalability;software deployment;stateless protocol;tunneling protocol	Aaron Striegel;G. Manimaran	2001		10.1109/ICC.2001.936538	real-time computing;the internet;quality of service;communication in small groups;computer science;size of groups, organizations, and communities;distributed computing;computer network	SE	-12.885579775721448	93.03749499259396	1
fedcd62f5d061d94e7a57129473cfbf9630d3edb	ofdm interference analysis in nonlinear time-varying radio channels with frequency offsets	time varying;wireless channels;wireless channels doppler shift ofdm modulation;frequency offsets;ofdm frequency offset interference nonlinear distortions;power amplifier;gain;nonlinear time varying radio channels;interference;radiofrequency interference;nonlinear distortion;doppler effect;maximal doppler shift;ofdm modulation;amplifier gain;ofdm;nonlinear distortions;linear gain;transmitter receiver;wide sense stationary uncorrelated scattering channel;doppler shift;ofdm interference analysis;transmitter receiver ofdm interference analysis nonlinear time varying radio channels frequency offsets wide sense stationary uncorrelated scattering channel maximal doppler shift nonlinear power amplifier amplifier gain linear gain;ofdm radiofrequency interference nonlinear distortion doppler shift gain scattering power amplifiers frequency estimation fluctuations transmitters;frequency offset;nonlinear power amplifier;noise;wide sense stationary uncorrelated scattering	Nonlinear distortions and frequency offsets are two major problems of OFDM transmission that essentially affect its reliability. In this paper, we analyze OFDM by taking into account both effects. For a wide-sense stationary uncorrelated scattering channel we derive an approximate analytical expression for the signal - to - total interference power ratio. The obtained formula directly involves maximal Doppler shift, transmitter - receiver frequency offset and characteristics of the nonlinear power amplifier (amplifier gain and linear gain). The simulation results confirm that the derived formula provides a close approximation of the exact expression for a wide range of Doppler shifts and transmitter-receiver frequency offsets. Simplicity and accuracy of the obtained expression as well as its fitting for realistic scenarios where the transmitter-receiver frequency offsets are random, makes it convenient for practical applications.	approximation algorithm;audio power amplifier;distortion;doppler effect;frequency offset;interference (communication);maximal set;nonlinear system;simulation;stationary process;transmitter	Natalia Y. Ermolova;Olav Tirkkonen	2008	2008 IEEE 19th International Symposium on Personal, Indoor and Mobile Radio Communications	10.1109/PIMRC.2008.4699605	orthogonal frequency-division multiplexing;doppler effect;telecommunications;control theory	Arch	41.98242476435059	77.51745573134794	2
682f8e385fa2d475805a10c0c7096c7d2fa715eb	efforts and efficiency in partial outsourcing and investment timing strategy under market uncertainty	outsourcing;investment decision;net present value;decision support system;real option;profitability;investment timing;capital budgeting;market uncertainty	Because outsourcing incurs hidden costs at the preparation stage and future profits are uncertain, outsourcing immediately is not always optimal. Thus, this paper studies when the optimal time to outsource is, by proposing a real option model. The timing strategy takes into account a firm's effort at the preparation stage and an outsourced proportion, because ex post future profits and consequently the optimal time are affected by how well prepared the outsourcing is and how large proportions a firm outsources. Based on the model, this research provides managerial implications about how outsourcing timing strategies should vary when outsourcing environments such as market uncertainty changes and a firm's effort. Our model shows that a firm can outsource earlier when an investment (effort) at the preparation stage is more efficiently made, when market becomes more stable, when it can expect higher marginal profits from the outsourcing, when it can outsource more proportion. Also, by comparing a widely used net present value model to our real option model, we show that the traditional method underestimates a firm's value for outsourcing and misleads a firm to outsource earlier. Finally, we provide a descriptive framework for a decision support system.	outsourcing	Yongma Moon	2010	Computers & Industrial Engineering	10.1016/j.cie.2010.02.010	net present value;decision support system;economics;marketing;operations management;knowledge process outsourcing;capital budgeting;management;commerce;profitability index;outsourcing	SE	1.6309093415653522	-6.250323020004572	3
c8ff017566f31a898e8a1fbee20eec07dae5c53f	a sociopragmatic study of apology speech act realization patterns in persian	traitement signal;oceanie;dominance;social dominance;apology speech act;speech acts;transformation cosinus discrete;status;analyse discours;new zealand english;language acquisition;nueva zelandia;analisis de contenido;anglais;discrete cosine transforms;signal processing;sociopragmatic;nouvelle zelande;english;new zealand;discourse analysis;ingles;procesamiento senal;oceania;social distance;ccsarp	This research study aimed at extracting and categorizing the range of strategies used in performing the speech act of apologizing in Persian. The first objective was to see if Persian apologies were formulaic in pragmatic structure as in English apologies are said to be [Holmes, J., 1990. Apologies in New Zealand English. Lang. Soc. 19, 155–200; Wolfson, N., Judd, E. (Eds.), 1983. Sociolinguistics and Language Acquisition. Rowley, Mass, Newbury House]. The other issue explored in this study was the investigation of the effect of the values assigned to the two context-external variables of social distance and social dominance on the frequency of the apology intensifiers. To this end, Persian apologetic utterances were collected via a Discourse Completion Test (DCT). The research findings indicated that Persian apologies are as formulaic in pragmatic structures. Also, the values assigned to the two context-external variables were found to have significant effect on the frequency of the intensifiers in different situations. 2007 Elsevier B.V. All rights reserved.	categorization;discrete cosine transform	Akbar Afghari	2007	Speech Communication	10.1016/j.specom.2007.01.003	language acquisition;speech recognition;discourse analysis;english;signal processing;linguistics;dominance;social distance	NLP	-13.502749967098545	-80.00926023105458	4
97782fc0df36622ed1cc44950d8db4545533d121	systems identification for noise and echo control using hidden psychoacoustical orthogonal sequences.	system identification			Mike Peters	1999			speech recognition	Robotics	76.29665641279308	-8.662124997672624	5
c6d9387ed4b49d9fd3b717f802420a6c5f657bca	making memories: applying user input logs to interface design and evaluation	human computer interaction;interface design;data model;collaborative interfaces;user behavior;usability;user interaction	In this paper, we describe our approach to designing interface components that automate the logging of user input. These recorded logs of user-system interactions can serve as a basis for usability assessment, and we present here the usability measures that can be automatically derived from this logged data. Making user logs an integral component of the system data model extends their usefulness beyond providing information on user behavior. In our prototype, logs are used for creating a more collaborative interface by increasing the system's contextual awareness of user interactions.	data model;interaction;prototype;usability	Tamara Babaian;Wendy T. Lucas;Heikki Topi	2006		10.1145/1125451.1125559	user interface design;user;10-foot user interface;user experience design;user modeling;interactive systems engineering;usability;human–computer interaction;data model;computer science;interface design;database;natural user interface;user interface;heuristic evaluation;world wide web;usability inspection	HCI	-42.68861905959522	-29.063443995941693	6
13e06074ee85d8980b3b0b15eb88682088eecf07	interprocedural constant propagation: a study of jump function implementations	cost effectiveness;constant propagation	An implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a jump function. While Callahan et al. propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the pass-through parameter jump function, as the most cost-effective in practice.	constant folding;software propagation	Dan Grove;Linda Torczon	1993		10.1145/155090.155099	real-time computing;simulation;cost-effectiveness analysis;computer science;programming language;constant folding;algorithm	Crypto	-61.83692469926702	34.74646967571893	7
e78d8911157ca5d25762e4718d8cbf8da272a1d9	formatshield: a binary rewriting defense against format string attacks	context information;intrusion detection;binary rewriting;system security;source code;format string attacks;reading and writing	Format string attacks allow an attacker to read or write anywhere in the memory of a process. Previous solutions designed to detect format string attacks either require source code and recompilation of the program, or aim to defend only against write attempts to security critical control information. They do not protect against arbitrary memory read attempts and non-control data attacks. This paper presents FormatShield, a comprehensive defense against format string attacks. FormatShield identifies potentially vulnerable call sites in a running process and dumps the corresponding context information in the program binary. Attacks are detected when malicious input is found at vulnerable call sites with an exploitable context. It does not require source code or recompilation of the program and can defend against arbitrary memory read and write attempts, including non-control data attacks. Also, our experiments show that FormatShield incurs minimal performance overheads and is better than existing solutions.	application security;binary file;experiment;linux;malware;rewriting;static program analysis;synthetic data;uncontrolled format string	Pankaj Kohli;Bezawada Bruhadeshwar	2008		10.1007/978-3-540-70500-0_28	intrusion detection system;computer science;internet privacy;world wide web;computer security;source code	Security	-56.35252140410784	56.98756705641997	8
ae1d26a36dec5dbdd1cfd6fa612bdf8f88d57047	advertising perception with immersive virtual reality devices	resists;virtual environments;urban areas;monitoring;solid modeling;advertising	This poster presents an initial study about people experience with advertising messages in Virtual Reality (VR) that simulates the urban space. Besides looking to the plastic and textual factors perceived by the users in the Virtual Environment (VE), this work also reflects about effects of immersion provided by different technological devices and its possible influences in the advertising message reception process — a head-mounted display (Oculus Rift DK2), a cavern automatic virtual environment (CAVE) and a desktop monitor (PC). To carry this empirical experiment, a 3D scenario that simulates a real city urban space was created and several advertising image formats were inserted on its landscape. User navigation through the urban space was designed in a firstperson perspective. In short, we intend to accomplish two objectives: a) to identify which factors lead people to pay attention to adverting in immersive VE; b) to verify the immersion effects produced by different VR interfaces in the perception of advertising.	desktop computer;emoticon;head-mounted display;image file formats;immersion (virtual reality);oculus rift;virtual reality	Eduardo Zilles Borba;Marcelo Knörich Zuffo	2017	2017 IEEE Virtual Reality (VR)	10.1109/VR.2017.7892331	computer vision;simulation;operating system;resist;multimedia;solid modeling;immersion	Visualization	-42.5173429487564	-38.53540520167359	9
3eb2fd7f42f507e8ee6e3d8325bfab0c1b12523a	bayesian quicknat: model uncertainty in deep whole-brain segmentation for structure-wise quality control		We introduce Bayesian QuickNAT for the automated quality control of whole-brain segmentation on MRI T1 scans. Next to the Bayesian fully convolutional neural network, we also present inherent measures of segmentation uncertainty that allow for quality control per brain structure. For estimating model uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC) samples from the posterior distribution are generated by keeping the dropout layers active at test time. Entropy over the MC samples provides a voxel-wise model uncertainty map, whereas expectation over the MC predictions provides the final segmentation. Next to voxel-wise uncertainty, we introduce four metrics to quantify structure-wise uncertainty in segmentation for quality control. We report experiments on four out-of-sample datasets comprising of diverse age range, pathology and imaging artifacts. The proposed structure-wise uncertainty metrics are highly correlated with the Dice score estimated with manual annotation and therefore present an inherent measure of segmentation quality. In particular, the intersection over union over all the MC samples is a suitable proxy for the Dice score. In addition to quality control at scan-level, we propose to incorporate the structure-wise uncertainty as a measure of confidence to do reliable group analysis on large data repositories. We envisage that the introduced uncertainty metrics would help assess the fidelity of automated deep learning based segmentation methods for large-scale population studies, as they enable automated quality control and group analyses in processing large data repositories.	artificial neural network;convolutional neural network;deep learning;dropout (neural networks);eli;elan;emoticon;experiment;fail-safe;graphics processing unit;image resolution;lateral computing;lateral thinking;lunar lander challenge;merck index;mesoscopic physics;monte carlo method;proxy server;voxel	Abhijit Guha Roy;Sailesh Conjeti;Nassir Navab;Christian Wachinger	2018	CoRR			Robotics	33.30333324802641	-80.93280022726067	10
c67b0739a8e98306fc7e9ad481babd4045812cc1	debugging data flows in reactive programs		Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console.  In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.		Herman Banken;Erik Meijer;Georgios Gousios	2018	2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)	10.1145/3180155.3180156	real-time computing;computer science;debugging;stream processing;visualization;reactive programming;program comprehension;data flow diagram	SE	-54.49292657675773	36.605377537153096	11
ca68ce63c19568e7cf0dd73aa07ce73b886a0095	robust multi-algorithm object recognition using machine learning methods	databases;machine learning algorithms;object recognition;service robots learning artificial intelligence object recognition robot vision;sensors;training;service robots;robot vision;object recognition training sensors image color analysis machine learning algorithms databases robustness;image color analysis;robustness;learning artificial intelligence;robust service robotics environment robust multialgorithm object recognition machine learning methods robotic applications hypothesis metrics	Robust object recognition is a crucial requirement for many robotic applications. We propose a method towards increasing reliability and flexibility of object recognition for robotics. This is achieved by the fusion of diverse recognition frameworks and algorithms on score level which use characteristics like shape, texture and color of the objects. Machine Learning allows for the automatic combination of the respective recognition methods' outputs instead of having to adapt their hypothesis metrics to a common basis. We show the applicability of our approach through several real-world experiments in a service robotics environment. Great importance is attached to robustness, especially in varying environments.	algorithm;experiment;machine learning;outline of object recognition;robot;robotics;robustness (computer science)	Tobias Fromm;Benjamin Staehle;Wolfgang Ertel	2012	2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2012.6343014	robot learning;computer vision;computer science;artificial intelligence;machine learning;3d single-object recognition;sketch recognition	Robotics	35.84635750817744	-45.72897236485131	12
7acc4520940ee089a343b495bc3a461ceb6f973a	the duality of second screens: a phenomenological study of multi-platform engagement and service experiences	technological innovation;digital service experiences second screen duality multiplatform engagement multiplatform service experiences internet traffic mobile devices service innovation research service innovation practice multidevice use service dominant logic sd logic digital entertainment services;engagement;duality second screen engagement service experience;duality;second screen;technological innovation interviews entertainment industry switches context games internet;internet;web services human factors mobile computing mobile handsets telecommunication traffic;games;interviews;switches;service experience;context;entertainment industry	Internet traffic from mobile devices surpassed traffic from traditional desktop PCs for the first time in 2014. Although users today access and experience digital services through multiple mobile and traditional devices simultaneously, little empirical research investigated the mechanisms driving this shift, or its implications for service innovation research and practice. Here, we report on a qualitative phenomenological study of multi-device use. Drawing on the emerging literature at the interface of service-dominant (SD) logic and service innovation, we conceptualize the individual devices used to access digital services as engagement platforms. We explore user behavior and experiences of digital entertainment services, and find that the perceived need to enhance or avoid elements of digital service experiences motivates users to switch between and/or supplement multiple engagement platforms with one another. We conceptualize our findings in two distinct theoretical models and delineate future research opportunities.	desktop computer;experience;mobile device;service innovation	Christoph F. Breidbach;Jennifer Chandler;Paul P. Maglio	2015	2015 48th Hawaii International Conference on System Sciences	10.1109/HICSS.2015.174	games;the internet;duality;simulation;interview;network switch;computer science;service delivery framework;marketing;multimedia;advertising;law;world wide web	HCI	-55.19890554615234	-41.2283467209054	13
cccef44d8c6d620b3b235d9173fad3f4e99554a3	langevin incremental mixture importance sampling	importance sampling;langevin diffusion;mixture density;optimal importance distribution;local approximation;kalman-bucy filter	This work proposes a novel method through which local information about the target density can be used to construct an efficient importance sampler. The backbone of the proposed method is the Incremental Mixture Importance Sampling (IMIS) algorithm of Raftery and Bao (2010), which builds a mixture importance distribution incrementally, by positioning new mixture components where the importance density lacks mass, relative to the target. The key innovation proposed here is to construct the mean vectors and covariance matrices of the mixture components by numerically solving certain differential equations, whose solution depends on the local shape of the target logdensity. The new sampler has a number of advantages: a) it provides an extremely parsimonious parametrization of the mixture importance density, whose configuration effectively depends only on the shape of the target and on a single free parameter representing pseudotime; b) it scales well with the dimensionality of the target; c) it can deal with targets that are not logconcave. The performance of the proposed approach is demonstrated on two synthetic non-Gaussian densities, one being defined on up to eighty dimensions, and on This work was partially funded by the Defence Science and Technology Laboratory through projects WSTC0058 and CDE36610. Matteo Fasiolo University of Bristol School of Mathematics Bristol, United Kingdom E-mail: matteo.fasiolo@bristol.ac.uk Flávio Eler de Melo and Simon Maskell University of Liverpool School of Electrical Engineering, Electronics and Computer Science Liverpool, United Kingdom a Bayesian logistic regression model, using the Sonar dataset. The Julia code implementing the importance sampler proposed here can be found at https:/github.c om/mfasiolo/LIMIS.		Matteo Fasiolo;Flavio Eler de Melo;Simon Maskell	2018	Statistics and Computing	10.1007/s11222-017-9747-5	econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	32.11607389231107	-26.27197810082346	14
62647a8f8a534db2ccfd0df7d513b4f084231d10	weighted som-face: selecting local features for recognition from individual face image	reconnaissance visage;image recognition;reconocimiento imagen;base donnee;facies;incertidumbre;uncertainty;database;base dato;intelligence artificielle;face recognition;local features;reconnaissance image;pattern recognition;artificial intelligence;incertitude;inteligencia artificial;reconnaissance forme;reconocimiento patron	In human face recognition, different facial regions have different degrees of importance, and exploiting such information would hopefully improve the accuracy of the recognition system. A novel method is therefore proposed in this paper to automatically select the facial regions that are important for recognition. Unlike most of previous attempts, the selection is based on the facial appearance of individual subjects, rather than the appearance of all subjects. Hence the recognition process is class-specific. Experiments on the FERET face database show that the proposed methods can automatically and correctly identify those supposed important local features for recognition and thus are much beneficial to improve the recognition accuracy of the recognition system even under the condition of only one single training sample per person.	feret (facial recognition technology);feret database;facial recognition system	Xiaoyang Tan;Jun Liu;Songcan Chen;Fuyan Zhang	2005		10.1007/11508069_46	facial recognition system;speaker recognition;computer vision;uncertainty;facies;feature;computer science;artificial intelligence;three-dimensional face recognition;3d single-object recognition;face hallucination;statistics	Vision	45.088104600722914	-59.084029244638394	15
c53f9b018f01c692dbabc58a749f05571238c74d	bandwidth enhancement of planar ebg structure using dissipative edge termination	ssn bandwidth enhancement planar type electromagnetic bandgap structure dissipative edge termination technique single ebg patches wideband suppression simultaneous switching noise;power distribution network pdn power integrity pi electromagnetic bandgap ebg structure power ground plane noise simultaneous switching noise ssn;photonic band gap bandwidth allocation integrated circuit interconnections;periodic structures;capacitors;metamaterials;bandwidth;periodic structures metamaterials bandwidth noise cutoff frequency capacitors;cutoff frequency;noise	In this paper, the bandwidth enhancement of planar type electromagnetic bandgap (EBG) structure using dissipative edge termination technique is investigated. Two termination schemes are presented; first with the termination elements along edges of every single EBG patches and second with termination along edges of the whole power plane. In the first case, simulations show that, both lower and upper cutoff frequency is improved for wideband suppression of simultaneous switching noise (SSN). For the second one, lower cutoff frequency is shifted downward from 800MHz of common planar EBG structure to 13MHz. which results in relative bandwidth of 63% higher than the commonly used EBGs. For fast simulation and validation of full-wave simulation, the circuit model of these structures is developed and simulation results are also presented.	bandwidth (signal processing);printed circuit board;simulation;subscriber identity module;zero suppression	Ali Sadr;Nasser Masoumi;Mohammad Sharifkhani	2015	2015 IEEE 13th International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2015.7182120	electronic engineering;telecommunications;engineering;electrical engineering	EDA	65.48284924200819	55.132454222659135	16
0daefe942b1ff15875c3aa2184848de882c19642	translating with scarce resources	ma chine translation;machine translation	Current corpus-based machine translation techniques do not work very well when given scarce linguistic resources. To examine the gap between human and machine translators, we created an experiment in which human beings were asked to translate an unknown language into English on the sole basis of a very small bilingual text. Participants performed quite well, and debriefings revealed a number of valuable strategies. We discuss these strategies and apply some of them to a statistical translation system.	statistical machine translation	Yaser Al-Onaizan;Ulrich Germann;Ulf Hermjakob;Kevin Knight;Philipp Koehn;Daniel Marcu;Kenji Yamada	2000			dynamic and formal equivalence;natural language processing;speech recognition;example-based machine translation;computer science;artificial intelligence;machine translation;machine translation software usability	AI	-25.938267291624037	-80.32632621140824	17
6d5d69881101d41d77530f699a06934e392ca2e1	a novel receiver design and maximum-likelihood detection for distributed mimo systems in presence of distributed frequency offsets and timing offsets		In distributed multi-input multi-output (MIMO) systems, antennas may not be able to utilize common oscillators at the transmitter side or the receiver side. Consequently, there are multiple carrier frequency offsets (CFOs) and multiple timing offsets (TOs) due to distributed antennas that may significantly degrade the system performance if not properly addressed. In this paper, we analyze the effect of multiple CFOs and multiple TOs on receiver signal energy at the front end of each receive antenna pulse matched filter output in a distributed MIMO system. We then propose a novel receiver design for distributed MIMO systems that can accommodate multiple CFOs and multiple TOs. The proposed receiver structure utilizes a bank of pulse matched filters at each receive antenna where each filter accommodates one CFO specifically. Each filter output is then sampled at the symbol rate, with sampling timing selected according to the corresponding TO, followed by an information symbol detector. For the proposed receiver configuration, we derive the maximum-likelihood (ML) symbol detector for both space-time block coded and uncoded distributed MIMO systems. In addition, we design sub-optimal detectors and show that under certain conditions, the sub-optimal detectors perform close to the ML-optimal detector with significantly lower complexities. Extensive simulation studies illustrate our theoretical development for distributed MIMO systems with various transceiver configurations and channel conditions. The simulation results indicate that the proposed receiver structure together with the ML detection offers significant performance gains compared to the current state of the art.	carrier frequency;mimo;matched filter;sampling (signal processing);sensor;simulation;transceiver;transmitter;transponder timing	Yi Cao;Weifeng Su;Stella N. Batalama	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2875391	mimo;control theory;transceiver;transmitter;front and back ends;mathematics;symbol rate;matched filter;detector;electronic engineering;communication channel	Mobile	51.85107893950817	78.71215071991895	18
728bf300a855329165e05cad2c95ca3abd1e430d	sensor fusion localization and navigation for visually impaired people		In this paper, we present an innovative cyber physical system for indoor and outdoor localization and navigation, based on the joint utilization of dead-reckoning and computer vision techniques on a smartphone-centric tracking system. The system is explicitly designed for visually impaired people, but it can be easily generalized to other users, and it is built under the assumption that special reference signals, such as colored tapes, painted lines, or tactile paving, are deployed in the environment for guiding visually impaired users along pre-defined paths. Differently from previous works on localization, which are focused only on the utilization of inertial sensors integrated into the smartphones, we exploit the smartphone camera as an additional sensor that, on one side, can help the visually impaired user to identify the paths and, on the other side, can provide direction estimates to the tracking system. We demonstrate the effectiveness of our approach, by means of experimental tests performed in a real outdoor installation and in a controlled indoor environment.		Giovanni Galioto;Ilenia Tinnirello;Daniele Croce;Federica Inderst;Federica Pascucci;Laura Giarré	2018	2018 European Control Conference (ECC)	10.23919/ECC.2018.8550373	inertial measurement unit;cyber-physical system;tracking system;sensor fusion;computer vision;exploit;computer science;image resolution;artificial intelligence	Robotics	-43.04051341221791	-42.85706956118994	19
de169dc2d361dd39a9a0a85f40144dfdba13ad58	sliding mode neural controller for nonlinear systems with higher-order and uncertainties	adaptive control;electric drives;neurocontrollers;nonlinear control systems;uncertain systems;variable structure systems;siso nonlinear system;adaptive sliding mode control;chattering problem;coupled drives ce8 system;equivalent control term;sliding mode neural controller;uncertainties	In this paper, we propose a new neural controller architecture which is derived from an adaptive sliding mode control framework for a SISO nonlinear system with higher-order and uncertainties. This neural controller can overcome some disadvantages inherent in sliding mode controllers such as the chattering problem, complex calculation of the equivalent control term and unavailable knowledge of the upper bounds of system uncertainties. Experimental results for a coupled drives CE8 system show that a real-time neural controller has been implemented successfully.	nonlinear system;real-time clock;soft-in soft-out decoder	Tri P. Nguyen;Hung T. Nguyen;Quang Phuc Ha	2004	IEEE Conference on Robotics, Automation and Mechatronics, 2004.		control engineering;open-loop controller;real-time computing;higher-order logic;sliding mode control;adaptive control;nonlinear system;computer science;engineering;control theory;upper and lower bounds	Robotics	64.31125453906986	-9.458620861681599	20
5209667afbb78993b3b63484e7fa0207612f0a86	iq-asymtre: synthesizing coalition formation and execution for tightly-coupled multirobot tasks	robot sensing systems;robot guidance;task analysis multi robot systems path planning resource allocation;invariant theory;resource allocation;path planning;information invariant theory;robot sensing systems robot kinematics cognition navigation current measurement computer architecture;satisfiability;tightly coupled tasks;multirobot tasks;coalition formation;computer architecture;navigation;general solution;current measurement;tightly coupled tasks multirobot tasks coalition formation task allocation algorithms information invariant theory robot guidance information quality;task analysis;cognition;information quality;multi robot systems;task allocation algorithms;measures of information;task allocation;robot kinematics	This paper presents the IQ-ASyMTRe architecture, which is aimed to address both coalition formation and execution for tightly-coupled multirobot tasks in a single framework. Many task allocation algorithms have been previously proposed without explicitly enabling the sharing of robot capabilities. Inspired by information invariant theory, ASyMTRe was introduced which enables the sharing of sensory and computational capabilities by allowing information to flow among different robots via communication. However, ASyMTRe does not provide a solution for how a coalition should satisfy sensor constraints introduced by the sharing of capabilities while executing the assigned task. Furthermore, conversions among different information types1 are hardcoded, which limits the flexibility of ASyMTRe. Moreover, relationships between entities (e.g., robots) and information types are not explicitly captured, which may produce infeasible solutions from the start, as the defined information type may not correspond well to the current environment settings. The new architecture introduces a complete definition of information type to guarantee the feasibility of solutions; it also explicitly models information conversions. Inspired by our previous work, IQ-ASyMTRe uses measures of information quality to guide robot coalitions to satisfy sensor constraints (introduced by capability sharing) while executing tasks, thus providing a complete and general solution. We demonstrate the capability of the approach both in simulation and on physical robots to form and execute coalitions that share sensory information to achieve tightly-coupled tasks.	algorithm;autonomous robot;computation;consensus dynamics;entity;hard coding;information quality;information security;memory management;real-time clock;real-time computing;requirement;robot;scalability;simulation	Yu Zhang;Lynne E. Parker	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5651186	invariant theory;navigation;real-time computing;simulation;cognition;resource allocation;computer science;artificial intelligence;task analysis;distributed computing;motion planning;information quality;robot kinematics;satisfiability	Robotics	-19.371217796441346	-9.27584753600163	21
c1b80ab5569747bd88a1395dd00c7a21f64a45d4	new extremal binary self-dual codes of lengths 66 and 68 from codes over r_k, m		In this work, four circulant and quadratic double circulant (QDC) constructions are applied to the family of the rings R k,m. Self-dual binary codes are obtained as the Gray images of self-dual QDC codes over R k,m. Extremal binary self-dual codes of length 64 are obtained as Gray images of λ-four circulant codes over R 2,1 and R 2,2. Extremal binary self-dual codes of lengths 66 and 68 are constructed by applying extension theorems to the F 2 and R 2,1 images of these codes. More precisely, 11 new codes of length 66 and 39 new codes of length 68 are discovered. The codes with these weight enumerators are constructed for the first time in literature. The results are tabulated.	binary code;circulant matrix;dual code;enumerator polynomial	Abidin Kaya;Nesibe Tüfekçi	2015	CoRR		binary code;linear code;discrete mathematics;combinatorics;quadratic equation;binary number;expander code;mathematics;circulant matrix	Theory	40.57968789068715	53.68850654419967	22
1268a05a03fe15bf2151606c1c4b3cec935d4aae	statistical properties of an lpc distance measure	theory and practice;distance measure;speech processing;additive noise;statistical test;linear predictive coding acoustic measurements additive noise speech processing statistical analysis statistical distributions parameter estimation equations covariance matrix acoustic testing;statistical properties;acoustic testing;linear predictive coding;statistical distributions;log likelihood ratio;statistical analysis;parameter estimation;acoustic measurements;covariance matrix	"""Several distance measures have been proposed for comparing sets of LPC coefficients. The most popular one has been the """"log likelihood ratio"""" proposed by Itakura [1]. In this paper we discuss this measure (strictly speaking, a somewhat generalized version of it) from both a theoretical and a practical point of view. We derive its statistical properties both when the reference vector is known and when it is estimated from the data. We also show how these properties are affected by windowing, additive noise, and preemphasis. We present results of extensive simulations in support of the theoretical predictions. Finally, we argue that de Souza's [2] recent criticism of this measure is unjustified."""	additive white gaussian noise;coefficient;emphasis (telecommunications);gaussian blur;lpc;linear system;microsoft windows;noise (electronics);signal-to-noise ratio;simulation;statistical model;utility functions on indivisible goods;white noise	José M. Tribolet;Lawrence R. Rabiner;Man Mohan Sondhi	1979		10.1109/ICASSP.1979.1170622	probability distribution;econometrics;covariance matrix;statistical hypothesis testing;linear predictive coding;speech recognition;likelihood-ratio test;speech processing;mathematics;estimation theory;statistics	ML	55.949142570881996	10.741735068919837	23
22d433ed5da653fa7c26036454d764eb2dfbd52a	interviewing secretaries in parallel	secretary problems;secretary problem;parallelization;competitive analysis;upper and lower bounds;time constraints;time constraint	Motivated by the parallel nature of on-line internet help-desks and human inspections, we introduce the study of interviewing secretaries in parallel, extending upon the study of the classical secretary problem. In our setting secretaries arrive into multiple queues, and are interviewed in parallel, with the aim of recruiting several secretaries in a timely manner. We consider a variety of new problems that fit this setting, and provide both upper and lower bounds on the efficiency of the corresponding interviewing policies, contrasting them with the classical single queue setting.	online and offline;secretary problem	Moran Feldman;Moshe Tennenholtz	2012		10.1145/2229012.2229053	competitive analysis;mathematical optimization;computer science;artificial intelligence;operations management;machine learning;mathematics;secretary problem;mathematical economics;upper and lower bounds;operations research;algorithm	ECom	14.728221194645625	11.102541267061941	24
1b63ddbc9948d5116eade1d615187b61dc0002dd	online stochastic ad allocation: efficiency and fairness	cs ds;allocative efficiency		fairness measure	Jon Feldman;Monika Henzinger;Nitish Korula;Vahab S. Mirrokni;Clifford Stein	2010	CoRR		financial economics;microeconomics;welfare economics	Robotics	-1.031897996505163	-4.227868460368219	25
d7b7cbcdbc9c98929a2282b1e69c26f0dc604d6b	development of surrogate spinal cords for the evaluation of electrode arrays used in intraspinal implants	biomechanical phenomena;animals;female;rats;electric stimulation;spinal cord;gelatin;elastic moduli;biomechanics;elongation surrogate spinal cord electrode array intraspinal implant mechanical suitability silicone elastomers gelatin hydrogels elastic modulus dynamic mechanical analysis human spinal cords formaldehyde crosslinked gelatin deformation;prosthetics;materials testing;analysis of variance animals biomechanical phenomena elastic modulus electric stimulation electrodes female gelatin materials testing neural prostheses rats rats sprague dawley silicone elastomers spinal cord;spinal cord materials electrodes testing force strain implants;elongation;electrodes;deformation;rats sprague dawley;analysis of variance;spinal cord injury functional electrical stimulation gelatin mechanical properties silicone elastomers;silicone elastomers;biomedical electrodes;neurophysiology;elastic modulus;neural prostheses;hydrogels;prosthetics biomechanics biomedical electrodes biomedical materials deformation elastic moduli elongation hydrogels neurophysiology;biomedical materials	We report the development of a surrogate spinal cord for evaluating the mechanical suitability of electrode arrays for intraspinal implants. The mechanical and interfacial properties of candidate materials (including silicone elastomers and gelatin hydrogels) for the surrogate cord were tested. The elastic modulus was characterized using dynamic mechanical analysis, and compared with values of actual human spinal cords from the literature. Forces required to indent the surrogate cords to specified depths were measured to obtain values under static conditions. Importantly, to quantify surface properties in addition to mechanical properties normally considered, interfacial frictional forces were measured by pulling a needle out of each cord at a controlled rate. The measured forces were then compared to those obtained from rat spinal cords. Formaldehyde-crosslinked gelatin, 12 wt% in water, was identified as the most suitable material for the construction of surrogate spinal cords. To demonstrate the utility of surrogate spinal cords in evaluating the behavior of various electrode arrays, cords were implanted with two types of intraspinal electrode arrays (one made of individual microwires and another of microwires anchored with a solid base), and cord deformation under elongation was evaluated. The results demonstrate that the surrogate model simulates the mechanical and interfacial properties of the spinal cord, and enables in vitro screening of intraspinal implants.	axon;brain implant;cns disorder;compression;cross link;dental implants, single-tooth;deploy;evaluation procedure;exhibits as topic;formaldehyde;gnu indent;gelatin;hydrogels;implantation procedure;implants;in vitro [publication type];interaction;ion implantation;medical device incompatibility problem;modulus of continuity;occur (action);procedure implants:finding:point in time:^patient:narrative;prospective search;silicone elastomers;silicones;software deployment;spinal cord;surrogate model;tissue damage;electrode	Cheng Cheng;Jonn Kmech;Vivian K. Mushahwar;Anastasia L. Elias	2013	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2013.2241061	elastic modulus;biomechanics;biological engineering;neurophysiology;physics;surgery;statistics	Visualization	27.54134551906974	-84.75413043270643	26
8b2b2c38f3ff2f45805d8a3b4cb5edf177682c1d	fast resilient jumbo frames in wireless lans	wireless links;rate adaptation wireless lan jumbo frame partial recovery;jumbo frame;propagation losses;wireless local area network;wireless link;wireless channels;wireless channel fast resilient jumbo frames wireless lan wireless local area network wireless link robustness;rate adaptation;wireless network;wireless lan performance loss throughput propagation losses wireless networks scheduling algorithm resilience robustness testing local area networks;spectrum;channel estimation;wireless lan wireless channels;probes;wireless channel;receivers;recovery rate;wireless communication;robustness;packet scheduling;partial recovery;wireless lan;fast resilient jumbo frames;throughput	With the phenomenal growth of wireless networks and applications, it is increasingly important to deliver content efficiently and reliably over wireless links. However, wireless performance is still far from satisfactory due to limited wireless spectrum, inherent lossy wireless medium, and imperfect packet scheduling. While significant research has been done to improve wireless performance, much of the existing work focuses on individual design space. We take a holistic approach to optimizing wireless performance and resilience. We propose Fast Resilient Jumbo frames (FRJ), which exploit the synergy between three important design spaces: (i) frame size selection, (ii) partial packet recovery, and (iii) rate adaptation. While these design spaces are seemingly unrelated, we show that there are strong interactions between them and effectively leveraging these techniques can provide increased robustness and performance benefits in wireless LANs. FRJ uses jumbo frames to boost network throughput under good channel conditions and uses partial packet recovery to efficiently recover packet losses under bad channel conditions. FRJ also utilizes partial recovery aware rate adaptation to maximize throughput under partial recovery. Using real implementation and testbed experiments, we show that FRJ out-performs existing approaches in a wide range of scenarios.	experiment;holism;interaction;jumbo frame;lossy compression;network packet;scheduling (computing);synergy;testbed;throughput	Anand Padmanabha Iyer;Gaurav Deshpande;Eric Rozner;Apurv Bhartia;Lili Qiu	2009	2009 17th International Workshop on Quality of Service	10.1109/IWQoS.2009.5201390	spectrum;throughput;real-time computing;telecommunications;computer science;jumbo frame;wireless network;wireless;robustness;computer network	Mobile	11.411807668246315	92.68397793727661	27
8100a19814abd3a4f08ae7ed668d6cd6ded2772f	voting prediction using new spatiotemporal interpolation methods	spatiotemporal;interpolation;presidential election;voting prediction;interpolation method	Most spatial and spatiotemporal interpolation methods give back a surface function as the result. Instead of that we consider interpolation methods that yield a single value as the final result. Voting prediction is a natural example that requires this type of spatiotemporal interpolation, because the final result is the total percentage vote for a party or candidate. We propose a new spatiotemporal interpolation method for voting prediction and similar problems. The approach can also be used in election data verification for effective government. We test the new method using USA presidential election data from the states of California, Florida, and Ohio between 1972 and 2004. The experimental results show that our method can produce comparatively precise predictions (e.g., the difference between prediction and actual result is 1.09% for Florida in 2004).	interpolation	Jun Gao;Peter Z. Revesz	2006		10.1145/1146598.1146678	econometrics;computer science;data mining;statistics	AI	87.7998453748829	-59.80741305083903	28
0010a4583d154d4b97207f283dc93cccd8eaab27	spintronic memristor as interface between dna and solid state devices	dna;magnetic domain walls;dna hybridization detection spintronic memristor solid state devices biomolecular computing platforms biomedical research biomedical practices dna molecular structures cmos molecular hybrid circuitry cmol circuitry biomolecular circuitry spintronic devices magnetic signals electrical signals magnetic sensor structures giant magnetoresistance gmr spin valve sensors tunnel magnetoresistance sensors tmr sensors on chip readout scheme telecommunication method frequency division multiplexing fdm;spintronic dna detection memristor;memristors;magnetic sensors;magnetic sensors magnetic domain walls magnetic domains dna memristors magnetic tunneling;spin valves biocomputing biomolecular electronics cmos integrated circuits dna frequency division multiplexing giant magnetoresistance magnetic sensors magnetoelectronics memristor circuits;magnetic domains;magnetic tunneling	Recently biomolecular computing platforms have been widely investigated with great potentials in both biomedical research and practices, such as using molecular structures of DNA to present the data bits and to operate the logic. Emerging CMOS/molecular hybrid (CMOL) circuitry demonstrates many overwhelming advantages compared with pure biomolecular circuitry, including the design flexibility and compatibility with the traditional CMOS process. In this work, spintronic devices are utilized to detect the spatial information of DNA by translating the magnetic signals associated with the DNA pieces to electrical signals. The physical mechanisms and sensing performances of various magnetic sensor structures are discussed and evaluated, including giant magneto-resistance (GMR) spin valve sensors, tunnel magneto-resistance (TMR) sensors and our newly proposed spintronic GMR/TMR memristor sensors. A on-chip readout scheme is also proposed with a telecommunication method-frequency division multiplexing (FDM) technique to efficiently transmit useful information and filtrate the noise which could achieve high SNR up to 70 dB. The new approach can be adopted as not only the interface between the DNA structure and CMOS circuitry, but also a promising sensing mechanism in the DNA hybridization detection technique.	cmos;carpal tunnel syndrome;electronic circuit;finite difference method;frequency divider;giant magnetoresistance;memristor;multiplexing;performance;sensor;signal-to-noise ratio;solid-state electronics;spintronics;triple modular redundancy	Jianlei Yang;Zhenyu Sun;Xiaobin Wang;Yiran Chen;Hai Li	2016	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2016.2547700	electronic engineering;memristor;magnetic domain;electrical engineering;nanotechnology;dna	EDA	52.56863549515593	50.324661600497265	29
0ef750e77224c2cf56392deb807ea66d976cc269	agent based dynamic execution of bpel documents	algoritmo paralelo;commerce electronique;multiagent system;cortafuego;business process execution language;langage execution processus metier;comercio electronico;parallel algorithm;agent based;processus metier;securite informatique;design flow;service web;web service;satisfiability;orientado servicio;qualite service;algorithme parallele;computer security;pare feu reseau;seguridad informatica;proceso oficio;oriente service;sistema multiagente;service oriented architecture;service quality;electronic trade;business process;servicio web;systeme multiagent;service oriented;calidad servicio;firewall	Web Services are the most promising innovative solution in order to remove business and technical obstacles for e-business. They support a true service oriented architecture that is designed to securely expose business logic beyond the firewall. As the momentum around Web Services increases, there is a growing need for effective mechanisms to coordinate the interaction among them. Business Process Execution Language for Web Services (BPEL) is a good candidate, such that it satisfies the needs of the business to enable this interaction. However, BPEL still is far from fulfilling all the requirements of composition. In this research, we constructed an architecture to improve performance of parallel executions in BPEL documents. Our architecture, by the help of our controller agent, allows dynamic execution of statically designed flow with respect to customer defined issues and QoS parameters at both implementation time and runtime.		Ali Emre Arpaci;Ayse Basar Bener	2005		10.1007/11569596_36	web service;firewall;simulation;business process execution language;computer science;design flow;operating system;service-oriented architecture;database;distributed computing;parallel algorithm;business process;computer security;service quality;satisfiability	DB	-40.78919351095465	23.99666820102186	30
575cc8aee5f8b32e44ff2b48eb59265b0c9d8b82	bf-classifier: background/foreground classification and segmentation of soundscape recordings	soundscape;classification;sound design;generative systems	Segmentation and classification is an important but time consuming part of the process of using soundscape recordings in sound design and research. Background and foreground are general classes referring to a signal's perceptual attributes, and used as a criteria by sound designers when segmenting sound files. We establish the background / foreground classification task within a musicological and production-related context, and present a method for automatic segmentation of soundscape recordings based on this task. We created a soundscape corpus with ground truth data obtained from a human perception study. An analysis of the corpus showed an average agreement of each class - background 92.5%, foreground 80.8%, and background with foreground 75.3%. We then used the corpus to train a machine learning technique using a Support Vector Machines classifier. An analysis of the classifier demonstrated similar results to the average human performance (background 96.7%, foreground 80%, and background with foreground 86.7%). We then report an experiment evaluating the classifier with different analysis windows sizes, which demonstrates how smaller window sizes result in a diminishing performance of the classifier.	brainfuck;ground truth;human reliability;machine learning;microsoft windows;statistical classification;support vector machine;text corpus;window function	Miles Thorogood;Jianyu Fan;Philippe Pasquier	2015		10.1145/2814895.2814926	sound design;generative systems;computer vision;speech recognition;soundscape;biological classification;computer science	AI	-18.182699648701753	-82.7997400867771	31
1afd0e1e71f69c6fe54198b77a722f2eca3ee4ec	partial sensing coverage with connectivity in lattice wireless sensor networks	deployment efficiency;partial sensing coverage;lattice sensor networks;performance evaluation;simulation;lattice wsns;mathematically modelling;network detectability;connectivity;wireless sensor networks	In this work, we investigate the partial sensing coverage with connectivity problem in structured lattice wireless sensor networks (WSNs) through mathematically modelling, theoretical derivation, computer-based simulations, and performance evaluation. Two popular patterns (square and equilateral triangle) are considered. We found: 1) a noticeable amount of sensors can be saved in lattice WSNs if a small percentage of application-tolerable sensing coverage is sacrificed; 2) there exists a threshold partial sensing coverage,π/4, below which both square and triangle patterns require the same amount of sensors, above which the triangle pattern is superior; 3) deployment pattern impacts the network detectability and 15.47% more sensors are required to guarantee the worst-case detectability in a triangular lattice WSN; and 4) there exists a critical ratio of communication range to sensing range in employing partial sensing coverage to trade for deployment efficiency. Simulation outcomes are shown to validate the analysis.	best, worst and average case;correctness (computer science);mathematical model;performance evaluation;sensor;simulation;software deployment	Yun Wang;William Chu;Yanping Zhang;Xiaolong Li	2013	IJSNet	10.1504/IJSNET.2013.059080	embedded system;simulation;wireless sensor network;telecommunications;computer science;connectivity;computer network	Mobile	15.022873854665896	86.72424600001081	32
afa43a2ed466b1719ef2766341c06e78ab485b84	emotion in decision making under uncertainty and cognitive control: a computational framework	emotion based cognitive model;decision making uncertainty cognition educational institutions technology management humans symbiosis computational intelligence appraisal finance;human performance;decision making under uncertainty;socio cultural variables;zajone lazarus controversy;intelligent behavior;immediate emotion;zajone lazarus controversy decision making cognitive control socio cultural variables human performance immediate emotion emotion based cognitive model intelligent behavior;cognition;cognitive control;decision making cognition;cognitive model;parallel processing	Models of decision making usually focus on cognitive, situational, and socio-cultural variables in accounting for human performance. However, emotion especially immediate emotion is rarely addressed within these models. This paper proposes an emotion-based cognitive model about how decisions are made which unifies emotion and cognition as two symbiotically integrated aspects of intelligent behavior. Within the general computational framework the influence of both immediate emotion and expected emotion are included. And to deal with the Zajone-Lazarus controversy concerns whether emotion depends upon appraisal of a situation or is triggered directly by stimulus features, the present study fits the compromise interpretation that there are parallel processes involved in the triggering of emotion. We also give a formalized description of the model to make it computable and illustrate it by an example of taxpayer's reporting decision.	cognition;cognitive model;computable function;computation;fits;gamergate controversy;human reliability;lazarus;process modeling	Ping Xu;Tao Wang	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.351	psychology;cognitive psychology;affective science;communication;social psychology	AI	-23.36710968373652	-15.017942574872745	33
80da7e5b77c136496fb7e6fd7bcac5fe4150403b	sharing learning experiences through correspondence on the www	libraries;emotional factors;information resources;groupware;correspondence;instruments;collaborative work;shared learning experiences;meta data groupware educational technology information resources internet;learning experience;world wide web collaborative work educational technology instruments tv casting libraries feedback psychology problem solving;social factors;portfolios;psychology;collaborative learning;learning networks;meta data descriptions;project work;learning communities;feedback;casting;internet;student meta knowledge capture;asynchronous learning networks;learning experience reflection;emotional factors shared learning experiences correspondence world wide web asynchronous learning networks learning communities knowledge sharing meta data descriptions portfolios project work learning experience reflection individual learning collaborative learning student meta knowledge capture social factors;knowledge sharing;world wide web;individual learning;meta data;tv;educational technology;problem solving	Aswchronous Learning Networks are the facilities and procediires to allow members of the learning coniniiinities to be more effective arid eflcient in their learning. One approach is to see how the ‘sharing’ of knowledge can be aiigniented through metadata descriptions attached to porfolios and project work. Another approach is to facilitate the reflection upon individiial or collaborative learning experiences (Okanioto, Cristea, Matsui, & Miwatu, 2000). The positiori to be defended in niv panel contribiition is that both the metadata approach and the atternpts to capture the students’ nzeta-knowledge are rather coniplicated because of social arid eniotional reasons.	www	Piet Kommers	2001		10.1109/ICALT.2001.943991	learning community;collaborative learning;cooperative learning;educational technology;social learning;casting;the internet;computer science;knowledge management;experiential learning;feedback;multimedia;metadata;world wide web;synchronous learning;pedagogy	ML	-81.18136742829395	-42.6923953258153	34
8407499ede6944ba16909d09a2a3e6e2c4b29499	lower bounds on the size of semi-quantum automata		In the literature, there exist several interesting hybrid models of finite automata which have both quantum and classical states. We call them semi-quantum automata. In this paper, we compare the descriptional power of these models with that of DFA. Specifically, we present a uniform method that gives a lower bound on the size of the three existing main models of semi-quantum automata, and this bound shows that semi-quantum automata can be at most exponentially more concise than DFA. Compared with a recent work (Bianchi, Mereghetti, Palano, Theoret. Comput. Sci., 551(2014), 102-115), our method shows the following two advantages: (i) our method is much more concise; and (ii) our method is universal, since it is applicable to the three existing main models of semi-quantum automata, instead of only a specific model.	automata theory;automaton;existential quantification;finite-state machine;semiconductor industry	Lvzhou Li;Daowen Qiu	2015	CoRR		combinatorics;discrete mathematics;continuous spatial automaton;quantum finite automata;computer science;nested word;ω-automaton;mathematics;dfa minimization;algorithm	Logic	0.32209799748777335	22.25537517320473	35
803757415eafae6f559771ae29caa3207f5f8fde	an fpga architecture for velocity independent backprojection in fmcw-based sar systems	microprocessors;computer architecture;radar imaging;field programmable gate arrays;signal processing algorithms;synthetic aperture radar	This paper introduces a new FPGA architecture optimized for Frequency Modulated Continuous Wave (FMCW) Synthetic Aperture Radar (SAR). The architecture implements a Global-Backprojection-Algorithm (GBP) which has been modified to be independent of platform velocity (start-stop-approximation). The design supports parallelism of dedicated GBP processing modules in order to provide high performance. Compared to a MATLAB implementation on a single core Intel i5 at 3.2 GHz the dedicated implementation on a ML605 board provides a minimum speed-up factor of 94. The entire FPGA system was tested with real-life SAR data.	algorithm;approximation;field-programmable gate array;image quality;matlab;modulation;parallel computing;ramp simulation software for modelling reliability, availability and maintainability;radar;real life;speedup;velocity (software development)	Fabian Cholewa;M. Wielage;Peter Pirsch;Holger Blume	2016	2016 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)	10.1109/ISSPIT.2016.7886044	embedded system;parallel computing;synthetic aperture radar;computer science;radar imaging;field-programmable gate array	Arch	3.078759040021569	45.73506915461534	36
dc9238a8b59324864ccda3ec6ea305a96d71dac8	anybody, anywhere, anytime - robotics with a social impact through a building block approach	sport educational computing educational robots innovation management mobile robots multi robot systems social aspects of automation;modular building blocks social impact embodied artificial intelligence user driven innovation high tech solutions physical manipulation physical reconfiguration abstract thinking innovative configurations modular robotic devices health improvements africa modular interactive tiles tanzania soccer tournaments asia europe;rehabilitation robotics modular robotics educational robotics embodied ai contextualised it;tiles robots context games abstracts light emitting diodes	In order to confront the challenge of creating robotics with a social impact, this paper speculates on how to allow anybody anywhere hands-on opportunity to make contextualized solutions. Inspired by embodied artificial intelligence and modular robotics, we present the building block approach as a way to achieve user-driven innovation of high tech solutions with a social impact. Modularity invites to physical manipulation and reconfiguration, and in the process of physical manipulation, the user starts understanding and developing the functionality. Through immediate action in the interaction, the user is not only learning about the abstract thinking that the system may represent, but at the same time, the user is able to try the system in new, innovative configurations. Hence, the system becomes a true engine for innovation allowing the user to creatively invent, build and test new developments as the exploration is happening. As instantiations of this building block approach and its social impact, we briefly review the development and use of modular robotic devices for education, health improvements, and business in Africa. We briefly outline the modular building blocks for education, modular interactive tiles for rehabilitation in Tanzania and for soccer tournaments in Africa, Asia and Europe.	anytime algorithm;artificial intelligence;hands-on computing;robot;robotics	Henrik Hautop Lund	2011	Advanced Robotics and its Social Impacts	10.1109/ARSO.2011.6301970	simulation;artificial intelligence;geography of robotics;self-reconfiguring modular robot;future of robotics;educational robotics	AI	-57.436023927870316	-36.06851110718046	37
952f99a1451e8f782a0212a2b0924ea8f7683b0e	a grasp approach for the extended car sequencing problem	metaheuristics;satisfiability;milp;scheduling;car sequencing problem;grasp	This paper presents a solution procedure for a new variant of the Car Sequencing Problem (CSP) based on the GRASP metaheuristic. In this variant, called xCSP (extended CSP), the aim is to satisfy the hard constraints of the CSP while scheduling the maximum possible number of cars with specific options at specific times of the day in order to satisfy other production requirements. Additional constraint ratios are likewise considered that force at least a minimum specific number of consecutive options. An extension of the CSP is formalized in this paper and computational results are presented using available on-line instances that verify the good performance of a GRASP procedure defined for the xCSP.	algorithm;computation;grasp;local optimum;metaheuristic;microsoft windows;online and offline;requirement;scheduling (computing);testbed;universal product code;verification and validation	Joaquín Bautista;Jordi Pereira;Belarmino Adenso-Díaz	2008	J. Scheduling	10.1007/s10951-007-0046-4	mathematical optimization;computer science;operating system;grasp;scheduling;algorithm;metaheuristic;satisfiability	AI	16.21659749175002	2.0740497890286775	38
d5aca18de1158e9b103504c2fbb548777509d521	impedance control and performance measure of series elastic actuators		Series elastic actuators (SEAs) have become prevalent in torque-controlled robots in recent years to achieve compliant interactions with environments and humans. However, designing optimal impedance controllers and characterizing impedance performance for SEAs with time delays and filtering are still underexplored problems. This article addresses the controller design problem by devising a critically damped gain design method for a class of SEA cascaded control architectures, which is composed of outer impedance and inner torque feedback loops. We indicate that the proposed gain design criterion solves optimal controller gains by maximizing phase-margin-based stability. Meanwhile, we observe a tradeoff between impedance and torque controller gains and analyze their interdependence in terms of closed-loop stability and overall impedance performance. Via the proposed controller design criterion, we adopt frequency-domain methods to thoroughly analyze the effects of time delays, filtering, and load inertia on SEA impedance performance. A novel impedance performance metric, defined as “Z-region,” is proposed to simultaneously quantify achievable impedance magnitude range (i.e., Z-width) and frequency range (i.e., Z-depth). Maximizing the Z-region enables SEA-equipped robots to achieve a wide variety of Cartesian impedance tasks without alternating the control structure. Simulations and experimental implementations are performed to validate the proposed method and performance metric.	characteristic impedance;computer simulation;control flow;feedback;frequency band;interaction;interdependence;nominal impedance;optimal control;phase margin;region-based memory management;robot	Nicholas Paine;Steven Jens Jorgensen;Luis Sentis	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2017.2745407	control theory;control theory;filter (signal processing);control engineering;torque;engineering;performance metric;impedance control;electrical impedance;impedance bridging;automatic frequency control	Robotics	68.37055635078676	-17.453740388453525	39
96d29a1ec84fd8bcca3cfe3c21c102ac527aceb8	in and out of actor-network theory: a necessary but insufficient journey	information systems;t technology general;social construction;information systems management;research method;science and technology studies;empirical evidence;he transportation and communications;research methods;is success;actor network theory;politics;is research;yield management;design methodology;management power	Purpose – This paper seeks to offer a retrospective look at an intellectual journey in and out of using actor‐network theory, which the author drew on to carry out an in‐depth case study of the troubled implementation of a computerised reservation system in a major transport company. The application of some key ANT concepts, i.e. human and non‐human actors, symmetry and translation, is reflected upon, highlighting their benefits and limitations.Design/methodology/approach – The paper's aims are accomplished through a confessional account of how it was done, rather than a normative post hoc justification. Some empirical evidence is provided to illustrate the difficulties and problems encountered in travelling back and forth between theory, methodology and data.Findings – In particular, ANT was very useful in focusing the paper on how to look at IS success and failure symmetrically and how social and technical distinctions are socially constructed, for instance in the conception and application of yield man...	network theory	Nathalie N. Mitev	2009	IT & People	10.1108/09593840910937463	politics;yield management;social science;empirical evidence;design methods;computer science;engineering;operations management;management science;sociology;management;operations research;law;information system;science, technology and society;research	HCI	-77.42794169387149	-2.567524730848042	40
4fa7e5388ef62f2abe42b84f057cec449aaa3eaf	design for diagnosable multiple-output digital systems	logic design;automatic testing;digital systems partitioning algorithms fault detection costs testing hardware very large scale integration circuit faults isolation technology educational institutions;logic testing automatic testing digital systems error detection fault location logic design;large size digital systems diagnosable system design multiple output digital systems error detection checkers partition algorithm partition algorithm automatic fault isolation computer implementation;digital systems;logic testing;error detection;fault isolation;fault location	A design for diagnosable multiple-output digital systems is presented, in which not only error detection is implemented by the minimum number of checkers but also fault isolation is realized by minimal additional hardware instead of traditional software diagnostic procedures such that the computation time and memory space for fault isolation are eliminated. A partition algorithm is utilized for partitioning a digital system specially into subsystems with each subsystem containing only one primary output. With the aid of this partition algorithm, an algorithm for automatic fault isolation is presented. The algorithm is suitable for computer implementation so that faults in large size digital systems can be isolated automatically. The overhead of the additional hardware to isolate faults automatically is less than 2.1%.<<ETX>>	algorithm;computation;dspace;digital electronics;error detection and correction;fault detection and isolation;overhead (computing);time complexity	He-De Ma;Ying Liu	1991	Digest of Papers 1991 VLSI Test Symposium 'Chip-to-System Test Concerns for the 90's	10.1109/VTEST.1991.208159	embedded system;electronic engineering;logic synthesis;real-time computing;error detection and correction;fault coverage;computer science;stuck-at fault;theoretical computer science;fault model;fault detection and isolation;statistics	EDA	21.117871344554143	49.48024924108161	41
777d5957ffbe2d53af0116bba8a224de59e69413	heuristic evaluation of interface usability for a web-based opac	online catalogues;interfase usuario;opac open public access catalog;t technology general;bibliotheque universitaire;user interface;interface design;worldwide web;asie;automated catalog;biblioteca universitaria;utilisabilite;malaisie;malaysia;interface utilisateur;heuristic evaluation;evaluation;evaluacion;usabilidad;catalogue automatise;catalogo automatizado;usability;malasia;university library;asia;iium	Purpose - The purpose of this paper is to investigate the usability of a web-based OPAC (WebPAC) user interface at the International Islamic University Malaysia (IIUM). It also looks at the applicability of heuristic evaluation in designing a user-centered WebPAC interface. Design/methodology/approach - Based on Nielsen's ten usability heuristic principles, the study focuses on three heuristics only, i.e. aesthetic and minimalist design, match between interface and the real world, and visibility of interface status. Findings - Results of the study found that the WebPAC interface conforms to at least 70 percent usability properties prescribed. Usability problems violated in the interface were identified. Practical implications - The study suggests that heuristic evaluation is applicable in libraries to asses the usability of user interface for online catalogs. Originality/value - Heuristic evaluation could assist libraries in designing user-centered interface for online catalogs.	heuristic evaluation;online public access catalog;usability;web application	Yushiana Mansor;Abdul Rani Widyawati	2007	Library Hi Tech	10.1108/07378830710840491	usability goals;pluralistic walkthrough;web usability;cognitive walkthrough;usability;human–computer interaction;computer science;system usability scale;interface design;evaluation;operating system;usability engineering;database;multimedia;user interface;heuristic evaluation;world wide web;usability lab;usability inspection	HCI	-62.22792846602357	-48.03640391779322	42
c2250ed485c3df3afab7817913bf6b93c555bdb4	a coupled neural oscillator model for recruitment and annihilation of the degrees of freedom of oscillatory movements	motor pattern;oscillations;coupled oscillators;bifurcation;coupled oscillator;degree of freedom;rhythmic movements;neural network model;neural oscillator;neurodynamics	"""We have constructed a neural network model for recruitment and annihilation of the degrees of freedom of the oscillatory activities which has been observed in experiments of human """"nger movements. The model is composed of two asymmetrically coupled oscillators which are a sti! oscillation system. The change of coupling strength is assumed to make the frequency of the oscillation change and invoke the behavior. By the change of the coupling coe$cient between units inside each oscillator, the oscillatory mode in the network transits from the horizontal mode to the synchronized mode, and then to the vertical mode as the frequency becomes higher. ( 1999 Elsevier Science B.V. All rights reserved."""		Hirofumi Nagashino;Minoru Kataoka;Yohsuke Kinouchi	1999	Neurocomputing	10.1016/S0925-2312(99)00051-X	computer science;machine learning;control theory;degrees of freedom;oscillation;artificial neural network	AI	16.88551961558631	-70.55204609854714	43
0bae3f76de15cccf40a473c76c922734827964b5	a concurrency control scheme for mobile transactions in broadcast disk environments	wireless channels;experimental analysis;broadcast channel;wireless broadcast channels;broadcast data;mobile databases;mobile computer;mobile database;concurrency control;performance analysis;data access;data broadcast;mobile transactions;read only transaction;broadcast disks;mobile computing;data dissemination	Broadcast disk technique has been often used to disseminate frequently requested data efficiently to a large volume of mobile clients over wireless channels. In broadcast disk environments, a server often broadcasts different data items with differing frequencies to reflect the skewed data access patterns of mobile clients. Previously proposed concurrency control methods for mobile transactions in wireless broadcast environments are focused on the mobile transactions with uniform data access patterns. These protocols perform poorly in broadcast disk environments where the data access patterns of mobile transactions are skewed. In broadcast disk environments, the time length of a broadcast cycle usually becomes large to reflect the skewed data access patterns. This will often cause read-only transactions to access old data items rather than the latest data items. Furthermore, updating mobile transactions will be frequently aborted and restarted in the final validation stage due to the update conflict of the same data items with high access frequencies. This problem will increase the average response time of the update mobile transactions and waste the uplink communication bandwidth. In this paper, we extend the existing FBOCC concurrency control method to efficiently handle mobile transactions with skewed data access patterns in broadcast disk environments. Our method allows read-only transactions to access the more updated data, and reduces the average response time of updating transactions through early aborts and restarts. Our method also reduces the amount of uplink communication bandwidth for the final validation of the update transactions. We present an in-depth experimental analysis of our method by comparing with existing concurrency control protocols. Our performance analysis shows that it significantly decreases the average response time and the amount of uplink bandwidths over existing methods.	concurrency (computer science);concurrency control	Sungwon Jung;Keunha Choi	2009	Data Knowl. Eng.	10.1016/j.datak.2009.02.008	real-time computing;mobile database;computer science;database;distributed computing;mobile computing	DB	-15.082623697540113	68.77580243468003	44
8e52f1e7b8751582b5bfc51a0d3cf1fac813899b	an efficient multi-resolution reconstruction scheme with motion compensation for 5d free-breathing whole-heart mri		In this work we propose a novel approach for the reconstruction of 3D isotropic, free-breathing cardiac cine MRI with 100% data e ciency. The main components are a continuous 3D Golden radial k-space data acquisition, a robust groupwise cardio-respiratory motion estimation technique and a multiresolution strategy introduced in a previously proposed compressed sensing reconstruction scheme. Initial results on simulated data show better reconstruction quality than the non-motion compensated counterpart and reduced reconstruction times with respect a single-resolution procedure for equivalent acceleration factors ranging 24.38 to 34.8.		Rosa-María Menchón-Lara;Javier Royuela-del-Val;Alejandro Godino-Moya;Lucilio Cordero-Grande;Federico Simmross-Wattenberg;Marcos Martín-Fernández;Carlos Alberola-López	2017		10.1007/978-3-319-67564-0_14	computer vision;acceleration;computer science;compressed sensing;artificial intelligence;real-time mri;ranging;motion compensation;data acquisition;motion estimation;data efficiency	Vision	49.4935081795632	-81.40338780948554	45
fbc7c3de4b23cea3be54a2ccc8449c9f153f094c	risk identification and project health prediction in it service management	key performance indicator;strategic outsourcing;outsourcing;project management;service provider;financial management;information technology;risk management;service management;contracts;probes;servers;risk management health care information technology management information systems outsourcing;risk identification;management information systems;predictive models;profitability;strategic outsourcing risk identification project health prediction it service management;book reviews;it service management;environmental management;proposals;quality management;project management risk management contracts proposals outsourcing quality management profitability costs environmental management financial management;project health prediction;health care	In this paper, we introduced ■ The lifecycle of IT service project management ■ The challenge of maintaining the health of strategic outsourcing projects ■ A framework and methodology to continuously probe and assess risks for ongoing projects ■ A predictive model built upon the training data collected from such probes and past project review data that can successfully predict the health status of ongoing projects	outsourcing;predictive modelling	Geraldine Abbott;Nikos Anerousis;Forrest Gordon;Allen Grussing;Sergey Makogon;Paul Manore;Fiona Humphries;Joan Sherry;Shu Tao	2010	2010 IEEE Network Operations and Management Symposium - NOMS 2010	10.1109/NOMS.2010.5488375	service provider;project management;quality management;risk management;service management;knowledge management;performance indicator;predictive modelling;management;information technology;health care;server;profitability index;outsourcing	SE	-70.36358078851757	15.783442093157348	46
f43bd70fca57f78996345792013ceb78b58016b2	diffusion strategies for distributed kalman filter with dynamic topologies in virtualized sensor networks		Network virtualization has become pervasive and is used in many applications.Through the combination of network virtualization and wireless sensor networks, it can greatly improve the multiple applications of traditional wireless sensor networks. However, because of the dynamic reconfiguration of topologies in the physical layer of virtualized sensor networks (VSNs), it requires a mechanism to guarantee the accuracy of estimate values by sensors. In this paper, we focus on the distributed Kalman filter algorithm with dynamic topologies to support this requirement. As one strategy of distributed Kalman filter algorithms, diffusion Kalman filter algorithm has a better performance on the state estimation. However, the existing diffusion Kalman filter algorithms all focus on the fixed topologies. Considering the dynamic topologies in the physical layer of VSNs mentioned above, we present a diffusion Kalman filter algorithm with dynamic topologies (DKFdt). Then, we emphatically derive the theoretical expressions of the mean and mean-square performance. From the expressions, the feasibility of the algorithm is verified. Finally, simulations confirm that the proposed algorithm achieves a greatly improved performance as compared with a noncooperative manner.	converge;dynamical system;kalman filter;peterson's algorithm;sensor;simulation	Shujie Yang;Tao Huang;Jianfeng Guan;Yongping Xiong;Mu Wang	2016	Mobile Information Systems	10.1155/2016/8695102	real-time computing;computer science;distributed computing;moving horizon estimation;computer network	Mobile	16.670583720127834	76.74368032973894	47
f982c32ada6c1e86a1f46ebdaf2ab110dd837014	creating a level playing field for all symbols in a discretization		In time series analysis research there is a strong interest in discrete representations of real valued data streams. The discretization process offers several desirable properties such as numerosity/dimensionality reduction, the removal of excess noise and the access to numerous algorithms that typically require discrete data. One approach that emerged over a decade ago and is still (along with its successors) considered state-of-the-art is the Symbolic Aggregate Approximation (SAX) algorithm proposed in Lin et al. [8] [9]. This discretization algorithm was the first symbolic approach that mapped a real-valued time series to a symbolic representation that was guaranteed to lower-bound Euclidean distance. The interest of this paper concerns the SAX assumption of data being highly Gaussian and the use of the standard normal curve to choose partitions to discretize the data. Though not necessarily, but generally, and certainly in its canonical form, the SAX approach chooses partitions on the standard normal curve that would produce an equal probability for each symbol in a finite alphabet to occur. This procedure is generally valid as a time series is normalized to have a μ = 0 and a σ = 1 before the rest of the SAX algorithm is applied. However there exists a caveat to this assumption of equi-probability due to the intermediate step of Piecewise Aggregate Approximation (PAA). What we will show in this paper is that when PAA is applied M. Butler Department of Computer Science, The University of York, Deramore Lane, York, YO10 5GH, United Kingdom Tel.: +44(0)1904-325500 Fax: +44(0)1904-325599 E-mail: mbutler@cs.york.ac.uk D. Kazakov Department of Computer Science, The University of York, Deramore Lane, York, YO10 5GH, United Kingdom Tel.: +44(0)1904-325500 Fax: +44(0)1904-325599 E-mail: kazakov@cs.york.ac.uk ar X iv :1 21 0. 51 18 v1 [ cs .D S] 1 8 O ct 2 01 2 2 Matthew Butler, Dimitar Kazakov the distribution of the data is indeed altered, resulting in a shrinking standard deviation that is proportional to the number of points used to create a segment of the PAA representation and the degree of auto-correlation within the series. Data that exhibits statistically significant auto-correlation is less affected by this shrinking distribution. As the standard deviation of the data contracts, the mean remains the same, however the distribution is no longer standard normal and therefore the partitions based on the standard normal curve are no longer valid for the assumption of equal probability.	aggregate function;amplifier;approximation algorithm;autocorrelation;computer science;data point;dimensionality reduction;discrete mathematics;discretization;euclidean distance;fax;genetic algorithm;matthew flatt;offset binary;time series	Matthew Butler;Dimitar Kazakov	2012	CoRR		mathematical optimization;combinatorics;discrete mathematics;calculus;mathematics;algorithm;statistics	ML	40.96522716147204	17.739530546746842	48
ee81ecd44515bba53d35969aa2c7c11bba48d41d	data and syntax centric anomaly detection for relational databases		Recent studies show that insider attacks that aim at exfiltrating data are very common and that these attacks are performed according to specific patterns. Protecting against such threats requires complementing existing security techniques, such as access control and encryption, with tools able to detect anomalies in data accesses. In this paper, we present a technique specifically tailored for detecting anomalous database accesses. Our technique extracts users’ access patterns based on both the syntax of the input queries and the amount of data in their output results. Our technique is based on mining SQL queries in database audit logs in order to form profiles of the normal users’ access patterns. New queries are checked upon these profiles, and deviations from these profiles are considered anomalous and thus indicative of possible attempts to exfiltrate or misuse the data. Our technique works under two application scenarios. The first is when the database has role-based access control (RBAC) in place. Under an RBAC system, users belong to roles and privileges are associated with roles rather than individual users. For this scenario, we form profiles of roles which make our approach usable for database management systems (DBMSs) that have a large user population; in this scenario, we apply the naive Bayesian classifier which shows accurate results in practice. We also employ multilabeling classification to enhance accuracy when the access patterns are common to multiple roles. The second application scenario is when the DBMS does not apply RBAC. In this scenario, we apply the COBWEB clustering method. Experimental results indicate that our techniques are very effective. © 2016 John Wiley & Sons, Ltd	anomaly detection;bayesian network;benchmark (computing);cluster analysis;data mining;data theft;database audit;encryption;john d. wiley;multi-level cell;naive bayes classifier;online transaction processing;random access;relational database;role-based access control;sql;sensor;statistical classification	Asmaa Sallam;Daren Fadolalkarim;Elisa Bertino;Qian Xiao	2016	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.1195	computer science;data mining;database;world wide web	Security	-60.52245618096578	61.304476523708125	49
2b319f237f205fec6579fe77f24fa641af180ab9	an emperical study on the test adequacy criterion based on coincidental correctness probability	mutation analysis;dynamic data flow analysis;test adequacy criteria;dynamic data;adequacy testing criterion;coincidental correctness;flow analysis	In recent years, coincidental correctness attracts many interests in testing area. Studies prove that it can adversely affect the effectiveness of testing. But current test adequacy criteria don't consider the effect of coincidental correctness. To address the problem, we design an approach to calculate the probability that an error in a given statement of the program under test is hidden by coincidental correctness. Such probability is used to measure the test adequacy on the statement. The test adequacy of a test suit is computed based on the adequacy on all the statement. A mutation analysis on five C programs is designed to compare the effectiveness of our adequacy criterion. The experimental results consistently approve our conjecture that such a test approach effectively quantizes the ability of test suite to find errors.	correctness (computer science);mutation testing;test suite	Jie Chen;Qian Li;Jianhua Zhao;Xuandong Li	2010		10.1145/2020723.2020743	reliability engineering;computer science;data mining;algorithm	SE	-60.484530268828266	36.567270299647575	50
a3bc60e546935d9fe868061d03f29c6f00c36bfb	subtyping supports safe session substitution		Session types describe the structure of bi-directional pointto-point communication channels by specifying the sequence and format of messages on each channel. A session type defines a communication protocol. Type systems that include session types are able to statically verify that communication-based code generates, and responds to, messages according to a specified protocol. It is natural to consider subtyping for session types, but the literature contains conflicting definitions. It is now folklore that one definition is based on safe substitutability of channels, while the other is based on safe substitutability of processes. We explain this point in more detail, and show how to unify the two views.	communications protocol	Simon J. Gay	2016		10.1007/978-3-319-30936-1_5	computer science;database;distributed computing;algorithm	PL	-31.16553933370421	31.708725674291312	51
fbaf99259b6cd6b9f7c98c7455132ea0de5b513c	on discrete minimax problems in r using interval arithmetic	minimax problem;funcion discreta;entropia;problema minimax;contrainte inegalite;inequality constraint;probleme minimax;discrete function;fonction discrete;constrenimiento desigualdad;entropie;aritmetica intervalo;entropy;interval arithmetic;arithmetique intervalle	Interval algorithms for bounding discrete minimax function values of problems in which the constituent minimax functions are continuously differentiable functions of one real variable in a bounded closed interval are presented, both with and without inequality constraints represented by continuously differentiable constraint functions.	interval arithmetic;minimax	Michael A. Wolfe	1999	Reliable Computing	10.1023/A:1009930013647	entropy;mathematical optimization;mathematical analysis;discrete mathematics;mathematics;minimax approximation algorithm	HPC	46.488386399631224	22.18012620354526	52
8fd07255e61f9d967f44b1992f66b6454ddf8b18	the value of rewards: exploring world of warcraft for gamification design	rewards;world of warcraft;gamification	Gamification design still lacks of a catalogue of game design elements defined on the basis of the players' experience, rather than of the game designers' expertise. In this work I outline an ongoing project that tries to define a catalogue of game elements for the gamification domain starting from the players' perspective. By investigating how World of Warcraft rewards its players through an ethnographic research, I describe how its rewards embody different values, as well as produce diversified effects on players' experience, and how these findings may be employed in the gamification domain.	gamification;world of warcraft	Amon Rapp	2016		10.1145/2968120.2987721	simulation;engineering;knowledge management;multimedia	HCI	-62.18620103305703	-36.376788226682415	53
273122c4194be7351ce88503d6803e07b99c1843	an on-demand scaling stereoscopic 3d video streaming service in the cloud	information systems applications incl internet;software engineering programming and operating systems;computer communication networks;special purpose and application based systems;computer system implementation;computer systems organization and communication networks	We describe a web service providing a complete stereoscopic 3D video multi-stream cloud application to serve a potentially very large number of clients over the Internet. The system architecture consists of a stream provider that leverages highly scalable and reliable cloud computing and storage services, with automatic load balancing capability for live and content streaming. By use of a suiting flash media plugin the content is displayed on a wide variety of 3D capable devices like for example 3D workstations or smart TV sets. Videos are made available by an on-line stream provider for live broadcasting or by cloud storage services. Compared to conventional 3D video streaming over satellite channels there are considerable savings in cost as well as a wider range of applicability and functional improvements. Possible areas of application are medical surgery, live concerts, and sports events.	cloud computing;cloud storage;flash memory;image scaling;load balancing (computing);online and offline;scalability;smart tv;software as a service;stereoscopic video game;stereoscopy;streaming media;systems architecture;web service;workstation	Maximilian Hoecker;Marcel Kunze	2013	Journal of Cloud Computing: Advances, Systems and Applications	10.1186/2192-113X-2-14	simulation;computer science;operating system;multimedia;world wide web	HPC	6.01014569879387	33.73906609249738	54
e86cce95c23e37dec8feefbc7072e3c8bf92ac2a	variational bayesian inference for stereo object tracking	bayes methods vectors inference algorithms object tracking accuracy signal processing algorithms;video signal processing;bayes methods;inference mechanisms;video signal processing bayes methods image sequences inference mechanisms object tracking stereo image processing;object tracking;stereo image processing;student s t stereo tracking variational inference;video channel variational bayesian inference stereo object tracking stereo video sequences single channel object tracker;image sequences	In this paper, we deal with object tracking in stereo video sequences. We introduce a Bayesian framework for utilizing the results of any conventional single channel object tracker, in order to accomplish the refinement of the tracking accuracy in the left/right video channel. In this Bayesian framework, a variational Bayesian algorithm is employed to this end, where a priori information about the object displacement (movement) over time is incorporated by means of a prior distribution. This a priori information is obtained in a pre-processing step, in which the object displacement over time is estimated. Experiments demonstrate the efficiency of the proposed post-processing methodology in terms of tracking accuracy.	algorithm;displacement mapping;experiment;preprocessor;refinement (computing);variational principle;video blog;video post-processing	Giannis K. Chantas;Nikos Nikolaidis;Ioannis Pitas	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638093	computer vision;computer science;machine learning;video tracking;pattern recognition	Vision	46.68841385999522	-48.382348996015345	55
49d88b1abf9ebe6edaa552fec550dc5415b6ec75	road extraction from remote sensing images by multiple feature pyramid network		Road extraction from high-resolution remote sensing images has been applied in many domains, but it is still full of challenges. We focus on the problem of slender roads, proposing a new multiple feature pyramid network (MFPN), which is composed of an effective feature pyramid and the tailored pyramid pooling module based on PSPNet. These two designs can address the sparsity of roads in remote sensing images via using multi-level semantic features. Experiments on remote sensing images from Quick Bird show that our MFPN model achieves competitive performance, especially for slender roads.	image resolution;sparse matrix	Xun Gao;Xian Sun;Menglong Yan;Hao Sun;Kun Fu;Yue Zhang;Zhipeng Ge	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8519093	remote sensing;computer vision;pyramid;feature extraction;pooling;computer science;image segmentation;artificial intelligence	Mobile	28.197138715384984	-51.77103694470793	56
33a96669e2b5f0da17c7a85f8395c3108f537fb6	approximate nonmyopic sensor selection via submodularity and partitioning	belief networks;sensor systems and applications;bayesian network;maximum information gain;polynomial time complexity;cost function;application software;bayesian networks bns;search space;medical tests;sensor selection;bayesian methods;greedy algorithms;submodular function;systems engineering and theory;cost function mutual information partitioning algorithms bayesian methods systems engineering and theory medical tests system testing military computing sensor systems and applications application software;optimal tradeoff case;computational complexity;budget limit case;submodular supermodular procedure;polynomial time;greedy algorithm;system testing;mutual information;cost effectiveness;sensor fusion;active fusion sensor selection partitioning procedure maximum information gain bayesian network mutual information budget limit case greedy algorithm computational efficiency optimal tradeoff case submodular supermodular procedure polynomial time complexity;computational efficiency;submodular function active fusion bayesian networks bns sensor selection;submodular functions;information gain;partitioning procedure;military computing;sensor fusion belief networks computational complexity greedy algorithms;partitioning algorithms;active fusion	As sensors become more complex and prevalent, they present their own issues of cost effectiveness and timeliness. It becomes increasingly important to select sensor sets that provide the most information at the least cost and in the most timely and efficient manner. Two typical sensor selection problems appear in a wide range of applications. The first type involves selecting a sensor set that provides the maximum information gain within a budget limit. The other type involves selecting a sensor set that optimizes the tradeoff between information gain and cost. Unfortunately, both require extensive computations due to the exponential search space of sensor subsets. This paper proposes efficient sensor selection algorithms for solving both of these sensor selection problems. The relationships between the sensors and the hypotheses that the sensors aim to assess are modeled with Bayesian networks, and the information gain (benefit) of the sensors with respect to the hypotheses is evaluated by mutual information. We first prove that mutual information is a submodular function in a relaxed condition, which provides theoretical support for the proposed algorithms. For the budget-limit case, we introduce a greedy algorithm that has a constant factor of (1 - 1/e) guarantee to the optimal performance. A partitioning procedure is proposed to improve the computational efficiency of the algorithms by efficiently computing mutual information as well as reducing the search space. For the optimal-tradeoff case, a submodular-supermodular procedure is exploited in the proposed algorithm to choose the sensor set that achieves the optimal tradeoff between the benefit and cost in a polynomial-time complexity.	authorization;bayesian network;computation;embedded system;emoticon;exponential search;greedy algorithm;ieee xplore;information gain in decision trees;kullback–leibler divergence;mutual information;polynomial;selection algorithm;sensor;sensor web;submodular set function;supermodular function;synthetic intelligence;time complexity;unified framework	Wenhui Liao;Qiang Ji;William A. Wallace	2009	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2009.2014168	mathematical optimization;greedy algorithm;computer science;theoretical computer science;machine learning;mathematics;statistics	Mobile	47.34604215974258	6.204074337068187	57
2ef9b794f99547669c12f603d5650c27fc7575f8	terrain classification through weakly-structured vehicle/terrain interaction	active sensor;legged locomotion;data mining legged locomotion robot sensing systems remotely operated vehicles terrain mapping cameras visual servoing pattern classification signal generators decoding;hidden markov model;qualitative analysis;information content;discriminant analysis;hidden markov models terrain classification weakly structured vehicle vehicle terrain interaction autonomous locomotion pattern classification legged robots gait bounce signal discriminant analysis;spatial pattern;hidden markov models;robot vision;feature extraction pattern classification legged locomotion hidden markov models terrain mapping robot vision;feature extraction;pattern classification;word recognition;terrain mapping;visual servoing;legged robot	We present a new terrain classification technique both for effective, autonomous locomotion over natural, unknown terrains and for the qualitative analysis of terrains for exploration and mapping. Our straight-forward approach requires a single camera with little processing of visual information. Specifically, we derived a gait bounce measure from visual servoing errors that result from vehicle-terrain interactions during normal locomotion. Characteristics of the terrain, such as roughness and compliance, manifest themselves in the spatial patterns of this signal and can be extracted using pattern classification techniques. For legged robots, different limb-terrain interactions generate gait bounce signals with different information content, thus deliberate limb motions can effect higher information content (i.e. the robot is an active sensor of terrain class). Segmentation of the gait cycle based on the limb-terrain interaction isolates portions of the gait bounce signal with high information content. The decoding of, then sequencing of, this content from each cycle segment yields a robust classification of terrain type from known benchmarks. To extract this spatio-temporal pattern of the gait bounce signal, we developed a meta-classifier using discriminant analysis and hidden Markov model. We present the gait bounce derivation. We demonstrate the viability of terrain classification for legged vehicles using gait bounce with a rigorous study of more than 700 trials, obtaining 84% accuracy. We describe how terrain classification can be used for gait adaptation, particularly in relation to an efficiency metric. We also demonstrate that our technique is generally applicable to other locomotion mechanisms such as wheels and treads.	autonomous robot;benchmark (computing);bounce keys;bounce message;gait analysis;hidden markov model;interaction;linear discriminant analysis;markov chain;self-information;spatiotemporal pattern;visual servoing;wheels	Amy C. Larson;Richard M. Voyles;Güleser Kalayci Demir	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1307154	computer vision;common spatial pattern;simulation;self-information;feature extraction;word recognition;computer science;qualitative research;machine learning;visual servoing;hidden markov model	Robotics	48.09263405153446	-36.58126977811156	58
4440329b96a3289ec5a6156ffec0066679837eed	navipoint: an input device for mobile information browsing	hand held device;input device;user interface;information browsing;mobile computer;pdas;mobile computing;hand held devices	A mobile computing environment imposes various restrictions on users. For example, most mobile devices have a liited screen size, and it may be di&ult to watch the screen closelyWhile the user is walking or standing in a bus or train, he or she may have only one hand free to manipulate the device. Therefore, some new operation method must be developed for comfortable information browsing in the mobile environment. In this paper, several existing methods are first introduced and compared from the viewpoint of their applicability in a mobile environment. A new input device for such an environment, named WaviPoint,” is then introduced NaviPoint is a specialized device for mobile information browsing. By using this device, a user cau perform three types of input “analog input,” “digital input.,” and “click input?’ with just one finger. After an explanation of the conceptual structure and a qualitative analysis of NaviPoint, the structure of a prototype is described Experiments using the prototype show that information browsing is possible with an overhead of less than 50% on the usual “mouse and scroll bar” method	digital data;display size;input device;mobile computing;mobile device;overhead (computing);prototype	Kiyokuni Kawachiya;Hiroshi Ishikawa	1998		10.1145/274644.274645	mobile search;mobile web;human–computer interaction;computer hardware;mobile database;computer science;operating system;mobile technology;multimedia;mobile station;user interface;mobile computing;input device	Mobile	-46.700829769103045	-42.4819945479012	59
89356490ab279b531c3f48c035bd5f041c545f89	the multiplicative consistency threshold of intuitionistic fuzzy preference relation		Abstract Intuitionistic fuzzy sets (IFSs) have shown to be a suitable and effective technique for expressing vague and uncertain information. In this paper, we discuss the multiplicative consistency of intuitionistic fuzzy preference relations (IFPRs) whose elements are IFSs, and propose a method to determine the multiplicative consistency threshold of IFPRs. At first, the half normal distribution density function of the consistency threshold of IFPRs is investigated based on the distance measure, and a numerical algorithm is presented to identify the standard deviation of the density function. Then, after simulation experiment, a table of critical values of the multiplicative consistency threshold is determined with different orders of IFPRs based on the Gauss–Lobatto method. Finally, a case study about the assessment of the crucial components of mud pump in offshore platform is conducted, and a comparative analysis using the other existing method with original multiplicative consistency threshold is completed to illustrate the effectiveness and applicability of the proposed multiplicative consistency threshold for IFPRs.		Yan Yang;Xinxin Wang;Zeshui Xu	2019	Inf. Sci.	10.1016/j.ins.2018.10.044	mathematics;probability density function;discrete mathematics;fuzzy logic;preference relation;mathematical optimization;half-normal distribution;multiplicative function;fuzzy set;standard deviation	AI	-3.4738963640135148	-20.2032610829555	60
806a98c4ce16a2df352ba5c0d824bcda7f6b45a4	automatic segmentation of the preterm neonatal brain with mri using supervised classification	brain;tissues;magnetic resonance imaging	Cortical folding ensues around 13-14 weeks gestational age and a qualitative analysis of the cortex around this period is required to observe and better understand the folds arousal. A quantitative assessment of cortical folding can be based on the cortical surface area, extracted from segmentations of unmyelinated white matter (UWM), cortical grey matter (CoGM) and cerebrospinal fluid in the extracerebral space (CSF). This work presents a method for automatic segmentation of these tissue types in preterm infants. A set of T1and T2-weighted images of ten infants scanned at 30 weeks postmenstrual age was used. The reference standard was obtained by manual expert segmentation. The method employs supervised pixel classification in three subsequent stages. The classification is performed based on the set of spatial and texture features. Segmentation results are evaluated in terms of Dice coefficient (DC), Hausdorff distance (HD), and modified Hausdorff distance (MHD) defined as 95th percentile of the HD. The method achieved average DC of 0.94 for UWM, 0.73 for CoGM and 0.86 for CSF. The average HD and MHD were 6.89 mm and 0.34 mm for UWM, 6.49 mm and 0.82 mm for CoGM, and 7.09 mm and 0.79 mm for CSF, respectively. The presented method can provide volumetric measurements of the segmented tissues, and it enables quantification of cortical characteristics. Therefore, the method provides a basis for evaluation of clinical relevance of these biomarkers in the given population.	hausdorff dimension;machine learning;pixel;relevance;supervised learning;sørensen–dice coefficient	Sabina M. Chita;Manon J. N. L. Benders;Pim Moeskops;Karina J. Kersbergen;Max A. Viergever;Ivana Isgum	2013		10.1117/12.2006753	computer vision;computer science;artificial intelligence;magnetic resonance imaging	Vision	32.352409595130474	-78.34765422271607	61
f57dcf5a38bd379a483558ffdbb86b6f2f56242a	tripimputor: real-time imputing taxi trip purpose leveraging multi-sourced urban data		Travel behavior understanding is a long-standing and critically important topic in the area of smart cities. Big volumes of various GPS-based travel data can be easily collected, among which the taxi GPS trajectory data is a typical example. However, in GPS trajectory data, there is usually little information on travelers’ activities, thereby they can only support limited applications. Quite a few studies have been focused on enriching the semantic meaning for raw data, such as travel mode/purpose inferring. Unfortunately, trip purpose imputation receives relatively less attention and requires no real-time response. To narrow the gap, we propose a probabilistic two-phase framework named TripImputor, for making the real-time taxi trip purpose imputation and recommending services to passengers at their dropoff points. Specifically, in the first phase, we propose a two-stage clustering algorithm to identify candidate activity areas (CAAs) in the urban space. Then, we extract fine-granularity spatial and temporal patterns of human behaviors inside the CAAs from foursquare check-in data to approximate the priori probability for each activity, and compute the posterior probabilities (i.e., infer the trip purposes) using Bayes’ theorem. In the second phase, we take a sophisticated procedure that clusters historical dropoff points and matches the dropoff clusters and CAAs to immerse the real-time response. Finally, we evaluate the effectiveness and efficiency of the proposed two-phase framework using real-world data sets, which consist of road network, check-in data generated by over 38 000 users in one year, and the large-scale taxi trip data generated by over 19 000 taxis in a month in Manhattan, New York City, USA. Experimental results demonstrate that the system is able to infer the trip purpose accurately, and can provide recommendation results to passengers within 1.6 s in Manhattan on average, just using a single normal PC.	approximation algorithm;cluster analysis;computation;feedback;geo-imputation;global positioning system;mobile device;real-time clock;real-time transcription;response time (technology);spark;smart city;time complexity;two-phase commit protocol;two-phase locking;uptime	Chao Chen;Shuhai Jiao;Shu Zhang;Weichen Liu;Liang Feng;Yasha Wang	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2771231	global positioning system;raw data;simulation;imputation (statistics);data mining;cluster analysis;engineering;data set;probabilistic logic;a priori probability;travel behavior	ML	-17.120815646203734	-32.668744335975894	62
fbd50953b33825081ce4580818bfa755de0b5d34	remark on algorithm: algorithm 321 [s14] t-test probabilities and algorithm 344: students t-distribution	t test;student s t statistic;approximation;distribution function	being computa-tionally fast, and has the capability of yielding results with as much precision as the hardware will permit. The algorithm does not break down when presented with a matrix which is not diag-onalizable; that is, a set of eigenvectors satisfying the eigen-equation is computed regardless of the existence of linearly independent eigenvectors. However, when a matrix is diagonalizable and degenerate, the algorithm does not yield well separated eigen-vectors corresponding to non-distinct eigenvalues. Another apparent disadvantage is the possible indication of completely successful computation (INDIC), even in clearly ill-conditioned situations where computational difficulties are inevitable. This latter property, however, is a common fault of other algorithms as well. ACKNOWLEDGMENTS. This author wishes to thank the editor and referee for their valuable critique and useful suggestions. and eigenvectors of a real general matrix. A method for the solution of roots of a nonlinear equation and for solution of the general eigenvalue problem. Algorithm 321, as published, was coded in CSIRO 3200 ALGOL and run on a CDC 3200 with programmed floating point operations. A FORTRAN equivalent of Algorithm 321 was run for comparison with the FORTRAN Algorithm 344, which uses the same recurrence relation based on Student's cosine formula as that used in Algorithm 321 for df degrees of freedom less than maxn. Numerical results agreed with 6-digit tabulated values [1] and double precision calculations indicate that accuracy is limited by truncation of intermediate results to the precision of the processor , with error in the final result increasing as the square root of dr. Timing tests rated Algorithm 344 at approximately (} d$+1½) msec; slightly faster than Algorithm 321, which required approximately (¼ df+2½)msec for df < maxn. For df >_ maxn Algorithm 321 uses Fisher's [2] fifth order approximation , whose accuracy is summarized in the diagram for df = 10(10)50 (see Figure 1). The shaded regions indicate values	algol;algorithm;approximation;box counting;computation;condition number;diagram;double-precision floating-point format;eigen (c++ library);fortran;nonlinear system;recurrence relation;shading;transponder timing;truncation	Geoffrey W. Hill;Mary Loughhead	1970	Commun. ACM	10.1145/362007.362051	mathematical optimization;combinatorics;student's t-test;distribution function;approximation;mathematics;statistics	ML	56.08035112071771	30.196957812590227	63
1d1f24e1fe5751020f06991c8923fc53b3bfe990	quantification of uncertainty in mineral prospectivity prediction using neural network ensembles and interval neutrosophic sets	quarrying industry;mineral prospectivity prediction;uncertainty handling geographic information systems minerals neural nets production engineering computing quarrying industry;uncertainty quantification;geographic information system;neural networks;neural nets;minerals;neural network ensemble;uncertainty handling;production engineering computing;indeterminacy membership;neutrosophic sets;geographic information systems;network architecture;truth membership;false membership vulurs;uncertainty intelligent networks minerals neural networks gold australia feedforward neural networks geographic information systems decision making databases;false membership vulurs uncertainty quantification mineral prospectivity prediction neural network interval neutrosophic sets support decision making mineral exploration geographic information systems truth membership indeterminacy membership;support decision making;mineral exploration;interval neutrosophic sets;neural network	Quantification of uncertainty in mineral prospectivity prediction is an important process to support decision making in mineral exploration. Degree of uncertainly can identify level of quality in the prediction. This paper proposes an approach to predict degrees of favourability for gold deposits together with quantification of uncertainty in the prediction. Geographic information systems (GIS) data is applied to the integration of ensemble neural networks and interval neutrosophic sets, three different neural network architectures are used in this paper. The prediction and its uncertainty are represented in the form of truth-membership, indeterminacy-membership, and false-membership values. Two networks arc created for each network architecture to predict degrees of favourability for deposit and non deposit, which are represented by truth and false membership values respectively. Uncertainty or indeterminacy-membership values are estimated from both truth and false membership values, The results obtained using different neural network ensemble techniques are discussed in this paper.	artificial neural network;gene prediction;geographic information system;indeterminacy in concurrent computation;network architecture	Pawalai Kraipeerapun;Kevin Kok Wai Wong;Lance Chun Che Fung;Warick Brown	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247262	uncertainty quantification;network architecture;uncertainty analysis;computer science;artificial intelligence;machine learning;data mining;sensitivity analysis;mineral exploration;artificial neural network	Robotics	3.922848543705352	-26.006157635122847	64
6c9b7a0c88ec68c88a85c95bfe8007df18853cf4	location based spectrum sensing evaluation in cognitive radio networks	signal image and speech processing;information systems applications incl internet;communications engineering networks	This letter addresses the problem of spectrum sensing over fading channel, in which a licensee and multiple unlicensed users coexist and operate in the licensed channel in a local area. We derive the overall average probabilities of detection and false alarm by jointly taking the fading and the location of SUs into account and employing the energy detection as the underlying detection scheme. Furthermore, we develop a statistical model of cumulate interference by the help of the overall average probabilities of detection. Based on the cumulate interference, we also obtain a closed-form expression of outage probability at the primary user’s receiver according to a specific distribution of the fading.	catastrophic interference;coexist (image);cognitive radio;downtime;interference (communication);statistical model	Haipeng Yao;Chenglin Zhao;Zheng Zhou	2011	EURASIP J. Wireless Comm. and Networking	10.1186/1687-1499-2011-74	telecommunications;computer science;computer security;computer network	Mobile	33.54331999962055	84.17922511824648	65
6f3965354350443a7c046b83aa0dc7d808a820ef	complexity results for the gap inequalities for the max-cut problem	max cut problem;cutting planes;hb economic theory;computational complexity	We prove several complexity results about the gap inequalities for the max-cut problem, including (i) the gap-1 inequalities do not imply the other gap inequalities, unless N P = Co N P ; (ii) there must exist non-redundant gap inequalities with exponentially large coefficients, unless N P = Co N P ; (iii) the associated separation problem can be solved in finite (doubly exponential) time. © 2012 Elsevier B.V. All rights reserved.	coefficient;gap theorem;maximum cut;time complexity	Laura Galli;Konstantinos Kaparis;Adam N. Letchford	2012	Oper. Res. Lett.	10.1016/j.orl.2012.01.010	mathematical optimization;maximum cut;combinatorics;mathematics;computational complexity theory;algorithm	Theory	24.355192063363585	14.679115551434556	66
0eb9774bd4c1938cb1ab9fbc60cdba3bdeb3611d	pquery: achieving privacy-preserving query with communication efficiency in internet of things		Internet of Things (IoT), as it can provide many promising IoT services to end users, has received considerable attention in recent years. However, IoT's security and privacy are still challenging today. In this paper, we propose a privacy- preserving query scheme, called PQuery, for fog computing-enhanced IoT. The proposed PQuery scheme is characterized by employing two privacy enhancing techniques, i.e., private information retrieval and oblivious transfer, to preserve the privacy for both the end user and the service provider in IoT query service. Though the computational cost is still high at the fog device, the communication overheads in PQuery can be greatly reduced between the fog device and the end user.	algorithmic efficiency;computation;fog computing;internet of things;oblivious transfer;personally identifiable information;privacy;private information retrieval	Nafiseh Izadi Yekta;Rongxing Lu	2017	2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)	10.1109/VTCFall.2017.8288344	end user;oblivious transfer;service provider;computer network;encryption;computer science;overhead (business);private information retrieval;internet of things	Security	-42.43745464097874	66.34250478912655	67
b2002a602cddb97c66bf415497303731a2eec48c	a theoretical study of nonclassical effects in optical and spin systems and their applications		Nonclassical states, having no classical analogue, promise advantage in the performance in several fields of technology, such as computation, communication, sensing. This led to an escalated interest in the recent years for the generation and characterization of nonclassical states in various physical systems of interest. Keeping this in mind, we examined generation of various lower- and higher-order nonclassical states in both codirectional and contradirectional asymmetric nonlinear optical couplers operating under second harmonic generation with the help of moments-based criteria. Using another such system (a symmetric nonlinear optical coupler), we have also established the possibility of observing quantum Zeno and anti-Zeno effects and further reduced the obtained results to yield the corresponding expressions for the asymmetric coupler. These studies have been accomplished using a complete quantum description of the system considering all the field modes as weak. Further, characterization of nonclassicality in spin systems using quasiprobability distributions has been performed, which has also provided us the dynamics of the amount of nonclassicality (using nonclassical volume). As the reconstruction of quasiprobability distributions is possible with the help of tomograms accessible in the experiments, tomograms are computed here for both finite and infinite dimensional systems. Finally, a set of controlled quantum communication (both insecure and secure) schemes with minimum number of entangled qubits has been proposed, which is also analyzed over Markovian channels. The optimized controlled secure quantum communication scheme is observed to be reducible to several cryptographic tasks. Using this interesting observation, we obtained the performance of a set of cryptographic schemes over non-Markovian channels, too.		Kishore Thapliyal	2018	CoRR		quantum mechanics;discrete mathematics;qubit;quantum zeno effect;quantum information science;second-harmonic generation;mathematics;nonlinear system;spin-½;quantum;physical system	HCI	57.104295958689576	33.77763158701256	68
d5eaf16b7a2f3202d0a77c8d8052e3fef8bae3f3	revisiting cycle shrinking	tratamiento paralelo;parallelisme;optimisation;metodologia;traitement parallele;optimizacion;estrategia optima;combined cycle;programacion paralela;optimal method;boucle programme;nested loops;transformacion;parallel programming;methodologie;bucle programa;recurrence;reduction cycle;optimal strategy;parallelism;paralelismo;particion;loop transformation;recurrencia;transformation boucle;indexation;partition;methode index decale;program loop;optimization;transformation;vecteur ordonnancement;methodology;strategie optimale;parallel processing;programmation parallele	Several loop transformations techniques have been designed to extract parallelism from nested loop structures. We rst review two important approaches, known as Generalized Cycle Shrinking presented by Shang, O'Keefe and Fortes and the Index Shift Method introduced by Liu, Ho and Sheu. The main result of the paper is a new methodology that permits to combine cycle shrinking techniques with the index shift method. We present a new optimization method that produces the best scheduling vector, and we show that we can outperform previous results by an arbitrary speedup factor.	control flow;loop invariant;mathematical optimization;parallel computing;scheduling (computing);speedup	Yves Robert;Siang Wun Song	1992	Parallel Computing	10.1016/0167-8191(92)90084-K	partition;transformation;parallel processing;parallel computing;combined cycle;nested loop join;computer science;calculus;methodology;mathematics;algorithm	Theory	4.189042982451356	37.48192492439153	69
d07d1cca99ffae5011f040e0cf2de6eec57d0b85	interference alignment techniques for multi-user mimo systems at millimeter-wave	millimeter wave spectrum;ing inf 03 telecomunicazioni;interference channel;interference alignment;interference channel mimo beamforming interference alignment millimeter wave spectrum;rayleigh channels antenna arrays mimo communication multi access systems radiofrequency interference;interference antenna arrays signal to noise ratio receivers mimo fading channels array signal processing;beamforming;mimo;frequency 30 ghz to 300 ghz interference alignment multi user mimo systems millimeter wave mobile devices antennas multiple input multiple output systems millimeter wave channel rayleigh fading channels	The increasing bandwidth required nowadays by mobile devices forces radio technology to start looking at the 30-300 GHz spectrum, corresponding to wavelength of the order of millimeters. The large attenuation belonging to this frequency range can be effectively mitigated by the use of multiple antennas that furthermore allow interference mitigation between users accessing simultaneously the channel. In this work a review of the state-of-the-art of modern multi-user multiple-input multiple-output (MIMO) systems is given, extending various Interference alignment (IA) techniques to the millimeter wave (MMW) channel. In particular, performance achieved by IA methods over the MMW and the classical Rayleigh fading channels are compared and some considerations are given. Specifically, it is seen that IA methods at MMW are very suitable even with many users which however cannot require an excessive rate.	algorithm;angle of arrival;frequency band;interference (communication);mobile device;multi-user mimo;multiplexing;next-generation network;rayleigh fading;signal subspace;signal-to-interference-plus-noise ratio;software propagation;waist-to-height ratio	Stefano Ciccotosto;Nevio Benvenuto	2015	2015 IEEE International Conference on Electronics, Circuits, and Systems (ICECS)	10.1109/ICECS.2015.7440394	3g mimo;electronic engineering;multi-user mimo;telecommunications;engineering;electrical engineering;co-channel interference;precoding	EDA	44.87087194019076	83.71264436313044	70
422951de7bf50a7fa74d77e539123babda452e2a	smt-based parameter synthesis for parametric timed automata		We present a simple method for representing finite executions of Parametric Timed Automata using Satisfiability Modulo Theories (SMT). The transition relation of an automaton is translated to a formula of SMT, which is used to represent all the prefixes of a given length of all the executions. This enables to underapproximate the set of parameter valuations for the undecidable problem of parametric reachability. We introduce a freely available, open-source tool PTA2SMT and show its application to the synthesis of parameter valuations under which a timed mutual exclusion protocol fails.	timed automaton	Michal Knapik;Wojciech Penczek	2016		10.1007/978-3-319-30165-5_1	timed automaton	Logic	-11.280528041836591	24.55354297101093	71
3cbf8137a4dafe1eabce71018c851a665f3f1d13	precise instruction scheduling without a precise machine model	optimizing compiler;instruction scheduling;machine model	A simple technique is presented which allows an optimizing compiler to more precisely compare the performance of alternative instruction sequences on a complex RISC architecture so that the better sequence can be chosen. This technique may be faster than current techniques, and has the advantage that minor modifications to the hardware do not require any changes to the compiler (not even recompilation), and yet have an immediate effect on instruction scheduling decisions.	instruction scheduling;optimizing compiler;scheduling (computing)	Henry G. Baker	1991	SIGARCH Computer Architecture News	10.1145/152766.152767	computer architecture;parallel computing;real-time computing;compiler correctness;computer science;optimizing compiler;instruction scheduling;programming language;functional compiler	Arch	-17.662724183419723	36.118678510178334	72
d851a4819df7180ad6735b35184816945ec176e4	design and implementation of a dynamic reconfigurable classroom for cooperative learning	information science;dynamic reconfiguration;classroom furniture dynamic reconfigurable classroom cooperative learning flexible ict complex interaction;computer aided instruction;collaboration;informing science;servers materials collaboration dynamic scheduling multicore processing information science education;materials;furniture;servers;design and implementation;multicore processing;furniture computer aided instruction;cooperative learning;dynamic reconfigurable classroom;dynamic reconfigurable classroom cooperative learning;dynamic scheduling	In order to practically examine a new type of class, in which cooperative learning is taken into consideration, we designed and built a classroom. This classroom, in which furniture and a flexible ICT basis are both designed reconfigurable, supports much more complex interactions between learners than in a traditional classroom. Two months' observation of the activities in the classroom shows that many aspects of the reconfigurations have encouraged complex interactions within cooperative learning.	interaction	Hideki Kondo;Hiroyuki Narahara	2011	2011 Third International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2011.72	multi-core processor;cooperative learning;simulation;dynamic priority scheduling;information science;computer science;operating system;multimedia;server;collaboration	Robotics	-81.17877739593978	-40.209908853966255	73
3a36ecdc716f5964f325b0103dfd6e8cc1b0d9ec	evaluation of new telecommunications services using stated preference techniques				Akiya Inoue;Hisao Yamamoto	1990			public economics;telecommunications service;microeconomics;business	Robotics	-69.64151706371717	-1.2858135046234	74
0b60e518edf6638e7e72159f326a3398c8258c27	parallelizing tree algorithms: overhead vs. parallelism	large scale	We consider techniques to efficiently exploit large scale parallel execution of tree operations. Both arbitrary and fixed order operations on trees are considered. An arbitrary order execution must be equivalent to a sequential execution of these operations in some order, while a fixed order execution specifies a specific order.	parallel computing	Jon A. Solworth;Bryan Reagan	1994		10.1007/BFb0025895	parallel computing;computer science;theoretical computer science;distributed computing;data parallelism;instruction-level parallelism;task parallelism	DB	-11.23567062528996	41.9639080412718	75
74ba3a3870d9a3379b6bf2433c1c4045c7475f03	reengineering for parallelism: an entry point into plpp for legacy applications	parallelization of legacy code;parallel programming;pattern language;design patterns	An apparatus useful for lifting a reel having cable wound thereabout for dispensing the cable includes oppositely positioned bearing housings operative to support spindle assemblies which are receivable into the reel to occasion rotation of the reel about its mandrel apertures or central axis. At least one of the bearing housings has a disk brake assembly operative therewith and associated controlling mechanisms for resisting rotation of one of the spindle assemblies. A braker arm attached to the spindle assembly operative with the disk brake assembly imparts a torque opposing the rotation of the reel for braking the reel. The apparatus is adapted to receive the forks of a forklift truck and be suspended thereby.	code refactoring;entry point	Berna L. Massingill;Timothy G. Mattson;Beverly A. Sanders	2007	Concurrency and Computation: Practice and Experience	10.1002/cpe.1147	software modernization;computer architecture;software design pattern;parallel computing;computer science;pattern language;programming language;task parallelism	PL	83.31383474066199	-20.01274297710137	76
6a6299c4a323eb48dfbed3310eea47cd06b525ed	a model of preventive maintenance for parallel, series, and single-item replacement systems based on statistical analysis	preventive maintenance;62p30;series and single item replacement systems;statistical analysis;parallel;cost	ABSTRACTPreventive maintenance (PM) scheduling of units is addressed as a crucial issue that effects on both economy and reliability of power systems. In this paper, we describe an application of statistical analysis for determining the best PM strategy in the case of parallel, series, and single-item replacement systems. A key aspect of industrial maintenance is the trade-off between cost and time of performing PM operations. The goals of this study is to determine the best time for performing PM operations in each system and also finding the number of spare parts and facilities in single-item replacement and parallel systems respectively so that the average cost per unit time is minimized. In this proposed maintenance strategy, PM operations are regularly performed on the production unit in equal time intervals. Finally, three examples are presented to demonstrate the effectiveness of the proposed models.		Mohammad Saber Fallahnezhad;E. Najafian	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1183781	preventive maintenance;parallel;mathematics;statistics	HPC	7.956031616876514	-1.085875204849114	77
8757a02ec150876ec422dbc06262076a74367d52	a deep cnn-based framework for enhanced aerial imagery registration with applications to uav geolocalization		In this paper we present a novel framework for geolocalizing Unmanned Aerial Vehicles (UAVs) using only their onboard camera. The framework exploits the abundance of satellite imagery, along with established computer vision and deep learning methods, to locate the UAV in a satellite imagery map. It utilizes the contextual information extracted from the scene to attain increased geolocalization accuracy and enable navigation without the use of a Global Positioning System (GPS), which is advantageous in GPS-denied environments and provides additional enhancement to existing GPS-based systems. The framework inputs two images at a time, one captured using a UAV-mounted downlooking camera, and the other synthetically generated from the satellite map based on the UAV location within the map. Local features are extracted and used to register both images, a process that is performed recurrently to relate UAV motion to its actual map position, hence performing preliminary localization. A semantic shape matching algorithm is subsequently applied to extract and match meaningful shape information from both images, and use this information to improve localization accuracy. The framework is evaluated on two different datasets representing different geographical regions. Obtained results demonstrate the viability of proposed method and that the utilization of visual information can offer a promising approach for unconstrained UAV navigation and enable the aerial platform to be self-aware of its surroundings thus opening up new application domains or enhancing existing ones.		Ahmed Nassar;Karim Amer;Reda ElHakim;Mohamed ElHelw	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00201	computer vision;onboard camera;satellite imagery;global positioning system;deep learning;feature extraction;computer science;blossom algorithm;artificial intelligence	Vision	29.474523714810065	-50.31364602748176	78
2e80f229a7b840ee81cc28f7f10168acd312cb7e	a relaxed matrix inversion method for retrieving water constituent concentrations in case ii waters: the case of lake kasumigaura, japan	imaging spectrometer;matrix inversion method mim;absorption;satellite data;water quality atmospheric optics calibration hydrological techniques;routine observations;inversion;root mean square error;lakes;biological system modeling;spectrum;matrix inversion;indexing terms;water quality;water lakes remote sensing noise absorption accuracy biological system modeling;journal;accuracy;routine observations case ii waters estimated specific inherent optical properties siops matrix inversion method mim;atmospheric optics;relative random uncertainty relaxed matrix inversion method water constituent concentrations lake kasumigaura japan case ii waters specific inherent optical properties lake water quality linear matrix inversion theory medium resolution imaging spectrometer instrument noise contaminated simulation meris data;remote sensing;estimated specific inherent optical properties siops;matrix;case ii waters;method;water;calibration;hydrological techniques;field survey;inherent optical properties;noise	The matrix inversion method (MIM) is an effective algorithm for estimating water constituent concentrations in case II waters. To apply this method, appropriate and accurate specific inherent optical properties (SIOPs) for each constituent in water are essential. However, many routine observations of lake water quality do not in fact provide SIOPs, thus limiting the application of the MIM. In this paper, an alternative MIM method based on linear matrix inversion theory was proposed to relax the requirement of SIOPs measurement. For this, so-called ESIOPs (Estimated SIOPs) were first derived by an unusual application of MIM based on adequate calibration samples; then the water constituent concentrations for the whole study area were retrieved by the standard application of MIM based on the derived ESIOPs. For each calibration sample, measurement of the reflectance spectrum and corresponding water constituent concentrations, which can be obtained from periodical satellite data and routine field surveys, is required. The performance of the proposed method was evaluated using the simulation data from Hydrolight and three MEdium Resolution Imaging Spectrometer Instrument (MERIS) images. The results showed that this method yielded satisfactory estimations of the water constituent concentrations for the noise-contaminated simulation data sets. For MERIS data in our study area (Lake Kasumigaura, Japan), the average bias (mean normalized bias or MNB) and relative random uncertainty (normalized root mean square error, or NRMS) were in the range of -11.2% to 3.4% and 4.8% to 29.7% for each water constituent concentration. These findings imply that the algorithm proposed in this study is theoretically reasonable and practically applicable.		Wei Yang;Bunkei Matsushita;Jin Chen;Takehiko Fukushima	2011	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2011.2126048	absorption;inversion;meteorology;spectrum;water;method;calibration;index term;hydrology;noise;mean squared error;accuracy and precision;optics;imaging spectrometer;atmospheric optics;physics;quantum mechanics;matrix;statistics;remote sensing	Robotics	84.73386509280398	-63.68281326457016	79
2a5560121be54c0dc25cb639ad00cf193716ccb4	a line-based pose estimation algorithm for 3-d polyhedral object recognition	image features;levenberg marquardt;object recognition;least square method;sampling strategy;pose estimation	  In this paper, we present a new approach to solve the problem of estimating the camera 3-D location and orientation from a  matched set of 3-D model and 2-D image features. An iterative least-square method is used to solve both rotation and translation  simultaneously. Because conventional methods that solved for rotation first and then translation do not provide good solutions,  we derive an error equation using roll-pitch-yaw angle to present the rotation matrix. From the modeling of the error equation,  we analytically extract the partial derivates for estimation parameters from the nonlinear error equation. To minimize the  error equation, Levenberg-Marquardt algorithm is introduced with uniform sampling strategy of rotation space to avoid stuck  in local minimum. Experimental results using real images are presented.    	3d pose estimation;algorithm;outline of object recognition;polyhedral	Tae-Jung Lho;Dong Joong Kang;Jong-Eun Ha	2004		10.1007/978-3-540-24768-5_97	computer vision;pose;levenberg–marquardt algorithm;3d pose estimation;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;3d single-object recognition;least squares;feature	Vision	50.9565714076715	-51.64801096163148	80
23bca2f6d4e0a4825b8663f16298c6cf2916a829	active disturbance rejection control of common rail pressure for gasoline direct injection engine	common rail;fuel systems;active disturbance rejection control;nonlinear state error feedback control;control system synthesis;extended state observer;disturbance rejection;pressure tracking problem;adrc;rail pressure control;gasoline direct injection;state feedback;gasoline direct injection engine;nonlinear control systems;external disturbance;internal uncertainty;pressure control;internal combustion engines;complex nonlinear system;fuel injection;gdi engine;engines;mathematical model;metals	The rail pressure control in Gasoline Direct Injection (GDI) engines is considered as one of key issues for precise control of fuel injection. This paper presents a rail pressure controller based on the framework of active disturbance rejection control (ADRC). The detailed nonlinear model of the fuel injection system for GDI engine is carried out. Suitable model simplification is introduced for the controller design. According to the complex nonlinear system which contains internal uncertainties and external disturbance, a nonlinear state error feedback control and an extended state observer are applied to deal with the pressure tracking problem. Simulation results are provided to demonstrate the effectiveness of the proposed controller.	controller (computing);feedback;flexible-fuel vehicle;graphics device interface;level of detail;nonlinear system;rejection sampling;simulation	Qifang Liu;Xun Gong;Yunfeng Hu;Hong Chen	2013	2013 American Control Conference		control engineering;engineering;automotive engineering;control theory	Robotics	67.2645152809903	-11.870373694183609	81
f2ebe0119191a3bd6e8ce13988473253cd568ee4	primary health care in mexico: a “non-isi” bibliometric analysis	analyse bibliometrique;paises en desarrollo;visibilite;america latina;visibilidad;information production;pays en developpement;amerique;medicina;information scientifique technique;amerique latine;publication nationale;1980 1990;litterature scientifique;medecine;production information;dato bibliografico;estudio impacto;primary health care;etude impact;visibility;cuidados salud primaria;literatura cientifica;estudio caso;america central;diffusion information;publicacion nacional;information dissemination;national publication;etude cas;medicine;latin america;soin sante primaire;bibliometric analysis;scientific technical information;difusion informacion;bibliographic data;america;informacion cientifica tecnica;scientific literature;central america;impact study;donnee bibliographique;mexique;developing countries;analisis bibliometrico;mexico;amerique centrale	This work reports the first results of a research in progress on the production, dissemination and impact of the literature on primary health care (PHC), as produced in Mexico during the period 1980–1992. The methodology used involved computerized searches in the MEDLINE, LILACS, and PERIODICA databases to identify the existing Mexican literature in the field. Results indicated a limited dissemination of the Mexican production through conventional databases. A total of 117 references were found in the field. Most of these references (72.65%) corresponded to journal articles. Over 55% of the documents were published by more than one author. Further research in the field as well as the implications of these results to PHC in Mexico are discussed by the author.	bibliometrics;database;literatura latino-americana e do caribe em ciências da saúde;medline	César A. Macías-Chapula	1995	Scientometrics	10.1007/BF02019173	developing country;visibility;latin americans;primary health care	HCI	-75.19417427204111	-22.63964970942324	82
4996c3ac67a0de1fd2d537f2cd8fa08ff2a3cedb	clico fs: an interactive web-based service of circos		UNLABELLED : We present ClicO Free Service, an online web-service based on Circos, which provides a user-friendly, interactive web-based interface with configurable features to generate Circos circular plots.   AVAILABILITY AND IMPLEMENTATION Online web-service is freely available at http://clicofs.codoncloud.com   CONTACT : soonjoo.yap@codongenomics.com   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;chord diagram;interface device component;usability;web application;web service	Wei-Hien Cheong;Yung-Chie Tan;Soon-Joo Yap;Kee-Peng Ng	2015		10.1093/bioinformatics/btv433	web application;the internet;world wide web;software;multimedia;computer science	Comp.	-4.01231390185574	-59.05885123319401	83
86009514aa870a6fa72f3c279214cd1b378912c9	coordinating crowd-sourced services	crowd sourced services;sensors;clocks;multi origin communication;smart phones;high level programming crowd sourced services smartphones personal connected computational devices sensors crowd sourced data multiorigin communication;aggregates;sensors clocks aggregates mobile communication real time systems smart phones;mobile communication;actors;coordination crowd sourced services actors multi origin communication;coordination;real time systems	"""The growing ubiquity of smartphones and similar personal connected computational devices, each with a number of sensors, has created an opportunity for useful services based on crowd-sourced data. A busy professional could find a restaurant to go to for a quick lunch based on information available from smartphones of people already there having lunch, waiting to be seated, or even heading there, a government could conduct a census in real-time, or """"sense"""" public opinion. Although the programming required for offering a new service of this sort can be significant if done from scratch, these applications have something in common: they use a similar pattern of coordinated communications between the various parties. This creates an opportunity to offer a set of coordination mechanisms as a platform to service designers, into which they can simply plug in their service specific code to offer a new service. This paper identifies the coordination mechanisms required for these crowd-sourced services as types of multi-origin communication. We present details of how these core mechanisms can be implemented using Actors, and introduce high-level programming constructs for launching a new service. Finally, we use examples to illustrate the implementation of services."""	actor model;android;computation;course (navigation);crowdsensing;crowdsourcing;goto;high- and low-level;high-level programming language;interrupt;mobile device;operating system;prototype;real-time transcription;routing;scsi initiator and target;sensor web;smartphone	Ahmed Abdel Moamen;Nadeem Jamali	2014	2014 IEEE International Conference on Mobile Services	10.1109/MobServ.2014.22	simulation;engineering;multimedia;computer security	Mobile	-39.86542603031642	53.94294829413945	84
59134403e819c403e4689716567ec5d03fb6ccbe	emerging phishing trends and effectiveness of the anti-phishing landing page	uniform resource locators electronic mail internet ip networks browsers organizations training;unsolicited e mail computer crime internet trusted computing;phishing e mails phishing trends antiphishing landing page trusted entity internet urls internet corporation for assigned names and numbers accredited registrars icann accredited registrars free subdomain registration services	Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity which compels them to share their personal, financial information. Acquired sensitive information is then used for personal benefits, like, gain access to money of the individuals from whom the information was taken. Phishing costs Internet users billions of dollars every year. A recent report highlighted phishing loss of around $448 million to organizations in April 2014. Researchers at Carnegie Mellon University (CMU) created an anti-phishing landing page supported by Anti-Phishing Working Group (APWG) with the aim to train users on how to prevent themselves from phishing attacks. It is used by financial institutions, phish site take down vendors, government organizations, and online merchants. When a potential victim clicks on a phishing link that has been taken down, he / she is redirected to the landing page. In this paper, we present the comparative analysis on two datasets that we obtained from APWG's landing page log files; one, from September 7, 2008 - November 11, 2009, and other from January 1, 2014 - April 30, 2014. We found that the landing page has been successful in training users against phishing. Forty six percent users clicked lesser number of phishing URLs from January 2014 to April 2014 which shows that training from the landing page helped users not to fall for phishing attacks. Our analysis shows that phishers have started to modify their techniques by creating more legitimate looking URLs and buying large number of domains to increase their activity. We observed that phishers are exploiting Internet Corporation for Assigned Names and Numbers (ICANN) accredited registrars to launch their attacks even after strict surveillance. We saw that phishers are trying to exploit free subdomain registration services to carry out attacks. In this paper, we also compared the phishing e-mails used by phishers to lure victims in 2008 and 2014. We found that the phishing e-mails have changed considerably over time. Phishers have adopted new techniques like sending promotional e-mails and emotionally targeting users in clicking phishing URLs.	authorization;automatic sounding;behavioral pattern;blog;canonical account;data logger;email;firefox;information sensitivity;internet explorer;landing page;mike lesser;money;naruto shippuden: clash of ninja revolution 3;on the fly;paging;personally identifiable information;phishing;plug-in (computing);preemption (computing);qualitative comparative analysis;social media;url redirection;user agent	Srishti Gupta;Ponnurangam Kumaraguru	2014	2014 APWG Symposium on Electronic Crime Research (eCrime)	10.1109/ECRIME.2014.6963163	phishing;telecommunications;operating system;internet privacy;management;world wide web;computer security	Security	-55.99562213086723	61.832577106581965	85
4bcfa8bbb6c9587a9a0d2600daaf4cbdf7fcbaab	strategy-driven reuse: bringing reuse from the engineering department to the executive boardroom	integrated approach;development strategy;cost saving;cost reduction;product line;software engineering;software development	Organizations have predominantly utilized reuse in Engineering Departments for the purposes of reducing the cost and improving the quality of the software they develop. While these strategies have been successful, we believe that the full potential of reuse can only be tapped when reuse is brought to the Executive Boardroom as well. We propose that organizations tap reuse not only for cutting costs, but also for strategic and widedranging business initiatives such as entering new markets, increasing agility in response to a dynamic marketplace, and competitive positioning and advantage. In order to do so effectively, organizations must harness the potential of reuse by migrating reuse into the company’s business and productdline planning processes. We present a framework for analyzing and changing reuse business practices. Such practices include costdreduction reuse, when the organization utilizes reuse for cost savings purposess reusedenabled business, when the organization uses reuse to create new business opportunitiess and strategyddriven reuse, when the organization incorporates reuse in the formulation of its business and productdline strategy for the purposes of obtaining competitive positioning and advantage. To determine whether or not reuse is the proper software development strategy to pursue, we utilize concepts in competitive software engineering, an integrated approach to software development that is attuned to the competitive demands of the marketplace. First, a framework is established by identifying and analyzing the organization’s goals, strengths, and limitations, its market and its competitive environment. Based on these analyses, possible business or product strategies are formulated and one or more are chosen that help achieve the organization’s goals. Finally, a development strategy is chosen. Following this choice, each step of the decision cycle should be redevaluated to ensure that it is consistent with the chosen development strategy.		Wayne C. Lim	1998	Ann. Software Eng.	10.1023/A:1018916206023	systems engineering;engineering;operations management;software development;software engineering;domain engineering;management science;management	SE	-68.98363188599114	18.061949537451465	86
892c4472f3c4508d94da098a5c061c122aa1fb64	design of the language replica for hybrid pram-numa many-core architectures	parallel computing;parallel programming multiprocessing systems parallel architectures;parallel programming;computer architecture phase change random access memory instruction sets hardware synchronization parallel programming libraries;parallel architectures;multi core parallel computing programming languages;datavetenskap datalogi;powerful synchronous shared memory architectures language replica design hybrid pram numa many core architectures parallel programming average programmer parallel architectures parallel language high level programming;multiprocessing systems;computer science;multi core;programming languages	Parallel programming is widely considered very demanding for an average programmer due to inherent asynchrony of underlying parallel architectures. In this paper we describe the main design principles and core features of Replica -- a parallel language aimed for high-level programming of a new paradigm of reconfigurable, scalable and powerful synchronous shared memory architectures that promise to make parallel programming radically easier with the help of strict memory consistency and deterministic synchronous execution of hardware threads and multi-operations.	asynchronous i/o;consistency model;high- and low-level;high-level programming language;manycore processor;parallel computing;parallel language;programmer;programming paradigm;scalability;shared memory	Jari-Matti Mäkelä;Erik Hansson;Daniel Åkesson;Martti Forsell;Christoph W. Kessler;Ville Leppänen	2012	2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2012.103	multi-core processor;fourth-generation programming language;computer architecture;parallel computing;declarative programming;language primitive;embarrassingly parallel;programming domain;reactive programming;computer science;operating system;distributed computing;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;bulk synchronous parallel;parallel programming model	Arch	-13.525975757661897	39.9004840760507	87
561c9297e18c0cd69ce060edf81f65bd2ce088a1	he timely computing base: timely actions in the presence of uncertain timeliness	air traffic control;collaborative work;timing real time systems navigation electronic switching systems air traffic control intelligent networks network servers multimedia systems collaborative software collaborative work;architectural construct;real time;real time systems application program interfaces fault tolerant computing;application program interface;multimedia systems;synchronous system;timely actions;application programming interface timely computing base timely actions uncertain timeliness real time behavior synchronous system models architectural construct timing functions;navigation;network servers;fault tolerant computing;timing functions;application program interfaces;real time behavior;timely computing base;electronic switching systems;intelligent networks;computer science;article;application programming interface;uncertain timeliness;collaborative software;real time systems;synchronous system models;timing	Real-time behavior is specified in compliance with timeliness requirements, which in essence calls for synchronous system models. However, systems often rely on unpredictable and unreliable infrastructures, that suggest the use of asynchronous models. Several models have been proposed to address this issue. We propose an architectural construct that takes a generic approach to the problem of programming in the presence of uncertain timeliness. We assume the existence of a component, capable of executing timely functions, which helps applications with varying degrees of synchrony to behave reliably despite the occurrence of timing failures. We call this component the Timely Computing Base, TCB. This paper describes the TCB architecture and model, and discusses the application programming interface for accessing the TCB services. The implementation of the TCB services uses fail-awareness techniques to increase the coverage of TCB properties.	algorithm;amplifier;application programming interface;computation;computational model;correctness (computer science);expect;experiment;linux;prototype;rtlinux;real-time clock;real-time computing;real-time transcription;requirement;synchronous circuit;systems design;timing failure;trusted computing base	Paulo Veríssimo;Antonio Casimiro;Christof Fetzer	2000		10.1109/ICDSN.2000.857587	embedded system;real-time computing;application programming interface;computer science;operating system;distributed computing;computer security;computer network	Embedded	-25.05205649102136	44.13492003238849	88
d962f937ebab6e4b544b6a72f82659c14286a9e6	regression test selection for distributed software histories		Regression test selection analyzes incremental changes to a codebase and chooses to run only those tests whose behavior may be affected by the latest changes in the code. By focusing on a small subset of all the tests, the testing process runs faster and can be more tightly integrated into the development process. Existing techniques for regression test selection consider two versions of the code at a time, effectively assuming a development process where changes to the code occur in a linear sequence. Modern development processes that use distributed version-control systems are more complex. Software version histories are generally modeled as directed graphs; in addition to version changes occurring linearly, multiple versions can be related by other commands, e.g., branch, merge, rebase, cherry-pick, revert, etc. This paper describes a regression test-selection technique for software developed using modern distributed version-control systems. By modeling different branch or merge commands directly in our technique, it computes safe test sets that can be substantially smaller than applying previous techniques to a linearization of the software history. We evaluate our technique on software histories of several large opensource projects. The results are encouraging: our technique obtained an average of 10.89× reduction in the number of tests over an existing technique while still selecting all tests whose behavior may differ.	computation;control system;directed graph;open-source software;rebasing;regression testing;software versioning;test case;version control;xojo	Milos Gligoric;Rupak Majumdar;Rohan Sharma;Lamyaa Eloussi;Darko Marinov	2014		10.1007/978-3-319-08867-9_19	regression testing;simulation;computer science;artificial intelligence;software regression;algorithm	Logic	-64.73041756967962	34.912570357326615	89
bfea34673a920a5ef804da6e9ce12c8ecae6ee63	effect of electrical and thermal stress on low-frequency noise characteristics of laser diodes	high temperature;active region;1 f noise;low frequency noise;multi quantum well;laser diode;indexation;thermal stress	In this paper, we study the electrical noise of commercially available laser diodes: a group of index guided AlGaInP diodes lasing at 635 nm (SDL3038-11) and a group of InGaAlP-multi-quantum-well diodes lasing at 670 nm (SVL71B). In particular, the eect of stress (high current and high temperature) on the noise is investigated. Measurements of the magnitude of the 1=f noise as a function of the operating current (10 nA to 10 mA) revealed an anomaly. After increasing proportionally with the current at small currents (10 nA to 10 lA), the 1=f noise tends to saturate with increasing currents in the range from 10 to 100 lA. For larger operating currents, the 1=f noise increases again with the current, proportional to I. This anomaly is even more pronounced after the devices have been stressed. We conclude that measurements of the 1=f noise at low bias currents are sensitive to the degradation of the active region of the laser diodes, while measurements of the 1=f noise at high bias currents can predict failure related to the quality of the crystal layers of the laser diodes. Ó 2001 Elsevier Science Ltd. All rights reserved.	anomaly detection;bandwidth-delay product;biasing;diode;elegant degradation;noise (electronics);quantum well	Xia Yu Chen;Anders Pedersen;A. D. van Rheenen	2001	Microelectronics Reliability	10.1016/S0026-2714(00)00201-8	flicker noise;noise spectral density;electronic engineering;y-factor;optoelectronics;noise;optics;stress;infrasound;physics	HCI	90.97688548118381	-11.737135228017712	90
47e7cbf983540c9bc626f2df1048e469ee450125	siren: a platform for deployment of vnfs in distributed infrastructures	vpp;programmable data planes;pvpp;domain specific languages dsl;compiler optimization;software switch	Fog computing is conceiving an Internet where general purpose compute is ubiquitous, in turn this is providing new infrastructures for Network Functions Virtualisation (NFV). However, current NFV designs focus on the Cloud, resulting in broken and suboptimal deployments when deploying to the Fog. Through a case study with preliminary results, this paper presents the effectiveness of Siren: a new prototype platform designed as a tool to deploy and manage Virtual Network Functions in Fog environments.	fog computing;general-purpose computing on graphics processing units;internet;network function virtualization;prototype	Lyndon Fawcett;Nicholas J. P. Race	2017		10.1145/3050220.3060611	embedded system;real-time computing;engineering;distributed computing	Networks	-15.403508071156612	82.35595239328504	91
85dd913b774103da6a9ff397f61dccec64653923	person re-identification based on part feature specificity		Person re-identification has become one of the most important problems in video surveillance system. In a multi-camera video surveillance system with non-overlapped area, the appearances of one person are much difference according different cameras or in the same camera at different times. On the other hand, different person may appears similar in one camera, so which made person re-identification a challenging problem. This paper carried out a person re-identification algorithm based on part feature specificity. This algorithm extract color, texture and shape features of different part of body first, then gather statistic specificity weight of these features for each part. At last, doing feature weighting both part weight and feature specificity in distance calculating, which make features with strong specificity more important. This algorithm indicates some parts of body are more important than others in re-identification, and the same part from different people with different appearance, the features with strong specificity are more important than the others. We have done our experiments at public datasets VIPeR and iLIDS, and evaluate the result by CMC. Result indicates this algorithm has higher re-identification rate, and more robust to viewing condition changes, illumination variations, background clutter and occlusion.	sensitivity and specificity	Dengyi Zhang;Qian Wang;Xiaoping Wu;Yu Cao	2015		10.1007/978-981-10-0356-1_22	clutter;statistic;mathematics;artificial intelligence;weighting;pattern recognition	Vision	34.765416694919885	-50.99165656529304	92
69c81ba4f3ce237d2f8826de2597633fa385bfad	reading blood glucose from subcutaneous electric current by means of a regularization in variable reproducing kernel hilbert spaces	clinical data;kernel;current;sugar blood kernel current measurement current navigation accuracy;blood glucose;accuracy;navigation;current measurement;reproducing kernel hilbert space;blood;sugar	In this paper we propose an adaptive kernel regularization algorithm for blood glucose reading from subcutaneous electric current. We illustrate the proposed algorithm with clinical data and quantify its clinical accuracy by means of the Clarke error grid analysis (EGA) and by the number of detected hypoglycemic events. We show that the proposed algorithm provides more accurate blood glucose reading than a commercially available system.	algorithm;edmund m. clarke;enhanced graphics adapter;freestyle;hilbert space;kernel (operating system);sampling (signal processing);sensor;spaces	Valeriya Naumova;Sergei V. Pereverzyev;Sivananthan Sampath	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160687	econometrics;mathematical optimization;navigation;kernel;current;reproducing kernel hilbert space;mathematics;accuracy and precision	Robotics	50.595902815075526	-84.20119115626726	93
7b56b5cb78712361918931d3d9ff29d455f67a1a	ambulatory monitoring of respiratory effort using a clothing-adhered biosensor		Accurate ambulatory sensing of patterns of respiratory effort has long eluded biomedical researchers. Though the data would inform clinical decision-making, the lack of sensors suitable for such data collection stymies clinical impact. This study describes an approach to sensing respiratory effort via thoracic and/or abdominal excursion in a form that affords longitudinal and continuous adherence. This approach leverages force-based sensors embedded in a clothing-adhered form factor to reduce user inconveniences. The primary benefit of this approach is user acceptance: it can monitor data longitudinally while addressing impactful user inconvenience issues with existing devices. Compared to a ground truth monitor of respiratory effort, and across both cognitive and physical tasks, the present approach resulted in a relative median error of 6.8% and mean absolute error of 1.8 breaths per min (SD=0.14). Sensor location affected performance, with chest-worn sensors outperforming waist-worn sensors. A more granular analysis of temporal markers of the respiratory cycle showed high agreement with ground truth; end-of-expiration temporal markers exhibited the least precision. The results indicate that this approach can be used to monitor respiratory effort accurately and theoretically with high adherence.	approximation error;embedded system;ground truth;maxima and minima;resident monitor;sensor	Mark Holt;Ben Yule;Dylan Jackson;Mary Zhu;Neema Moraveji	2018	2018 IEEE International Symposium on Medical Measurements and Applications (MeMeA)	10.1109/MeMeA.2018.8438678	data collection;real-time computing;ambulatory;wearable computer;mean absolute error;ground truth;respiratory effort;computer science	Mobile	11.406540191952953	-87.3743032314339	94
af78a25b122d12ef6cde90dfd4313c8c048cb9ad	network-based pi control for output tracking of continuous-time systems with time-varying sampling and network-induced delays		Abstract For a continuous-time linear system with constant reference input, the network-based proportional-integral (PI) control is developed to solve the output tracking control problem by taking time-varying sampling and network-induced delays into account. A traditional PI control system is introduced to obtain the equilibriums of state and control input. Using the equilibriums, a discrete-time PI tracking controller in a network environment is constructed. The resulting network-based PI control system is described by an augmented system with two input delays and the output tracking objective is transformed into ensuring asymptotic stability of the augmented system. A delay-dependent stability condition is established by a discontinuous augmented Lyapunov–Krasovskii functional approach. The PI controller design result of in-wheel motor as a case study is provided in terms of linear matrix inequalities. Matlab simulation and experimental results resorting to a test-bed for ZigBee-based control of in-wheel motor are given to validate the proposed method.	sampling (signal processing)	Dawei Zhang;Zhiyong Zhou;Xinchun Jia	2018	J. Franklin Institute	10.1016/j.jfranklin.2018.04.041	control theory;pid controller;pi;sampling (statistics);linear system;matrix (mathematics);control theory;mathematics;exponential stability;control system	ML	67.54979266625132	-2.1396566956192977	95
93701c720da0c842ebd242321315eecc390e7b68	automated segmentation of cerebellum using brain mask and partial volume estimation map	female;models neurological;brain;cerebellum;male;brain mapping;image interpretation computer assisted;adult;magnetic resonance imaging;algorithms;humans;databases factual;young adult;computational biology;models anatomic	While segmentation of the cerebellum is an indispensable step in many studies, its contrast is not clear because of the adjacent cerebrospinal fluid, meninges, and cerebra peduncle. Thus, various cerebellar segmentation methods, such as a deformable model or a template-based algorithm might exhibit incorrect segmentation of the venous sinuses and the cerebellar peduncle. In this study, we propose a fully automated procedure combining cerebellar tissue classification, a template-based approach, and morphological operations sequentially. The cerebellar region was defined approximately by removing the cerebral region from the brain mask. Then, the noncerebellar region was trimmed using a morphological operator and the brain-stem atlas was aligned to the individual brain to define the brain-stem area. The proposed method was validated with the well-known FreeSurfer and ITK-SNAP packages using the dice similarity index and recall and precision scores. As a result, the proposed method was significantly better than the other methods for the dice similarity index (0.93, FreeSurfer: 0.92, ITK-SNAP: 0.87) and precision (0.95, FreeSurfer: 0.90, ITK-SNAP: 0.93). Therefore, it could be said that the proposed method yielded a robust and accurate segmentation result. Moreover, additional postprocessing with the brain-stem atlas could improve its result.	alignment;atlases;cerebellar peduncle;cerebrospinal fluid;cervical atlas;clinical use template;cortical cell layer of the cerebellum;freesurfer;hl7publishingsubsection <operations>;itk gene;itk-snap;mathematical morphology;nasal sinus;peduncle - plant part;precision and recall;segmentation action;whole earth 'lectronic link;algorithm;biologic segmentation	Dong-Kyun Lee;Uicheul Yoon;Ki-Chang Kwak;Jong-Min Lee	2015		10.1155/2015/167489	neuroscience;radiology;young adult;computer science;artificial intelligence;magnetic resonance imaging;brain mapping;anatomy	Vision	40.25706396188344	-80.00484312552328	96
eebcf264de357b9875ce5aa113a2ebc9d0db8605	learning rules for chinese prosodic phrase prediction	prediction rule;two-level prosodic structure;speech corpus;better prediction accuracy;chinese prosodic phrase prediction;corpus text;phrase prediction;prosodic phrase prediction;example database;corresponding prosodic boundary label	This paper describes a rule -learning approach towards Chinese prosodic phrase prediction for TTS systems. Firstly, we prepared a speech corpus having about 3000 sentences and manually labelled the sentences with two-level prosodic structure. Secondly, candidate features related to prosodic phrasing and the corresponding prosodic boundary labels are extracted from the corpus text to establish an example database. A series of comparative experiments is conducted to figure out the most effective features from the candidates. Lastly, two typical rule learning algorithms (C4.5 and TBL) are applied on the example database to induce prediction rules. The paper also suggests general evaluation parameters for prosodic phrase prediction. With these parameters, our methods are compared with RNN and bigram based statistical methods on the same corpus. The experiments show that the automatic rule-learning approach can achieve better prediction accuracy than the non-rule based methods and yet retain the advantage of the simplicity and understandability of rule systems. Thus it is justified as an effective alternative to prosodic phrase prediction.	bigram;c4.5 algorithm;database;experiment;feature selection;julia;machine learning;netware file system;programmer;random neural network;speech corpus;test data;text corpus	Sheng Zhao;Jianhua Tao;Lianhong Cai	2002			natural language processing;speech recognition;computer science;linguistics	NLP	-21.044265886563775	-80.82156615996654	97
0286195e985f64d2b404754ed9a665a5c9eff85b	a sparse kernel density estimation algorithm using forward constrained regression	endnotes;kernel density estimate;pubications	Using the classical Parzen window (PW) estimate as the target function, the sparse kernel density estimator is constructed in a forward constrained regression manner. The leave-one-out (LOO) test score is used for kernel selection. The jackknife parameter estimator subject to positivity constraint check is used for the parameter estimation of a single parameter at each forward step. As such the proposed approach is simple to implement and the associated computational cost is very low. An illustrative example is employed to demonstrate that the proposed approach is effective in constructing sparse kernel density estimators with comparable accuracy to that of the classical Parzen window estimate.	algorithm;algorithmic efficiency;computation;estimation theory;jackknife resampling;kernel (operating system);kernel density estimation;linear least squares (mathematics);sparse matrix;test set	Xia Hong;Sheng Chen;Christopher J. Harris	2007		10.1007/978-3-540-74282-1_151	kernel;principal component regression;kernel regression;kernel density estimation;econometrics;mathematical optimization;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;multivariate kernel density estimation;mathematics;variable kernel density estimation;polynomial kernel;statistics	ML	29.948226790779742	-26.6446425301993	98
46d2e0b490e032b711b634730a156005a4227106	a 5ghz digital fractional- $n$ pll using a 1-bit delta–sigma frequency-to-digital converter in 65 nm cmos		"""A highly digital two-stage fractional-<inline-formula> <tex-math notation=""""LaTeX"""">$N$ </tex-math></inline-formula> phase-locked loop (PLL) architecture utilizing a first-order 1-bit <inline-formula> <tex-math notation=""""LaTeX"""">$ \Delta \!\Sigma $ </tex-math></inline-formula> frequency-to-digital converter (FDC) is proposed and implemented in a 65nm CMOS process. Performance of the first-order 1-bit <inline-formula> <tex-math notation=""""LaTeX"""">$ \Delta \!\Sigma $ </tex-math></inline-formula> FDC is improved by using a phase interpolator-based fractional divider that reduces phase quantizer input span and by using a multiplying delay-locked loop that increases its oversampling ratio. We also describe an analogy between a time-to-digital converter (TDC) and a <inline-formula> <tex-math notation=""""LaTeX"""">$ \Delta \!\Sigma $ </tex-math></inline-formula> FDC followed by an accumulator that allows us to leverage the TDC-based PLL analysis techniques to study the impact of <inline-formula> <tex-math notation=""""LaTeX"""">$ \Delta \!\Sigma $ </tex-math></inline-formula> FDC characteristics on <inline-formula> <tex-math notation=""""LaTeX"""">$ \Delta \!\Sigma $ </tex-math></inline-formula> FDC-based fractional-<inline-formula> <tex-math notation=""""LaTeX"""">$N$ </tex-math></inline-formula> PLL (FDCPLL) performance. Utilizing proposed techniques, a prototype PLL achieves 1 MHz bandwidth, −101.6 dBc/Hz in-band phase noise, and <inline-formula> <tex-math notation=""""LaTeX"""">$ {\textrm {1.22 ps}_{\textrm {rms}}}$ </tex-math></inline-formula> (1 kHz–40 MHz) jitter while generating 5.031GHz output from 31.25MHz reference clock input. For the same output frequency, the stand-alone second-stage fractional-<inline-formula> <tex-math notation=""""LaTeX"""">$N$ </tex-math></inline-formula> FDCPLL achieves 1MHz bandwidth, −106.1dBc/Hz in-band phase noise, and <inline-formula> <tex-math notation=""""LaTeX"""">$ {\textrm {403 fs}_{\textrm {rms}}}$ </tex-math></inline-formula> jitter with a 500MHz reference clock input. The two-stage PLL consumes 10.1mW power from a 1V supply, out of which 7.1 mW is consumed by the second-stage FDCPLL."""	1-bit architecture;accumulator (computing);arnold tongue;cmos;cpu multiplier;delay-locked loop;first-order predicate;first-order reduction;floppy-disk controller;oversampling;phase noise;phase-locked loop;prototype;quantization (signal processing);time-to-digital converter	Mrunmay Talegaonkar;Tejasvi Anand;Ahmed Elkholy;Amr Elshazly;Romesh Kumar Nandwana;Saurabh Saxena;Brian Young;Woo-Seok Choi;Pavan Kumar Hanumolu	2017	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2017.2718670	dbc;computer science;electronic engineering;oversampling;phase-locked loop;control theory;phase noise;delta-sigma modulation;architecture;jitter;analogy	EDA	61.01295712248262	50.08795980045911	99
22313f77258f86923cbd63b118067fd01d3d7188	heuristic strategies for using computers to enrich education	feedback mechanism;educational methods;educational media;technological advancement;animal behaviour;computer assisted instruction;educational strategies;computer science;educational technology;instructional innovation;teaching methods	Computers differ from other technology in an important way—they are part and parcel of a growing body of insights about human problem-solvers. The accomplishments of young students, in particular, suggest that certain aspects of computing are directly related to a deep view of education. This paper examines the basis for such a relationship, and proposes a heuristic methodology for bringing it about. The methodology is derived from the view that the primary function of education is to liberate human potential. Four heuristic strategies for using student-controlled computing to support this view are given. The relation of such a use of technology to the role of human teachers, and to the technology of CAI are also discussed. An example of how the heuristic methodology has been used to design a new approach to math education (Soloworks) is described.	heuristic	Thomas A. Dwyer	1974	International Journal of Man-Machine Studies	10.1016/S0020-7373(74)80001-5	educational technology;simulation;human–computer interaction;computer science;artificial intelligence;teaching method;feedback;management science;multimedia	Arch	-70.82128742041039	-38.26609256293517	100
f3e8cdb58e29ee636c961c64638913f9f822cb6f	minimizing the convergence error for the smart grid with duration-limited communications		In this paper, we investigate the issue of the convergence error of the consensus based algorithm for the economic dispatch problem (EDP) in the smart grid, where the incremental cost (IC) acts as the consensus variable. The algebraic expression of the convergence error in the context of duration-limited communications is presented. The performance in terms of the convergence error varies with the set of generators selected to be informed the external message associated with the optimal IC. We consider the case of minimizing the convergence error subject to a fixed cardinal number and the case of minimizing the size of the set of generators selected subject to a given bound. By formulating them as corresponding optimization problems, we present algorithm to obtain the optimal set of generators for both cases. The effectiveness of the proposed algorithms is verified by examples.	algorithm;dynamic dispatch;electronic data processing;mathematical optimization;optimization problem	Ruimeng Gan;Jinliang Shao;Yue Xiao;Yulan Gao;Bin Fu	2018	2018 IEEE International Conference on Communications Workshops (ICC Workshops)	10.1109/ICCW.2018.8403782	cardinal number;mathematical optimization;smart grid;computer science;real-time computing;algebraic expression;genetic algorithm;economic dispatch;marginal cost;convergence (routing);optimization problem	Robotics	33.69343317954017	1.8925283793793124	101
a5d5332dbbe60fbf7dd5c5add767bcb9ec4cb3f3	an efficient k nearest neighbors searching algorithm for a query line	plus lointain voisin;complejidad espacio;algoritmo busqueda;preprocesor;time complexity;geometrie algorithmique;preprocesseur;geometria combinatoria;preprocessor;algorithme recherche;search algorithm;computational geometry;duality;vecino mas cercano;dualite;complexite temps;nearest and farthest neighbor;combinatorial geometry;informatique theorique;arreglo;space complexity;plus proche voisin;nearest neighbour;k nearest neighbor;geometria computacional;dualidad;geometrie combinatoire;arrangement;farthest neighbor;complexite espace;complejidad tiempo;ligne interrogation;query line;computer theory;informatica teorica	We present an algorithm for finding k nearest neighbors of a given query line among a set of n points distributed arbitrarily on a two-dimensional plane. Our algorithm requires O(n2) time and O(n2/log n) space to preprocess the given set of points, and it answers the query for a given line in O(k + log n) time, where k may also be an input at the query time. Almost a similar technique works for finding k farthest neighbors of a query line, keeping the time and space complexities invariant. We also show that if k is known at the time of preprocessing, the time and space complexities for the preprocessing can be reduced keeping the query times unchanged.	k-nearest neighbors algorithm;search algorithm	Subhas C. Nandy;Sandip Das;Partha P. Goswami	2003	Theor. Comput. Sci.	10.1016/S0304-3975(02)00322-5	time complexity;combinatorics;duality;computational geometry;computer science;artificial intelligence;mathematics;dspace;k-nearest neighbors algorithm;preprocessor;algorithm;search algorithm	ECom	15.679306149222002	27.415678489537665	102
d6b115a07c07812d839d6b1242eacd7a06b71a83	blind channel estimation using cooperative subcarriers for ofdm systems		This work introduces a novel blind channel estimation technique for orthogonal frequency division multiplexing (OFDM) systems over time- varying mobile channels. The proposed estimator exploits the channel correlation over consecutive OFDM symbols to estimate the channel parameters blindly. In the new estimator, particular subcarriers are modulated using M-ary phase shift keying (MPSK), and subcarriers with the same indices in the consecutive OFDM symbol are modulated using M-ary amplitude shift keying (MASK), replacing the pilots in pilot-aided systems. Consequently, all subcarriers are data-bearing, which leads to spectral efficiency improvement. The proposed estimator uses the feature that MPSK and MASK modulated symbols have sufficient channel state information (CSI) that enable them to cooperate in order to detect the MPSK symbols coherently and blindly. Then, the CSI at the corresponding MPSK symbols can be acquired in a decision-directed (DD) fashion. The performance of the proposed estimator is evaluated in terms of symbol error rate (SER) and mean-squared error (MSE), where an exact analytical formula is obtained for the SER of binary phase shift keying (BPSK) symbols in mobile radio channels with various time-varying rates. The obtained results show that the proposed estimator produces accurate channel estimates as compared to pilot-aided and state-of-the-art systems without additional complexity.	amplitude-shift keying;channel state information;frequency divider;key (cryptography);mean squared error;modulation;multiplexing;spectral efficiency;subcarrier	Anas Saci;Abdallah Shami;Arafat J. Al-Dweik	2018	2018 IEEE International Conference on Communications (ICC)	10.1109/ICC.2018.8422355	phase-shift keying;real-time computing;estimator;channel state information;word error rate;control theory;amplitude-shift keying;spectral efficiency;orthogonal frequency-division multiplexing;computer science;communication channel	Mobile	49.68987889278519	77.570733746427	103
78f90821542346775962d91d6ae6477b804e55b4	kernels in cartesian products of digraphs		A kernel J of a digraph D is an independent set of vertices of D such that for every vertex w ∈ V (D)\J there exists an arc from w to a vertex in J . In this paper we have obtained results for the existence and nonexistence of kernels in Cartesian products of certain families of digraphs, and characterized T Cn, T Pn and Cm Cn which have kernels, where T is a tournament, and Pn and Cn are, respectively, the directed path and the directed cycle of order n. Finally, we have introduced and studied kernel-partitionable digraphs.	carrier-to-noise ratio;cartesian closed category;cycle (graph theory);directed graph;disk partitioning;independent set (graph theory);kernel (operating system);path (graph theory)	R. Lakshmi;S. Vidhyapriya	2016	Australasian J. Combinatorics		discrete mathematics;mathematics;cartesian product	Theory	29.137128640031914	32.06071404398461	104
7672a6dcb407ea04f332b7cf61eaa849475d7104	supervisory control for coordinating volt/var control devices on a distribution system	distribution systems;power loss reduction;volt var control	This paper proposes a computationally efficient method for supervisory control of Volt/Var control devices-voltage regulators and capacitor banks - on a distribution system. The method searches for the best control settings for these devices as the operating conditions change on the system in real-time. The objective of the supervisory control is that of a volt/var optimization - minimize the power losses on the distribution circuit while keeping the node voltages within the acceptable range. The method considers multiple voltage regulators and capacitor banks on a distribution feeder. The test results show the effectiveness of method in terms of providing the global optimal solution with fast computational speed.	algorithmic efficiency;mathematical optimization;real-time clock;voltage regulator	Gulcihan Ozdemir;Selcuk Emiroglu;Mesut Baran	2016	2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)	10.1109/ISGT.2016.7781162	control engineering;electronic engineering;engineering;control theory	Robotics	4.296845234419511	4.636610973218851	105
09a3c969224ca71f7d29d9715b39b7cf199f1024	extracting gait velocity and stride length from surrounding radio signals		Gait velocity and stride length are critical health indicators for older adults. A decade of medical research shows that they provide a predictor of future falls, hospitalization, and functional decline among seniors. However, currently these metrics are measured only occasionally during medical visits. Such infrequent measurements hamper the opportunity to detect changes and intervene early in the impairment process.  In this paper, we develop a sensor that uses radio signals to continuously measure gait velocity and stride length at home. Our sensor hangs on a wall like a picture frame. It does not require the monitored person to wear or carry a device on her body. Our approach builds on recent advances in wireless systems which have shown that one can locate people based on how their bodies impact the surrounding radio signals. We demonstrate the accuracy of our method by comparing it to the gold standard in clinical tests, and the VICON motion tracking system. Our experience from deploying the sensor in 14 homes indicates comfort with the technology and a high acceptance rate.	acceptance testing;kerrison predictor;tracking system;velocity (software development)	Chen-Yu Hsu;Yuchen Liu;Zachary Kabelac;Rumen Hristov;Dina Katabi;Christine Liu	2017		10.1145/3025453.3025937	wireless;real-time computing;simulation;human–computer interaction;gait;health indicator;continuous monitoring;computer science;match moving;stride	HCI	9.480029645588143	-86.61525806867456	106
914e86f7d7a4844d2f3cf29ce2cb6dfdfc928f87	path diversity for ds-ssma communications in nakagami fading channels	rayleigh fading;path diversity;average error probability;performance comparison;nakagami fading;error correction code;direct sequence spread spectrum;fading channel;multiple access interference;outage probability;quadrature rule;multiple access;reed solomon code	Average error probability and outage probability for an asynchronous direct sequence spread spectrum multiple access communications through slow nonselective Nakagami fading channels are evaluated for nondiversity and diversity receptions. Using the Gauss quadrature rule, the moments of the self-interference and the multiple access interferences are used to evaluate average error probability and outage probability. Combining the diversity technique and error correcting codes, comparisons between the uncoded nondiversity DS-SSMA system and that of the coded diversity system are shown for the Gold Code of codelength 127. Using fourth-order diversity and the Reed-Solomon code, the maximum achievable number of users is 12 percent of the codelength for Rayleigh fading, when the average probability is 10 -3. The corresponding outage probability is less than 5 percent. Performance comparisons between Rician and Nakagami fading channels are made. Since the system is interference limited, the performance seems to show no significant difference for the two fading channel models when the number of users is large.	code;downtime;forward error correction;gaussian quadrature;gauss–kronrod quadrature formula;interference (communication);rayleigh fading;reed–solomon error correction;sun outage	Kou Tan Wu;Shuenn An Tsaur	1994	Telecommunication Systems	10.1007/BF02110143	fading distribution;error detection and correction;telecommunications;computer science;rayleigh fading;direct-sequence spread spectrum;diversity scheme;channel state information;fading;reed–solomon error correction;statistics	ML	40.84850470451691	74.83149521945438	107
4fb0188a73da617c0f8b2bb03ea63709a048a32c	development of saap3d force field and the application to replica-exchange monte carlo simulation for chignolin and c-peptide	force field;single amino acid potential;peptide;monte carlo simulation;nmr analysis	Single amino acid potential (SAAP) would be a prominent factor to determine peptide conformations. To prove this hypothesis, we previously developed SAAP force field for molecular simulation of polypeptides. In this study, the force field was renovated to SAAP3D force field by applying more accurate three-dimensional main-chain parameters, instead of the original two-dimensional ones, for the amino acids having a long side-chain. To demonstrate effectiveness of the SAAP3D force field, replica-exchange Monte Carlo (REMC) simulation was performed for two benchmark short peptides, chignolin (H-GYDPETGTWG-OH) and C-peptide (CHO-AETAAAKFLRAHA-NH2). For chignolin, REMC/SAAP3D simulation correctly produced native β-turn structures, whose minimal all-atom root-mean-square deviation value measured from the native NMR structure (except for H) was 1.2 Å, at 300 K in implicit water, along with misfolded β-hairpin structures with unpacked aromatic side chains of Tyr2 and Trp9. Similar results were obtained for chignolin analog [G1Y,G10Y], which folded more tightly to the native β-turn structure than chignolin did. For C-peptide, on the other hand, the α-helix content was larger than the β content on average, suggesting a significant helix-forming propensity. When the imidazole side chain of His12 was protonated (i.e., [His12Hip]), the α content became larger. These observations as well as the representative structures obtained by clustering analysis were in reasonable agreement not only with the structures of C-peptide that were determined in this study by NMR in 30% CD3CD in H2O at 298 K but also with the experimental and theoretical behaviors having been reported for protonated C-peptide. Thus, accuracy of the SAAP force field was improved by applying three-dimensional main-chain parameters, supporting prominent importance of SAAP for peptide conformations.		Michio Iwaoka;Toshiki Suzuki;Yuya Shoji;Kenichi Dedachi;Taku Shimosato;Toshiya Minezaki;Hironobu Hojo;Hiroyuki Onuki;Hiroshi Hirota	2017	Journal of computer-aided molecular design	10.1007/s10822-017-0084-8	side chain;computational chemistry;c-peptide;chemistry;monte carlo method;protonation;force field (physics);amino acid;peptide;crystallography	ML	97.36704020354608	-4.503501374626915	108
fb28652dcd4b329d37be18a0677135d0ce16aa1d	on minimization of test application time for ras	circuit testing costs power dissipation design for testability routing built in self test very large scale integration flip flops writing clocks;decoding;clocks;routing;dft random access scan ras scan design;flip flops;supercomputer education research centre;scan design;random access scan ras;testing;computer architecture;vlsi integrated circuit testing;low power;electronic systems engineering formerly cedt centre for electronic design technology;word length;vlsi testing random access scan conventional ras test application time low power dissipation test data volume cluster based techniques serial input random access scan variable word length random access scan;integrated circuit testing;vlsi;speech recognition;dft;random access	Conventional Random access scan (RAS) for testing has lower test application time, low power dissipation, and low test data volume compared to standard serial scan chain based design. In this paper, we present two cluster based techniques, namely, Serial Input Random Access Scan and Variable Word Length Random Access Scan to reduce test application time even further by exploiting the parallelism among the clusters and performing write operations on multiple bits. Experimental results on benchmarks circuits show on an average 2-3 times speed up in test write time and average 60% reduction in write test data volume.	benchmark (computing);parallel computing;random access;speedup;test data	Raghavendra Adiga;Gandhi Arpit;Virendra Singh;Kewal K. Saluja;Hideo Fujiwara;Adit D. Singh	2010	2010 23rd International Conference on VLSI Design	10.1109/VLSI.Design.2010.61	embedded system;routing;electronic engineering;scan chain;parallel computing;real-time computing;computer science;test compression;discrete fourier transform;software testing;very-large-scale integration;programming language;random access;computer network	EDA	19.835684285275853	53.10057413750728	109
1ca80b539a03b59f0bb41932fbd6325c5beb7136	elasticon; an elastic distributed sdn controller	control systems;protocols;standards;data center networks;computer architecture;servers;load management;scalability;software defined networks	Software Defined Networking (SDN) has become a popular paradigm for centralized control in many modern networking scenarios such as data centers and cloud. For large data centers hosting many hundreds of thousands of servers, there are few thousands of switches that need to be managed in a centralized fashion, which cannot be done using a single controller node. Previous works have proposed distributed controller architectures to address scalability issues. A key limitation of these works, however, is that the mapping between a switch and a controller is statically configured, which may result in uneven load distribution among the controllers as traffic conditions change dynamically. To address this problem, we propose ElastiCon, an elastic distributed controller architecture in which the controller pool is dynamically grown or shrunk according to traffic conditions. To address the load imbalance caused due to spatial and temporal variations in the traffic conditions, ElastiCon automatically balances the load across controllers thus ensuring good performance at all times irrespective of the traffic dynamics. We propose a novel switch migration protocol for enabling such load shifting, which conforms with the Openflow standard. We further design the algorithms for controller load balancing and elasticity. We also build a prototype of ElastiCon and evaluate it extensively to demonstrate the efficacy of our design.	algorithm;centralized computing;data center;elasticity (data store);fault tolerance;ibm notes;load balancing (computing);multitenancy;network switch;openflow;programming paradigm;prototype;scalability;software-defined networking	Advait Abhay Dixit;Fang Hao;Sarit Mukherjee;T. V. Lakshman;Ramana Rao Kompella	2014	2014 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)	10.1145/2658260.2658261	communications protocol;real-time computing;scalability;computer science;control system;operating system;distributed computing;software-defined networking;computer security;server;computer network	Networks	-13.78899905214718	81.78010067060163	110
74599fc7d45503ba53d9b842ecae1dbdc30865a7	visual security assessment for cipher-images based on neighborhood similarity	security assessment	In the recent decades, many practical algorithms have been put forward for images and videos encryption. However, there is no objective security assessment algorithm or calculation index has been proposed at present. According to the differences of pixel value and neighborhood distribution between cipher-images and original images, we present a visual security assessment algorithm based on neighborhood similarity. The experiment result shows that the scheme can provide an objective assessment which is match up to subjective assessment, and is also suitable for the security assessment of cipher-images produced by other selective encryption algorithms.	algorithm;authorization;cipher;encryption;pixel	Ye Yao;Zhengquan Xu;Jing Sun	2009	Informatica (Slovenia)		computer science;data mining;internet privacy;computer security	ML	38.10394150096548	-10.183366871812582	111
0ad580eb781f31a89e9b1d9bd1fca954730beaae	coverability in two dimensions		A word is quasiperiodic (or coverable) if it can be covered by occurrences of another finite word, called its quasiperiod. This notion was previously studied in the domains of text algorithms and combinatorics of right infinite words. We extend several results to two dimensions. We also characterize all rectangular words that cover non-periodic two-dimensional infinite words. Then we focus on two-dimensional words with infinitely many quasiperiods. We show that such words have zero entropy. However, contrarily to the one-dimensional case, they may not be uniformly recurrent.	algorithm;quasiperiodicity;recurrent neural network;recurrent word	Guilhem Gamard;Gwénaël Richomme	2015		10.1007/978-3-319-15579-1_31	combinatorics on words;discrete mathematics;combinatorics;mathematics;quasiperiodic function	Theory	36.56344111428718	36.851226348076196	112
db3b60b6a4e738de3abd2fdcd56d1fe54e93715c	modeling of the front end of a new capacitive finger-type angular-position sensor	automotive electronics;front end;system modeling;transfer functions;carrier frequency system capacitive finger type angular position sensor front end modeling angular speed sensor automotive applications metrology applications capacitive sensor principle response speed resolution compact sensor outlines dirt insensitivity condensation insensitivity finger type geometry working principle sensor layout sensor design front end topology input circuit transfer function angular rotor position measurement results ratiometric measurement algorithm;capacitive sensor;angular measurement;indexing terms;signal processing equipment;angular velocity measurement;signal processing equipment capacitive sensors position measurement angular measurement angular velocity measurement automotive electronics transfer functions;transfer function;position measurement;capacitive sensors prototypes robustness automotive engineering metrology geometry sensor phenomena and characterization circuit topology transfer functions position measurement;capacitive sensors	A robust capacitive finger-type angular-position and angular-speed sensor for automotive and metrology applications is presented. The advantages of the capacitive sensor principle are simplicity, high response speed and resolution (/spl plusmn/0.04/spl deg/), compact sensor outlines, and insensitivity to dirt and condensation. Additionally the presented finger-type geometry allows easy mounting and replacement. The paper describes the working principle, the design and the layout of a prototype sensor. It models the front-end topology and derives the transfer function of the input circuit. The required relationships for computing the angular rotor position are presented, and measurement results obtained from a prototype sensor are discussed.	angularjs	Georg Brasseur	2001	IEEE Trans. Instrumentation and Measurement	10.1109/19.903887	control engineering;computer vision;electronic engineering;computer science;engineering;electrical engineering;proximity sensor;capacitive displacement sensor;capacitive sensing;transfer function	Embedded	78.99232776359995	-19.571751905686547	113
88bd7b25463381905080be1c6d82c7e1453e62c9	packet structure and receiver design for low latency wireless communications with ultra-short packets		Fifth generation wireless standards require much lower latency than what current wireless systems can guarantee. The main challenge in fulfilling these requirements is the development of short packet transmission, in contrast to most of the current standards, which use a long data packet structure. Since the available training resources are limited by the packet size, reliable channel and interference covariance estimation with reduced training overhead are crucial to any system using short data packets. In this paper, we propose an efficient receiver that exploits useful information available in the data transmission period to enhance the reliability of the short packet transmission. In the proposed method, the receive filter (i.e., the sample covariance matrix) is estimated using the received samples from the data transmission without using an interference training period. A channel estimation algorithm to use the most reliable data symbols as virtual pilots is employed to improve quality of the channel estimate. Simulation results verify that the proposed receiver algorithms enhance the reception quality of the short packet transmission.	algorithm;channel state information;elegant degradation;exploit (computer security);fifth generation computer;hp 48 series;interference (communication);interrupt latency;mimo;network packet;overhead (computing);requirement;simulation;the matrix;throughput	Byungju Lee;Sunho Park;David James Love;Hyoungju Ji;Byonghyo Shim	2018	IEEE Transactions on Communications	10.1109/TCOMM.2017.2755012	latency (engineering);latency (engineering);computer science;estimation of covariance matrices;packet generator;data transmission;real-time computing;network packet;computer network;communication channel;signal-to-noise ratio	Mobile	43.49089549851854	83.83143906846763	114
01bbad5d650fd61099186f7e32576d7ee3adfb18	implementing a semantic service provision platform		  Zusammenfassung  Die nahtlose Integration von Informationssystemen spielt eine wichtige Rolle bei der Entwicklung und Wartung von Softwareprodukten.  In den heutigen, dynamischen Märkten ist Veränderung eher die Regel als die Ausnahme. Daher ist die Möglichkeit, Softwareprodukte  effizient an Veränderungen der Anwendungslandschaft anzupassen, ein wichtiger Vorteil für eine Unternehmung. Die Vision dienstorientierter  Ansätze geht dahin, dass unternehmensrelevante Funktionalitäten von existierenden Softwaresystemen als Dienste bereitgestellt  werden, die dann durch eine Komposition der Dienste zu weiteren oder neuen Anwendungen zusammengestellt werden. Leider ist  diese Vision bis heute noch nicht in einem hinreichenden Maß in der Praxis umgesetzt worden.      Der Artikel präsentiert eine Plattform, die eine semantische Suche, Komposition und Ausführung von Diensten unterstützt und  die oben genannte Vision umsetzt. Die Realisierung der Vision wurde prototypisch für ein begrenztes Anwendungsszenario umgesetzt,  um ihre grundsätzliche Implementierbarkeit aufzuzeigen. Erkenntnisse aus der prototypischen Umsetzung sind: Erstens, die Anforderungen  an die logischen Reasoner sind hoch, wenn man „echte“ Problemstellungen lösen möchte. Zweitens, die formale Spezifikation  „realer“ Dienste ist eine aufwändige Aufgabe und drittens, es ist zurzeit nicht einfach, reale Szenarien für eine dynamische  Dienstkomposition zu finden, weil das Vertrauen in diese Technologie fehlt, weil die Frage nach der Verantwortung im Fehlerfalle  nicht geklärt ist und weil die Investitionen in die semantische Modellierung von Diensten sich nur für hochdynamische Umgebungen  auszahlen.        		Dominik Kuropka;Mathias Weske	2008	Wirtschaftsinformatik	10.1007/s11576-007-0016-7	web service;volatility;computer science;information integration;service-oriented architecture;process control;formal specification;semantics;landscape;service discovery;confidence;business process;law;information technology;world wide web;computer security	Vision	-102.01511236896476	34.72946969103514	115
5851850e383b73cb27efaa819c1dc2ffcbea3723	the frisch scheme in dynamic system identification	sistema lineal;systeme multivariable;systeme discret;dynamic system;linear system;identificacion sistema;system identification;sistema multivariable;estimacion parametro;multivariable system;parameter estimation;estimation parametre;sistema discreto;systeme lineaire;identification systeme;discrete system	The use of the Frisch scheme in the identification of linear dynamic systems is investigated in order to describe the whole family of models that can explain given input-output noisy sequences. Unlike the algebraic case, it is shown that, in general, only a single model is compatible with the data. These results are first proposed for single-input single-output systems and then generalized to the multivariable case.	dynamical system;system identification;uriel frisch	Sergio Beghelli;Roberto Guidorzi;Umberto Soverini	1990	Automatica	10.1016/0005-1098(90)90168-H	control engineering;system identification;dynamical system;discrete system;calculus;control theory;mathematics;linear system;estimation theory	Robotics	59.60905042742338	6.924475050479548	116
3a3679965ab4ef93409097be5898c68a51d81ea8	a knowledge-tracing model of learning from a social tagging system	cognitive models;user models;latent dirichlet allocation;lda;topic models;spartag.us;social tagging;social web	We propose a user model to support personalized learning paths through online material. Our approach is a variant of student modeling using the computer tutoring concept of knowledge tracing. Knowledge tracing involves representing the knowledge required to master a domain, and, from traces of online user behavior, diagnosing user knowledge states as a profile over those elements. The user model is induced from documents tagged by an expert in a social tagging system. Tags identified with “expertise” in a domain can be used to identify a corpus of domain documents. That corpus can be fed to an automated process that distills a topic model representation characteristic of the domain. As a learner navigates and reads online material, inferences can be made about the degree to which topics in the target domain have been learned. We validate this knowledge tracing approach against data from a social tagging study. As part of this evaluation, we match the predictions of the knowledge-tracing model to individual participant responses made to individual question items used to test domain knowledge.	folksonomy;personalization;text corpus;topic model;tracing (software);user modeling	Peter Pirolli;Sanjay Kairam	2012	User Modeling and User-Adapted Interaction	10.1007/s11257-012-9132-1	computer science;artificial intelligence;data science;machine learning;data mining;world wide web	Web+IR	-31.8746752309364	-60.746545538805435	117
2340c1b3a9b64d7bf4414c1e08432194dcc75565	lossless compression of bayer mask images using an optimal vector prediction technique	abstracts image resolution complexity theory image edge detection vectors radio access networks;compression algorithm;lossless compression;image resolution bayes methods data compression image coding image colour analysis image reconstruction;full resolution;compression ratio;source code;colour band mixing lossless compression technique bayer mask image optimal vector prediction technique lossy method full colour image reconstruction full resolution image coding	In this paper a lossless compression technique for Bayer pattern images is presented. The common way to save these images was to colour reconstruct them and then code the full resolution images using one of the lossless or lossy methods. This solution is useful to show the captured images at once, but it is not convenient for efficient source coding. In fact, the resulting full colour image is three times greater than the Bayer pattern image and the compression algorithms are not able to remove the correlations introduced by the reconstruction algorithm. However, the Bayer pattern images present new problems for the coding step. In fact, adjacent pixels belong to different colour bands mixing up different kinds of correlations. In this paper we present a lossless compression procedure based on an optimal vector predictor, where the Bayer pattern is divided into non-overlapped 2×2 blocks, each of them predicted as a vector. We show that this solution is able to exploit the existing correlation giving a good improvement of the compression ratio with respect to other lossless compression techniques, e.g., JPEG-LS.	algorithm;bayer filter;color image;data compression;jpeg;kerrison predictor;lossless compression;lossy compression;pixel	Stefano Andriani;Giancarlo Calvagno;Daniele Menon	2006	2006 14th European Signal Processing Conference		data compression;lossy compression;color cell compression;lossless jpeg;computer vision;data compression ratio;block truncation coding;image compression;jbig2;computer science;theoretical computer science;jpeg;lossless compression;chain code;fractal transform;context-adaptive binary arithmetic coding;texture compression;computer graphics (images)	Vision	43.631042404496	-15.156765040505256	118
8b06a5e7d55ab97a9f38002ed784d18259206f55	clustering with lattices in the analysis of graph patterns	graph mining;artificial intelligent;data structure	Mining frequent subgraphs is an area of research where we have a given set of graphs (each graph can be seen as a transaction), and we search for (connected) subgraphs contained in many of these graphs. In this work we will discuss techniques used in our framework Lattice2SAR for mining and analysing frequent subgraph data and their corresponding lattice information. Lattice information is provided by the graph mining algorithm gSpan; it contains all supergraph-subgraph relations of the frequent subgraph patterns — and their supports. Lattice2SAR is in particular used in the analysis of frequent graph patterns where the graphs are molecules and the frequent subgraphs are fragments. In the analysis of fragments one is interested in the molecules where patterns occur. This data can be very extensive and in this paper we focus on a technique of making it better available by using the lattice information in our clustering. Now we can reduce the number of times the highly compressed occurrence data needs to be accessed by the user. The user does not have to browse all the occurrence data in search of patterns occurring in the same molecules. Instead one can directly see which frequent subgraphs are of interest.	algorithm;browsing;cluster analysis;data mining;database transaction;graph (discrete mathematics);preprocessor;structure mining;xslt/muenchian grouping	Edgar H. de Graaf;Joost N. Kok;Walter A. Kosters	2007	CoRR		combinatorics;data structure;computer science;artificial intelligence;graph theory;theoretical computer science;forbidden graph characterization;comparability graph;data mining;mathematics;voltage graph;distance-hereditary graph;molecule mining	ML	-6.991877562625812	-37.92932104478511	119
ad336b392fbbe26792e4cf2cd7350cf7cf582de0	subforests of bipartite digraphs -- the minimum degree condition	minimum degree	Abstract   Let D be a bipartite digraph and let F be an oriented forest of size  k −1. We consider two conditions on the minimum indegree and the minimum outdegree of the digraph D guaranteeing that D contains F. These conditions extend older results concerning oriented trees of size  k −1.		Irmina A. Ziolo	2001	Discrete Mathematics	10.1016/S0012-365X(00)00453-2	combinatorics;discrete mathematics;topology;mathematics	Theory	29.663838485003865	28.85494175446213	120
0d4c33263d90f768cfa02d590730c2400dc848e8	karbon: eine web-basierte ar-anwendung für die kommunikationsunterstützung in der bauindustrie			eine and zwei	Ralf Dörner;Jonas Etzold;Paul Grimm;Jörg Schweitzer	2012				NLP	-98.92770272544455	25.25982700031428	121
f5fe7c08e84fa9127374185cf8420b150b76d035	on the hardware-software partitioning: the classic general model (cgm)	single processor;microprocessors;object oriented partitioning algorithm;object oriented partitioning algorithm hardware software partitioning classic general model mathematical modeling tool hardware software codesign single processor multiprocessor objective function;permutation arrays;object oriented methods;hardware software codesign;generic model;multiprocessor;system modeling;user generated content object oriented modeling partitioning algorithms computer architecture hardware java mathematical model algorithm design and analysis multiprocessing systems microprocessors;mathematical modeling tool;classic general model;object oriented partitioning hardware software partitioning system modeling;object oriented partitioning;objective function;computer architecture;object oriented methods hardware software codesign;object oriented;hardware software partitioning;mathematical model;multiprocessing systems;user generated content;algorithm design and analysis;object oriented modeling;partitioning algorithms;hardware;java	In this paper we introduce a mathematical modeling tool (called classic general model: CGM) for the general problem of hardware-software codesign so that different partitioning algorithms can be easily and quickly developed and compared in this same framework. CGM introduces a simple but efficient model which supports single/multiprocessor, primal and dual approaches, fine or coarse granularity. CGM determines solutions by stating mapping, implementation and permutation arrays. For judging among solutions of a certain algorithm, an objective function is defined. After modeling the problem by CGM we have a classic problem: finding the best values for elements of three arrays to optimize the objective function. We have modeled an object-oriented partitioning algorithm based on CGM. The promising results show the benefits of the CGM in development and comparison of partitioning algorithms	algorithm;device driver synthesis and verification;loss function;mathematical model;multiprocessing;optimization problem;partition problem	Hani JavanHemmat;Maziar Goudarzi;Shaahin Hessabi	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277800	algorithm design;computer architecture;parallel computing;real-time computing;multiprocessing;systems modeling;computer science;operating system;mathematical model;object-oriented programming;user-generated content;java	EDA	1.4190318933810835	52.7671824413764	122
e5f50d9c852864da52249f39293d20922f0d3299	noninvasive assessment of the complexity and stationarity of the atrial wavefront patterns during atrial fibrillation	mixing matrix structure noninvasive atrial wavefront pattern assessment atrial wavefront pattern complexity assessment atrial wavefront pattern stationarity assessment atrial fibrillation atrial activity spatiotemporal organization degree surface recordings body surface potential maps bspm wavefront pattern spatial complexity wavefront pattern temporal stationarity surface ecg principal component analysis pca;cardiopathie;singular value decomposition svd;trouble de l excitabilite;arritmia;spatial topographies;heart disease;distribucion espacial;metodo estadistico;excitability disorder;principal component analysis diseases electrocardiography medical signal processing pattern recognition;analisis componente principal;noninvasive atrial wavefront pattern assessment;wavefront;analyse amas;atrial fibrillation principal component analysis surface topography surface waves surface treatment decision making contracts reflection electrocardiography singular value decomposition;decomposition valeur singuliere;electrodiagnostic;pathologie de l appareil circulatoire;aged aged 80 and over analysis of variance atrial fibrillation body surface potential mapping cluster analysis electrocardiography female heart atria humans male middle aged principal component analysis signal processing computer assisted;body surface potential maps;surface potential;non invasive method;genie biomedical;wavefront pattern spatial complexity;spatial diversity;orejuela;surface recordings;singular value decomposition;trouble du rythme cardiaque;clinical decision making;hombre;statistical method;contracts;trastorno excitabilidad;body surface potential mapping bspm;bspm;cartographie;electrocardiographie;atrial activity spatiotemporal organization degree;surface ecg;atrial wavefront pattern complexity assessment;carta de datos;surface topography;methode non invasive;potencial superficie;electrodiagnostico	A novel automated approach to quantitatively evaluate the degree of spatio-temporal organization in the atrial activity (AA) during atrial fibrillation (AF) from surface recordings, obtained from body surface potential maps (BSPM), is presented. AA organization is assessed by measuring the reflection of the spatial complexity and temporal stationarity of the wavefront patterns propagating inside the atria on the surface ECG, by means of principal component analysis (PCA). Complexity and stationarity are quantified through novel parameters describing the structure of the mixing matrices derived by the PCA of the different AA segments across the BSPM recording. A significant inverse correlation between complexity and stationarity is highlighted by this analysis. The discriminatory power of the parameters in identifying different groups in the set of patients under study is also analyzed. The obtained results present analogies with earlier invasive studies in terms of number of significant components necessary to describe 95% of the variance in the AA (four for more organized AF, and eight for more disorganized AF). These findings suggest that automated analysis of AF organization exploiting spatial diversity in surface recordings is indeed possible, potentially leading to an improvement in clinical decision making and AF treatment.	anisotropic filtering;atrial fibrillation;body surface;chaos theory;decision making;heart atrium;map;norm (social);patients;principal component analysis;principal component analysis;sample variance;signal processing;spatiotemporal pattern;stationary process;ventricular fibrillation	Pietro Bonizzi;María de la Salud Guillem;Andreu M. Climent;José Millet-Roig;Vicente Zarzoso;Francisco Castells;Olivier Meste	2010	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2010.2052619	electronic engineering;electrodiagnosis;computer science;machine learning;mathematics;geometry;physics;principal component analysis	Visualization	21.481286691814265	-85.2707033783621	123
bb3f92bca271f8883d36075715242e7d29f1e6c5	vision-based registration for augmented reality with integration of arbitrary multiple planes	projection geometrique;vision ordenador;realite virtuelle;image processing;realidad virtual;geometrical projection;virtual reality;procesamiento imagen;projective geometry;proyeccion geometrica;traitement image;computer vision;projective plane;realite augmentee;plano proyectivo;realidad aumentada;sistema coordenadas;geometria proyectiva;image sequence;espacio proyectivo;plan projectif;projective space;vision ordinateur;secuencia imagen;systeme coordonnee;augmented reality;geometrie projective;espace projectif;sequence image;coordinate system	We propose a novel vision-based registration approach for Augmented Reality with integration of arbitrary multiple planes. In our approach, we estimate the camera rotation and translation by an uncalibrated image sequence which includes arbitrary multiple planes. Since the geometrical relationship of those planes is unknown, for integration of them, we assign 3D coordinate system for each plane independently and construct projective 3D space defined by projective geometry of two reference images. By integration with the projective space, we can use arbitrary multiple planes, and achieve high-accurate registration for every position in the input images.	ar (unix);augmented reality	Yuko Uematsu;Hideo Saito	2005		10.1007/11553595_19	projective plane;computer vision;augmented reality;projective space;projective geometry;topology;image processing;computer science;coordinate system;mathematics;geometry;virtual reality	Vision	49.98986468070409	-57.34852141901053	124
09ff2f654c787c3cb84838c89742808190a64840	integrity management in component based systems	middleware;object-oriented programming;software architecture;software maintenance;component based systems;software integrity management	There is a need for mechanisms for maintaining and restoring software integrity on deployed systems. Dynamic replacement, removal and addition of components in deployed systems is supported by most component models. This is needed to enable the software on a device to evolve in the period that it is owned by a consumer, but endangers the integrity of the software on these devices. For high volume consumer devices the challenge is to keep the devices operating reliable and robust in the period that it is owned, used and possibly reconfigured by a consumer. To this end, we propose mechanisms and tools, for maintaining system integrity on deployed systems.	system integrity	Johan Muskens;Michel R. V. Chaudron	2004	Proceedings. 30th Euromicro Conference, 2004.	10.1109/EURMIC.2004.1333429	reliability engineering;long-term support;verification and validation;computing;real-time computing;software sizing;software project management;computer science;systems engineering;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;software design description;middleware;software construction;application lifecycle management;resource-oriented architecture;software deployment;software system;software peer review	SE	-50.04632258750173	40.956048992270986	125
ba63bc62a9e4744bafca4225ae4384df99a93f8b	audio enhancing with dnn autoencoder for speaker recognition	neural networks;speech microphones speaker recognition noise measurement artificial neural networks training speech enhancement;distant microphone dnn autoencoder audio enhancing speech enhancement distant microphones noisy data fisher database text independent speaker recognition systems speaker recognition system;speech enhancement microphones speaker recognition;speaker recognition;denoising;dnn speaker recognition denoising de reverberation neural networks;dnn;de reverberation	In this paper we present a design of a DNN-based autoencoder for speech enhancement and its use for speaker recognition systems for distant microphones and noisy data. We started with augmenting the Fisher database with artificially noised and reverberated data and trained the autoencoder to map noisy and reverberated speech to its clean version. We use the autoencoder as a preprocessing step in the later stage of modelling in state-of-the-art text-dependent and text-independent speaker recognition systems. We report relative improvements up to 50% for the text-dependent system and up to 48% for the text-independent one. With text-independent system, we present a more detailed analysis on various conditions of NIST SRE 2010 and PRISM suggesting that the proposed preprocessig is a promising and efficient way to build a robust speaker recognition system for distant microphone and noisy data.	autoencoder;microphone;prism (surveillance program);preprocessor;signal-to-noise ratio;speaker recognition;speech enhancement	Oldrich Plchot;Lukás Burget;Hagai Aronowitz;Pavel Matejka	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472647	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;noise reduction	Robotics	-14.906287666064637	-89.55118559936393	126
626269e2d3885d2af62dafd1c8b94af99d15b0b0	signal and noise adapted filters for differential motion estimation	modelizacion;optimisation;statistical data;estimation mouvement;analisis estadistico;image processing;optimizacion;linear filter;estimacion movimiento;gradiente;procesamiento imagen;motion estimation;gradient;probabilistic approach;filtro lineal;traitement image;modelisation;local structure;statistical analysis;filtre lineaire;enfoque probabilista;approche probabiliste;image sequence;analyse statistique;donnee statistique;pattern recognition;invariante;secuencia imagen;optimization;estructura local;structure locale;reconnaissance forme;dato estadistico;reconocimiento patron;modeling;adaptive filter;invariant;sequence image	Differential motion estimation in image sequences is based on measuring the orientation of local structures in spatio-temporal signal volumes. Filters estimating the local gradient are applied to the image sequence for this purpose. Whereas previous approaches to filter optimization concentrate on the reduction of the systematical error of filters and motion models, the method presented in this paper is based on the statistical characteristics of the data. We present a method for adapting linear shift invariant filters to image sequences or whole classes of image sequences. Therefore, it is possible to simultaneously optimize derivative filters according to the systematical errors as well as to the statistical ones.	autocorrelation matrix;filter design;gradient;mathematical optimization;motion estimation;structure tensor;velocity (software development)	Kai Krajsek;Rudolf Mester	2005		10.1007/11550518_59	adaptive filter;computer vision;systems modeling;image processing;computer science;invariant;linear filter;motion estimation;mathematics;geometry;gradient	Vision	49.34437364622484	-58.96482794273419	127
5c4145fbfdd4c5d64dea27ee3b350ab427f243cf	detection and measurement of thin dielectric layers using reflection of frequency scanned millimetric waves	thickness measurement dielectric thin films fourier transforms microwave photonics microwave reflectometry millimetre wave detectors millimetre wave measurement;millimetre wave detectors;dielectric thin films;fourier transform;optical transmitters;optical reflection;optical surface waves;microwave reflectometry;size 10 mm to 100 mm;thin concealed dielectric layers;frequency measurement;frequency scanned millimetric wave reflectometry;three dimensional;burg transformation;microwave photonics;dielectric measurements frequency measurement optical reflection optical receivers optical transmitters optical interferometry optical harmonic generation optical surface waves lenses thickness measurement;fourier transformation;fourier transforms;optical properties;lenses;millimetre wave measurement;dielectric measurements;optical interferometry;size 10 mm to 100 mm thin concealed dielectric layers frequency scanned millimetric wave reflectometry burg transformation fourier transformation microwave optical properties;thickness measurement;optical receivers;optical harmonic generation;microwave optical properties	A method is described for detection and measurement of thin concealed dielectric layers using frequency scanned millimetre wave reflectometry, and carrying out a Burg or Fourier transformation of the results. Three dimensional data may potentially be obtained by moving the beam over the surface of the sample. Multiple layers of thickness ranging from 10 mm to 100 mm are readily detected and monitored through their microwave optical properties.	microwave;reflectometry;thickness (graph theory)	Nicholas Bowring;John G. Baker;John F. Alder	2007	2007 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2007.372818	fourier transform;electronic engineering	Robotics	90.33898540554503	-15.742737432741247	128
487b1c711d8f8fba6cad7f7455e21c42490ec468	the user side of sustainability: modeling behavior and energy usage in the home	energy;anomaly detection;machine learning;smart environments	Society is becoming increasingly aware of the impact that our lifestyle choices make on energy usage and the environment. As a result, research attention is being directed toward green technology, environmentally-friendly building designs, and smart grids. This paper looks at the user side of sustainability. In particular, it looks at energy consumption in everyday home environments to examine the relationship between behavioral patterns and energy consumption. It first demonstrates how data mining techniques may be used to find patterns and anomalies in smart home-based energy data. Next, it describes a method to correlate homebased activities with electricity usage. Finally, it describes how this information could inform users about their personal energy consumption and to support activities in a more energy-efficient manner. These approaches are validated by using real energy data collected in a set of smart home testbeds.	algorithm;behavioral pattern;data mining;feedback;home automation;image resolution;machine learning;mobile device;sensor;smart environment;uninterruptible power supply;usage data	Chao Chen;Diane J. Cook;Aaron S. Crandall	2013	Pervasive and Mobile Computing	10.1016/j.pmcj.2012.10.004	embedded system;anomaly detection;simulation;energy;computer science;world wide web;computer security	HCI	1.1274411465655423	32.948445872840374	129
cc609f0ba99c8f317d9f0a09652c0bbc0a2b309f	multiparty session types within a canonical binary theory, and beyond		A widespread approach to software service analysis uses session types. Very different type theories for binary and multiparty protocols have been developed; establishing precise connections between them remains an open problem. We present the first formal relation between two existing theories of binary and multiparty session types: a binary system rooted in linear logic, and a multiparty system based on automata theory. Our results enable the analysis of multiparty protocols using a much simpler type theory for binary protocols, ensuring protocol fidelity and deadlock-freedom. As an application, we offer the first theory of multiparty session types with behavioral genericity. This theory is natural and powerful; its analysis techniques reuse results for binary session types.		Luís Caires;Jorge A. Pérez	2016		10.1007/978-3-319-39570-8_6	computer science;theoretical computer science;distributed computing	ML	-35.663729404631454	72.88791271703926	130
6d5e000222784d75b1d1743a8ed330f65aff1708	machine learning techniques for content-based information retrieval. (méthodes d'apprentissage automatique pour la recherche par le contenu de l'information)			information retrieval;linear algebra;machine learning	Sanaa Chafik	2017				Theory	-107.19901884319941	13.138229028079069	131
bd2dcca948801b68091f9ff9278049d7e4d87756	facial feature localization using statistical models and sift descriptors	databases;gentleboost classifier;facial features shape gaussian distribution lighting histograms noise robustness principal component analysis human robot interaction face parameter estimation;histograms;local appearance model;pose illumination and expression;facial feature localization;training;image classification;testing;face alignment;statistical model;image interpretation;face recognition;shape;scale invariant feature transform;image representation;pixel;scale invariant feature transform descriptor;facial features;face;parameter estimation;image representation face recognition gaussian distribution image classification;model parameter estimation;facial landmark;active shape model;gaussian distribution;intensity profile;intensity profile facial feature localization statistical model active shape model image interpretation face alignment model parameter estimation gaussian distribution scale invariant feature transform descriptor image representation gentleboost classifier facial landmark local appearance model	Active Shape Model (ASM) is a powerful statistical tool for image interpretation, especially in face alignment. In the standard ASM, local appearances are described by intensity profiles, and the model parameter estimation is based on the assumption that the profiles follow a Gaussian distribution. It suffers from variations of poses, illumination and expressions. In this paper, an improved ASM framework, GentleBoost based SIFT-ASM is proposed. Local appearances of landmarks are originally represented by SIFT (Scale-Invariant Feature Transform) descriptors, which are gradient orientation histograms based representations of image neighborhood. They can provide more robust and accurate guidance for search than grey-level profiles. Moreover, GentleBoost classifiers are applied to model and search the SIFT features instead of the unnecessary assumption of Gaussian distribution. Experimental results show that SIFT-ASM significantly outperforms the original ASM in aligning and localizing facial features.	active shape model;algorithm;database;estimation theory;gradient;illumination (image);scale-invariant feature transform;statistical model	Zisheng Li;Jun-ichi Imai;Masahide Kaneko	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326323	normal distribution;active shape model;facial recognition system;face;statistical model;computer vision;contextual image classification;shape;computer science;machine learning;pattern recognition;scale-invariant feature transform;histogram;software testing;estimation theory;pixel	Vision	43.95965459775371	-52.792461414394644	132
ea3d7e923305863b8fb7834addcaae48621f2f16	task partitioning optimization algorithm for energy saving and load balance on noc-based mpsocs	minimization;load balance partitioning algorithm homogeneous mpsoc noc performance evaluation energy efficiency;sa task partitioning optimization algorithm energy saving load balance noc based mpsoc multiprocessor system on chip network on chip processor elements pe performance requirements intercommunicating task mapping efficiency process partition reduce pr map reduce algorithm energy consumption minimization ec minimization lb simulated annealing;simulated annealing;energy consumption;three dimensional displays;heuristic algorithms;algorithm design and analysis;partitioning algorithms energy consumption algorithm design and analysis heuristic algorithms three dimensional displays minimization simulated annealing;simulated annealing energy conservation energy consumption network on chip;partitioning algorithms	Multiprocessor System-on-Chip (MPSoC) based on Network-on-Chip (NoC) integrates a large amount of Processor Elements (PEs) to fulfill the performance requirements of several applications. These applications are composed of a set of intercommunicating tasks, which are dynamically mapped onto PEs of the target architecture. However, the efficient task-mapping requires some previous steps, among them partitioning, which organizes tasks considering their interaction before applying a mapping process. This paper introduces Partition Reduce (PR) - a task partitioning approach based on the MapReduce algorithm targeting homogeneous NoC based MPSoCs. We analyze the efficiency of PR for energy consumption (EC) minimization and load balance (LB). The results obtained from a set of experiments, with large number of tasks, demonstrate that PR is more effective on processing time and result quality when compared to the classic Simulated Annealing (SA). In addition, PR produces partitions with low energy consumption and rigorous load balance.	algorithm;experiment;lattice boltzmann methods;load balancing (computing);mpsoc;mapreduce;mathematical optimization;multiprocessing;network on a chip;requirement;simulated annealing;simulation;task allocation and partitioning of social insects	Marco P. Stefani;Thais Webber;Ramon Fernandes;Rodrigo Cataldo;Leticia B. Poehls;César A. M. Marcon	2015	Sixteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2015.7085412	algorithm design;computer architecture;parallel computing;real-time computing;simulated annealing;computer science;algorithm	Arch	-5.561421908968667	57.35909287848002	133
4c92bd412cfeb8c786045815fa14098c25019cf7	analog joint source channel coding using space-filling curves and mmse decoding	awgn channels analog joint source channel coding mmse decoding gaussian sources laplacian sources;channel coding;gaussian sources;least mean squares methods;analog coding;decoding;combined source channel coding;probability density function;discrete time;laplacian sources;space filling curves;rate distortion theory;joint source channel coding;laplace equations;mmse decoding;awgn channels;channel capacity;mmse decoding analog coding space filling curves;maximum likelihood decoding;conference report;performance analysis;additive white noise;least mean squares methods combined source channel coding decoding;space filling curve;channel coding maximum likelihood decoding signal to noise ratio awgn channels channel capacity rate distortion theory performance analysis laplace equations additive white noise delay;signal to noise ratio;analog joint source channel coding	We investigate the performance of a discrete-time all-analog-processing joint source channel coding system for the transmission of i.i.d. Gaussian and Laplacian sources over AWGN channels. In the encoder, two samples of an i.i.d. source are mapped into a channel symbol using a space-filling curve. Different from previous work in the literature, MMSE decoding instead of ML decoding is considered, and we focus on both high and low channel SNR regions. The main contribution of this paper is to show that the proposed system presents a performance very close to the theoretical limits, even at low SNR, as long as the curve parameters are properly optimized.	additive white gaussian noise;british undergraduate degree classification;channel capacity;decibel;decoding methods;encoder;forward error correction;numerical analysis;signal-to-noise ratio;space-filling curve	Yichuan Hu;Javier Garcia-Frias;Meritxell Lamarca	2009	2009 Data Compression Conference	10.1109/DCC.2009.45	discrete time and continuous time;probability density function;speech recognition;rate–distortion theory;channel code;telecommunications;computer science;mathematics;signal-to-noise ratio;channel capacity;statistics	Vision	39.68950908753022	68.30414494063321	134
0a1c6eddecebe732c6f37e7c6c2d6ac3a7797860	a no-reference objective image sharpness metric based on just-noticeable blur and probability summation	testing video compression biomedical measurements extraterrestrial measurements image quality image enhancement digital images monitoring costs image sensors;probability image enhancement;probability;image assessment;hvs;indexing terms;objective;image enhancement;perceptual based sharpness metric no reference objective image sharpness metric probability summation image blurriness metric just noticeable blur;image quality;sharpness metric;perception;sharpness metric image quality image assessment perception hvs no reference objective;no reference	This work presents a perceptual-based no-reference objective image sharpness/blurriness metric by integrating the concept of just noticeable blur (JNB) into a probability summation model. Unlike existing objective no-reference image sharpness/blurriness metrics, the proposed metric is able to predict the relative amount of blurriness in images with different content. Results are provided to illustrate the performance of the proposed perceptual-based sharpness metric. These results show that the proposed sharpness metric correlates well with the perceived sharpness.	gaussian blur	Rony Ferzli;Lina J. Karam	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379342	image quality;computer vision;mathematical optimization;index term;pattern recognition;probability;mathematics;perception;statistics	Vision	62.330356421266714	-63.57217177081433	135
8c8b2e6ce0ce3f0724fd7b84b8a114c56e7e7095	editorial: designing high quality system/software architectures	software architecture;quality system	Just about everything has an architecture - be it a software system, an organization, or even a human body - consisting of parts and interactions among the parts. Of increasingly crucial concerns in Software Engineering are the system architecture which describes the parts, and interactions between the parts, of the system - be it the enterprise architecture in which the projected software system is to function or the architecture of the projected software system itself.		Lawrence Chung;José Luis Garrido;Nary Subramanian;Manuel Noguera;Kawtar Benghazi Akhlaki	2010	Sci. Comput. Program.	10.1016/j.scico.2010.01.006	multilayered architecture;enterprise architecture framework;functional software architecture;reference architecture;software architecture;architecture tradeoff analysis method;quality management system;database-centric architecture;computer science;architecture domain;applications architecture;component-based software engineering;software development;software design description;software construction;solution architecture;software architecture description;view model;resource-oriented architecture;software deployment;systems architecture;software system	EDA	-52.32674002568178	26.72773902494691	136
355a23132a1f472b0c15131f87c091c14fb83b5e	multiple errors produced by single upsets in fpga configuration memory: a possible solution	field programmable gate array;random access memory;multiple errors;circuit faults;fault simulation;routing;place algorithm;network routing;route algorithm;integrated circuit reliability fpga configuration memory very high integration level sram field programmable gate arrays single event upsets multiple errors routing resources place algorithm route algorithm;redundancy;single event upsets;fault tolerance;logic testing sram chips field programmable gate arrays integrated circuit reliability network routing fault simulation;single event transient;logic testing;field programmable gate arrays single event transient circuit faults signal processing algorithms redundancy single event upset routing costs random access memory fault tolerance;place and route;sram;single event upset;fpga configuration memory;field programmable gate arrays;integrated circuit reliability;signal processing algorithms;very high integration level;fault injection;sram chips;routing resources	The very high integration levels reached by SRAM-based field programmable gate arrays (FPGAs) lead to high occurrence rate of single event upsets (SEUs) in their configuration memory, which can produce multiple errors affecting routing resources. Based on detailed analysis of this phenomenon, we devised a reliability-oriented place and route algorithm able to significantly improve the reliability of SRAM-based FPGAs with limited costs in terms of performance degradation and resource occupation. To evaluate the effectiveness of the algorithm we performed extensive fault injection experiments.	algorithm;elegant degradation;experiment;fault injection;field-programmable gate array;place and route;routing;single event upset;static random-access memory	Matteo Sonza Reorda;Luca Sterpone;Massimo Violante	2005	European Test Symposium (ETS'05)	10.1109/ETS.2005.29	reliability engineering;embedded system;routing;electronic engineering;parallel computing;real-time computing;computer science;field-programmable gate array	EDA	7.829085206816139	59.50246755979459	137
90d441ce7588c9c1cb5a4386a7bcd390a5d30c64	drawing density core-sets from incomplete relational data		Incompleteness is a ubiquitous issue and brings challenges to answer queries with completeness guaranteed. A density core-set is a subset of an incomplete dataset, whose completeness is approximate to the completeness of the entire dataset. Density core-sets are effective mechanisms to estimate completeness of queries on incomplete datasets. This paper studies the problems of drawing density core-sets on incomplete relational data. To the best of our knowledge, there is no such proposal in the past. (1) We study the problems of drawing density core-sets in different requirements, and prove the problems are all NP-Complete whether functional dependencies are given. (2) An efficient approximate algorithm to draw an approximate density core-set is proposed, where an approximate Knapsack algorithm and weighted sampling techniques are employed to select important candidate tuples. (3) Analysis of the proposed approximate algorithm shows the relative error between completeness of the approximate density core-set and that of a density core-set with same size is within a given relative error bound with high probability. (4) Experiments on both real-world and synthetic datasets demonstrate the effectiveness and efficiency of the algorithm.		Yongnan Liu;Jianzhong Li;Hong Gao	2017		10.1007/978-3-319-55699-4_32	data mining;computer science;completeness (statistics);tuple;knapsack problem;relational database;approximation error;relational theory;artificial intelligence;functional dependency;data quality;pattern recognition	ML	-8.28847321603757	-33.62710374698137	138
0dd8d9c5dbf056b50e2bfbba4eed1285af327106	treewidth and minimum fill-in on d-trapezoid graphs	interval graph;permutation graph	We show that the minimum fill-in and the minimum interval graph completion of ad-trapezoid graph can be computed in time O(n). We also show that the treewidth and the pathwidth of a d-trapezoid graph can be computed by an O(n tw(G) ) time algorithm. For both algorithms, d is supposed to be a fixed positive integer and it is required that a suitable intersection model of the given d-trapezoid graph is part of the input. As a consequence, the minimum fill-in and the minimum interval graph completion as well as the treewidth and the pathwidth of a given trapezoid graph (or permutation graph) can be computed in time O(n), even if no intersection model is part of the input.	algorithm;diagram;np-completeness;p (complexity);p versus np problem;pathwidth;polynomial;scan line;time complexity;treewidth	Hans L. Bodlaender;Ton Kloks;Dieter Kratsch;Haiko Müller	1998	J. Graph Algorithms Appl.		1-planar graph;outerplanar graph;block graph;pathwidth;split graph;combinatorics;discrete mathematics;interval graph;topology;graph bandwidth;clique-width;comparability graph;permutation graph;mathematics;voltage graph;distance-hereditary graph;tree-depth;circle graph;partial k-tree;graph minor;complement graph;line graph;string graph;planar graph	Theory	25.140618016030906	26.00717805383987	139
9363668226e78a1d4cc5cac951be73b6f0ab0bce	a big-data based and process-oriented decision support system for traffic management		Data analysis and monitoring of road networks in terms of reliability and performance are valuable but hard to achieve, especially when the analytical information has to be available to decision makers on time. The gathering and analysis of the observable facts can be used to infer knowledge about traffic congestion over time and gain insights into the roads safety. However, the continuous monitoring of live traffic information produces a vast amount of data that makes it difficult for business intelligence (BI) tools to generate metrics and key performance indicators (KPI) in nearly real-time. In order to overcome these limitations, we propose the application of a big-data based and process-centric approach that integrates with operational traffic information systems to give insights into the road network's efficiency. This paper demonstrates how the adoption of an existent process-oriented DSS solution with big-data support can be leveraged to monitor and analyse live traffic data on an acceptable response time basis.	big data;decision support system;information system;network congestion;observable;real-time clock;response time (technology)	Alejandro Vera Baquero;Ricardo Colomo Palacios	2018	EAI Endorsed Trans. Scalable Information Systems	10.4108/eai.29-5-2018.154810	performance indicator;data mining;computer science;decision support system;information system;response time;big data;business intelligence;continuous monitoring;traffic congestion	OS	-15.24627364622309	-29.960458308101835	140
f5c3037320be2a15c46c97b7a5fdc491ec0bdc69	transform domain technique for windowing the dct and dst	real time;data communication;discrete cosine transform;real time signal processing;edge effect;signal processing;adaptive system;financial market;discrete sine transform;fast transform;windowing;software implementation	In this paper, an algorithm is developed to apply Hann, Hamming, Blackman and related windows directly in the transform domain for the discrete cosine transform and discrete sine transform. These algorithms are useful in applications where windowing is required in order to minimize edge effects caused by implicit symmetries in the transform domain that are not replicated in the real-world data. Examples of such applications include data communication, adaptive system identification and filtering, real-time analysis of financial market data, etc. Software implementations in C language are also given.	discrete cosine transform;discrete sine transform	Barry G. Sherlock;Yogendra P. Kakad	2002	J. Franklin Institute	10.1016/S0016-0032(01)00058-8	discrete hartley transform;constant q transform;transform coding;speech recognition;hartley transform;s transform;harmonic wavelet transform;lapped transform;modified discrete cosine transform;short-time fourier transform;continuous wavelet transform;computer science;edge effects;theoretical computer science;adaptive system;fractional fourier transform;discrete sine transform;discrete fourier transform;signal processing;discrete cosine transform;discrete fourier transform;fast wavelet transform;frequency domain;financial market;computer graphics (images);discrete frequency domain	Logic	57.03500627729193	19.755138722515838	141
8b2ceb0c62cb8cbeddca98e9fcc78f135ad0f8dd	visualization of solid reaction-diffusion systems	visualization chemicals laplace equations automata voting large scale systems graphics biological tissues solid modeling very large scale integration;computer graphics;multiple scalar variables solid reaction diffusion systems visualization large scale features reaction sites cellular automaton based reaction diffusion system computer graphics applications;reaction diffusion;pattern generation;reaction diffusion system;computer graphic;image texture;large scale;cellular automata;cellular automaton;cellular automata computer graphics image texture	In 1952, mathematician Alan Turing introduced reaction diffusion. This mechanism permits large-scale features to emerge in the concentrations of chemicals that react and diffuse within an array of reaction sites, or cells. Recently, we used a cellular automaton-based reaction-diffusion system to generate textures for computer graphics applications. We extended the original binary nature of cellular automata to encompass multiple scalar variables, increasing the range and variety of patterns generated. >		Peter Chambers;Alyn P. Rockwood	1995	IEEE Computer Graphics and Applications	10.1109/38.403820	cellular automaton;image texture;simulation;computer science;theoretical computer science;computer graphics;reaction–diffusion system;algorithm;computer graphics (images)	Visualization	74.01262970028434	-46.12598118423311	142
4dd61a12a7341ccc5fec7d159f90868c7fa54e25	build or merge: locational decisions in mobile access networks		Mergers between mobile network operators involve merging their respective networks. Though this may represent a chance to optimize the network structure, merging may not represent the cost-optimal solution. In this paper, we compare two different evolution paths, where the networks to be merged are separately upgraded to cover the whole traffic demand or a single network is optimized as the result of the merger (Build vs Merge). Our preliminary analyses show that the Merge approach may lead to 50% higher costs, due in particular to the high costs borne to switch off redundant access points. Any Build vs Merge decision should therefore consider the sunk costs due to the inherited networks, as well as the possible benefits associated to a merger.		Maurizio Naldi;Andrea Pacifici;Alessio Tagliacozzo;Gaia Nicosia	2018	2018 UKSim-AMSS 20th International Conference on Computer Modelling and Simulation (UKSim)	10.1109/UKSim.2018.00035	merge (version control);sunk costs;gsm;operator (computer programming);access network;distributed computing;modeling and simulation;cellular network;computer science	Vision	-12.436860242172328	87.67926421491062	143
5644c59aeb4f1a4cb36f55e222b76005015751a9	algebraic rewritings for optimizing regular path queries	use;objet;base relacional dato;automata no determinista;remote access;memoire;complet;optimisation;problem;base donnee;acceso remoto;dato;aplicacion;data integrity;optimizacion;automata estado finito;data;acces a distance;interrogation base donnee;database;interrogacion base datos;base dato;transducers;object;regular language;pregunta documental;query optimization;automaton;relational database;calculo automatico;definicion;probleme;product;integration;non deterministic automaton;utilizacion;computing;informacion;question documentaire;algorithme;calcul automatique;lenguaje racional;memory access;semistructured data;algorithm;utilisation;automata;completo;definition;requete a chemin regulier;partiel;memoria;optimisation requete;donnee;producto;integracion;partial;automate;reecriture;theory;teoria;regular path query;data warehousing;tecnica;langage rationnel;base donnee relationnelle;query;cached views;automate non deterministe;number;produit;regular path queries;optimization;finite automaton;transductor;parcial;problema;automate fini;query answering;rewriting;transducer;nombre;application;transducteur;objeto;query rewriting;database query;technique;information;numero;memory;reescritura;theorie;complete;algoritmo;ideal;donnee semistructuree	Rewriting queries using views is a powerful technique that has applications in query op timization data integration data warehousing etc Query rewriting in relational databases is by now rather well investigated However in the framework of semistructured data the problem of rewriting has received much less attention In this paper we focus on extracting as much information as possible from algebraic rewritings for the purpose of optimizing regular path queries The cases when we can nd a complete exact rewriting of a query using a set a views are very ideal However there is always information available in the views even if this information is only partial We introduce lower and possibility par tial rewritings and provide algorithms for computing them These rewritings are algebraic in their nature i e we use only the algebraic view de nitions for computing the rewritings This fact makes them a main memory product which can be used for reducing secondary memory and remote access We give two algorithms for utilizing the partial lower and partial possibility rewritings in the context of query optimization	algorithm;auxiliary memory;computer data storage;linear algebra;mathematical optimization;query optimization;relational database;rewriting	Gösta Grahne;Alex Thomo	2003	Theor. Comput. Sci.	10.1016/S0304-3975(02)00739-9	transducer;computer science;theoretical computer science;data warehouse;database;mathematics;automaton;finite-state machine;algorithm	DB	-26.9411782332209	5.7027303737713195	144
9cff73cf2d0faff1f98e3148e7219fc9c5746e68	minimum redundancy cut in ontologies for semantic indexing	busqueda informacion;ontologie;semantic indexing;redundancia;information retrieval;semantics;intelligence artificielle;indexing terms;semantica;semantique;redundancy;indexing;recherche information;indexation;indizacion;artificial intelligence;ontologia;inteligencia artificial;ontology;redondance	This paper presents a new method that aims at improving semantic indexing while reducing the number of indexing terms. Indexing terms are determined using a minimum redundancy cut in a hierarchy of conceptual hypernyms provided by an ontology (e.g. WordNet, EDR). The results of some information retrieval experiments carried out on several standard document collections using the EDR ontology are presented, illustrating the benefit of the method.	bluetooth;experiment;information retrieval;ontology (information science);wordnet	Florian Seydoux;Jean-Cédric Chappelier	2005		10.1007/11595014_64	search engine indexing;index term;computer science;artificial intelligence;ontology;data mining;database;semantics;redundancy;information retrieval	Web+IR	-35.338897660072156	-60.57346922222103	145
9501204dd7063ae70d6166cb360b4490cf90eb72	malicious pdf documents explained	software;windows security;software bugs;document handling;computer crime;windows security malware pdf;security researchers;malicious pdf documents;malware authors malicious pdf documents operating system software bugs pdf language security researchers;pdf language;software computer crime malware portable document format;operating system;malware;portable document format;malware authors;pdf;program debugging;security of data;operating systems computers;security of data document handling operating systems computers program debugging	What makes a PDF file malicious? PDF designers and the PDF reader software architects never intended for files to be able to modify the operating system running the PDF reader. But security researchers and malware authors found ways to exploit PDF readers' software bugs and to creatively use the PDF language, enabling them to produce PDF documents that execute arbitrary code. Embedded files are a good example of this design philosophy. The PDF language allows files to be embedded inside PDF documents.PDF reader software designers have begun using Windows security features such as data execution prevention (DEP) and address space layout randomization (ASLR) to prevent exploits from executing.	address space layout randomization;arbitrary code execution;embedded system;executable space protection;exploit (computer security);list of pdf software;malware;microsoft windows;operating system;portable document format;software architect;software bug	Didier Stevens	2011	IEEE Security & Privacy	10.1109/MSP.2011.14	software bug;computer science;database;malware;world wide web;computer security	Security	-57.684600008927575	56.66371139631814	146
4f8b3b43948b881dc669795675cb23a8659939fd	conceptual graph interchange format for mining financial statements	information extraction;text mining;financial performance;qa75 electronic computers computer science;conceptual graph;deviation detection;interchange format;conceptual graph interchange format	This paper addresses the automatic transformation of financial statements into conceptual graph interchange format (CGIF). The method mainly involves extracting relevant financial performance indicators, parsing it to obtain syntactic sentence structure and to generate the CGIF for the extracted text. The required components for the transformation are detailed out with an illustrative example. The paper also discusses the potential manipulation of the resulting CGIF for knowledge discovery and more precisely for deviation detection.	conceptual graph	Siti Sakira Kamaruddin;Abdul Razak Hamdan;Azuraliza Abu Bakar;Fauzias Mat Nor	2009		10.1007/978-3-642-02962-2_73	conceptual graph;knowledge interchange format;text mining;computer science;data science;data mining;database;information extraction	ML	-40.30836122831122	-70.0186140944592	147
3cb784b31d14a24b8c6f1c4d7ee23d73af13a900	electronic conferencing on the internet: the first electronic computational chemistry conference	computational chemistry		computational chemistry;internet	Steven M. Bachrach	1995	Journal of Chemical Information and Computer Sciences	10.1021/ci00025a011	chemistry;computer science;theoretical computer science;multimedia	HPC	-61.01026539561368	-12.46441709512624	148
7da963f3ca42622cb9463162008164574d5cad8f	on the advice complexity of online bipartite matching and online stable marriage	stable marriage;bipartite matching;on line algorithms;advice complexity	In this paper, we study the advice complexity of the online bipartite matching problem and the online stable marriage problem. We show that for both problems, ⌈log2(n!)⌉ bits of advice are necessary and sufficient for a deterministic online algorithm to be optimal, where n denotes the number of vertices in one bipartition in the former problem, and the number of men in the latter.		Shuichi Miyazaki	2014	Inf. Process. Lett.	10.1016/j.ipl.2014.06.013	combinatorics;stable marriage problem;bipartite graph;3-dimensional matching;theoretical computer science;mathematics;stable roommates problem;algorithm	Theory	25.675343948623514	24.771373642734925	149
8f375c3786fe08c88e8202b8410950481fc4aa04	a low phase noise 8.8 ghz vco based on isf manipulation and dual-tank technique		This paper presents a novel 8.8 GHz voltage-controlled oscillator (V CO), which employs a dual-tank structure and the impulse sensitivity function (ISF) manipulation technique. By utilizing these techniques, the behavior of both the tail and the cross-coupled transistors is optimized compared to the conventional VCO; therefore, the noise from the transistors and the low quality of tank is reduced; resulting in better phase noise performance. The circuit is implemented in 130nm BiCMOS technology, and achieves −115.26dBc/Hz @ 1MHz offset with 4 percent frequency tuning range. The power consumption is between 80mW and 100mW from a 2.5V power supply, and the figure of merit (FoM) is 175dBc/Hz.	bicmos;ink serialized format;phase noise;power supply;transistor;voltage-controlled oscillator	Rong Jiang;Hossein Noori;Foster F. Dai;Jun Fu;Wei Zhou;Yudong Wang	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050669	electronic engineering;control theory;phase noise;noise measurement;computer science;figure of merit;voltage-controlled oscillator;bicmos;noise figure;noise spectral density;noise temperature	EDA	61.52522647518946	49.49612008427786	150
beeb60df694457fb55828f2c11466f8bb7298a70	robust ue receiver with interference cancellation in lte advanced heterogeneous network	radio receivers;long term evolution;maximum likelihood estimation;crs ic robust ue receiver lte advanced heterogeneous network hetnet deployment network capacity base stations downlink interference mitigation algorithms almost blank subframe scenario abs scenario non colliding case common reference symbols colliding case serving cell dominant interfering cell robust equalizer low complexity variant space alternating generalized expectation maximization sage map maximum a posteriori criterion crs interference cancellation;interference suppression;channel estimation interference robustness equalizers transmitting antennas receiving antennas;radio receivers expectation maximisation algorithm interference suppression long term evolution maximum likelihood estimation;expectation maximisation algorithm	Heterogeneous network (HetNet) deployment is a key feature to increase the network capacity in LTE- Advanced and systems beyond. One major obstacle is the interference from neighbor base-stations which can significantly degrade the system performance. In this paper, we present downlink interference mitigation and cancellation algorithms for HetNets in the so-called almost blank subframe (ABS) scenario. In the ABS scenario, one must differentiate between two cases, namely (1) the non- colliding case: the common reference symbols (CRS) of the serving and the dominant interfering cell are not overlapped, and (2) the colliding case: the CRS of the serving cell and the dominant interfering cell are overlapped. For case (1) we propose a robust equalizer and a low-complexity variant of it. For case (2) we present the application of the space alternating generalized expectation-maximization (SAGE) with a maximum a-posteriori (MAP) criterion. Based on extensive realistic link-level simulations, the outcome for case (1) is that the proposed robust equalization technique is almost as good as the receiver with traditional CRS interference cancellation (IC) but with lower complexity and latency. For case (2), we find that the SAGE-MAP with three SAGE cycles/iterations is sufficient to achieve the same performance as the receiver with ideal CRS-IC.	compaq lte;complexity;equalization (communications);expectation–maximization algorithm;field electron emission;interference (communication);iteration;scenario testing;simulation;software deployment;telecommunications link	Basuki Endah Priyanto;Shashi Kant;Fredrik Rusek;Sha Hu;Jianjun Chen;Chris Wugengshi	2013	2013 IEEE 78th Vehicular Technology Conference (VTC Fall)	10.1109/VTCFall.2013.6692396	electronic engineering;simulation;telecommunications;engineering;mathematics;maximum likelihood;radio receiver;statistics	Mobile	43.17446751811274	83.35216551773708	151
71cc99c171318e4b25f6810e0dca59b73c3de010	discoverylink: a system for integrated access to life sciences data sources	life sciences	Vast amounts of life sciences data reside today in specialized data sources, with specialized query processing capabilities. Data from one source often must be combined with data from other sources to give users the information they desire. There are database middleware systems that extract data from multiple sources in response to a single query. IBM's DiscoveryLink is one such system, targeted to applications from the life sciences industry. DiscoveryLink provides users with a virtual database to which they can pose arbitrarily complex queries, even though the actual data needed to answer the query may originate from several different sources, and none of those sources, by itself, is capable of answering the query. We describe the DiscoveryLink offering, focusing on two key elements, the wrapper architecture and the query optimizer, and illustrate how it can be used to integrate the access to life sciences data from heterogeneous data sources.		Laura M. Haas;Peter M. Schwarz;Prasad Kodali;Elon Kotlar;Julia E. Rice;William C. Swope	2001	IBM Systems Journal	10.1147/sj.402.0489	web query classification;computer science;data science;data mining;database;view;query language	HPC	-37.06768465556223	2.156576218516	152
7c41609638ad52db924114f2f232aad7fc790632	high speed synchronization of processors using fuzzy barriers	distributed system;systeme reparti;compilateur;multiprocessor;multiprocessor systems;programacion paralela;sistema informatico;parallelizing compilers;parallel programming;computer system;compiler;synchronisation;computer architecture;sistema repartido;architecture ordinateur;synchronization;risc;systeme informatique;arquitectura ordenador;sincronizacion;multiprocesador;parallel programs;materiel informatique;high speed;material informatica;parallel processing;compilador;software implementation;programmation parallele;hardware;multiprocesseur	Parallel programs are commonly written using barriers to synchronize parallel processes. Upon reaching a barrier, a processor must stall until all participating processors reach the barrier. A software implementation of the barrier mechanism using shared variables has two major drawbacks. Firstly, the execution of the barrier may be slow since it requires execution of several instructions. Secondly, processors that are stalled waiting for other processors to reach the barrier cannot do any useful work. In this paper, the notion of thefuzzy barrier is presented, that avoids these drawbacks. The first problem is avoided by implementing the mechanism in hardware. The second problem is solved by extending the barrier concept to include a region of statements that can be executed by a processor while it awaits synchronization. The barrier regions are constructed by a compiler and consist of instructions such that a processor is ready to synchronize upon reaching the first instruction and must synchronize before exiting the region. When synchronization does occur, the processors could be executing at any point in their respective barrier regions. The larger the barrier region, the more likely it is that none of the processors will have to stall. Hardware fuzzy barriers have been implemented as part of a RISC-based multi-processor system. Results based on a software implementation of the fuzzy barrier on the Encore multiprocessor indicate that the synchronization overhead can be greatly reduced using the mechaism.	central processing unit;compiler;multiprocessing;overhead (computing);parallax barrier;shared variables	Rajiv Gupta;Michael Epstein	1990	International Journal of Parallel Programming	10.1007/BF01407864	embedded system;parallel processing;synchronization;parallel computing;real-time computing;memory barrier;computer science;operating system;programming language;algorithm	Arch	-15.723255846270744	45.42630362656053	153
cb0699ceced53071254f960ffec09f29d63639d4	the importance of cognitive fit in mobile information systems	user interface		information system	Andrew Urbaczewski;Matti Koivisto	2008	CAIS		knowledge management;engineering;human–computer interaction;information system;user interface;cognition	HCI	-53.23796535363101	-34.705446529961314	154
009ee183b894a96f9f9a7a94ba31c36e0e2e090f	goal-oriented requirements engineering, part ii	home care;systems analysis database management systems formal specification object oriented programming;formal specification;database management systems;programming databases history software design collaborative work data engineering knowledge engineering knowledge management software engineering fellows;business process design;database conceptual schema;goal oriented requirements engineering;agent oriented software development methodology;object oriented programming;systems analysis;software development;database conceptual schema goal oriented requirements engineering software development agent oriented software development methodology tropos database design techniques;database design techniques;tropos	"""Summary form only given. The last fifteen years have seen the rise of a new phase in software development which is concerned with the acquisition, modelling and analysis of stakeholder purposes (""""goals"""") in order to derive functional and nonfunctional requirements. The history of ideas and research results for this new phase was reviewed in a RE'04 keynote presentation by Axel van Lamsweerde. We revisit the topic and sketch on-going research on a number of fronts. Specifically, we discuss an agent-oriented software development methodology - called Tropos - that is founded on the concepts of goal, actor as well as inter-actor dependencies. We also show how goal models that characterize a space of possible solutions for meeting stakeholder goals can be used as a basis for designing high variability software. In addition, we report on early work to extend database design techniques to support the generation of a database conceptual schema from stakeholder goals. The research reported is the result of collaborations with colleagues at the Universities of Toronto and Trento"""	actor model;conceptual schema;database design;non-functional requirement;requirements engineering;software development process;spatial variability	John Mylopoulos	2006	14th IEEE International Requirements Engineering Conference (RE'06)	10.1109/RE.2006.27	schema migration;systems analysis;requirements analysis;software requirements specification;information engineering;entity–relationship model;computer science;systems engineering;engineering;knowledge management;three schema approach;software design;social software engineering;component-based software engineering;software development;software design description;requirement;software engineering;software construction;formal specification;database;systems development life cycle;programming language;object-oriented programming;resource-oriented architecture;computer-aided software engineering;database schema;business process modeling;software development process;software requirements;software system	DB	-53.96072957734697	24.289342371388756	155
d98d3d3b15a227d3503d3101e8bec88382925f33	fuzzy hypervector spaces	hypervector space;fuzzy hypervector space;level sub-hyperspace;fundamental theorem;fuzzy subset;algebraic nature;fuzzy quotient hypervector space;certain condition;fuzzy cosets;fuzzy vector space	The aim of this paper is the generalization of the notion of fuzzy vector spaces to fuzzy hypervector spaces. In this regard, by considering the notion of fuzzy hypervector spaces, we characterized a fuzzy hypervector space based on its level sub-hyperspace. The algebraic nature of fuzzy hypervector space under transformations is studied. Certain conditions are obtained under which a given fuzzy hypervector space can or cannot be realized as a union of two fuzzy hypervector spaces such that none is contained in the other. The construction of a fuzzy hypervector space generated by a given fuzzy subset of a hypervector space is given. The set of all fuzzy cosets of a fuzzy hypervector space is shown to be a hypervector space. Finally, a fuzzy quotient hypervector space is defined and an analogue of a consequence of the “fundamental theorem of homomorphisms” is obtained.	fuzzy set;linear algebra;spaces	Reza Ameri;Omid Reza Dehghan	2008	Adv. Fuzzy Systems	10.1155/2008/295649	algebraic number;homomorphism;fuzzy logic;discrete mathematics;vector space;coset;quotient;mathematics;fundamental theorem	DB	43.63006892701793	26.003687534751347	156
4dcb58e2be56dfc8df0f868c7591083b03a5c9b2	efficient parallel gpu algorithms for bdd manipulation	graphics processing units binary decision diagrams;boolean functions graphics processing units arrays instruction sets kernel indexes;binary decision diagrams;magnitude speedup parallel gpu algorithms bdd manipulation binary decision diagram manipulation graphics processing units sequential cpu based package;graphics processing units;parallel execution binary decision diagrams bdds boolean satisfiability formal verification graphics processing unit gpu	We present parallel algorithms for Binary Decision Diagram (BDD) manipulation optimized for efficient execution on Graphics Processing Units (GPUs). Compared to a sequential CPU-based BDD package with the same capabilities, our GPU implementation achieves at least 5 orders of magnitude speedup. To the best of our knowledge, this is the first work on using GPUs to accelerate a BDD package.	binary decision diagram;central processing unit;graphics processing unit;parallel algorithm;speedup	Miroslav N. Velev;Ping Gao	2014	2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2014.6742980	computer architecture;parallel computing;computer science;theoretical computer science;general-purpose computing on graphics processing units	EDA	0.7029857807804357	41.12360438888491	157
73d774b8a2b91b41e984822651fd873b2a5ad733	symmetric gobos in a finite projective plane	finite projective plane	A gobo G in any incidence structure K is a (perhaps degenerate) tactical configuration having the property that no three points in G are collinear and no three lines in G are concurrent. General results are obtained where K is a finite projective plane of order n and G has k points and k lines such that each point (line) lies on r lines (points) of C. Particular attention is called to the contrast between the case r = 1 and the case r # 1 when k = n.	incidence matrix	George E. Martin	1971	J. Comb. Theory, Ser. A	10.1016/0097-3165(71)90047-1	projective plane;line at infinity;fano plane;plane curve;projective space;duality;discrete mathematics;finite geometry;topology;affine plane;blocking set;real projective plane;non-desarguesian plane;collineation;mathematics;geometry;complete quadrangle;configuration;perspective;correlation;cross-ratio;real projective line;pencil;line coordinates	Theory	40.61602148718066	31.310417581824638	158
81ecd071b2080ff142390693cac0364b02b8fcad	visualization of implicit surfaces using adaptive tetrahedrizations	implicit surface;computer graphics;automatic logic units;visualization;current measurement;region of interest;visualization computer graphics automatic logic units current measurement	Implicitly defined surfaces f(x,y,z)=0 are usually visualized by an approximating mesh of polygons. One approach of calculating an approximating mesh, practiced in the past, is to subdivide the regions of interests into spatial cells from which the surface is extracted by compiling surface patterns of those cells which are traversed by the surface. Our solution basically follows this approach, but improves it with respect to several aspects. The space is partitioned adaptively using a scheme which neither requires multiple passes nor storing neighborhood information. Furthmore, the cells of the approximating surface mesh are part of the resulting spatial cell decomposition. A post-processing step of ''vertex snapping'' in order to improve the surface mesh is not required.	compiler;video post-processing	Heinrich Müller;Michael Wehle	1997	Scientific Visualization Conference (dagstuhl '97)		computer vision;computer science;theoretical computer science;computer graphics (images)	Visualization	67.81784060427935	-49.30481434189109	159
813b427575169eb72c0236a6c09351ae88f560b0	a new class of control laws for spacecraft attitude tracking using switching and trajectory rescaling	automatic control;control systems;time scale;convergence;quaternion feedback;closed loop systems;nonlinear control systems;strain control;space vehicles attitude control trajectory quaternions space missions automatic control strain control control systems feedback loop convergence;convergence rate;quaternion feedback spacecraft attitude control tracking trajectory rescaling model based control nonlinear control systems gain switching strategy closed loop systems;attitude control;feedback;trajectory;feedback loop;model based control;controller design;space missions;trajectory tracking;gain switching strategy;exponential convergence;spacecraft;trajectory rescaling;tracking;quaternions;space vehicles;closed loop systems space vehicles attitude control tracking nonlinear control systems feedback	"""We present some new results in controller design for spacecraft attitude trajectory tracking. A model-based nonlinear controller and a controller gain switching strategy are presented. The nonlinear controller together with the switching strategy result in exponential convergence and also provide a near method to overcome problems associated with closed loop behavior near an unstable equilibrium that are inherent to controllers with quaternion feedback. Bounds of convergence rates and """"turn around time"""" to the stable equilibrium are explicitly calculated. A """"time-scaling"""" approach for partial reference trajectory redesign is then presented to achieve tracking when constraints on the magnitude of the available input torques are present."""		Pablo Arambel;Vikram Manikonda	2000		10.1109/ROBOT.2000.845360	control engineering;simulation;convergence;control system;trajectory;space exploration;automatic control;spacecraft;feedback loop;control theory;feedback;mathematics;attitude control;tracking;rate of convergence;quaternion	Robotics	65.48159672709858	-12.875052487468754	160
059cb9f390fbe49b3c299e90c65b1b891c620386	the modular snake robot open project: turning animal functions into engineering tools	manipulators;mobile robots;research and development;research and development manipulators mobile robots;robot sensing systems actuators software robot kinematics animals;bioinspired gaze animal functions engineering tools km robota modular snake robot open project research and development activities colombian institution bioinspired project animal physiology recategorization loco manipulation controllers simulation tools robot architecture software hardware modular snake robot research locomotion	This paper introduces the KM-RoBoTa's Modular Snake Robot Open Project, as a part of the research and development activities of this Colombian institution. The contents of the paper are divided in two main parts. First, the aim of this bio-inspired project is presented, which is built upon the relationships between animal physiology and engineering tools. Second, the extension and re-categorization of the entire project using different research and development achievements of our institution, is presented. This includes developments of hardware, which can be summarized as the description of the machine, the sensors and the processing unit. Developments in software as loco-manipulation controllers, simulation tools and a robot architecture implementation to integrate either software and hardware. This paper is an insight of the outlooks in modular snake robot research by our institution, particularly modeling locomotion and implementing or improving perception and task capabilities under the bio-inspired gaze.	british informatics olympiad;categorization;loco linux;procurement;robot;sensor;simulation	Kamilo Melo;Juan León;Alvaro di Zeo;Vanesa Rueda;Diego Roa;Manuel Parraga;Daniel Gonzalez;Laura Paez	2013	2013 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2013.6719368	mobile robot;simulation;engineering;artificial intelligence;social robot;robot control	Robotics	65.61166600913468	-28.267348641559995	161
9e3774c6039559a7fa53b8eabecca290c748fd83	integration of shareable containers with distributed hash tables for storage of structured and dynamic data	peer to peer network shareable container distributed hash table spatial temporal data data structure storage network space based computing container;telematics;dis tributed hash table;space based computing;peer to peer network;distributed hash table;routing;intelligent transportation systems;storage management;temporal data;distributed computing;shareable container;p2p;storage network;sensor network;storage management data structures peer to peer computing;computer architecture;telecommunication traffic;dynamic data;mobile service;containers peer to peer computing space technology roads computer architecture telecommunication traffic routing intelligent transportation systems intelligent structures distributed computing;space based computing container;roads;data structures;ip networks;space technology;peer to peer computing;intelligent structures;dht;data structure;spatial temporal data;containers;distributed architecture;p2p dht space based computing telematics	Structured, spatial-temporal data arises in many applications areas such as transportation, sensor networks or mobile services. Peer to peer networks are the natural choice for the distributed architecture required by these applications. However, a closer analysis of the available distributed hash table (DHT) based approaches shows their inefficiency since the data structure gets lost and the short liveness of the data leads to a high signalling traffic. In this work we propose a novel storage network based on an overlay of Space-based Computing Containers for storing, accessing and manipulating dynamic data.	data structure;distributed computing;distributed hash table;dynamic data;liveness;peer-to-peer;storage area network	eva Kühn;Richard Mordinyi;Hannu-Daniel Goiss;Thomas Moser;Sandford Bessler;Slobodanka Dana Kathrin Tomic	2009	2009 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2009.53	computer science;database;distributed computing;computer network	DB	-34.54082305048457	48.45527166114006	162
b4c0f69e245de9eb334138183df31c407ce82053	automatic bio-mems platforms for fast disease diagnosis	genetic analysis;chip;electrokinetics;stimulated emission;microfluidics;enzyme;infectious disease;dna;rna;molecular biology;molecular biophysics;nucleic acid;genetics;microfluidic chip	The paper reports two platforms for fast diagnosis of infectious diseases using enabling Bio-MEMS (Bio-Micro-Electro-Mechanical-Systems) technology. The first platform uses molecular biology techniques for DNA-based diagnosis. An integrated microfluidic chip capable of performing DNA/RNA amplification, electrokinetic sample injection and separation, and on-line optical detection of nucleic acid products is developed. To demonstrate the functionality of this platform, RNA-based detection for Dengue 2 virus has been preformed in an automatic format with less sample and reagent volumes in a shorter period thanks to lower thermal inertia of the micro systems. The second platform uses ELISA (Enzyme-linked immunosorbent assay) techniques for disease detection by employing an enzyme label for detection of antibody-antigen complexes. An automatic disease diagnosis system could be realized by integrating micropumps and microvalves. HCY (hepatitis C virus) and syphilis serum samples are successfully tested on the developed chips using ELISA procedure. These two Bio-MEMS platforms could be crucial for molecular biology, genetic analysis, infectious disease detection, and other biomedical applications. KeywordsMicrofluidics; peR; Electrophoresis; Micropump; DNA; RNA; ELISA	british informatics olympiad;microelectromechanical systems;online and offline	Fu-Chun Huang;Chia-Sheng Liao;Chih-Hao Wang;Gwo-Bin Lee	2005	2005 IEEE International Conference on Robotics and Biomimetics - ROBIO	10.1109/ROBIO.2005.246385	chip;electrokinetic phenomena;nucleic acid;enzyme;microfluidics;rna;lab-on-a-chip;infectious disease;nanotechnology;stimulated emission;genetic analysis;dna;molecular biophysics	Robotics	95.2556410949688	-18.896618223417054	163
251fd23d19cd751dcd87824d2967f25609458b8e	decision procedure for entailment of symbolic heaps with arrays		This paper gives a decision procedure for the validity of entailment of symbolic heaps in separation logic with Presburger arithmetic and arrays. The correctness of the decision procedure is proved under the condition that sizes of arrays in the succedent are not existentially bound. This condition is independent of the condition proposed by the CADE-2017 paper by Brotherston et al, namely, one of them does not imply the other. For improving efficiency of the decision procedure, some techniques are also presented. The main idea of the decision procedure is a novel translation of an entailment of symbolic heaps into a formula in Presburger arithmetic, and to combine it with an external SMT solver. This paper also gives experimental results by an implementation, which shows that the decision procedure works efficiently enough to use.	algorithm;code;computer aided verification;correctness (computer science);decision problem;esop;experiment;formal verification;frame language;identifier;immutable object;lecture notes in computer science;linked list;presburger arithmetic;satisfiability modulo theories;separation logic;simultaneous multithreading;solver;symposium on logic in computer science;symposium on principles of programming languages;tree automaton	Daisuke Kimura;Makoto Tatsuta	2017		10.1007/978-3-319-71237-6_9	theoretical computer science;mathematics;discrete mathematics;presburger arithmetic;algorithm;correctness;satisfiability modulo theories;separation logic;logical consequence;heap (data structure)	Logic	-16.364905330215297	23.129971285191008	164
8e8ddf566209cb242c45d8d553632e0e084b90b4	speaker diarization using direction of arrival estimate and acoustic feature information: the i2r-ntu submission for the nist rt 2007 evaluation	speaker diarization;direction of arrival	This paper describes the IR/NTU system submitted for the NIST Rich Transcription 2007 (RT-07) Meeting Recognition evaluation Multiple Distant Microphone (MDM) task. In our system, speaker turn detection and clustering is done using Direction of Arrival (DOA) information. Purification of the resultant speaker clusters is then done by performing GMM modeling on acoustic features. As a final step, nonspeech & silence removal is done. Our system achieved a competitive overall DER of 15.32% for the NIST Rich Transcription 2007 evaluation task.	acoustic cryptanalysis;cluster analysis;direction of arrival;google map maker;master data management;medical transcription;microphone;nist hash function competition;network interface device;pc speaker;purification of quantum state;resultant;speaker diarisation;windows rt	Chin-Wei Eugene Koh;Hanwu Sun;Tin Lay Nwe;Trung Hieu Nguyen;Bin Ma;Chng Eng Siong;Haizhou Li;Susanto Rahardja	2007		10.1007/978-3-540-68585-2_45	speech recognition;acoustics;engineering;communication	NLP	-14.731104997698464	-88.0415332318144	165
4f1587aee51cc94afb0acf1d72ac1bf8604c57db	the design and evaluation of a generic method for generating mosaicked multispectral filter arrays	exact registration;sensitivity and specificity;evaluation performance;computer aided design;design principle;sensor arrays digital filters digital cameras multispectral imaging optical filters optical imaging costs robustness optical arrays photoreceptors;image segmentation;performance evaluation;image processing;diminution cout;generic algorithm;spectrum analysis;optical filters;evaluacion prestacion;procesamiento imagen;digital camera;color filter arrays;photography;demosaicking algorithms;equipment failure analysis;color filter array cfa;traitement image;consistency coefficient;digital camera industry;generic msfa generation method;signal processing computer assisted;full color image;equipment design;reconstruction image;color filter;image enhancement;spectral consistency;general methods;optical arrays;image interpretation computer assisted;estudio caso;reconstruccion imagen;arbol binario;multispectral filter array msfa;spatial uniformity;image color analysis;image colour analysis;image reconstruction;robustesse;color filter array;arbre binaire;reproducibility of results;colorimetry;etude cas;filtro colorado;filtre colore;algorithms;optical filters image colour analysis image segmentation optical arrays;robustness;spectral band;hexagonal grid;consistency coefficient mosaicked multispectral filter array generation color filter arrays digital camera industry exact registration representative spectral bands full color image demosaicking algorithms tessellation mechanisms generic msfa generation method checkerboard pattern spatial uniformity spectral consistency static coefficient;demosaicking;banda espectral;bande spectrale;multispectral filter array msfa binary tree checkerboard pattern color filter array cfa demosaicking hexagonal grid mosaicking;reduccion costes;algoritmo optimo;algorithme optimal;imagen color;optimal algorithm;mosaicked multispectral filter array generation;representative spectral bands;information storage and retrieval	The technology of color filter arrays (CFA) has been widely used in the digital camera industry since it provides several advantages like low cost, exact registration, and strong robustness. The same motivations also drive the design of multispectral filter arrays (MSFA), in which more than three spectral bands are used. Although considerable research has been reported to optimally reconstruct the full-color image using various demosaicking algorithms, studies on the intrinsic properties of these filter arrays as well as the underlying design principles have been very limited. Given a set of representative spectral bands, the design of an MSFA involves two issues: the selection of tessellation mechanisms and the arrangement/layout of different spectral bands. We develop a generic MSFA generation method starting from a checkerboard pattern. We show, through case studies, that most of the CFAs currently used by the industry can be derived as special cases of MSFAs generated using the generic algorithm. The performance of different MSFAs are evaluated based on their intrinsic properties, namely, the spatial uniformity and the spectral consistency. We design two metrics, static coefficient and consistency coefficient, to measure these two parameters, respectively. The experimental results demonstrate that the generic algorithm can generate optimal or near-optimal MSFAs in both the rectangular and the hexagonal domains	algorithm;bands;chroma subsampling;circuit complexity;coefficient;color image;demosaicing;design pattern;digital camera;focal (programming language);generic drugs;generic programming;greater than;hexagonal sampling;image quality;image stitching;motivation;multispectral image;requirement;sampling (signal processing);sampling - surgical action;staring array;tessellation (computer graphics);umbrella filters	Lidan Miao;Hairong Qi	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.877315	iterative reconstruction;computer vision;hexagonal tiling;color filter array;spectral bands;spectrum analyzer;genetic algorithm;color image;colorimetry;binary tree;image processing;computer science;photography;optical filter;mathematics;image segmentation;robustness;computer graphics (images)	Visualization	54.018830195110745	-62.802440807939846	166
46166672032bd120f90117d28a55bf5d54b9e89e	a new bee colony optimization algorithm with idle-time-based filtering scheme for open shop-scheduling problems	bee colony optimization;artificial intelligent;scheduling algorithm;scheduling problem;open shop scheduling;profitability;optimal algorithm;problem solving	Open shop scheduling problems (OSSP) are one of the most time-consuming works in scheduling problems. Currently, many artificial intelligence algorithms can reduce the problem-solving time to an acceptable time range, and even can further downsize the range of solution space. Although the range of solution space is technically downsized, in most scheduling algorithms every partial solution still needs to be completely solved before this solution can be evaluated. For example, if there is a schedule with 100 operations, then all 100 operations must be scheduled before the scheduler can evaluate its fitness. Therefore, the time–cost of unnecessary partial solutions is no longer saved. In order to improve the weakness stated above, this paper proposes a new bee colony optimization algorithm, with an idle-time-based filtering scheme, according to the inference of ‘‘the smaller the idle-time, the smaller the partial solution’’, and ‘‘the smaller the makespan (Cmax) will be’’. It can automatically stop searching a partial solution with insufficient profitability, while the scheduler is creating a new scheduling solution, and therefore, save time–cost for the remaining partial solution. The architecture and details of the bee colony optimization heuristic rule is detailed in this paper. 2010 Elsevier Ltd. All rights reserved.	artificial intelligence;bees algorithm;feasible region;fitness function;heuristic;makespan;mathematical optimization;open-shop scheduling;problem solving;scheduling (computing)	Yueh-Min Huang;Jin-Chen Lin	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.10.010	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;job shop scheduling;open-shop scheduling;mathematical optimization;simulation;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;two-level scheduling;scheduling;profitability index	AI	20.550694660337463	-1.2536600739243828	167
8da1be5e589c55e173ecff6d1a477cbc589aae7d	a geodesic framework for analyzing molecular similarities	conformational analysis;nonlinear mapping;drug design;side effect;multidimensional scaling;self organization;connected component	A fast self-organizing algorithm for extracting the minimum number of independent variables that can fully describe a set of observations was recently described (Agrafiotis, D. K.; Xu, H. Proc. Natl. Acad. Sci.U.S.A. 2002, 99, 15869-15872). The method, called stochastic proximity embedding (SPE), attempts to generate low-dimensional Euclidean maps that best preserve the similarities between a set of related objects. Unlike conventional multidimensional scaling (MDS) and nonlinear mapping (NLM), SPE preserves only local relationships and, by doing so, reveals the intrinsic dimensionality and metric structure of the data. Its success depends critically on the choice of the neighborhood radius, which should be consistent with the local curvature of the underlying manifold. Here, we describe a procedure for determining that radius by examining the tradeoff between the stress function and the number of connected components in the neighborhood graph and show that it can be used to produce meaningful maps in any embedding dimension. The power of the algorithm is illustrated in two major areas of computational drug design: conformational analysis and diversity profiling of large chemical libraries.	algorithm;chemical library;computation;connected component (graph theory);drug design;graph - visual representation;image scaling;library (computing);map;multidimensional scaling;netware loadable module;ninety nine;nonlinear system;organizing (structure);paraffin embedding;self-organization;small molecule libraries;activated t cell autonomous cell death;manifold	Dimitris K. Agrafiotis;Huafeng Xu	2003	Journal of chemical information and computer sciences	10.1021/ci025631m	combinatorics;discrete mathematics;self-organization;connected component;topology;multidimensional scaling;computer science;machine learning;mathematics;side effect;drug design;statistics	ML	3.9978635184076676	-57.39343607173694	168
9572335a8b11671f5d3b253f7e080aaf804bc291	design and implementation of fault current limiters in distribution system using internet of things	distribution system;solid state fault current limiter;hybrid fault current limiter;ieee 6-bus system;internet of things	One of the essential execution of a wireless sensor network is observing hardware. Wireless Sensor Network are capable for taken a toll effective checking over gigantic geo location. Development of keen network depends on the Internet of Things (IoT). Nowadays the fault current levels are increased because of the increasing power demand. Fault Current Limiter(FCL) is used as a protective device to reduce the fault current in distribution system. This paper presents Solid State Fault Current Limiter (SSFCL) and Hybrid Fault Current Limiter (HFCL) to limit the fault current levels for different fault conditions. Also this paper presents the comparison between these two devices with fault current limiting ratio. The performance of the two devices is carried over in IEEE 6-bus system. The simulation and experimental results validate the performance of these two devices to reduce the fault current. The proposed model is executed with SSFCL and HFCL for 60 W three-phase load under various fault conditions. All the fault conditions are observed and controlled through IoT cloud framework.	internet of things	S. Gomathi;T. Venkatesan;D. Sri Vidhya	2018	Wireless Personal Communications	10.1007/s11277-018-5281-9	computer science;computer network;real-time computing;limiter;fault current limiter;wireless sensor network;cloud computing;geolocation;internet of things;limiting;fault (power engineering)	Mobile	2.720714644820123	29.521292596765846	169
5631ffe509181d2627479374b5b4933b39e048d2	temporalising tractable description logics	history;el temporalisation;el temporalisation temporal language temporal description logic constant binary relations dl lite decidability;logic design;temporal logic;dl lite;constant binary relations;automatic logic units;automatic logic units ontologies computer science educational institutions computational complexity history logic design biomedical informatics;first order;temporal description logic;temporal logic decidability;computational complexity;binary relation;ontologies;computer science;description logic;temporal language;biomedical informatics;decidability	It is known that for temporal languages, such as first-order LTL, reasoning about constant (time-independent) relations is almost always undecidable. This applies to temporal description logics as well: constant binary relations together with general concept subsumptions in combinations of LTL and the basic description logic ALC cause undecidability. In this paper, we explore temporal extensions of two recently introduced families of 'weak' description logics known as DL-Lite and EL. Our results are twofold: temporalisations of even rather expressive variants of DL-Lite turn out to be decidable, while the temporalisation of EL with general concept subsumptions and constant relations is undecidable.	adobe flash lite;decision problem;description logic;display resolution;encode;first-order predicate;fully qualified name;logical connective;temporal logic;unary operation;undecidable problem	Alessandro Artale;Roman Kontchakov;Carsten Lutz;Frank Wolter;Michael Zakharyaschev	2007	14th International Symposium on Temporal Representation and Reasoning (TIME'07)	10.1109/TIME.2007.62	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-14.23684157705618	14.026121360323913	170
6ec09ec28e3519975f2868bc5191b84342e18870	grammatical error correction as multiclass classification with single model		This paper describes our system in the shared task of CoNLL-2013. We illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling. Our system achieves the F1 score of 17.13% on the standard test set.	error detection and correction;f1 score;multiclass classification;test set	Zhongye Jia;Peilu Wang;Hai Zhao	2013			speech recognition;computer science;machine learning;multiclass classification;pattern recognition;bayes error rate	NLP	-23.515584588864936	-76.22407588116798	171
4521a95b7b3a01b3b3bff9b33a348be1b7d6b111	a performance comparison of orthogonal code division multiple-access techniques for mobile satellite communications	multipath fading;satellite communication;fading;multiaccess communication satellite communication robustness interference bandwidth time division multiple access frequency division multiaccess europe performance analysis fading;mobile radiocommunication;performance evaluation;mobile satellite communication;estudio comparativo;power efficiency;performance comparison;telecomunicacion via satelite;evanouissement;telecommunication par satellite;fading radio;radiocommunication service mobile;time domain system simulation mobile satellite communications orthogonal code division multiple access performance comparison cdma mobile terrestrial communication systems low power flux density emission interference power efficiency cochannel self noise voice activation frequency reuse europe usa frequency selective multipath fading satellite transponder nonlinearity;etude comparative;transponders mobile satellite communication land mobile radio code division multiple access performance evaluation multipath channels fading;code division multiple access;low power;acces multiple code;land mobile radio;communication channels information theory;canal transmission;analyse performance;comparative study;performance analysis;time domain;multipath channels;transponders;radiocomunicacion servicio movil;satellite telecommunication;acceso multiple codificado;system simulation;analisis eficacia	In recent years, code division multiple-access (CDMA) techniques have received a great deal of attention for mobile terrestrial/satellite communication systems. Primarily considered for the noteworthy features of low power flux density emission and robustness to interference and multipath, CDMA is known to bear reduced bandwidth and power efficiency when compared to traditional TDMA and FDMA due to the intrinsic cochannel self-noise. Early attempts to increase the capacity of CDMA-based systems for commercial applications relied on voice activation and frequency reuse. More recently, practical solutions to implement (synchronous) orthogonal CDMA signaling are being developed independently in Europe and in the USA. This paper is focused on the comparative performance analysis of those two orthogonal CDMA schemes in the operating renditions of a mobile satellite communications system. In particular, the two CDMA systems are compared in the presence of that and frequency-selective multipath fading and a typical satellite transponder nonlinearity. Most numerical results are derived through a time-domain system simulation that confirms and integrates the theoretical findings. >	communications satellite	Riccardo De Gaudenzi;Tobias Garde;Filippo Giannetti;Marco Luise	1995	IEEE Journal on Selected Areas in Communications	10.1109/49.345877	multipath propagation;code division multiple access;electrical efficiency;telecommunications;time domain;computer science;comparative research;transponder;cdma spectral efficiency;near-far problem;fading;communications satellite;computer network	Mobile	29.923221272983525	82.16794595375403	172
a394877cb187f16fe21f4e56fb290752e3d6b3a5	towards a practical specification language	software metrics;software testing;formal specification;software quality assurance;software defects;software research;software systems;software configuration management;software engineering;specification language;software requirements;software practice;software development;software life cycle;software reliability	Recognition of the value of formal specifications in the design and verification of large software systems is becoming more widespread. Specification languages themselves, however, are difficult to develop in part because of the inherent conflict between the goals of clarity and formalism required by these languages. This paper discusses the role of specification languages, examples of specifications in two currently implemented languages, AFFIRM and SPECIAL, and makes some suggestions towards a more practical specification language.	formal specification;semantics (computer science);software system;specification language	Anne-Marie G. Discepolo	1981		10.1145/800175.809858	personal software process;software requirements specification;verification and validation;software sizing;software verification;computer science;systems engineering;package development process;software design;social software engineering;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;programming language;software measurement;software deployment;software requirements;software quality;software quality analyst;software system;software peer review	SE	-58.21808277614718	26.91945517392794	173
4a370f7e33275083d45bfce9b3b15be4fa460288	hybrid misclassification minimization	global solution;weighted averaging;satisfiability;fast algorithm;cross validation;np complete problem	Given two finite point setsA andB in then-dimensional real spaceRn, we consider the NP-complete problem of minimizing the number of misclassified points by a plane attempting to divideRn into two halfspaces such that each open halfspace contains points mostly ofA orB. This problem is equivalent to determining a plane {x | xTw=γ} that maximizes the number of pointsx ∈A satisfying inxTw>γ, plus the number of pointsx ∈B satisfyingxTw<γ. A simple but fast algorithm is proposed that alternates between (i) minimizing the number of misclassified points by translation of the separating plane, and (ii) a rotation of the plane so that it minimizes a weighted average sum of the distances of the misclassified points to the separating plane. Existence of a global solution to an underlying hybrid minimization problem is established. Computational comparison with a parametric approach to solve the NP-complete problem indicates that our approach is considerably faster and appears to generalize better as determined by tenfold cross-validation.		Chunhui Chen;Olvi L. Mangasarian	1996	Adv. Comput. Math.	10.1007/BF02124738	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;np-complete;machine learning;mathematics;geometry;algorithm;cross-validation;statistics;satisfiability	Robotics	26.801327792958105	14.406610480167357	174
2df162565d5004ab26f16b22a4fb28645bbb69e6	automatically produced algorithms for the generalized minimum spanning tree problem		The generalized minimum spanning tree problem consists of finding a minimum cost spanning tree in an undirected graph for which the vertices are divided into clusters. Such spanning tree includes only one vertex from each cluster. Despite the diverse practical applications for this problem, the NP-hardness continues to be a computational challenge. Good quality solutions for some instances of the problem have been found by combining specific heuristics or by including them within a metaheuristic. However studied combinations correspond to a subset of all possible combinations. In this study a technique based on a genotype-phenotype genetic algorithm to automatically construct new algorithms for the problem, which contain combinations of heuristics, is presented. The produced algorithms are competitive in terms of the quality of the solution obtained. This emerges from the comparison of the performance with problem-specific heuristics and with metaheuristic approaches.	algorithm;file spanning;minimum spanning tree;steiner tree problem	Carlos Contreras Bolton;Carlos Rey Barra;Sergio Ramos-Cossio;Claudio Rodríguez;Felipe Gatica;Víctor Parada	2016	Scientific Programming	10.1155/2016/1682925	euclidean minimum spanning tree;mathematical optimization;kruskal's algorithm;minimum degree spanning tree;spanning tree;prim's algorithm;minimum spanning tree;machine learning;gomory–hu tree;connected dominating set;k-minimum spanning tree;reverse-delete algorithm;minimum spanning tree-based segmentation;distributed minimum spanning tree	Theory	24.879277133481107	5.548544681462576	175
00f5f0612aea7d59245bf79d654e565a941e54fd	framework for educational software quality assurance in lithuania	evaluation of educational software;quality assurance;quality of educational software;educational software;educational content	In 2003 a working group established on the initiative of the Centre of Information Technology of Education under the Ministry of Education and Science, has been reviewing regulations for the assessment and certification of educational software. This article analizes the main aspects of the proposed orders. It discusses general structure of quality assurance system, the procedures for the certification of standard and permanently renewable digital educational resources, and the main criteria and principles of the evaluation of educational software in Lithuania. In parallel, it investigates and summarizes research results and foreign policies and practices on quality assurance. Finally, the article compares them with Lithuanian regulations.	electronic commerce regulations 2002;software quality assurance	Lina Markauskaite	2004	Informatics in Education		engineering management;systems engineering;engineering;software engineering;educational evaluation;software quality control	SE	-78.14213649898569	-28.57956242866951	176
92045bce7cd6242f2a31a3203213f5096f46b500	learning binary hash codes for large-scale image search		Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy. This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinitypreserving binary codes. Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures. We focus on defining the algorithms, and illustrate the main points with results using millions of images. Kristen Grauman Dept. of Computer Science, University of Texas, Austin e-mail: grauman@cs.utexas.edu Rob Fergus Courant Institute, New York University e-mail: fergus@cs.nyu.edu	algorithm;artificial neural network;binary code;boosting (machine learning);computer science;courant–friedrichs–lewy condition;data structure;email;hash function;hash table;image retrieval;information;kernel method;machine learning;outline of object recognition;random projection;scalability;similarity measure;spectral density estimation;spectral method;supervised learning;time complexity;unsupervised learning;window function	Kristen Grauman;Rob Fergus	2013		10.1007/978-3-642-28661-2_3	computer science;theoretical computer science;machine learning;data mining	ML	18.117444460103513	-46.61199071384428	177
a1f809d8866c268560e7992787bf38a4f1fed5eb	icpram 2015 - proceedings of the international conference on pattern recognition applications and methods, volume 2, lisbon, portugal, 10-12 january, 2015			icpram;pattern recognition		2015				Robotics	-51.13246457469573	-8.102892011281602	178
557d3ed5eaad43e97a96b69f6fdda5e263a9e5fc	systematically manipulating t-cell signaling dynamics via multiple model informed open-loop controller design	predictive models adaptation models mathematical model computational modeling data models training data in vitro;predictive control;open loop systems;mathematical analysis;control system synthesis;predictive control control system synthesis mathematical analysis open loop systems;controller design t cell signaling dynamics multiple model approach open loop control t cell signaling pathways mathematical models model predictive control	A multiple-model approach to open-loop control of T-cell signaling pathways is presented. Mathematical models of the T-cell signaling pathway are used to inform the controller design. The proposed framework employs a model predictive control strategy to reduce the computational complexity of the open loop control problem. Predictions from each model are weighted using adaptive Akaike weights that are iteratively computed for each controller update step based upon the most relevant training data subsets. This process accounts for the fact that models differ in their ability to accurately reflect the system dynamics under different experimental conditions. The algorithm is evaluated in silico and simulations demonstrate how the model weighting strategy more effectively manages the inaccuracies of any single model. Furthermore, the multiple-model control strategy is evaluated in vitro to direct T-cell signaling. The controller-derived input sequence successfully drives the relative concentration of phosphorylated Erk along the desired trajectory when implemented in the laboratory.	akaike information criterion;algorithm;cell signaling;computational complexity theory;control theory;gene regulatory network;mathematical model;simulation;system dynamics	Jeffrey P. Perley;Judith Mikolajczak;Vu C. Dinh;Marietta L. Harrison;Gregery T. Buzzard;Ann E. Rundell	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426023	control engineering;open-loop controller;simulation;computer science;engineering;control theory;mathematics;model predictive control	Robotics	55.759481817847664	-7.821672178802989	179
74675ff1286398ded9b1075b2f2fd9a5e95524ac	understanding knowledge outcome improvement at the post-adoption stage in a virtual community		Purpose#R##N##R##N##R##N##R##N##R##N#The purpose of this paper is to solve the challenges in knowledge outcome (e.g. knowledge contribution, knowledge exploration) improvement at the post-adoption phase in the context of e-communities. This study develops a model by integrating dedication-constraint framework and self-presentation theory. The model proposes that knowledge outcomes at the post-adoption phase rely on relationship development between community members, conceptualized as commitment. The authors also hypothesize that members’ perceived online self-presentation quality, theorized as personal control and social influence, serves as the key means to motivate members’ commitment.#R##N##R##N##R##N##R##N##R##N#Design/methodology/approach#R##N##R##N##R##N##R##N##R##N#This study used survey instrument to collect data and adopted partial least squares to test the proposed hypotheses.#R##N##R##N##R##N##R##N##R##N#Findings#R##N##R##N##R##N##R##N##R##N#The results show that perceived online self-presentation quality positively affects relationship development, which in turn affects continuance intention for knowledge outcomes.#R##N##R##N##R##N##R##N##R##N#Research limitations/implications#R##N##R##N##R##N##R##N##R##N#This study expands the dedication-constraint framework by integrating the self-presentation theory. This study contributes new knowledge by proposing a model that delineates the relationship between online self-presentation quality, relationship development, and knowledge outcomes at the post-adoption stage.#R##N##R##N##R##N##R##N##R##N#Practical implications#R##N##R##N##R##N##R##N##R##N#This study shows that members’ perceived online self-presentation quality affects both affective commitment and calculative commitment, which in turn affect knowledge outcomes, suggesting the important role of the perceived quality in stimulating a member’s post-adoption reactions.#R##N##R##N##R##N##R##N##R##N#Originality/value#R##N##R##N##R##N##R##N##R##N#This study contributes to the research on post-adoption behavior in an e-community context by accounting for the influence of e-community features in self-presentation quality and dedication-constraint mechanisms on post-adoption phenomena.	virtual community	Shih-Wei Chou;I-Hua Hung	2016	IT & People	10.1108/ITP-05-2015-0121	knowledge management;management science;management;social psychology	HCI	-85.33677888575924	-0.7889004561336866	180
d9ff72fdf59a8dd791fc29db901be20de50ed6be	robustness to adversarial examples through an ensemble of specialists		Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications. However, CNNs have a high risk of being broken by adversarial examples, by fooling them consistently with the addition of small perturbations to natural images, undetectable by the human eyes.	computer vision;convolutional neural network;perturbation theory;real life	Mahdieh Abbasi;Christian Gagné	2017	CoRR		artificial intelligence;machine learning;data mining	AI	19.50438922999517	-51.67426046354092	181
8a8cc8e738f98d0d142c6e157dcd5356f077e5c9	bearings-only target motion analysis by estimation of densities	bearings only target motion analysis btma entropy density filtering approximation hmm hidden markov model extended kalman filtering imm interacting multiple model tracking performance observer density estimation;sensors;kalman filters;observers;target tracking direction of arrival estimation entropy hidden markov models kalman filters nonlinear filters observers;hidden markov models;hidden markov models mathematical model kalman filters sensors observers;mathematical model	The aim of this paper is to compare, in the domain of the Bearings-only Target Motion Analysis (BTMA) with two observers, three approaches in terms of tracking performances : the Interacting Multiple Models (IMM) based on the Extended Kalman Filtering, the Hidden Markov Models (HMM) and the approximated densities filtering based on the maximum of entropy.	approximation algorithm;hidden markov model;kalman filter;markov chain;performance;target motion analysis	Marc Spigai;Jean-Francois Grandin	1998	9th European Signal Processing Conference (EUSIPCO 1998)		maximum-entropy markov model;pattern recognition;control theory;mathematics;markov model;hidden markov model;statistics	Vision	55.75785229795734	5.382661381809182	182
b8360b07c3f93f745a28ef1a5b36a58e231d6ba6	a silicon implementation of the fly's optomotor control system	observability;systeme commande;sistema control;system structure;dispositivo optomecanico;otomotor response;charge coupled device;optomotor control system;controlabilidad;observabilidad;real time;controllability;adaptive control;systeme silicium;dispositif optomecanique;observabilite;systeme commande optomoteur;visual motion;dispositif ccd;chip;stochastic system;identificacion sistema;controlabilite;dispositivo transferencia carga;control system;optoelectronic device;low power;system identification;optomechanical device;control adaptativo;commande stochastique;structure systeme;silicon system;commande adaptative;reponse optomotrice;biological systems;stochastic control;control estocastico;dispositif optoelectronique;sistema estocastico;identification systeme;dispositivo optoelectronico;systeme stochastique;estructura sistema	Flies are capable of stabilizing their body during free flight by using visual motion information to estimate self-rotation. We have built a hardware model of this optomotor control system in a standard CMOS VLSI process. The result is a small, low-power chip that receives input directly from the real world through on-board photoreceptors and generates motor commands in real time. The chip was tested under closed-loop conditions typically used for insect studies. The silicon system exhibited stable control sufficiently analogous to the biological system to allow for quantitative comparisons.	biological system;cdisc adas-cog - commands summary score;cmos;control system;diptera;low-power broadcasting;on-board data handling;photoreceptors;silicon;very-large-scale integration	Reid R. Harrison;Christof Koch	2000	Neural Computation	10.1162/089976600300014944	chip;observability;controllability;stochastic control;adaptive control;system identification;control system;control theory;mathematics;charge-coupled device	Robotics	73.79493571759654	-6.830277950804251	183
6eacde67615003abdc04a19a06f74c53ade17056	location-correcting codes	optimal codes;sphere packing;channel coding;error correction codes;decoding;decoding error correction codes error correction redundancy upper bound linear code computer errors information theory computer science materials science and technology;upper bounds;galois fields error correction codes;decoding location correcting codes linear codes galois field channel errors correction erasure correction singleton type bound redundancy code length sphere packing lower bound upper bound optimal code constructions;linear codes;indexing terms;code length;upper bound;materials science and technology;error correcting codes;redundancy;erasure correction;error correction code;error correction;linear code;galois field location correcting codes channel errors correction erasure correction decomposability distance hamming metric error correcting codes sphere packing bounds singleton type bound redundancy optimal codes code length upper bounds asymptotic length;singleton type bound;sphere packing bounds;decomposability distance;asymptotic length;computer science;location correcting codes;telecommunication channels;galois field;sidon set;redundancy linear codes error correction codes channel coding telecommunication channels galois fields decoding;decoding error correction codes redundancy error correction upper bound computer errors parity check codes computer science laboratories milling machines;channel errors correction;computer errors;lower bound;information theory;galois fields;hamming metric	We study codes over GF (q) that can correct t channel errors assuming the error values are known. This is a counterpart to the well-known problem of erasure correction, where error values are found assuming the locations are known. The correction capabilities of these so called t-location correcting codes (t-LCCs) are characterized by a new metric, the decomposability distance, which plays a role analogous to that of the Hamming metric in conventional error-correcting codes (ECCs). Based on the new metric, we present bounds on the parameters of tLCCs that are counterparts to the classical Singleton, sphere packing and GilbertVarshamov bounds for ECCs. In particular, we show examples of perfect LCCs, and we study optimal (MDS-like) LCCs that attain the Singleton-type bound on the redundancy. We show that these optimal codes are generally much shorter than their erasure (or conventional ECC) analogues: The length n of any t-LCC that attains the Singleton-type bound for t > 1 is bounded from above by t + O( √ q), compared to length q+1 which is attainable in the conventional ECC case. We show constructions of optimal t-LCCs for t ∈ {1, 2, n−2, n−1, n} that attain the asymptotic length upper bounds, and constructions for other values of t that are optimal, yet their lengths fall short of the upper bounds. The resulting asymptotic gap remains an open research problem. All the constructions presented can be efficiently decoded.	closing (morphology);error detection and correction;forward error correction;grammatical framework;hamming code;hamming distance;open research;set packing;whole earth 'lectronic link	Ron M. Roth;Gadiel Seroussi	1996	IEEE Trans. Information Theory	10.1109/18.485724	combinatorics;discrete mathematics;error detection and correction;information theory;theoretical computer science;mathematics;upper and lower bounds;statistics	Theory	39.411749410614085	57.258244524714875	184
6af7332d499c187199f170df83954f19ff485ece	doppler processing of coherent radar backscatter for ocean surface wave measurements	remote sensing by radar doppler radar ocean waves oceanographic techniques;peak wave period doppler processing coherent radar backscatter ocean surface wave measurements wave period wave direction radar backscattering intensity spectral density wave height modulation transfer function doppler velocity;coherent radar backscatter;wave height;radar backscattering intensity;peak wave period;spectral density;surface wave height;reliability assessment;remote sensing by radar;wave direction;sea surface;doppler effect;doppler velocity;surface wave;modulation transfer function;ocean surface wave measurements;doppler radar;surface waves;wave period;doppler effect doppler radar surface waves sea surface radar measurements sea measurements;coherent radar;radar measurements;ocean wave;oceanographic techniques;ocean waves;doppler processing;sea measurements;coherent radar surface wave height doppler velocity;significant wave height;radial velocity	The technique for extracting wave period and wave direction from a navigation radar backscattering intensity is well developed but the determination of spectral density or wave height is hindered by the complex nature of the modulation transfer function. In contrast to backscattering intensity, Doppler velocity from coherent radar is the radial velocity of the scattering objects. Its oscillatory component is contributed by ocean waves. The spectral peak component of Doppler velocity is close to the peak wave period measured by a nearby buoy and the significant wave height can be accurately calculated. With radar range coverage on the order of ten dominant wavelengths, reliable assessment of peak wave period and significant wave height is achievable with radar data as short as one second. Wave direction can also be determined with a scanning system.	coherence (physics);doppler effect;modulation;radar;radial (radio);spectral density;surface wave;transfer function;velocity (software development)	Paul A. Hwang;Mark A. Sletten;Jakov V. Toporkov;Dennis B. Trizna	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5653056	meteorology;wind wave;particle velocity;plane wave;wave shoaling;geodesy;wave propagation;surface wave;wave;pulse-doppler radar;phase velocity;physics;remote sensing	Embedded	81.50964450070757	-66.28885787485044	185
4c4295eab2a1b7ff4dcfcfb1a64af081d1ad9c69	shuffling by semi-random transpositions	complexity analysis;theorem proving markov processes random processes computational complexity cryptography;fixed point;theorem proving;upper bound;nonreversible markov chains semirandom transposition shuffle cyclic to random shuffle cryptographic system mixing time complex valued test function uniform random permutations complex analysis tools;computational complexity;cryptography;mixing time;random processes;random permutation;markov processes;lower bound;markov chain;upper bound statistics cryptography mathematics computer science testing fasteners state space methods algorithm design and analysis sampling methods	"""In the cyclic-to-random shuffle, we are given n cards arranged in a circle. At step k, we exchange the kth card along the circle with a uniformly chosen random card. The problem of determining the mixing time of the cyclic-to-random shuffle was raised by Aldous and Diaconis in 1986. Mironov used this shuffle as a model for the cryptographic system known as RC4, and proved an upper bound of O(n log n) for the mixing time. We prove a matching lower bound, thus establishing that the mixing time is indeed of order /spl Theta/(n log n). We also prove an upper bound of O(n log n) for the mixing time of any """"semirandom transposition shuffle"""", i.e., any shuffle in which a random card is exchanged with another card chosen according to an arbitrary (deterministic or random) rule. To prove our lower bound, we exhibit an explicit complex-valued test function which typically takes very different values for permutations arising from few iterations of the cyclic-to-random-shuffle and for uniform random permutations. Perhaps surprisingly, the proof hinges on the fact that the function e/sup z/ - 1 has nonzero fixed points in the complex plane. A key insight from our work is the importance of complex analysis tools for uncovering structure in nonreversible Markov chains."""	cryptography;cryptosystem;distribution (mathematics);fisher–yates shuffle;fixed point (mathematics);freedman–diaconis rule;iteration;markov chain;rc4;semiconductor industry;the circle (file system)	Elchanan Mossel;Yuval Peres;Alistair Sinclair	2004	45th Annual IEEE Symposium on Foundations of Computer Science	10.1109/FOCS.2004.60	stochastic process;combinatorics;random permutation;discrete mathematics;random function;mathematics;upper and lower bounds;algorithm;statistics	Theory	10.677801029328224	22.52890224088287	186
2c41f5aebdc3d2ea3ac07e1bd605c03664bf7f87	maintaining coherent perceptual information using anchoring	mobile robot;datavetenskap datalogi;datavetenskap;computer and systems science;experimental validation;computer science	The purposeof this paperis to address the problemof maintainingcoherentperceptual information in a mobile roboticsystemworking over extended periods of time, interacting with a userandusing multiple sensingmodalities to gatherinformation about the environment and specific objects. We presenta systemwhich is able to usespatialand olfactory sensorsto patrol a corridor andexecute userrequestedtasks.Tocopewith perceptualmaintenancewe presentan extension of the anchoring framework capable of maintaining the correspondencebetweensensordataand the symbolic descriptionsreferring to objects.It is alsocapableof trackingandacquiring information from observationsderived from sensor -dataaswell asinformation from a priori symbolicconcepts. Thegeneral systemis described andanexperimentalvalidation onamobilerobot is presented .	coherent;interaction;mobile robot	Amy Loutfi;Silvia Coradeschi;Alessandro Saffiotti	2005			mobile robot;computer vision;simulation;computer science;artificial intelligence;machine learning	AI	-29.43428668911335	-39.985136493460594	187
38884fc29c5cf5c0693515be6da07c0ddc875606	segmentation of densely populated cell nuclei from confocal image stacks using 3d non-parametric shape priors	shape three dimensional displays image segmentation level set training databases solid modeling;confocal microscopy densely populated cell nuclei segmentation confocal image stacks 3d nonparametric shape priors nuclei boundaries extraction cell migration maximum a posteriori estimation 3d alignment algorithm;shape measurement biomedical optical imaging cellular biophysics image segmentation maximum likelihood estimation medical image processing microfluidics optical microscopy	An approach to jointly estimate 3D shapes and poses of stained nuclei from confocal microscopy images, using statistical prior information, is presented. Extracting nuclei boundaries from our experimental images of cell migration is challenging due to clustered nuclei and variations in their shapes. This issue is formulated as a maximum a posteriori estimation problem. By incorporating statistical prior models of 3D nuclei shapes into level set functions, the active contour evolutions applied on the images is constrained. A 3D alignment algorithm is developed to build the training databases and to match contours obtained from the images to them. To address the issue of aligning the model over multiple clustered nuclei, a watershed-like technique is used to detect and separate clustered regions prior to active contour evolution. Our method is tested on confocal images of endothelial cells in microfluidic devices, compared with existing approaches.	active contour model;algorithm;cell nucleus;endothelial cells;estimated;estimation theory;memory segmentation;microchip analytical devices;microscopy, confocal;migration, cell;population;published database;watershed (image processing);algorithm;biologic segmentation	Lee-Ling S. Ong;Mengmeng Wang;Justin Dauwels;H. Harry Asada	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944878	computer vision;mathematics;image segmentation;optics;scale-space segmentation	Vision	43.884692859493235	-76.13702139245243	188
f7b4151d2f98683a95335ec1f77674a54b307028	performance study of scan-and-wait combining over nakagami-q (hoyt) fading channels	nakagami channels;signal retransmission scan and wait combining nakagami q fading channel hoyt fading channel outage probability average output signal to noise ratio bit error rate maximal ratio combining selection combining time delay;diversity reception;error statistics;nakagami channels diversity reception error statistics;signal to noise ratio diversity reception bit error rate fading receivers switches modulation	The performance of scan-and-wait combining (SWC) over independent non-identically distributed (i.n.d.) slow and flat Nakagami-q (Hoyt) fading channels is examined. The outage probability and average output signal-to-noise ratio (SNR) expressions are derived. Also, the bit-error rate expression for the SWC with different binary modulation schemes over the i.n.d. Hoyt-fading channels is presented. It is shown that the SWC can perform better than the maximal-ratio combining and selection combining with the same number of channel estimates when the predetermined SNR threshold values are optimized. However, the SWC scheme has a drawback of time delay for retransmission of signals.	adobe swc file;bit error rate;broadcast delay;maximal set;modulation;retransmission (data networks);signal-to-noise ratio;sun outage	Beng Soon Tan;Ly-Minh-Duy Le;Kwok Hung Li;Kah Chan Teh	2013	2013 IFIP Wireless Days (WD)	10.1109/WD.2013.6686452	electronic engineering;telecommunications;statistics	Mobile	36.5783622777107	77.74002678201063	189
b448ad5491c5845f797acec6a59c9038faebc7bc	rt-unnid: a practical solution to real-time network-based intrusion detection using unsupervised neural networks	misuse detection;network security;anomaly detection;real time;intrusion detection;neural net;network traffic;self organizing map;intrusion detection systems;unsupervised neural network;self organized map;intrusion detection system;adaptive resonance theory;neural network	With the growing rate of network attacks, intelligent methods for detecting new attacks have attracted increasing interest. The RT-UNNID system, introduced in this paper, is one such system, capable of intelligent real-time intrusion detection using unsupervised neural networks. Unsupervised neural nets can improve their analysis of new data over time without retraining. In previous work, we evaluated Adaptive Resonance Theory (ART) and Self-Organizing Map (SOM) neural networks using offline data. In this paper, we present a real-time solution using unsupervised neural nets to detect known and new attacks in network traffic. We evaluated our approach using 27 types of attack, and observed 97% precision using ART nets, and 95% precision using SOM nets. a 2006 Elsevier Ltd. All rights reserved.	adaptive resonance theory;artificial neural network;intrusion detection system;network traffic control;online and offline;real-time clock;real-time computing;self-organizing map;sensor;windows rt	Morteza Amini;Rasool Jalili;Hamid Reza Shahriari	2006	Computers & Security	10.1016/j.cose.2006.05.003	anomaly-based intrusion detection system;intrusion detection system;computer science;artificial intelligence;machine learning;data mining;time delay neural network;computer security;artificial neural network	AI	6.67235129887058	-36.58206812423741	190
b95c648e918ad19bfcd23a8164d42002d1019c30	multi-gpu image-based visual hull rendering	categories and subject descriptors according to acm ccs i 3 3 computer graphics picture image generation display algorithms i 3 1 computer graphics hardware architecture parallel processing i 3 1 computer graphics hardware architecture graphics processors	Many virtual mirror and telepresence applications require novel viewpoint synthesis with little latency to user motion. Image-based visual hull (IBVH) rendering is capable of rendering arbitrary views from segmented images without an explicit intermediate data representation, such as a mesh or a voxel grid. By computing depth images directly from the silhouette images, it usually outperforms indirect methods. GPU-hardware accelerated implementations exist, but due to the lack of an intermediate representation no multi-GPU parallel strategies and implementations are currently available. This paper suggests three ways to parallelize the IBVH-pipeline and maps them to the sorting classification that is often applied to conventional parallel rendering systems. In addition to sort-first parallelization, we suggest a novel sort-last formulation that regards cameras as scene objects. We enhance this method’s performance by a block-based encoding of the rendering results. For interactive systems with hard real-time constraints, we combine the algorithm with a multi-frame rate (MFR) system. We suggest a combination of forward and backward image warping to improve the visual quality of the MFR rendering. We observed the runtime behavior of the suggested methods and assessed how their performance scales with respect to input and output resolutions and the number of GPUs. By using additional GPUs, we reduced rendering times by up to 60%. Multi-frame rate viewing can even be ten times faster.	algorithm;compositing;data (computing);geo warping;graphics processing unit;image warping;input/output;interactive media;interactivity;intermediate representation;map;microsoft fingerprint reader;parallel computing;parallel rendering;real-time clock;real-time computing;set packing;sorting;visual hull;voxel	Stefan Hauswiesner;Rostislav Khlebnikov;Markus Steinberger;Matthias Straka;Gerhard Reitmayr	2012		10.2312/EGPGV/EGPGV12/119-128	computer vision;tiled rendering;parallel computing;scientific visualization;image-based modeling and rendering;3d rendering;rendering;computer science;theoretical computer science;operating system;parallel rendering;real-time computer graphics;real-time rendering;texture memory;computer graphics;alternate frame rendering;volume rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	66.91187992024611	-51.50699345351059	191
3272d0b6b44ffcbc95a544d1d5a655ca4b2a0bb6	on the use of prime implicates in conformant planning	prime implicate;conformant planning	The paper presents an investigation of the use of two alternative forms of CNF formulae—prime implicates and minimal CNF—to compactly represent belief states in the context of conformant planning. For each representation, we define a transition function for computing the successor belief state resulting from the execution of an action in a belief state; results concerning soundness and completeness are provided. The paper describes a system (PIP) which dynamically selects either of these two forms to represent belief states, and an experimental evaluation of PIP against state-of-the-art conformant planners. The results show that PIP has the potential of scaling up better than other planners in problems rich in disjunctive information about the initial state. Introduction and Motivation Conformant planning (Smith and Weld 1998) is the problem of planning in presence of incomplete information about the initial state. One of the most important questions in conformant planning is how to represent the information about the initial situation, which is often referred as the initial belief state. In a domain with n propositions, the size of the initial belief state can be 2. In the literature, the description of the initial belief state is often given as a CNF formula with some additional constructs (as discussed later). The representation method used to encode belief states affects the performance of a conformant planner in several ways. It can quickly increase the memory usage of the planner, leading to undesirable out-of-memory situations, if the size of the belief state is large. It also directly affects the time complexity in computing the successor belief states, since this task often requires the planner to test for the satisfaction of a conjunction of literals in a belief state, which is a NP-hard problem. In the past, several representations have been developed. An indirect representation of belief states is used in CFF (Brafman and Hoffmann 2004), while ordered binary decision diagram (OBDD) is employed in POND (Bryce, Kambhampati, and Smith 2006); approximation states has ∗Partial supported by NSF grants IIS-0812267, CBET0754525, and CREST-0420407. Copyright c © 2011, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. been introduced in CPA (Tran et al. 2009); the work presented in DNF (To, Pontelli, and Son 2009) relies on explicit disjunctive formulae. The investigation presented in (To, Pontelli, and Son 2009) also discusses in more details the advantages and disadvantages of each of the aforementioned representations. It is worth mentioning that, regardless of the representation of belief states, all planners have difficulties scaling up when the size of the initial belief state is large. For example, all planners fail to find a solution of a modified version of the coins-21 problem instance from the IPC-2006 competition, either because they run out of memory or the plan computation exceeds reasonable time limit. The initial belief state of this problem contains 10 states, compared to the “easy instances” in the same domain—e.g., the coins-20 instance has an initial belief state containing less than 10 possible states, and can be solved by all planners in less than two minutes. The above issues motivated us to investigate alternative representations of belief states in conformant planning. Inspired by recent developments in other areas (e.g., the dDNNF representation of SAT (Darwiche 2001)) and aware of the difficulties involved in using OBDD in computing the successor state, our goal is to identify a belief state representation with two desirable properties. First, the size of the representation should be minimal (as defined later). Second, the representation should facilitate a simple and efficient way for determining the satisfaction of a set of literals given a belief state. In this paper, we investigate two different CNF-based belief state representations. The first representation employs prime implicates, called pi-formula. In this representation, a formula is replaced by its set of prime implicates or its pi-form. This representation exhibits a number of desirable properties. The complexity of checking tautological in a piform of a formula is linear for a literal (and polynomial for a clause) in the number of the propositions. Second, the representation is unique among the set of equivalent CNF formulae, making tractable the problem of checking for repetitions of belief states in a search tree. Third, this representation is, in many cases, very compact. Finally, the computation of the successor belief states under this representation can be done efficiently. The main disadvantage of this representation is that the size of the pi-form of a formula is sometimes very large. To this end, we investigate an alternative representation, called minimal CNF, whose size is often significantly smaller than its equivalent pi-form and can compensate for the pi-form representation in many cases. In this paper, we provide a formal description of the two representations and design a best-first search conformant planner (PIP) that adopts them. We provide an experimental comparison of PIP against other planners. Our experiments show that PIP is comparable in speed with state-of-the-art planners and scales up better in domains rich in disjunctive information about the initial belief state. Background: Conformant Planning A planning problem is a tuple P = 〈F,O, I,G〉, where F is a set of propositions, O is a set of actions, I describes the initial state of the world, and G describes the goal. A literal is either a proposition p ∈ F or its negation ¬p.  ̄̀ denotes the complement of a literal `. For a set of literals L, L = { ̄̀ | ` ∈ L}. A conjunction of literals is often represented as the set of its literals. A set of literals X is consistent if there is no p ∈ F such that {p,¬p} ⊆ X , and complete if, for each p ∈ F , either p ∈ X or ¬p ∈ F . A state s is a consistent and complete set of literals. A belief state is a set of states. We will often use lowercase (resp. uppercase) letter, possibly with indices, to represent a state (resp. a belief state). Each action a in O is associated with a precondition φ (denoted by pre(a)) and a set of conditional effects of the form ψ → ` (also denoted by a : ψ → `), where φ and ψ are sets of literals and ` is a literal. A state s satisfies a literal `, denoted by s |= `, if ` ∈ s. s satisfies a conjunction of literals X , denoted by s |= X , if it satisfies every literal belonging to X . The satisfaction of a formula in a state is defined in the usual way. Likewise, a belief state S satisfies a literal `, denoted by S |= `, if s |= ` for every s ∈ S. Given a state s, an action a is executable in s if s |= pre(a). The effects of executing a in s is e(a, s) = {` | ∃(a : ψ → `). s |= ψ} The transition function, denoted by Φ, in the planning domain of P is defined by Φ(a, s) = { s \ e(a, s) ∪ e(a, s) s |= pre(a) ⊥ otherwise (1) where ⊥ denotes a failed state. We can extend the function Φ to define Φ̂, a transition function which maps sequences of actions and belief states to belief states. Φ̂ is used to reason about the effects of plans. Let S be a belief state. We say that an action a is executable in a belief state S if it is executable in every state belonging to S. Let [a1, . . . , an] be a sequence of actions: • If n = 0 then Φ̂([ ], S) = S; • If n > 0 then ◦ if Φ̂([a1, . . . , an−1], S) = ⊥ or if an is not executable in Φ̂([a1, . . . , an−1], S), then Φ̂([a1, . . . , an], S) = ⊥; ◦ if Φ̂([a1, . . . , an−1], S) 6= ⊥ and an is executable in Φ̂([a1, . . . , an−1], S) then Φ̂([a1, . . . , an], S) = {Φ(an, s) | s ∈ Φ̂([a1, . . . , an−1], S)}. The initial state of the world (I) is a belief state and is represented by a formula. In all benchmarks, I consists of a conjunction of a set of literals, a set of oneof-statements— representing an exclusive-or of its components—and a set of or-statements—representing the logical or of its components. By SI we denote the set of all states satisfying I . The goal description G can contain literals and or-clauses. A sequence of actions [a1, . . . , an] is a solution of P if Φ̂([a1, . . . , an], SI) satisfies the goal G. In this paper, for an action a, we will denote with Ca the set of conditional effects of a. The function Φ̂ can be used in the implementation of a best-first search based conformant planner. As we have discussed earlier, one of the important factors to this effort lies in the representation of belief states. CNF Representations for Belief States In this section, we explore the use of prime implicates in representing belief states for the development of a conformant planner. Observe that the development of a new representation of belief states needs to come with a set of operations such as checking for the satisfaction of a condition and/or updating a belief state with a set of effects. A clause α is a set of fluent literals. α is tautological if {f,¬f} ⊆ α for some f ∈ F and it is a unit clause if |α| = 1. A CNF formula is a set of clauses. A literal l is in a CNF formula φ, denoted by l ∈ φ, if there exists α ∈ φ such that l ∈ α. By φl (resp. φl̄) we denote the set of clauses in φ which contain l (resp. l̄). A clause α subsumes a clause β (or β is subsumed by α) if α ⊂ β. Given a CNF formula φ, a clause α in φ is said to be trivially redundant for φ if it is tautological or it is subsumed by another clause in φ. The technique of simplifying a CNF formula by removing subsumed clause(s) from that formula is called subsumption. A clause α is said to be resolvable with another clause β if there exists a literal ` such that ` ∈ α,  ̄̀ ∈ β, and their resolvent α|β, defined by α|β = (α \ {`}) ∪ (β \ { ̄̀}), is a non-tautological clause. In this case, we say that α is resolvable with β on `. Observe that, if α and β are two clauses in a CNF formula φ and there exists a clause in φ which is s	approximation;array data structure;artificial intelligence;best-first search;binary decision diagram;bryce;cobham's thesis;complement (complexity);computation;conjunctive normal form;cost per action;disjunctive normal form;encode;exclusive or;executable;experiment;fluent calculus;heuristic;ibm notes;image scaling;influence diagram;literal (mathematical logic);map;np-hardness;out of memory;polynomial;precondition;resolution (logic);search tree;state (computer science);subsumption architecture;time complexity;unit propagation;pip	Son Thanh To;Tran Cao Son;Enrico Pontelli	2010			computer science;artificial intelligence;algorithm	AI	-19.060643948859408	14.377312925361025	192
34d417cd073a21df5c7c7bb9fcf147b20edfe15c	improving data forwarding in mobile social networks with infrastructure support: a space-crossing community approach	mobile social networks;social networking online human factors mobile computing;ap spreading mobile social networks infrastructure support space crossing community detection approach msn hybrid underlying networks mobile users access points long distance node communication physical proximity community social attraction space crossing communities saas algorithm mit reality mining university of illinois movement social community based data forwarding algorithms nguyens routing algorithms bubble rap algorithms;space crossing;data forwarding;infrastructure support;080502 mobile technologies;community approach;communities mobile communication local activities mobile computing social network services educational institutions vectors;089999 information and computing sciences not elsewhere classified	In this paper, we study two tightly coupled issues: space-crossing community detection and its influence on data forwarding in Mobile Social Networks (MSNs) by taking the hybrid underlying networks with infrastructure support into consideration. The hybrid underlying network is composed of large numbers of mobile users and a small portion of Access Points (APs). Because APs can facilitate the communication among long-distance nodes, the concept of physical proximity community can be extended to be one across the geographical space. In this work, we first investigate a space-crossing community detection method for MSNs. Based on the detection results, we design a novel data forwarding algorithm SAAS (Social Attraction and AP Spreading), and show how to exploit the space-crossing communities to improve the data forwarding efficiency. We evaluate our SAAS algorithm on real-life data from MIT Reality Mining and University of Illinois Movement (UIM). Results show that space-crossing community plays a positive role in data forwarding in MSNs in terms of delivery ratio and delay. Based on this new type of community, SAAS achieves a better performance than existing social community-based data forwarding algorithms in practice, including Bubble Rap and Nguyen's Routing algorithms.	algorithm;dot-com bubble;operand forwarding;real life;social network;software as a service;uim	Zhongqiao Li;Cheng Wang;Siqian Yang;Changjun Jiang;Ivan Stojmenovic	2014	IEEE INFOCOM 2014 - IEEE Conference on Computer Communications	10.1109/INFOCOM.2014.6848134	simulation;computer science;distributed computing;world wide web	DB	0.5826718765331353	78.67784896402851	193
acb6bb918ba2103870b44c36bfe5b50fc26173b8	swift valuation of discretely monitored arithmetic asian options		Abstract In this work, we propose an efficient and robust valuation of discretely monitored arithmetic Asian options based on Shannon wavelets. We employ the so-called SWIFT method, a Fourier inversion numerical technique with several important advantages with respect to the existing related methods. Particularly interesting is that SWIFT provides mechanisms to determine all the free-parameters in the method, based on a prescribed precision in the density approximation. The method is applied to two general classes of dynamics: exponential Levy models and square-root diffusions. Through the numerical experiments, we show that SWIFT outperforms state-of-the-art methods in terms of accuracy and robustness, and shows an impressive speed in execution time.		Álvaro Leitao;Luis Ortiz-Gracia;Emma I. Wagner	2018	J. Comput. Science	10.1016/j.jocs.2018.07.004	wavelet;mathematical optimization;fourier transform;swift;robustness (computer science);arithmetic;computer science;valuation of options;asian option;exponential function;inversion (meteorology)	Theory	79.02913092707543	16.642532631548214	194
966526ad0dfbcc8b1bb573f799f084074433f574	is there any need to mention induction?		Induction is the process by which seen data becomes the basis for prediction of unseen data. There has long been a desire to explain the procedure in a context-free way. But Hume’s circularity problem and the no-free-lunch theorems both seem to suggest the logical impossibility of any context-free mechanism. Machine Learning takes the position that no such mechanism exists. But an alternative comes from Epistemology. Popper’s falsificationist theory holds that there is a general mechanism, but that it does not perform induction. Inductive effects arise implicitly, through pursuit of a non-inductive goal. Less plausibly, the mechanism is taken to be uninformed exploration of hypotheses. But as the paper shows, Popper’s solution can be reworked using information theory. Increasing the informational efficiency with which representations predict seen data can be shown to produce inductive effects. With representation optimization taking the place of hypothesis-search in the argument, it then becomes possible to explain induction in a context-free way.	context-free language;hume (programming language);information theory;logical possibility;machine learning;mathematical induction;mathematical optimization;no free lunch in search and optimization	Chris Thornton	2011				ML	-13.927007594470854	2.3375879968131397	195
683c85d6919723d38b58ab90730414eec1630f28	a revised hilbert–huang transform and its application to fault diagnosis in a rotor system	hilbert–huang transform;empirical mode decomposition;end effects;fault diagnosis;mode mixing;rotor system	As a classical method to deal with nonlinear and nonstationary signals, the Hilbert⁻Huang transform (HHT) is widely used in various fields. In order to overcome the drawbacks of the Hilbert⁻Huang transform (such as end effects and mode mixing) during the process of empirical mode decomposition (EMD), a revised Hilbert⁻Huang transform is proposed in this article. A method called local linear extrapolation is introduced to suppress end effects, and the combination of adding a high-frequency sinusoidal signal to, and embedding a decorrelation operator in, the process of EMD is introduced to eliminate mode mixing. In addition, the correlation coefficients between the analyzed signal and the intrinsic mode functions (IMFs) are introduced to eliminate the undesired IMFs. Simulation results show that the improved HHT can effectively suppress end effects and mode mixing. To verify the effectiveness of the new HHT method with respect to fault diagnosis, the revised HHT is applied to analyze the vibration displacement signals in a rotor system collected under normal, rubbing, and misalignment conditions. The simulation and experimental results indicate that the revised HHT method is more reliable than the original with respect to fault diagnosis in a rotor system.		Yongjian Ji;Hongjun Wang	2018		10.3390/s18124329		AI	78.8602778528514	-38.66553127433421	196
13cd3b91afc9aae5ba9da9adce2a00cf83f0341e	physical activity estimation using accelerometer and facility information for elderly healthcare	daily activity physical activity estimation accelerometer elderly healthcare user acceleration data facility information energy expenditure physical activity scale metabolic equivalent to task met physical activity monitoring road map land use land cover map elderly people living activity monitoring;agriculture roads senior citizens monitoring estimation sensors conferences;patient monitoring acceleration measurement accelerometers biomedical measurement gait analysis geriatrics health care	This paper proposes a novel framework to estimate the amount of physical activity at a place where people stayed, by utilizing facility information and user's acceleration data. The total amount of physical activities, energy expenditure of a physical activity is a good scale. Our framework provides a physical activity scale based on typical energy expenditure of the activity given by existing researches already. To estimates the energy expenditure, we use typical value of metabolic equivalents to task (MET) which is used as practical scale. Unlike the other studies for monitoring physical activity, we estimate the type of user's activity using facility information which are obtained from road map and land-use/land-cover map. To confirm the feasibility of our approach, we have conducted a long-term experiment on monitoring the activity of elderly people living in less-populated area. As a result, our framework provides good summarization of daily activities of participants.	activity recognition;population;sensor;smartphone	Masayuki Hayashi;Masayuki Kanbara;Norimichi Ukita;Norihiro Hagita	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6973984	simulation	HCI	8.122322551149264	-86.02485382128195	197
0721643e83c67c186567531a8c85480ba3e65d27	rodi: benchmarking relational-to-ontology mapping generation quality		Accessing and utilizing enterprise or Web data that is scattered across multiple data sources is an important task for both applications and users. Ontology-based data integration, where an ontology mediates between the raw data and its consumers, is a promising approach to facilitate such scenarios. This approach crucially relies on high-quality mappings to relate the ontology and the data, the latter being typically stored in relational databases. A number of systems to help mapping construction have recently been developed. A generic and effective benchmark for reliable and comparable evaluation of mapping quality would make an important contribution to the development of ontology-based integration systems and their application in practice. We propose such a benchmark, called RODI, and evaluate various systems with it. It offers test scenarios from conference, geographical, and oil and gas domains. Scenarios are constituted of databases, ontologies, mappings, and queries to test expected results. Systems that compute relational-to-ontology mappings can be evaluated using RODI by checking how well they can handle various features of relational schemas and ontologies, and how well computed mappings work for query answering. Using RODI we conducted a comprehensive evaluation of six systems.	benchmark (computing);ontology (information science);ontology-based data integration;relational database;semantic integration;web ontology language	Christoph Pinkel;Carsten Binnig;Ernesto Jiménez-Ruiz;Evgeny Kharlamov;Wolfgang May;Andriy Nikolov;Ana Sasa;Martin G. Skjæveland;Alessandro Solimando;Mohsen Taheriyan;Christian Heupel;Ian Horrocks	2018	Semantic Web	10.3233/SW-170268	data science;linguistics;semantic integration;philosophy;benchmarking	SE	-38.24969057085738	3.947534810590557	198
2f3fc78592eba99df26a876759ac4277560e8c6e	some new results for ternary linear codes of dimension 5 and 6	graph theory;notice of violation;mathematics;error correction codes;decoding;hamming weight;optimal linear code;distributed computing;linear codes;strongly regular graph;quasicyclic code;linear code distributed computing decoding error correction codes hamming weight mathematics;linear code;cyclic codes;graph theory linear codes cyclic codes;two weight code;quasicyclic code ternary linear codes two weight code strongly regular graph optimal linear code;ternary linear codes	"""It is proved that ternary codes with parameters [40,5,25] and [148,5,98] do not exist. A new ternary code is constructed with parameters [47,5,30]. The results solve the problem of finding the smallest n for which a ternary [n, 5, d] code exists for d = 25, 29, 30, and 98. Several new ternary linear codes of dimension 6 are found, including one two-weight code giving rise to a new strongly regular graph. Zdez T e m Optimal linear code, ternary linear code, quasicyclic code, two-weight code. I. NONEXISTENCE RESULTS Consider the quadratic form Q from GF (3)"""" to GF ( 3 ) given by Q ( x ) = x: + x; + ... + xz, where X I , x2,"""". ,xn denote the respective coordinates of z E GF (3) n. Let the Hamming weight of a word z be denoted by wt (z). Then obviously Q(z) = wt (z) mod 3. Let C be a subspace of dimension k of GF (3)"""" and let QC be the qudratic form @ restricted to C. Although Q itself is nondegenerate, Qc can be degenerate. The dimension of the radical of QC and the type of Qc detemne the number of words of weight 0, 1, and 2 mod 3 in C, respectively, and vice versa. Let A, denote the number of words of weight w in C and let S, denote the number where j runs over all integers for which 0 < 3j + i 5 n. Let r denote the dimension of the radical and let A denote the determinant of &c (which is +1 or -1 and is related to the type of Qc). Then from [14] we have the following. If k r is even, then 3k-1 + 2 . (-I)%? . A . 3 V I s1 = 2 ( 3 k 1 (-I)% . A . 3 9 l (2) (3) 3k-1 (-I)% . A . 3W-I). A ternary [n, K, d] code C can be seen as a k-dimensional subspace of GF(3)n (with the property that the smallest nonzero weight is d). The code C is called self-orthogonal if every codeword of C is orthogonal to every codeword of C with the usual inner product (thus corresponding to the case that r = k ) . From [6] we have the following lemmas: Manuscript received Nov. 30, 1994; revised May 1, 1995 The author is with the Department of Mathematics and Computing Science, Eindhoven University of Technology, P.O. Box 513, 56000 MB Eindhoven, The Netherlands. EEE Log Number 9414730. 0018-9448/95$04.00"""	code word;grammatical framework;hamming weight;like button;linear code;strongly regular graph;ternary numeral system;window function	Marijn van Eupen	1995	IEEE Trans. Information Theory	10.1109/18.476334	strongly regular graph;hamming weight;combinatorics;discrete mathematics;graph theory;theoretical computer science;linear code;mathematics;ternary golay code;statistics;balanced ternary	Theory	40.09997905413677	53.10734377124841	199
148cdb12105aeb818cccfa21badf4b4abefe5363	sig.ma: live views on the web of data	web of data;aggregated search;semantic web;rdfa	We demonstrate Sig.ma, both a service and an end user application to access the Web of Data as an integrated information space.  Sig.ma uses an holistic approach in which large scale semantic web indexing, logic reasoning, data aggregation heuristics, ad hoc ontology consolidation, external services and responsive user interaction all play together to create rich entity descriptions. These consolidated entity descriptions then form the base for embeddable data mashups, machine oriented services as well as data browsing services. Finally, we discuss Sig.ma's peculiar characteristics and report on lessions learned and ideas it inspires.	data aggregation;heuristic (computer science);hoc (programming language);holism;holomovement;mashup (web application hybrid);semantic web;semiconductor consolidation;web indexing;world wide web	Giovanni Tummarello;Richard Cyganiak;Michele Catasta;Szymon Danielczyk;Renaud Delbru;Stefan Decker	2010		10.1145/1772690.1772907	web service;web development;web modeling;data web;web mapping;web standards;computer science;semantic web;social semantic web;web page;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web	Web+IR	-41.74178170448688	9.170284753328737	200
c33ff21b23c7ec087b4d2cfe31c379ad8f345e2b	probabilistic threshold join over distributed uncertain data	joins;bloom filters;distributed query processing;uncertain data	Large amount of uncertain data is collected by many emerging applications which contain multiple sources in a distributed manner. Previous efforts on querying uncertain data in distributed environment have only focus on ranking and skyline, join queries have not been addressed in earlier work despite their importance in databases. In this paper, we address distributed probabilistic threshold join query, which retrieves results satisfying the join condition with combining probabilities that meet the threshold requirement from distributed sites. We propose a new kind of bloom filters called Probability Bloom Filters (PBF) to represent set with probabilistic attribute and design a PBF based Bloomjoin algorithm for executing distributed probabilistic threshold join query with communication efficiency. Furthermore, we provide theoretical analysis of the network cost of our algorithm and demonstrate it by simulation. The experiment results show that our algorithm can save network cost efficiently by comparing to original Bloomjoin algorithm in most scenarios.	uncertain data	Lei Lei Deng;Fei Wang;Benxiong Huang	2011		10.1007/978-3-642-23535-1_8	hash join;computer science;bloom filter;data mining;database;distributed computing;programming language	DB	-24.191243158762607	2.920680666174304	201
a444565b423e58594bafca32f263ade846968166	facets and measures of gene ontology annotation quality in model organism databases	gene ontology annotation	Model organism databases are important repositories of data and information for biomedical research, but are useful to scientists only if the information they contain meets certain levels of quality. This project describes six facets of information quality applicable to Gene Ontology (GO) annotations in model organism databases, and defines corresponding metrics to be used in measuring the types and degrees variation of GO annotations made by one or more human database curators. The defined facets and measures of annotation quality are: Consistency, Reliability, Specificity, Completeness, Accuracy, and Validity. Contextual factors, and factors affecting internal and external validity, are also discussed. Poster and paper available at: http://macmullen.com/conferences/asist Acknowledgments This work benefited from discussions with curators from multiple model organism databases at the 2005 and 2006 Gene Ontology Annotation Camps held at Stanford University. The Stanford Department of Genetics and the Saccharomyces Genome Database (SGD) provided travel support. W.J.M. was supported in part by an Individual Biomedical Informatics Fellowship (F37 LM009194) from the National Library of Medicine, and an unrestricted research gift from Microsoft Research to the Annotation of Structured Data research group in the School of Information and Library Science at the University of North Carolina at Chapel Hill. GO ANNOTATION QUALITY FACETS Annotation instances per curator by paper	chapel;experiment;external validity;fast analog computing with emergent transient states;gene ontology;informatics;information quality;library science;microsoft research;saccharomyces genome database (sgd);sensitivity and specificity;task analysis	W. John MacMullen	2006		10.1002/meet.14504301260	computer science;bioinformatics;data mining;information retrieval	DB	-46.146249878896256	-65.3575256122594	202
3b4b6799446aa5a311d09093e1d1dac3232471bf	an alternative approach to mining association rules.	data mining;association rule	An alternative approach to mining association rules is described. Some special techniques and algorithms are used that lead to a much richer syntax of association rules with only linear complexity of computation. A free and open system LISp-Miner implements these algorithms and can serve as a demonstration of used techniques. The same techniques can be used in other kinds of mining e.g. multi-relation mining and conditional frequency analysis.	algorithm;association rule learning;computation;context of computational complexity;frequency analysis;lisp	Jan Rauch;Milan Simunek	2005		10.1007/11498186_13	concept mining;association rule learning;computer science;data science;machine learning;data mining;k-optimal pattern discovery	DB	-31.750042887675757	6.356532958413753	203
77d45a62fde50ef591400d068212a64a7a72fe53	explicit fuzzy modeling of shapes and positioning for handwritten chinese character recognition	model design;structure methods;relative position;fuzzy reasoning;handwriting recognition;shape recognition fuzzy reasoning handwritten character recognition learning artificial intelligence natural languages;prototypes;training;text analysis;shape recognition;natural languages;fuzzy relative positioning fuzzy inference system handwritten character recognition chinese character recognition statistical structural method;fuzzy sets;incremental training;handwritten chinese character recognition;process design;fuzzy modeling;shape;dictionaries;statistical structural method;fuzzy inference system;pattern recognition;writing;structural component positioning;character structure;shape character recognition handwriting recognition pattern recognition process design labeling writing fuzzy systems dictionaries text analysis;incremental training fuzzy modeling handwritten chinese character recognition character structure shape recognition fuzzy inference system structural component positioning;learning artificial intelligence;fuzzy relative positioning;character recognition;fuzzy systems;handwritten character recognition;labeling;fuzzy model;chinese character recognition	In this paper, we present a new method for on-line Chinese character recognition that relies on an explicit description of characters structure. Contrary to most of known structural approaches, this model can describe characters written in a fluent style, thanks to a flexible fuzzy modeling of shapes and positioning of their structural components (primitives and radicals). We designed a process for incremental training of the models cooperated with automatic structural labeling for minimizing the required manual task in model design. First experiments show that the method is able to recognize non-regularly written characters and has a convincing generalization ability.	experiment;fuzzy concept;fuzzy logic;inference engine;online and offline;optical character recognition	Adrien Delaye;Éric Anquetil;Sébastien Macé	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.141	process design;labeling theory;speech recognition;shape;computer science;machine learning;pattern recognition;prototype;handwriting recognition;fuzzy set;natural language;writing	Robotics	32.08091562269999	-66.5043039312687	204
1d8171ca6c0ad7f94e50f309b2ed6184246008ef	deadlock detection for distributed process networks	distributed process networks;memory requirement predictions;turing complete model;concurrent processes;multi threading;yarn;concurrent computing;detection algorithms;process network model;run time deadlock detection;multiprocessor systems;processor scheduling;first in first out;distributed processing;deadlock detection;streaming data functional parallelism;process network;multithreaded systems;local deadlock detection algorithm;scheduling algorithm;system recovery;processor scheduling concurrency control multiprocessing systems parallel processing multi threading;signal processing;workstations;bounded memory scheduling algorithm;concurrency control;multiprocessing systems;parallel processing local deadlock detection algorithm process network model distributed process networks concurrent processes first in first out unidirectional queues streaming data functional parallelism multiprocessor systems multithreaded systems turing complete model memory requirement predictions bounded memory scheduling algorithm run time deadlock detection mitchell merritt algorithm;signal processing algorithms;concurrent process;mitchell merritt algorithm;first in first out unidirectional queues;parallel processing;system recovery parallel processing detection algorithms concurrent computing workstations signal processing algorithms sonar yarn signal processing laboratories;sonar	The process network (PN) model, which consists of concurrent processes communicating over first-in first out unidirectional queues, is useful for modeling and exploiting functional parallelism in streaming data applications. The PN model maps easily onto multi-processor and/or multi-threaded targets. Since the PN model is Turing complete, memory requirements cannot be predicted statically. In general, any bounded-memory scheduling algorithm for this model requires run-time deadlock detection. The few PN implementations that perform deadlock detection detect only global deadlocks. Not all local deadlocks, however, will cause a PN system to reach global deadlock. In this paper, we present the first local deadlock detection algorithm for PN models. The proposed algorithm is based on the Mitchell and Merritt algorithm and is suitable for both parallel and distributed PN implementations.	algorithm;deadlock;kahn process networks;multiprocessing;parallel computing;requirement;scheduling (computing);stream (computing);thread (computing);turing completeness	Alex G. Olson;Brian L. Evans	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416243	parallel processing;parallel computing;real-time computing;multithreading;workstation;concurrent computing;fifo and lifo accounting;computer science;deadlock;concurrency control;signal processing;distributed computing;edge chasing;scheduling;deadlock prevention algorithms;sonar	HPC	-13.757021768073834	44.35784563594327	205
ccfff2033d8a4dd3a2f8f3c61e08fb3d767a3228	an extensible framework for multicore response time analysis	multicore scheduling;timing analysis;verification	In this paper, we introduce a multicore response time analysis (MRTA) framework, which decouples response time analysis from a reliance on context-independent WCET values. Instead, the analysis formulates response times directly from the demands placed on different hardware resources. The MRTA framework is extensible to different multicore architectures, with a variety of arbitration policies for the common interconnects, and different types and arrangements of local memory. We instantiate the framework for single level local data and instruction memories (cache or scratchpads), for a variety of memory bus arbitration policies, including: Round-Robin, FIFO, Fixed-Priority, Processor-Priority, and TDMA, and account for DRAM refreshes. The MRTA framework provides a general approach to timing verification for multicore systems that is parametric in the hardware configuration and so can be used at the architectural design stage to compare the guaranteed levels of real-time performance that can be obtained with different hardware configurations. We use the framework in this way to evaluate the performance of multicore systems with a variety of different architectural components and policies. These results are then used to compose a predictable architecture, which is compared against a reference architecture designed for good average-case behaviour. This comparison shows that the predictable architecture has substantially better guaranteed real-time performance, with the precision of the analysis verified using cycle-accurate simulation.		Robert I. Davis;Sebastian Altmeyer;Leandro Soares Indrusiak;Claire Maiza;Vincent Nélis;Jan Reineke	2017	Real-Time Systems	10.1007/s11241-017-9285-4	parallel computing;memory bus;architecture;real-time computing;extensibility;cache;computer science;multi-core processor;reference architecture;response time;static timing analysis	Embedded	-2.559438935645729	50.7590743161503	206
5b1e8b310080d10379dae3bb7cb7665997e477d8	sparse kernel recursive least squares using l1 regularization and a fixed-point sub-iteration	recursive estimation adaptive filters iterative methods least squares approximations;kernel signal processing algorithms adaptive filters algorithm design and analysis vectors testing;kaf algorithm l 1 regularization sparse kernel recursive least squares skrls fixed point subiteration kernel adaptive filtering;fixed point iteration kernel adaptive filtering krls skrls l1 regularization	A new kernel adaptive filtering (KAF) algorithm, namely the sparse kernel recursive least squares (SKRLS), is derived by adding a ℓ1-norm penalty on the center coefficients to the least squares (LS) cost (i.e. the sum of the squared errors). In each iteration, the center coefficients are updated by a fixed-point sub-iteration. Compared with the original KRLS algorithm, the proposed algorithm can produce a much sparser network, in which many coefficients are negligibly small. A much more compact structure can thus be achieved by pruning these negligible centers. Simulation results show that the SKRLS performs very well, yielding a very sparse network while preserving a desirable performance.	adaptive filter;algorithm;coefficient;elastic net regularization;fixed point (mathematics);fixed-point arithmetic;iteration;kaf-10500;kernel (operating system);nonlinear system;online and offline;recursion;recursive least squares filter;simulation;sparse matrix;taxicab geometry	Badong Chen;Nanning Zheng;José Carlos Príncipe	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854606	least squares support vector machine;mathematical optimization;kernel embedding of distributions;kernel adaptive filter;kernel principal component analysis;machine learning;pattern recognition;mathematics;variable kernel density estimation;recursive least squares filter	Robotics	21.38724991505687	-37.44662224883895	207
bc8e094f303822a31372e08178dde8f42605294e	interactive decision support for multiobjective cots selection	analytic hierarchy process;decision support;pareto efficiency;priori preference information interactive decision support commercial off the shelf components multiobjective cots selection component based software engineering scientific community weighted scoring method analytical hierarchy process pareto efficient alternatives;object oriented programming;decision maker;component based software;software engineering;multiple objectives;commercial off the shelf;software engineering costs business computer industry space exploration project management programming security statistics industrial economics;decision support systems;scientific communication;software selection decision making decision support systems object oriented programming software engineering software packages;software packages;software selection	"""In the past decade component-based software engineering (CBSE) has gained considerable attention from both industry and the scientific community. Obviously, selecting the """"best"""" combination of commercial off-the-shelf (COTS) components plays a critical role in CBSE. This task becomes demanding, as multiple objectives and constraints have to be taken into account. So far, the selection process has been tackled by such traditional techniques as the weighted scoring method or the analytical hierarchy process. This paper introduces a decision support approach that more properly addresses that challenge. The approach involves first determining (feasible) Pareto-efficient alternatives, then allowing decision makers to interactively explore the solution space until they find the most appealing solution. It not only works without extensive a priori preference information (such as criteria weights), it can also be easily integrated into existing COTS selection frameworks"""	analytical hierarchy;component-based software engineering;decision support system;feasible region;interactivity;pareto efficiency	Thomas Neubauer;Christian Stummer	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.283	decision-making;analytic hierarchy process;decision support system;computer science;artificial intelligence;software engineering;data mining;database;management science;object-oriented programming	SE	13.716837437206276	-4.5077265978686345	208
bc9a527f473f965b3ffbf5fb7dce1728e5ab75f6	a naturally occurring niche and species phenomenon: the model and first results			niche blogging	Yuval Davidor	1991			ecology;niche;computer science;phenomenon	Theory	-5.295336013883101	-47.1053956965047	209
5fd6073d24a474f479ed7f2273ea79e15a90177e	an agent-based model for quantitatively analyzing and predicting the complex behavior of emergency departments	emergency department;agent based model;complex adaptive system;decision support system	Hospital based emergency departments (EDs) are highly integrated service units devoted primarily to handling the needs of patients arriving without prior appointment, and with uncertain conditions. In this context, analysis and management of patient flows play a key role in developing policies and decisions for overall performance improvement. However, patient flows in EDs are considered to be very complex because of the different pathways patients may take and the inherent uncertainty and variability of healthcare processes. The agent-based model provides a flexible platform for studying ED operations, as it predicts the system-level behavior from individual level interactions. In this way, policies such as staffing can be changed and the effect on system performance, such as waiting times and throughput, can be quantified. The overall goal of this study is to develop tools to better understand the complexity, evaluate policy and improve efficiencies of ED units. The main contribution of this paper includes: an agent-based model of ED, a flexible atomic data monitoring layer for agent state tracing, and a master/worker based framework for efficiently executing the model and analyzing simulation data. The presented model has been calibrated to imitate a real ED in Spain, the simulation results have proven the feasibility of using agent-based model to study ED system. c © 20xx Published by Elsevier Ltd.	agent-based model;interaction;simulation;spatial variability;throughput	Zhengchun Liu;Dolores Rexachs;Francisco Epelde;Emilio Luque	2017	J. Comput. Science	10.1016/j.jocs.2017.05.015	emergency department;agent-based model;staffing;management science;theoretical computer science;performance improvement;simulation;decision support system;computer science;tracing;complex adaptive system	ML	-16.6355274408428	-24.500890232263252	210
612a3ec8d4687e1418dd332b95f43f556e6b37e7	optimizing data sharing and address translation for the cell be heterogeneous chip multiprocessor	storage allocation;software;microprocessors;data sharing;optimisation;memory management;heterogeneous systems;logic design;perforation;storage management;multiprogramming environment dma based data sharing optimization address translation cell broadband engine heterogeneous chip multiprocessor design optimization memory access mechanism cpu protocol;software management;chip multiprocessor;software systems;multiprogramming;design optimization;hardware engines design optimization application software access protocols performance analysis software performance memory management computer architecture memory architecture;cell broadband engine;chip;memory access;computer architecture;synchronization;execution environment;coherence;system architecture;storage management logic design microprocessor chips multiprogramming optimisation storage allocation;microprocessor chips;hardware	Heterogeneous Chip Multiprocessors (HMPs), such as the Cell Broadband Engine, offer a new design optimization opportunity by allowing designers to provide accelerators for application specific domains. Data sharing between CPUs and accelerators, and memory access mechanisms and protocols are crucial decisions in the design of an HMP. In this article, we analyze the choices between hardware and software managed coherence between CPU and accelerators for DMA-based data sharing, and find that hardware-coherent DMA shows a performance benefit of up to 3x, even for simple workloads.We explore memory address translation architecture choices for DMA-based data sharing. In multiprogramming environments, address translation is commonly used to separate processes. For efficiency, direct access to system memory requires address translation capabilities in the accelerator. We find that hardware managed address translation shows a performance benefit of up to 5x, even for simple workloads, by avoiding the costs of accelerator/CPU communication and supervisor management of the translation context and the introduction of a serial bottleneck on the CPU.	cpu cache;cell (microprocessor);central processing unit;coherence (physics);computer multitasking;direct memory access;host media processing;mathematical optimization;memory address;memory management unit;microsoft foundation class library;multi-core processor;multiprocessing;optimizing compiler;page table;process (computing);random access;systems architecture;systems design	Michael Gschwind	2008	2008 IEEE International Conference on Computer Design	10.1109/ICCD.2008.4751904	chip;embedded system;synchronization;computer architecture;parallel computing;logic synthesis;real-time computing;multidisciplinary design optimization;coherence;computer multitasking;telecommunications;computer science;operating system;computer network;systems architecture;software system;memory management	Arch	-9.084728271001989	49.24722675066498	211
82b27bcb5a67e5384d969f449355e89946832eef	analysis of acoustic wave propagation in a thin moving fluid	piecewise linear;global solution;35b35;grupo de excelencia;mathematical analysis;galbrun equations;35q35;stability;35c15;fourier laplace method;ciencias basicas y experimentales;matematicas;aeroacoustics;asymptotic model;acoustic waves;velocity profile;35q72;euler equation	We study the propagation of acoustic waves in a uid that is contained in a thin two-dimensional tube, and that it is moving with a velocity pro le that only depends on the transversal coordinate of the tube. The governing equations are the Galbrun equations, or, equivalently, the linearized Euler equations. We analyze the approximate model that was recently derived by Bonnet-Bendhia, Duru é and Joly to describe the propagation of the acoustic waves in the limit when the width of the tube goes to zero. We study this model for strictly monotonic stable velocity pro les. We prove that the equations of the model of Bonnet-Bendhia, Duru é and Joly are well posed, i.e., that there is a unique global solution, and that the solution depends continuously on the initial data. Moreover, we prove that for smooth pro les the solution grows at most as t as t → ∞, and that for piecewise linear pro les it grows at most as t. This establishes the stability of the model in a weak sense. These results are obtained constructing a quasi-explicit representation of the solution. Our quasi-explicit representation gives a physical interpretation of the propagation of acoustic waves in the uid and it provides an e cient way to compute numerically the solution. ∗AMS 2000 classi cation: 35Q72; 35Q35; 35B35; 35C15. †On leave of absence from Departamento de Métodos Matemáticos y Numéricos. Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas. Universidad Nacional Autónoma de México. Apartado Postal 20-726, México DF 01000. Fellow of the Sistema Nacional de Investigadores. 1 ar X iv :0 90 7. 55 62 v1 [ m at hph ] 3 1 Ju l 2 00 9	acoustic cryptanalysis;approximation algorithm;bibliothèque de l'école des chartes;computational aeroacoustics;duru–kleinert transformation;euler;existential quantification;méxico indígena;numerical analysis;piecewise linear continuation;postal;software propagation;velocity (software development)	Patrick Joly;Ricardo Weder	2010	SIAM Journal of Applied Mathematics	10.1137/09077237X	acoustic wave;mathematical optimization;mathematical analysis;stability;piecewise linear function;calculus;mathematics;aeroacoustics;euler equations;physics	ML	84.2538193586598	7.551577849341145	212
300130d84e6d082a3e4002afbf03942ec71c6613	web-based digital shop floor: implementation of business service management and managerial implications	manufacturing systems;managerial implications;remote control;e shopfloor;sensors;collaboration;service management;e manufacturing;business service management;parallel kinematic machines;web based shopfloor;distributed manufacturing;pkm;remote monitoring;digital shopfloor	In this paper, a web-based collaborative framework called Wise-ShopFloor (Web-based integrated sensor-driven e-ShopFloor) is proposed. With appropriate open architecture for effective collaboration among a dispersed engineering team, the Wise-ShopFloor can serve real-time data from bottom up and can function as a constituent component of e-manufacturing for remote monitoring and control. This paper presents the basis of the framework for building web-based collaborative systems that can be used for distributed manufacturing. A proof-of-concept prototype system is developed on top of the framework to demonstrate one of its potential applications on shop floor monitoring and control.	distributed manufacturing;interaction;java 3d;multitier architecture;network traffic control;open architecture;prototype;real-time clock;real-time data;top-down and bottom-up design;web application	Dan Zhang;Lihui Wang;Terry Wu	2007	International Journal of Internet and Enterprise Management	10.1504/IJIEM.2007.011592	economics;service management;computer science;sensor;operations management;database;management;remote control;rmon;collaboration	DB	-55.13530027884099	12.83384163629555	213
239af6314e75cc98c7aa05e3f2505819f9e6d11d	knowledge-based system for structured document recognition	system architecture;control structure;knowledge base;document structure;knowledge based system	"""This paper discribes a document analysis system broadly consisting of a knowledge base, a blackboard and a set of tasks having their own set of spacialists for segmentation, recognition and for inheritance. The knowledge base contains a generic hierarchical description of the document structure in terms of layout objects labeled logically. This allows the generation of hypothetic networks of linked objects in the blackboard. The specialists cooperate indirectly through the blackboard by updating the layout object descriptors. GRAPHEIN is a general-purpose system that could deal effectively with a variety of document classes. It is able to organize and control the diverse document recognition p r e cesses in a flexible and efficient manner. Section 2 presents the classes of document structure adopted and the knowledge sources taken into account in the GRAPHEIN project. The system architecture and the control structure will be detailed respectively in sections 3 and 4. Finally, we conclude with a discussion on the opportunity of such an architecture and propose further improvements. A blackboard modification causes an """"event"""" to propagate up to some specific tasks. A task could then choose another 2 Document structures subset of specialists to carry on with the process. Finally, a synthesized blackboard summary allows a task selector to focus efficiently on the most useful layout object t o process."""	control flow;general-purpose modeling;knowledge base;knowledge-based systems;systems architecture;text segmentation	Abdel Belaïd;Jean-Jules Brault;Yannick Chenevoy	1990			document processing;mathematics;data mining;information retrieval;knowledge-based systems;document structure description;knowledge base;systems architecture;structured document;document management system;document clustering	AI	-36.19058312140509	-70.84656975899452	214
9c4742bfbded15a400d680ce9e854c3a5e0898c3	sub-sampling for multi-armed bandits	reinforcement learning;multi armed bandits;sub sampling	The stochastic multi-armed bandit problem is a popular model of the exploration/exploitation trade-off in sequential decision problems. We introduce a novel algorithm that is based on sub-sampling. Despite its simplicity, we show that the algorithm demonstrates excellent empirical performances against stateof-the-art algorithms, including Thompson sampling and KL-UCB. The algorithm is very flexible, it does need to know a set of reward distributions in advance nor the range of the rewards. It is not restricted to Bernoulli distributions and is also invariant under rescaling of the rewards. We provide a detailed experimental study comparing the algorithm to the state of the art, the main intuition that explains the striking results, and conclude with a finite-time regret analysis for this algorithm in the simplified two-arm bandit setting.	algorithm;bernoulli polynomials;decision problem;experiment;multi-armed bandit;need to know;performance;regret (decision theory);sampling (signal processing);thompson sampling	Akram Baransi;Odalric-Ambrym Maillard;Shie Mannor	2014		10.1007/978-3-662-44848-9_8	mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;thompson sampling;reinforcement learning	ML	23.688461996038697	-18.208243705162314	215
39d01b1bce28e4d7d7a4fffb4d496cba81e49adb	http-based smart transportation of dns queries and applications	dns;http;security;privacy;network	In this paper we introduce a system, called DJSON, which enables HTTP transport of Domain Name System traffic. DJSON enables re-encoding of the existing Domain Name System message format, so that it can traverse hostile territory with confidence without modifying the underlying Domain Name System design. In DJSON, Domain Name System messages are sent and received with a properly formatted HTML using a JSON encoding that allows bidirectional mapping to and from traditional Domain Name System transport encodings. This guarantees that interoperability is no worse than it is today. HTTP can be used to work around the problem where middle boxes have interoperability problems. DJSON aims to solve several real-world and operational problems. DJSON is designed to “bridge” Domain Name System across areas where Domain Name System packets might be mangled, deliberately modified or blocked. DJSON further aims to enable and address improved reliability, availability, and security. Detailed discussions, experiments run on a prototype of DJSON, and analysis show the effectiveness and relevance of our work.	experiment;html;hypertext transfer protocol;interoperability;json;prototype;relevance;traverse	Aziz Mohaisen;Manar Mohaisen	2015	Smart CR	10.6029/smartcr.2015.04.001	dns hijacking;root name server;hosts;resolv.conf;computer science;dns spoofing;internet privacy;zone file;name server;world wide web;computer security;nsupdate;domain name system;domain masking	Networks	-55.77785033588682	68.09139554524711	216
8616423b2172845ebecda729688a29ea15206504	network interface design for multi-gbit/s wdm optical networks	real time;gigabit networks;packet switching;optical networks;performance guarantees;network interface;atm	We report on the design of a distributed, small, fast packet switched network interface that provides electronic buffering and switching for the data channel in reconfigurable multi-Gbps WDM optical networks. The high-speed interface has been constructed and tested in one such WDM network, STARNET, which is based on a physical passive star topology and is intended for backbone applications in Campus Networks. A two-node experimental STARNET is being implemented at the Optical Communications Research Laboratory at Stanford University. The interface prototype was completed in September 1993; the printed circuit board version of the interface has been operational without modification since January 1994. We have since developed software from the driver level to the applications layer.	gigabit;network interface;wavelength-division multiplexing	Charles F. Barry;Sanjay K. Agrawal;Nina L. Taranenko;Ciro Aloisio Noronha;Leonid G. Kazovsky	1995	J. High Speed Networks	10.3233/JHS-1995-4105	real-time computing;telecommunications;computer science;network interface;atmosphere;packet switching;computer network	HPC	-20.874888780908158	90.7645931464523	217
80d9b82e0bc1b7ad3a55b4953a8b5009e9b71135	a bit collision detection based hybrid query tree protocol for anti-collision in rfid system	protocols;reliability;binary tree algorithm;mqt protocol;telecommunication congestion control;delay effects;rfid system binary tree algorithm bhqt mqt protocol;trees mathematics;acceleration;bhqt;trees mathematics mobile communication protocols radiofrequency identification telecommunication congestion control;protocols mobile communication radiofrequency identification algorithm design and analysis delay effects acceleration reliability;rfid system;mobile communication;algorithm design and analysis;bhqt protocol bit collision detection mechanism hybrid query tree protocol anti collision protocol rfid systems radio frequency identification aloha based protocols binary query tree algorithms stochastic timeslots qt based protocols collision avoidance tag identification mobile identification;radiofrequency identification	In radio frequency identification (RFID) systems, the anti-collision protocol is a key topic that has attracted a great deal of research interest. The protocols can be divided into two categories: ALOHA-based and binary query tree (QT) algorithms. The ALOHA-based protocols avoid collisions by distributing tags into different stochastic timeslots. In contrast, the QT-based protocols achieve a reliable throughput of identification and avoid collisions by using prefix matching. This paper proposes an improved QT protocol called BHQT (Bit collision detection based Hybrid Query Tree) which combining the basic principle of QT and a new bit collision detection mechanism for tag identification by a mo-bile reader. Analysis shows that the BHQT protocol reduces tag collisions, which accelerates the tag identification process in mobile identification. Simulation re¬sults substantiate the significant performance improvement by the BHQT protocol for tag identification.	algorithm;collision detection;qt (software);radio frequency;radio-frequency identification;simulation;tag (metadata);throughput;vii	Haosong Gou;Younghwan Yoo	2011	2011 IEEE 11th International Conference on Computer and Information Technology	10.1109/CIT.2011.77	acceleration;communications protocol;algorithm design;real-time computing;mobile telephony;computer science;operating system;reliability;distributed computing;statistics;computer network	Mobile	10.59000581262637	93.7048465664135	218
c656a917545f31bba1547e23cd96b23a129b50bc	image compression using an efficient edge cartoon + texture model	efficient edge cartoon model;image coding;computed tomography;data compression;cartoon;transform coding;image texture;wavelet transforms;wavelet coding;image compression;chromium;efficient texture model;edge cartoon;texture image coding;wavelet coding image compression efficient edge cartoon model efficient texture model wavelet based image coders smooth region representation two stage image coder edge information coding multiscale wedgelet decomposition texture image coding;wedgelets;smooth region representation;edge information coding;image coding chromium computed tomography data compression;wavelet based image coders;multiscale wedgelet decomposition;wavelet transforms image coding data compression transform coding image texture;wavelets;two stage image coder	"""Wavelet-based image coders optimally represent smooth regions and isolated point singularities. However, w av elet coders are less adept at representing perceptually important edge singularities, and coding performance su ers signi cantly as a result. In this paper, w epropose a no veltw o-stageimage coder framework based on modeling images as edge cartoons + textures. In stage 1, w einfer and eÆciently code the edge information from the image using a multiscale wedgelet decomposition. In stage 2, we code the residual, \edgeless"""" texture image using a standard w av elet coder. Our preliminary coder improves signi cantly o ver standard wavelet coding techniques in terms"""	data compression;dictionary coder;image compression;ver (command);wavelet transform	Michael B. Wakin;Justin K. Romberg;Hyeokho Choi;Richard G. Baraniuk	2002		10.1109/DCC.2002.999942	data compression;image texture;computer vision;chromium;transform coding;speech recognition;image compression;computer science;mathematics;computed tomography;statistics;computer graphics (images)	Vision	44.71612242953801	-14.778198642879564	219
94b8f7337221cc6c1ead7fb58918a1f71cf76bf5	passive keyless entry system for long term operation	intrabody communication;long term operation;automobiles;vehicles passive keyless entry system intrabody communication updating scheme authentication vulnerability eavesdropping shared secret extraction long term operation;radio equipment;cryptographic key updating;authentication vulnerability;shared secret extraction;wireless communication;artificial neural networks;system design;cryptographic key updating passive keyless entry system intrabody communication;cryptography;telecommunication security;vehicles;telecommunication security automobiles cryptography radio equipment;eavesdropping;updating scheme;passive keyless entry system	We propose a passive keyless entry system which applies intrabody communication and updating scheme. Recently authentication vulnerability of the system has been reported. And most of attacks eavesdrop on authentication to extract the shared secret. The passive keyless entry system must be safe at least the life time of the vehicle. Our system uses intrabody communication instead of the wireless communication against eavesdropping. Furthermore in case the shared secret is extracted, our system updates the shared secret frequently. In this paper we describe the system design and the demo system.	authentication;shared secret;systems design	Hisashi Oguma;Naoki Nobata;Kazunari Nawa;Tsutomu Mizota;Mitsuru Shinagawa	2011	2011 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks	10.1109/WoWMoM.2011.5986125	telecommunications;computer science;cryptography;computer security;artificial neural network;computer network	Arch	-49.02917556497585	74.04587083137469	220
9197d8ae4dca547306b63fb2bd0c8597b1508471	usage of dds data-centric middleware for remote monitoring and control laboratories	control engineering education;computer aided instruction;middleware laboratories java servers real time systems computer architecture synchronization;publish subscribe paradigm dds data centric middleware remote monitoring control laboratories communication middleware;remote monitoring and control dds middleware networked embedded systems remote laboratory;computer architecture;servers;remote laboratory;remote monitoring and control;synchronization;networked embedded systems;middleware;control engineering computing;dds;middleware computer aided instruction control engineering computing control engineering education;java;real time systems	Communication middleware technologies are powerful instruments to develop and deploy remote laboratories for students to practice the control of real application cases. Middleware allows the rapid development and deployment of actual distributed settings since it abstracts the specifics of the different hardware platforms and communication media involved in the access of students to the lab material. This paper describes the practical settings carried out in the a case of a successful laboratory experience that allows students to simulate the on-line monitoring and control of remote systems, i.e., the train traffic in a simulated metro system. The innovative contribution of this experiment is the usage of data-centric middleware based on the publish/subscribe paradigm that has real-time properties. Up to our knowledge, such deployment with this particular middleware type has not been done, and it has given rise to a flexible setting that grows with the contributions of students. We have validated the idea by gathering the feedback of students with respect to the operation of the assignment, showing that this motivates the students' learning and enrollment in the assignment.		Marisol García-Valls;Pablo Basanta-Val	2013	IEEE Trans. Industrial Informatics	10.1109/TII.2012.2211028	embedded system;synchronization;middleware;real-time computing;computer science;message oriented middleware;operating system;middleware;remote monitoring and control;distributed computing;java;server;computer network	Embedded	-34.20232693338822	46.065027364237615	221
69675a71bb9b4d6ba92c44f7cd17b006071a9631	recalage et fusion d'images sonar multivues : critère de dissimilarité et ignorance		Résumé. Ce papier présente une application pour le recalage et la fusion d’images sonar classifiées. Nous adaptons ici la méthode présentée dans un précédent papier à des données multivues. Pour la caractérisation de fond marin, nous avons besoin de fusionner des images sonar multivues afin d’améliorer les résultats. Néanmoins, avant de pouvoir fusionner ces images, il faut les recaler. Notre approche de recalage s’appuie sur un critère de dissimilarité calculé à partir du conflit issu de la combinaison des fonctions de croyance. L’utilisation de la théorie des fonctions de croyance offre un cadre théorique adequat qui permet une bonne modélisation des imperfections, et qui a déjà prouvé son intérêt pour la fusion de classifieurs en traitement d’images.	bibliothèque de l'école des chartes;déjà vu;image registration;linear algebra;performance;sonar (symantec)	Cedric Rominger;Arnaud Martin	2011			computer vision;sonar;ignorance;fusion;image registration;artificial intelligence;computer science	Crypto	-104.97606124508279	11.97572579247738	222
3494171fe21ff0f752c87400bb4b6622a7e4007d	mirageprinter: interactive fabrication on a 3d printer with a mid-air display		The rapid proliferation of digital fabrication machines has resulted in creating an environment that enables more people to make various creations. From a viewpoint of Human Computer Interaction, it is often pointed out that interfaces bridging between works in the digital environment and the physical environment are necessary to support design for personal fabrication [WILLIS 2011] [WEICHEL 2014].	3d printing;bridging (networking);digital environment;digital modeling and fabrication;human computer;human–computer interaction;printer (computing)	Junichi Yamaoka;Yasuaki Kakehi	2016		10.1145/2929484.2929489	multimedia;computer graphics (images)	HCI	-45.24075646361014	-38.041507901708876	223
fe5a917cbb3d5094846b3f75903785aac91f5654	development of engineering education in bahrain	bahrain engineering education;service industry;engineering education;private sector;engineering education companies government law force control refining petroleum industry investments aluminum legal factors;social status;bahrain	Engineering education in Bahrain has evolved along with the country's political and economic shift from British control to national sovereignty. Beginning with UK-based craft and technician studies, engineering education evolved into a two-track US-influenced system, including diplomas for technicians and B.Sc. degrees for engineers. Currently motivated by higher salaries and social status, students prefer graduating as engineers even though the demand for technicians is higher. The presence of foreign nationals in engineering, mainly from India and Egypt remains significant, especially in the private sector. Engineers now work in the service industry as sales engineers as well as in management. Some engineers also have converted to other disciplines such as finance, marketing, and insurance where their skills are much sought after by the employers. In short, the evolution of engineering education and practice in Bahrain continues to follow the country's political and economic position in a very complex region of the world	diploma	A-Imam Al-Sammak;Hisham Al-Shehabi	2006	IEEE Technology and Society Magazine	10.1109/MTAS.2006.1649023	tertiary sector of the economy;engineering management;engineering education;social status;engineering;civil engineering;management;law;private sector	DB	-72.34846873167128	-4.508391833866128	224
a836f110fc7afcfe46561d6b34c44b298bb15445	correlational analysis of complex systems	complex system		complex systems	Burton Voorhees	1999	Cybernetics and Human Knowing		cognitive psychology;cognitive science;complex system;complex system;psychology	Robotics	-25.34247380701918	-15.39525528867246	225
21adfa693395c2aa502942809b9fe6d622494e56	a tree-based algorithm for mining diverse social entities	data mining;social network analysis	DiSE-growth, a tree-based (pattern-growth) algorithm for mining DIverse Social Entities, is proposed and experimentally assessed in this paper. The algorithm makes use of a specialized data structure, called DiSE-tree, for effectively and efficiently representing relevant information on diverse social entities while successfully supporting the mining phase. Diverse entities are popular in a wide spectrum of application scenarios, ranging from linked Web data to Semantic Web and social networks. In all these application scenarios, it has become important to analyze high volumes of valuable linked data and discover those diverse social entities. We complement our analytical contributions by means of an experimental evaluation that clearly shows the benefits of our tree-based diverse social entity mining algorithm. c © 2014 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	abstract syntax tree;algorithm;computation;data structure;entity;experiment;linked data;maxima and minima;real life;scalability;semantic web;social network;sparse matrix;tree structure	Peter Braun;Alfredo Cuzzocrea;Carson Kai-Sang Leung;Richard Kyle MacKinnon;Syed Khairuzzaman Tanbeer	2014		10.1016/j.procs.2014.08.102	computer science;data science;data mining;world wide web	ML	-26.796845891951484	-57.458802135196166	226
51b8fe1d2efc82ddace05d97407432926bb5a70f	electronic music, vol. iii, msica viva competition prize winners 200420052006: adrian moore, joshua goldman, panayiotis kokoras, pedro almeida, santiago dez fischer, ingrid obled, manuella blackburn, thomas peter			d-grid;michael j. fischer;tridiagonal matrix algorithm	Ian Whalley	2009	Computer Music Journal	10.1162/comj.2009.33.4.91		ML	-54.820717936092606	-14.574043599810867	227
49e985896f6c1ff70487d5812994b285547db000	pricing currency option based on the extension principle and defuzzification via weighting parameter identification	期刊论文	We present a fuzzy version of theGarman-Kohlhagen (FG-K) formula for pricing European currency option based on the extension principle. In order to keep consistent with the real market, we assume that the interest rate, the spot exchange rate, and the volatility are fuzzy numbers in the FG-K formula. The conditions of a basic proposition about the fuzzy-valued functions of fuzzy subsets are modified. Based on the modified conditions and the extension principle, we prove that the fuzzy price obtained from the FG-K formula for European currency option is a fuzzy number. To simplify the trade, the weighted possibilistic mean (WPM) value with a weighting function is adopted to defuzzify the fuzzy price to a crisp price. The numerical example shows our method makes the α-level set of fuzzy price smaller, which decreases the fuzziness. The example also indicates that the WPM value has different approximation effects to real market price by taking different values of weighting parameter in the weighting function. Inspired by this example, we provide a method, which can identify the optimal parameter.		Jixiang Xu;Yanhua Tan;Jinggui Gao;Enmin Feng	2013	J. Applied Mathematics	10.1155/2013/623945	mathematical optimization;defuzzification;type-2 fuzzy sets and systems;fuzzy number;mathematics;mathematical economics	AI	-1.6290276068745935	-18.915255565461962	228
ae585b9564f2b4eeef880522c84400f652e1dff6	an adaptive network coding scheme for unreliable multi-hop wireless networks	wireless networks;network coding encoding throughput relays toxicology wireless networks;multi hop wireless network;toxicology;unreliable wireless link network coding multi hop wireless network;network coding;cope adaptive network coding scheme multihop wireless network anc mechanism network reliability neighbors overhearing link quality measurement nolm overhearing channel qualnet;wireless mesh networks adaptive codes network coding radio networks relay networks telecommunication telecommunication network reliability;unreliable wireless link;relays;encoding;throughput	In this paper, we propose the Adaptive Network Coding (ANC) mechanism for multi-hop wireless networks. The ANC mechanism takes an adaptive approach to the network coding-based forwarding and focuses on improving the robustness and the reliability of the network in the unreliable wireless environment. This paper introduces the Neighbor's Overhearing Link quality Measurement (NOLM) which allows the intermediate node to measure the quality of the overhearing channel of each of its one-hop neighbors in an indirect manner. The performance of our proposed mechanism is evaluated using the network simulator QualNet. The simulation results show that our ANC provides an average throughput gain of 56% in the fixed topology over the existing COPE.	exclusive or;linear network coding;lossy compression;overhead (computing);simulation;throughput	Pedram Khayyat Khoshnevis;Sang Hyun Ahn;Hayoung Oh	2016	2016 International Conference on Big Data and Smart Computing (BigComp)	10.1109/BIGCOMP.2016.7425932	wireless mesh network;wireless ad hoc network;linear network coding;wireless wan;heterogeneous network;telecommunications;engineering;radio resource management;wireless network;maximum throughput scheduling;distributed computing;key distribution in wireless sensor networks;wi-fi array;computer network	Mobile	7.2009740181220305	85.73494728739972	229
42dae3844ecd7a1af0faddc6d65b0462f39544f4	a new approach to dynamic bandwidth allocation in quality of service networks: performance and bounds	teletrafic;evaluation performance;dynamic bandwidth allocation;funcion utilidad;performance evaluation;modele mathematique;ingegneria industriale e dell informazione;resource allocation;implementation;fonction utilite;bandwidth allocation;evaluacion prestacion;exigence usager;simulation;resource management;exigencia usuario;utility function;network performance;simulacion;modelo matematico;satisfiability;guaranteed rate;qualite service;algorithme;upper bound;service model;algorithm;gestion recursos;teletrafico;dynamic allocation;mathematical models;user requirement;teletraffic;mathematical model;gestion ressources;asignacion recurso;asignacion dinamica;quality of service;allocation ressource;implementacion;borne superieure;allocation dynamique;quality of ser vice;service quality;allocation bande passante;cota superior;calidad servicio;algoritmo	Efficient dynamic resource provisioning algorithms are necessary to the development and automation of Quality of Service (QoS) networks. The main goal of these algorithms is to offer services that satisfy the QoS requirements of individual users while guaranteeing at the same time an efficient utilization of network resources. In this paper we introduce a new service model that provides per-flow bandwidth guarantees, where users subscribe for a guaranteed rate; moreover, the network periodically individuates unused bandwidth and proposes short-term contracts where extra-bandwidth is allocated and guaranteed exclusively to users who can exploit it to transmit at a rate higher than their subscribed rate. To implement this service model we propose a dynamic provisioning architecture for intra-domain Quality of Service networks. We develop a set of dynamic on-line bandwidth allocation algorithms that take explicitly into account traffic statistics and users’ utility functions to increase users’ benefit and network revenue. Further, we propose a mathematical formulation of the extra-bandwidth allocation problem that maximizes network revenue. The solution of this model allows to obtain an upper bound on the performance achievable by any on-line bandwidth allocation algorithm. We demonstrate through simulation in realistic network scenarios that the proposed dynamic allocation algorithms are superior to static provisioning in providing resource allocation both in terms of total accepted load and network revenue, and they approach, in several network scenarios, the ideal performance provided by the mathematical model. 2006 Elsevier B.V. All rights reserved.	algorithm;mathematical model;memory management;online and offline;overselling;provisioning;quality of service;requirement;simulation;static random-access memory;web traffic	Jocelyne Elias;Fabio Martignon;Antonio Capone;Guy Pujolle	2007	Computer Networks	10.1016/j.comnet.2006.12.003	simulation;telecommunications;computer science;resource management;mathematical model;dynamic bandwidth allocation;computer network	Networks	0.7427854284324267	102.00619348232613	230
35cf4e2bae2cd1dee525297133edb60035078b82	real-time signal frequency analysis in variable speed drives using the sparse fast fourier transform (sfft)		This paper investigates the implementation and the use of the sparse fast Fourier transform algorithm in the converter control of a variable speed drive. The algorithm is proposed due to the reduction in computational complexity compared to the conventional fast Fourier transform for the special case of sparse signals. After discussing the theory and a simulation model, experimental results obtained using a field programmable gate array (FPGA) implementation are presented, showing the effectiveness of the proposed solution.	algorithm;computational complexity theory;fast fourier transform;field-programmable gate array;frequency analysis;simulation;sparse matrix;unix signal	Mehanathan Pathmanathan;Luca Peretti	2016	IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2016.7792931	electronic engineering;parallel computing;speech recognition;computer science	Robotics	59.051358336432	21.914854898349297	231
35d78f0bc9b9de5491533e3b8b7278ee61f819bc	semantics of links and document structure discovery	document structure	This paper presents a novel algorithm to discover the hierarchical document structure by classifying the links between the document pages. This link classi cation adds metadata to the links that can be expressed using Resource Description Framework Syntax [7]. Several well-known programs automatically generate HTML web pages from di erent document formats such as LaTeX, Powerpoint, Word, etc. Our interest is in the intertwined HTML web pages generated by the LaTeX2HTML program [6]. We use the web robot of the WWWPal System [11] to save the structure of the web document in a webgraph. Then the web analyzer of the system applies our algorithm to discover the semantics of the links and infer the hierarchical structure of the document.	algorithm;html;latex;microsoft word for mac;resource description framework;web crawler;web page;webgraph	John R. Punin;Mukkai S. Krishnamoorthy	2002			well-formed document;static web page;computer science;document structure description;document type declaration;web page;data mining;database;world wide web;information retrieval;design document listing	Web+IR	-35.00723571007047	6.594436383626475	232
9e2ce478712ef0b6d9b7215cea0e61d6bfd22bac	proper noun detection in document images	nom propre;characteristic;image processing;noun;deteccion;lettre capitale;procesamiento imagen;character;caracteristica;detection;classification;traitement image;extraccion;caractere;palabra;word recognition;pattern recognition;caracteristique;caracter;word;reconnaissance forme;reconocimiento patron;character recognition;clasificacion;extraction;mot	Abstract   An algorithm for the detection of proper nouns in document images printed in mixed upper and lower case is presented. Analysis of graphical features of words in a running text is performed to determine words that are likely to be names of specific persons, places, or objects (i.e. proper nouns). This algorithm is a useful addition to contextual post-processing (CPP) or whole word recognition techniques where word images are matched to entries in a dictionary. Due to the difficulty of creating a comprehensive list of proper nouns, a methodology of locating such words prior to recognition will allow for the use of specialized recognition strategies for those words only. Experimental results demonstrate that about 90% of all occurrences of proper nouns were located and over 97% of the unique proper nouns in a document were found using this algorithm.		Ginige L. De Silva;Jonathan J. Hull	1994	Pattern Recognition	10.1016/0031-3203(94)90062-0	natural language processing;noun;extraction;speech recognition;image processing;biological classification;word recognition;computer science;word;characteristic;character	Vision	34.873648751785716	-67.28205646045272	233
061dcaa0e28c09e2016d73ca1fe770ab8b6239f5	reversible anonymization of dicom images using automatically generated policies.	automatic generation	Many real-world applications in the area of medical imaging like case study databases require separation of identifying (IDATA) and non-identifying (MDATA) data, specifically those offering Internet-based data access. These kinds of projects also must provide a role-based access system, controlling, how patient data must be organized and how it can be accessed. On DICOM image level, different image types support different kind of information, intermixing IDATA and MDATA in a single object. To separate them, it is possible to reversibly anonymize DICOM objects by substituting IDATA by a unique anonymous token. In case that later an authenticated user needs full access to an image, this token can be used for re-linking formerly separated IDATA and MDATA, thus resulting in a dynamically generated, exact copy of the original image. The approach described in this paper is based on the automatic generation of anonymization policies from the DICOM standard text, providing specific support for all kinds of DICOM images. The policies are executed by a newly developed framework based on the DICOM toolkit DCMTK and offer a reliable approach to reversible anonymization. The implementation is evaluated in a German BMBF-supported expert network in the area of skeletal dysplasias, SKELNET, but may generally be applicable to related projects, enormously improving quality and integrity of diagnostics in a field focused on images. It performs effectively and efficiently on real-world test images from the project and other kind of DICOM images.	authentication;dicom;data access;data anonymization;database;execution;expert network;medical imaging;physical object;policy;abnormal cellular structure or growth	Michael Onken;Jörg Riesmeier;Marcel Engel;Adem Yabanci;Bernhard Zabel;Stefan Després	2009	Studies in health technology and informatics	10.3233/978-1-60750-044-5-861	computer science;data mining;database;dicom;world wide web	ML	-50.63090262297958	-63.01169605119066	234
a3d8af5b56d28963d58386f2c49829f033cf78c0	energizing or depleting? understanding the effects of agile methodologies on individual software developers' resources				Lea Fortmann-Müller	2018				SE	-64.86641812480342	22.868590957856846	235
5ce44ef4c3cee1c852e83f95492b5d76a3afb155	support vector machine for 3d modelling from sparse geological information of various origins	computadora;tratamiento datos;computers;geologic maps;experimental studies;systeme information geographique;three dimensional models;modelo 3 dimensiones;north america;america del norte;amerique du nord;ordinateur;etude experimentale;3d modelling;modele 3 dimensions;gocad;coupe geologique;canada este;data processing;traitement donnee;three dimensional;algorithme;canada est;interpretacion;canada;geographic information systems;visual analysis;interpretation;algorithms;quebec;svm;geologic sections;support vector machine;corte geologico;geological reconstruction;carte geologique;eastern canada;mapa geologico;algoritmo	Three-dimensional (3D) geological models are a powerful way of visualization, analysis and interpretation of geological information. However, manual modelling with available GIS tools is a challenging and time-consuming task. Here we propose the use of the support vector machine (SVM) in order to automate the creation of such models. We experiment with various input data and hyperparameters in order to demonstrate that the SVM can be efficiently applied in 3D geological reconstructions overcoming some limitations of previously used methods.	3d modeling;sparse matrix;support vector machine	Alex Smirnoff;Eric Boisvert;Serge J. Paradis	2008	Computers & Geosciences	10.1016/j.cageo.2006.12.008	support vector machine;data processing;computer science;machine learning;operations research	AI	77.22274129235609	-54.452502662035215	236
e6ecb3d6861349bf9e726a0134ba42097f4bbf5c	silicon-micromachined gas chromatographic columns for the development of portable detection device		We report the fabrication of a gas chromatographic column module integrated on a silicon substrate and usable as a portable measurement device dedicated to the selective detection of various chemical compounds (gas or vapour). PDMS, PEG, and F13-TEOS stationary phases have been prepared in order to coat the inside walls of microchannels. The microcolumn tests were performed with a mixture of hydrocarbons and ketone. After having evaluated the effectiveness of such a separation module, we showed an application by coupling a GC microcolumn with a metal oxide-based gas sensor. The best results were obtained at a low isothermal temperature mode of the GC micro-column (near the ambient temperature). The coupling between the GC microcolumn and a metal oxide gas sensor enables to obtain a rapid, reliable, and selective analysis of various chemical compounds.	columns	Jean-Baptiste Sanchez;Aline Schmitt;Franck Berger;Christophe Mavon	2010	J. Sensors	10.1155/2010/409687	chromatography;chemistry;analytical chemistry;organic chemistry	Robotics	92.82687314160916	-16.616515210480692	237
1446f98f3125768d21bed0f8b9e8d53112445327	resco: a middleware component for reliable service composition in pervasive systems	databases;middleware open systems cameras microphones cryptography laboratories computer science reliability engineering adaptive systems availability;computers;adaptive systems dynamic service composition reliability security;nodes;reliability;data transmission security;high level application services;service composition;general and miscellaneous mathematics computing and information science;service composition reliability;computer network security;reliable service composition middleware component;information security;adaptive middleware component;trustworthy compositions;dynamic system;pervasive system;satisfiability;network topology;symposia;mathematical methods and computing;adaptive systems;lead;r codes;resco;adaptive system;data processing security;open system;communications networks;algorithms;service sector;ubiquitous computing;middleware;communications protocols;electronic security;trust establishment mechanism resco pervasive system high level application services dynamic system open system reliable service composition middleware component trustworthy compositions adaptive middleware component service composition reliability;pervasive systems;open systems;security;trust establishment mechanism;software reliability;security of data;dynamic service composition;ubiquitous computing middleware open systems security of data software reliability	Service composition schemes create high-level application services by combining several basic services. Service composition schemes for dynamic, open systems, such as those found in pervasive environments, must be cognizant of the possibility of failures and attacks. In open systems, it is seldom feasible to guarantee the reliability of each node prior to access; however, there may be several possible ways to compose the same high-level service, each having a different (though possibly overlapping) set of nodes that can satisfy the composition. We approach this problem with a Reliable Service Composition middleware component, ReSCo, to determine trustworthy compositions and nodes for service composition in dynamic, open systems. ReSCo is a modular, adaptive middleware component that selects from possible composition paths and nodes to enhance reliability of service compositions. ReSCo can work with a broad range of both service composition algorithms and trust establishment mechanisms.	algorithm;entity;high- and low-level;middleware;open system (computing);personal digital assistant;pervasive informatics;service composability principle;simulation;ubiquitous computing;while	Brent Lagesse;Mohan Kumar;Matthew K. Wright	2010	2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PERCOMW.2010.5470620	embedded system;computer science;information security;adaptive system;operating system;database;distributed computing;open system;computer security;computer network	Robotics	-44.3644868536153	54.38649105281025	238
ba54bb6caa6cac90ab2fc13c5cf601817926a97e	high-performance, space-efficient, automated object locking	persistent objects;programming language;reduced instruction set computing;software performance evaluation;computer languages virtual machining concurrency control data structures java relational databases transaction databases sun laboratories handicapped aids;shared memory systems;risc instructions high performance space efficient automated object locking lock manager designs overhead persistent programming language lock state sharing lock management methods lock data structures shared data structure transactions bookkeeping memoization techniques lock release lock request processing;concurrency control;persistent objects object oriented databases concurrency control software performance evaluation reduced instruction set computing shared memory systems;object oriented databases;high performance;data structure	The paper studies the impact of several lock manager designs on the overhead imposed to a persistent programming language by automated object locking. Our study reveals that a lock management method based on lock state sharing outperforms more traditional lock management designs. Lock state sharing is a novel lock management method that represents all lock data structures with equal values with a single shared data structure. Sharing the value of locks has numerous benefits: (i) it makes the space consumed by the lock manager small and independent of the number of locks acquired by transactions, (ii) it eliminates the need for expensive book-keeping of locks by transactions, and (iii) it enables the use of memoization techniques for whole locking operations. These advantages add up to make the release of locks practically free, and the processing of over 99% of lock requests between 8 to 14 RISC instructions.	compiler;concurrent data structure;experiment;image scaling;java development kit (jdk);just-in-time compilation;lock (computer science);memoization;overhead (computing);persistent programming language;program analysis;sparc;ultrasparc;vendor lock-in;virtual machine;working set	Laurent Daynès;Grzegorz Czajkowski	2001		10.1109/ICDE.2001.914825	double-checked locking;giant lock;lock;lock;reduced instruction set computing;parallel computing;real-time computing;data structure;ticket lock;distributed lock manager;computer science;concurrency control;readers–writer lock;database;distributed computing;programming language;lock convoy	DB	-14.711180040755202	48.674728593088346	239
c34d87a82f15e26e663bc988cbb6ae7de981526a	machine learning techniques for taming the complexity of modern hardware design		The continual quest to improve performance and efficiency for new generations of IBM servers leads to a corresponding increase in system complexity. As hardware complexity increases, i.e., more complicated hardware architectures requiring more design choices, the level of sophistication in automation also increases to manage the design challenges. The number of design choices in modern hardware design calls for intelligent automated techniques to navigate the design space. This paper covers three machine learning-based automation techniques used during the design and lifetime of IBM systems. In particular, we describe applying these techniques to the IBM z13 mainframe. During the presilicon design phase, a software system called synthesis tuning system is employed to optimize the parameters of the synthesis program vital to hardware implementation. During both the presilicon and postsilicon phases of the design, a framework called MicroProbe automatically generates microbenchmarks, i.e., small programs, to determine power, performance, and resilience characteristics of the system. Following system product deployment in customer environments, the Call Home facility monitors and analyzes a wide range of in-field usage metrics to help administrators understand current system behavior and improve future designs. Beyond existing IBM system contributions, this high-level overview paper also describes additional machine learning (and related) techniques in the field of hardware design, along with future directions for such work.	machine learning	Matthew M. Ziegler;Ramon Bertran Monfort;Alper Buyuktosunoglu;Pradip Bose	2017	IBM Journal of Research and Development		software system;automation;software deployment;sophistication;psychological resilience;ibm;computer science;musical tuning;computer hardware;machine learning;server;artificial intelligence	Theory	-24.722638223549026	56.044828928609576	240
532150edf4905a2f34a71f90731b64bb74f1598c	existence conditions for coons patches interpolating geodesic boundary curves	concepcion asistida;computer aided design;hermite interpolation;geodesic structure;ajustamiento curva;interpolation hermite;polynomial interpolation;estructura geodesica;courbure;compatibilidad;surface reconstruction;satisfiability;constraint satisfaction;geodesique;satisfaction contrainte;reconstruction surface;interpolacion hermite;geodesic;smoothing;structure geodesique;geodesico;hermite coons interpolation;surface smoothing;compatibility;alisamiento;conception assistee;curvatura;ajustement courbe;curvature;compatibilite;geodesic curves;interpolacion polinomial;satisfaccion restriccion;reconstruccion superficie;curve fitting;lissage;interpolation polynomiale	Given two pairs of regular space curves r1(u), r3(u) and r2(v), r4(v) that define a curvilinear rectangle, we consider the problem of constructing a C^2 surface patch R(u,v) for which these four boundary curves correspond to geodesics of the surface. The possibility of constructing such a surface patch is shown to depend on the given boundary curves satisfying two types of consistency constraints. The first constraint is global in nature, and is concerned with compatibility of the variation of the principal normals along the four curves with the normal to an oriented surface. The second constraint is a local differential condition, relating the curvatures and torsions of the curves meeting at each of the four patch corners to the angle between those curves. For curves satisfying these constraints, the surface patch is constructed using a bicubically-blended Coons interpolation process.	bézier curve;contingency (philosophy);coons patch;interpolation;polynomial;steven anson coons	Rida T. Farouki;Nicolas Szafran;Luc Biard	2009	Computer Aided Geometric Design	10.1016/j.cagd.2009.01.003	mathematical analysis;geodesic;topology;surface reconstruction;constraint satisfaction;polynomial interpolation;hermite interpolation;computer aided design;mathematics;geometry;family of curves;curvature;compatibility;statistics;smoothing;curve fitting;algebra;satisfiability	Graphics	68.40574025389529	-40.23522061336732	241
8627306651670df4666fb968da399e54ff2128a6	analysis of a combined cg1-dg2 method for the transport equation	65n30;a priori error estimates;finite element discretization;65n50;reduced discontinuous galerkin method;65n15;convective transport;65n12	In this paper, we introduce a reduced discontinuous Galerkin method in which the space of continuous piecewise-linear functions (CG1) is enriched with discontinuous piecewise-quadratics (DG2). The resultant finite element approximation is continuous at the vertices of the mesh and discontinuous across edges/faces. We analyze the properties of the CG1-DG2 discretization in the context of a steady linear transport equation. The presented a priori error estimate shows that the discontinuous enrichment stabilizes the continuous coarse-scale component and delivers optimal convergence rates. Numerical studies for steady and unsteady convection problems confirm this result.		Roland Becker;Melanie Bittl;Dmitri Kuzmin	2015	SIAM J. Numerical Analysis	10.1137/13093683X	discretization error;mathematical optimization;mathematical analysis;discontinuous galerkin method;calculus;mathematics	Theory	89.20924111594795	12.020377086430598	242
5b0dc5a7b20cdca36af6b61fc604d771347acc0c	locally non-linear embeddings for extreme multi-label learning		The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches make training and prediction tractable by assuming that the training label matrix is low-rank and hence the effective number of labels can be reduced by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies or scale to large problems as the low rank assumption is violated in most real world applications. This paper develops the X1 classifier to address both limitations. The main technical contribution in X1 is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows X1 to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that X1 can make significantly more accurate predictions then the state-of-the-art methods including both embeddings (by as much as 35%) as well as trees (by as much as 6%). X1 can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.	benchmark (computing);cobham's thesis;data point;experiment;multi-label classification;multiclass classification	Kush Bhatia;Himanshu Jain;Purushottam Kar;Prateek Jain;Manik Varma	2015	CoRR		combinatorics;machine learning;pattern recognition;mathematics;statistics	ML	21.68203301193454	-43.138010050586374	243
ab4ef600953f0062a84be6e11404dc5e9d1d4d41	contrast pattern mining with gap constraints for peptide folding prediction	pattern mining	1 In this paper, we propose a peptide folding prediction method which discovers contrast patterns to differentiate and predict peptide folding classes. A contrast pattern is defined as a set of sequentially associated amino acids which frequently appear in one type of folding but significantly infrequent in other folding classes. Our hypothesis is that each type of peptide folding has its unique interaction patterns among peptide residues (amino acids). The role of contrast patterns is to act as signatures or features for prediction of a peptide’s folding type. For this purpose, we propose a two phase peptide folding prediction framework, where the first stage is to discover contrast patterns from different types of contrast datasets, followed by a learning process which uses all discovered patterns as features to build a supervised classifier for folding prediction. Experimental results on two benchmark protein datasets will indicate that the proposed framework can outperform simple secondary structure prediction based approaches for peptide folding prediction.	algorithm;benchmark (computing);data mining;electronic signature;gene prediction;interaction;machine learning;mined;protein structure prediction	Chinar C. Shah;Xingquan Zhu;Taghi M. Khoshgoftaar;Justin Beyer	2008			computer science;bioinformatics;data science;machine learning	ML	9.451537886971787	-55.31251552453857	244
221351788e6f54a5ab8650bef5f061acd1dcc994	big data applications using workflows for data parallel computing	parallel programming big data bioinformatics;big data workflow;next generation sequencing data analysis big data applications workflow systems data parallel computing techniques data analytics easy to use scalable approach actor oriented modeling bioinformatics use cases;data storage systems;data analysis;engines;information management;scientific computing big data workflow actor oriented programming data parallelization bioinformatics application scientific programming;distributed databases;scientific computing;actor oriented programming;scientific programming;terrestrial atmosphere;big data information management data handling data storage systems programming distributed databases parallel processing;data handling;data parallelization;programming;parallel processing;bioinformatics application;bioinformatics	In the Big Data era, workflow systems need to embrace data parallel computing techniques for efficient data analysis and analytics. Here, the authors present an easy-to-use, scalable approach to build and execute Big Data applications using actor-oriented modeling in data parallel computing. They use two bioinformatics use cases for next-generation sequencing data analysis to verify the feasibility of their approach.	big data;bioinformatics;data parallelism;parallel computing;scalability	Jianwu Wang;Daniel Crawl;Ilkay Altintas;Weizhong Li	2014	Computing in Science & Engineering	10.1109/MCSE.2014.50	programming with big data in r;computational science;parallel processing;programming;computer science;data science;group method of data handling;data-intensive computing;data mining;database;information management;data analysis;management	HPC	-40.386560203304974	-0.9693895051506685	245
05d0c649163dcdb645b9f38b09a830e71c739094	a new identification scheme based on the perceptrons problem	smart card;syndrome decoding;linear equations;point of view;zero knowledge;np complete problem	Identification is a useful cryptographic tool. Since zero-knowledge theory appeared [3], several interactive identification schemes have been proposed (in particular Fiat-Shamir [2] and its variants [4, 6, 5], Schnorr [9]). These identifications are based on number theoretical problems. More recently, new schemes appeared with the peculiarity that they are more efficient from the computational point of view and that their security is based on NP-complete problems: PKP (Permuted Kernels Problem) [10], SD (Syndrome Decoding) [12] and CLE (Constrained Linear Equations) [13]. We present a new NP-complete linear problem which comes from learning machines: the Perceptrons Problem. We have some constraints, m vectors X i of {−1, +1}, and we want to find a vector V of {−1, +1} such that X · V ≥ 0 for all i. Next, we provide some zero-knowledge interactive identification protocols based on this problem, with an evaluation of their security. Eventually, those protocols are well suited for smart card applications.	computation;conformal loop ensemble;constraint (mathematics);cryptography;decoding methods;identification scheme;karp's 21 np-complete problems;like button;linear equation;linear least squares (mathematics);linear programming;np-completeness;perceptron;smart card;zero-knowledge proof	David Pointcheval	1995		10.1007/3-540-49264-X_26	smart card;combinatorics;np-complete;computer science;theoretical computer science;mathematics;linear equation;computer security;algorithm;zero-knowledge proof;statistics;algebra	Crypto	-38.893313555985365	80.00652822576816	246
8eb497ecb3f4739a2f5177052b6cc31dca100d69	nbdt: an efficient p2p indexing scheme for web service discovery	busqueda informacion;p2p indexing;web service discovery;information retrieval;peer to;service web;p2p;p2p computing;web service;peer networks;intercambio electronico de datos;distributed data structures;indexing;recherche information;indexation;web services;indizacion;echange donnee informatise;information system;service discovery;peer to peer networks;systeme information;electronic data interchange;servicio web;sistema informacion	In this paper, we propose a new infrastructure for web services discovery in P2P networks, the Nested Balanced Distributed Tree (NBDT). Peers that store web services information, such as data item descriptions, are efficiently located using a scalable and robust data indexing structure based on a NBDT. The key innovation is that the solution is based on a totally new infrastructure, which is not vulnerable to classic disadvantages. We present a theoretical analysis backed up by experimental results, which shows that the communication cost of query and update operations scale sublogarithmically in the worst case with the number of NBDT nodes, outperforming three of the most popular decentralised infrastructures: Chord (and some of its successors), BATON (and its successor) and Skip Graphs. Furthermore, we show that the network is robust to failures providing quality of web services requirements.	peer-to-peer;service discovery;web service	Spyros Sioutas	2008	Int. J. Web Eng. Technol.	10.1504/IJWET.2008.016106	web service;computer science;operating system;data mining;database;law;world wide web	Web+IR	-29.337513383319187	0.5640759399206301	247
69fd5441025a83d5a6dffab18810a7b797862850	categorizing users in behavior change support systems based on cognitive dissonance	cognitive dissonance;behavior change support systems;endnotes;behavior change;persuasive technology;persuasive systems design;pubications	Most developers of behavior change support systems (BCSS) employ ad hoc procedures in their designs. This paper presents a novel discussion concerning how analyzing the relationship between attitude toward target behavior, current behavior, and attitude toward change or maintaining behavior can facilitate the design of BCSS. We describe the three-dimensional relationships between attitude and behavior (3D-RAB) model and demonstrate how it can be used to categorize users, based on variations in levels of cognitive dissonance. The proposed model seeks to provide a method for analyzing the user context on the persuasive systems design model, and it is evaluated using existing BCSS. We identified that although designers seem to address the various cognitive states, this is not done purposefully, or in a methodical fashion, which implies that many existing applications are targeting users not considered at the design phase. As a result of this work, it is suggested that designers apply the 3D-RAB model in order to design solutions for targeted users.	3d film;categorization;hoc (programming language);persuasive technology;requirement;systems design	Isaac Wiafe;Keiichi Nakata;Stephen R. Gulliver	2014	Personal and Ubiquitous Computing	10.1007/s00779-014-0782-3	simulation;behavior change methods;human–computer interaction;knowledge management;persuasive technology;behavior change;cognitive dissonance	HCI	-59.99898751543838	-40.05674200424434	248
66aa346ba1a248cfca257b2f6e25909d5cca1291	odra: a next generation object-oriented environment for rapid database application development	application development;virtual machine;query language;programming language;distributed computing;object database;development tool;rapid application development;object oriented;next generation	ODRA (Object Database for Rapid Application development) is an object-oriented application development environment currently being constructed at the Polish-Japanese Institute of Information Technology. The aim of the project is to design a next-generation development tool for future database application programmers. The tool is based on the query language SBQL (Stack-Based Query Language), a new, powerful and high level object-oriented programming language tightly coupled with query capabilities. The SBQL execution environment consists of a virtual machine, a main memory DBMS and an infrastructure supporting distributed computing. The paper presents design goals of ODRA, its fundamental mechanisms and some relationships with other solutions.	actionscript;communications protocol;computer data storage;distributed computing;experiment;high- and low-level;high-level programming language;interoperability;mathematical optimization;odra (computer);programmer;programming tool;prototype;query language;rapid application development;runtime system;software development kit;stack-oriented programming language;test case;virtual machine	Michal Lentner;Kazimierz Subieta	2004		10.1007/978-3-540-75185-4_11	data control language;computer science;theoretical computer science;database;rdf query language;programming language;rapid application development	DB	-30.470534585036734	41.8841664039604	249
55236223cfcb64a3989f733e3a9adbe3f262a307	dynamic resource allocation of smart home workloads in the cloud	resource management;servers;time factors;quality of service;queueing analysis;smart homes;cloud computing	Cloud computing offers provision for elastic and scalable infrastructure resource allocation across the network that allows deployment of services for controlling home devices and appliances. Data generated from heterogeneous smart home devices are processed in different application services deployed in the cloud data center. The primary challenge of smart home service provider's is to optimize the cloud resource allocation while satisfying the Quality of Service(QoS) constraints of the application services. Service execution time is one of the most vital QoS parameters. In this paper, a queuing theoretic approach is proposed to model the smart home workload. First, M/M/c queue model is applied to find the response time of smart home tasks with light variation over the arrival rate. Then, Markovian Modulated Poisson Process (MMPP) is used to extend the model to a more advanced type of smart home processing workloads. Next, the optimal number of Virtual Machines(VMs) required deploying the application servers that can satisfy the execution time constraint of incoming workloads is calculated. Finally, total service time of a smart home application is calculated considering into account the possible level of concurrency and dependency among tasks of an application service. In the end, some numerical and simulation examples are provided to validate our findings.	application server;approximation algorithm;cloud computing;concurrency (computer science);data center;home automation;java virtual machine;modulation;numerical analysis;quality of service;queueing theory;response time (technology);run time (program lifecycle phase);scalability;simulation;software deployment	Shahin Vakilinia;Mohamed Cheriet;Jananjoy Rajkumar	2016	2016 12th International Conference on Network and Service Management (CNSM)	10.1109/CNSM.2016.7818449	embedded system;real-time computing;simulation;quality of service;cloud computing;computer science;resource management;operating system;server;computer network	Metrics	-23.987052578498986	62.45677311235348	250
7531794fd326094b06792a010617e42d5f3c6e53	bayesian hierarchical modelling of traffic flow - with application to malta's road network	bayes methods;road traffic;vehicle routing;bayes theorem;traffic flow;malta;statistical distributions;origin and destination;monte carlo method;route choice;markov processes;monte carlo methods;measurement errors;markov chains	A Bayesian Hierarchical Model is presented to estimate route choice preferences between OD pairs. The methodology adopted utilizes both Origin-Destination (OD) information and traffic counts observed on some of the links in the network to estimate route choice probabilities. Route choice preferences are represented by multinomial distributions and estimated via a Markov Chain Monte Carlo (MCMC) algorithm. The proposed model takes into account measurement errors in the link counts, the uncertanties present in OD data and alternative routes choices both inside or outside the network of study. The proposed method is validated on both a synthetic example and the traffic network of Malta.	algorithm;hierarchical database model;markov chain monte carlo;monte carlo method;multinomial logistic regression;synthetic intelligence	Luana Chetcuti Zammit;Maria Attard;Kenneth Scerri	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728423	econometrics;simulation;markov chain monte carlo;geography;statistics	Robotics	25.922679111732084	-22.014957233002168	251
19040bf1f29c587478410600d1afb2aa717a701c	class-dependent modeling for dialog translation	desciframiento;modelizacion;evaluation performance;model specification;traduccion automatica;performance evaluation;decodage;decoding;dialog translation;frase;evaluacion prestacion;statistical machine translation;metric;tratamiento lenguaje;probabilistic approach;modelisation;sentence;evaluation metric;traduction automatique;mixture model;language processing;enfoque probabilista;approche probabiliste;system integration;traitement langage;metrico;teoria mezcla;phrase;mixture theory;modeling;theorie melange;class dependent;metrique;machine translation;automatic translation	This paper presents a technique for class-dependent decoding for statistical machine translation (SMT). The approach differs from previous methods of class-dependent translation in that the class-dependent forms of all models are integrated directly into the decoding process. We employ probabilistic mixture weights between models that can change dynamically on a sentence-by-sentence basis depending on the characteristics of the source sentence. The effectiveness of this approach is demonstrated by evaluating its performance on travel conversation data. We used this approach to tackle the translation of questions and declarative sentences using class-dependent models. To achieve this, our system integrated two sets of models specifically built to deal with sentences that fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built with all of the data to handle the general case. The technique was thoroughly evaluated on data from 16 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial.	dialog	Andrew M. Finch;Eiichiro Sumita;Satoshi Nakamura	2009	IEICE Transactions	10.1587/transinf.E92.D.2469	natural language processing;speech recognition;systems modeling;transfer-based machine translation;example-based machine translation;metric;computer science;artificial intelligence;mixture model;machine translation;rule-based machine translation;specification;algorithm;statistics;system integration	Vision	-23.72535506996766	-79.66413795366593	252
28d9ade6f69e2c989e5853fd50ee88f834bd252c	modelscope: inspecting executable models during run-time	software tool;correspondence;anti unification;reuse;similarity	This paper presents a software tool called ModelScope which is a co-debugging platform that, during run-time, allows insight into a model running on various different target systems with different views and diagrams for different aspects of the run-time state.	debugging;diagram;executable;programming tool	Philipp Graf;Klaus D. Müller-Glaser	2008		10.1145/1370175.1370195	similarity;computer science;theoretical computer science;data mining;reuse;algorithm	SE	-54.49731678881452	34.64720381415688	253
ebf08975a8c7d5a6cbed18b1dc4ddec981e685b5	simulation for a class of networked cascade control systems by pid control	control systems;intelligent actuators;fuzzy pid control method;three term control cascade control discrete time systems distributed control fuzzy control simulation;comprehensive performance index networked cascade control systems discrete time pid control fuzzy pid control method;real time;networked cascade control systems;fuzzy control;niobium;simulation;discrete time systems;performance index;discrete time pid control;discrete time;electrical equipment industry;cascade control;upper bound;three term control;control system synthesis;pid control;industrial control;control system synthesis three term control control systems fuzzy control industrial control process control electrical equipment industry intelligent sensors real time systems intelligent actuators;performance analysis;process control;distributed control;comprehensive performance index;intelligent sensors;real time systems	The simulation for a class of networked cascade control systems by PID control is investigated in this paper. Cascade control system, wherein the control loops are closed via real-time networks, is named networked cascade control system(NCCS for short). The configuration for a class of NCCSs is presented and the locations of the networks inserted into the control loops are pointed out. Discrete time PID control method and fuzzy PID control method are both introduced. A comprehensive performance index, is proposed to evalutate and compare the performance of the NCCS. The simulation of conventional PID control method and fuzzy self-tuning PID control methods in the NCCS are implemented based on MATLAB/Simulink and TrueTime toolbox. The effectiveness of the fuzzy self-tuning PID control method and its superiority over the conventional PID control method is verified.	control flow;control system;matlab;pid;real-time clock;self-tuning;simulation;simulink	Congzhi Huang;Yan Bai;Xinli Li	2010	2010 International Conference on Networking, Sensing and Control (ICNSC)	10.1109/ICNSC.2010.5461617	pid controller;control engineering;niobium;discrete time and continuous time;process performance index;real-time computing;computer science;networked control system;engineering;control system;process control;control theory;upper and lower bounds;intelligent sensor	Robotics	63.100350345522706	-6.9070839449713715	254
ab2bc965f10e7e45a827ed8d4fd3489bc021424f	on linear dynamic systems with parametrized perturbations and time delays	eigenvalues and eigenfunctions;observability;linear systems;perturbation techniques linear systems nonlinear dynamical systems;so nominal system possess linear dynamic systems parametrized perturbations time delays;nonlinear dynamical systems;time delays;controllability;dynamic system;dynamic systems;perturbation techniques;time delay;poles and zeros;trajectory;controllability filtering theory observability trajectory eigenvalues and eigenfunctions poles and zeros;time delays dynamic systems perturbations;linear dynamical system;perturbations;filtering theory	This paper presents a unifying approach to the problems of computing the admissible sets of parametrical multi-perturbations in a class of bounded sets. Some fundamental properties are maintained provided that the so-nominal system possesses the corresponding property.	dynamical system	Manuel de la Sen;Raul Nistal	2012	2012 Second International Conference on Digital Information and Communication Technology and it's Applications (DICTAP)	10.1109/DICTAP.2012.6215355	mathematical optimization;dynamical system;control theory;mathematics	Robotics	68.96295912699766	2.139179754723994	255
087ee0f793ca2387acd9a97b0b99c81e286e0daa	habit learning and brain–machine interfaces (bmi): a tribute to valentino braitenberg’s “vehicles”	pedestrian safety;skill learning;poison control;habits;injury prevention;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;neuroprosthetics;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;brain computer interfaces;user computer interface;poisoning prevention;falls;ergonomics;instrumental and classical conditioning;suicide prevention	Brain–Machine Interfaces (BMI) allow manipulation of external devices and computers directly with brain activity without involvement of overt motor actions. The neurophysiological principles of such robotic brain devices and BMIs follow Hebbian learning rules as described and realized by Valentino Braitenberg in his book “Vehicles,” in the concept of a “thought pump” residing in subcortical basal ganglia structures. We describe here the application of BMIs for brain communication in totally locked-in patients and argue that the thought pump may extinguish—at least partially—in those people because of extinction of instrumentally learned cognitive responses and brain responses. We show that Pavlovian semantic conditioning may allow brain communication even in the completely paralyzed who does not show response-effect contingencies. Principles of skill learning and habit acquisition as formulated by Braitenberg are the building blocks of BMIs and neuroprostheses.	basal (phylogenetics);basal ganglia;brain neoplasms;brain–computer interface;computer;computers;conditioning (psychology);drug vehicle;electroencephalography;embrace, extend and extinguish;extinction, psychological;hebbian theory;learning disorders;neuroprosthetics;patients;robot;rule (guideline);neuroprosthesis	Niels Birbaumer;Friedhelm Hummel	2014	Biological Cybernetics	10.1007/s00422-014-0595-5	psychology;brain–computer interface;neuroscience;simulation;developmental psychology;habit;engineering;suicide prevention;artificial intelligence;human factors and ergonomics;injury prevention	ML	-35.48629323284569	-40.019249358249574	256
253dbc3abf5dae0f3e733bbc325cc13f7c728d4f	glm analysis of time resolved nirs data of motor activation during different motor tasks	bioelectric potentials;time resolved spectra;haemodynamics;medical disorders;near infrared spectra time resolved nirs data analysis motor activation motor task hemodynamic response motor ability unilateral impairment finger movement task handgrip task variance signal general linear model approach time resolved nirs data processing continuous wave signal;data analysis;time resolved spectra bioelectric potentials data analysis haemodynamics medical disorders medical signal processing;medical signal processing;thumb correlation market research time series analysis hemodynamics photonics	The hemodynamic response to motor activation was investigated by time-resolved NIRS in healthy subjects and patients with unilateral impairment in motor ability. Healthy subjects performed a simple and a complex finger movement task, patients a handgrip task. A General Linear Model approach (GLM) was applied during NIRS data processing. In general, compared to the integral (continuous wave signal), higher significance of activation was found for the variance signal that selectively represents changes in the deep compartment. A discussion of GLM results with respect to task complexity and difficulty is provided.	anatomical compartments;brain injuries;cerebrovascular accident;clinic;general linear model;generalized linear model;hemodynamics;hemodynamics;multi-compartment model;patients;phylogenetic comparative methods;sample variance;sampling (signal processing)	Erika Molteni;Heidrun Wabnitz;Anna M. Bianchi;Oliver Steinkellner;Tilmann Sander-Thoemmes;Frederik Geisler;Bruno-Marcel Mackert;Stefanie Leistner;Sergio Cerutti	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6609868	psychology;simulation;speech recognition;medicine;hemodynamics;data analysis;communication;physiology;statistics	Robotics	19.715408399121614	-84.80770956502869	257
afad3cf6f0599aeff854ac62ec6660268fe9f7ac	fabrication technologies for three-dimensional integrated circuits (invited)	cu;fabrication;3dic design;3d integrated circuits;epitaxial growth;cmos technology;cad tools;integrated circuit;integrated circuit metallisation;fabrication technologies;cu fabrication technologies 3d vlsi low temperature cu cu wafer bonding 3d integrated circuits face to back bonded device wafers short vertical vias cu cu pads inter wafer throughway reliability criteria cu cu bond structural integrity cu cu contact electrical characteristics process flow efficiency process flow repeatability cad tools 3dic design 3dic layout;3dic layout;3d vlsi;circuit cad wafer bonding integrated circuit manufacture vlsi integrated circuit interconnections integrated circuit metallisation interface structure integrated circuit reliability;cu cu bond structural integrity;system on a chip;short vertical vias;three dimensional;low temperature;materials science and technology;process flow efficiency;reliability criteria;integrated circuit technology;interface structure;cu cu pads;cu cu contact electrical characteristics;fabrication integrated circuit technology three dimensional integrated circuits wafer bonding epitaxial growth integrated circuit interconnections computer science system on a chip cmos technology materials science and technology;integrated circuit interconnections;process flow repeatability;low temperature cu cu wafer bonding;vlsi;circuit cad;computer science;integrated circuit reliability;wafer bonding;structural integrity;face to back bonded device wafers;integrated circuit manufacture;three dimensional integrated circuits;inter wafer throughway	The MIT approach to 3-D VLSI integration is based on low-temperature Cu-Cu wafer bonding. Device wafers are bonded in a face-to-back manner, with short vertical vias and Cu-Cu pads as the inter-wafer throughway. In our scheme, there are several reliability criteria, which include: a) Structural integrity of the Cu-Cu bond, b) CuCu contact electrical characteristics, and c) Process flow efficiency and repeatability. In addition, CAD tools are needed to aid in design and layout of 3DICs. This paper will discuss recent results in all these areas.	computer-aided design;emoticon;integrated circuit;repeatability;very-large-scale integration;via (electronics);wafer (electronics);wafer bonding	Rafael Reif;Andy Fan;Kuan-Neng Chen;Shamik Das	2002		10.1109/ISQED.2002.996687	system on a chip;embedded system;three-dimensional space;electronic engineering;epitaxy;computer science;electrical engineering;integrated circuit;very-large-scale integration;fabrication;cmos;engineering drawing	EDA	12.998674479830965	55.33453438711661	258
bfa0d42421a9b146a55a7593db205b92478a0099	deterministic hypergraph coloring and its applications	lovasz local lemma;comptage;hipergrafico;coloracion grafo;algorithm performance;mathematiques discretes;temps polynomial;matematicas discretas;derandomisation;approximation algorithm;05c65;discrete mathematics;derandomization;desaleatorizacion;contaje;68wxx;conjunto parcialmento ordenado;41a10;partially ordered set;coloration graphe;resultado algoritmo;ensemble contour;06axx;algoritmo aproximacion;polynomial time;performance algorithme;counting;edge set;ensemble partiellement ordonne;algorithme polynomial;approximate counting;hypergraph;algorithme approximation;68r05;68w25;graph colouring;hypergraphe;tiempo polinomial	Given a hypergraph and a set of colors, we want to find a vertex coloring to minimize the size of any monochromatic set in an edge. We give a deterministic polynomial time approximation algorithm with performance close to the best bound guaranteed by an existential argument. This can be applied to support divide and conquer approaches to various problems. We give two examples. For deterministic DNF approximate counting, this helps us explore the importance of a previously ignored parameter, the maximum number of appearances of any variable, and we construct algorithms that are particularly good when this parameter is small. For partially ordered sets, we are able to constructivize the dimension bound given by Furedi and Kahn [Order, 3 (1986), pp. 15--20].	graph coloring	Chi-Jen Lu	2004	SIAM J. Discrete Math.	10.1137/S0895480100367664	partially ordered set;time complexity;mathematical optimization;combinatorics;discrete mathematics;lovász local lemma;mathematics;geometry;approximation algorithm;counting;algorithm;algebra	Theory	21.682360298824015	24.81955568713738	259
9a76fa26b672214c2e6c8d945bf0adccb1e021dd	bitwise partial-sum: a new tool for integral analysis against arx designs			arx;bitwise operation	Yu F Sasaki;Lei Wang	2015	IEICE Transactions			EDA	44.1405606690237	42.717960645356584	260
ba1c3827cea420faa5055a647d33eb0f49ecc937	computer-assisted adjuncts for aneurysmal morphologic assessment: toward more precise and accurate approaches	neck;angiography;morphological analysis;computing systems	Neurosurgeons currently base most of their treatment decisions for intracranial aneurysms (IAs) on morphological measurements made manually from 2D angiographic images. These measurements tend to be inaccurate because 2D measurements cannot capture the complex geometry of IAs and because manual measurements are variable depending on the clinician's experience and opinion. Incorrect morphological measurements may lead to inappropriate treatment strategies. In order to improve the accuracy and consistency of morphological analysis of IAs, we have developed an image-based computational tool, AView. In this study, we quantified the accuracy of computer-assisted adjuncts of AView for aneurysmal morphologic assessment by performing measurement on spheres of known size and anatomical IA models. AView has an average morphological error of 0.56% in size and 2.1% in volume measurement. We also investigate the clinical utility of this tool on a retrospective clinical dataset and compare size and neck diameter measurement between 2D manual and 3D computer-assisted measurement. The average error was 22% and 30% in the manual measurement of size and aneurysm neck diameter, respectively. Inaccuracies due to manual measurements could therefore lead to wrong treatment decisions in 44% and inappropriate treatment strategies in 33% of the IAs. Furthermore, computer-assisted analysis of IAs improves the consistency in measurement among clinicians by 62% in size and 82% in neck diameter measurement. We conclude that AView dramatically improves accuracy for morphological analysis. These results illustrate the necessity of a computer-assisted approach for the morphological analysis of IAs.		Hamidreza Rajabzadeh-Oghaz;Nicole Varble;Jason M. Davies;Ashkan Mowla;Hakeem J. Shakir;Ashish Sonig;Hussain Shallwani;Kenneth V. Snyder;Elad I. Levy;Adnan H. Siddiqui;Hui Meng	2017	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2255553	morphological analysis	SE	38.89430540096391	-81.77472893090844	261
e761565f000a7d2ae2c2d94751965bcae9ae985a	differential learning algorithms for decorrelation and independent component analysis	unsupervised learning;traitement signal;cambio variable;hebbian learning;learning algorithm;critere stabilite;metodo diferencial;signal estimation;separacion ciega;separation aveugle source;maximum likelihood;parametro variable;latent variable;blind source separation;simulacion numerica;maximum vraisemblance;gradiente;local stability;criterio estabilidad;intelligence artificielle;apprentissage non supervise;algorithme apprentissage;gradient;independent component analysis;higher order;differential method;parametre variable;blind separation;modele marche aleatoire;maximum likelihood estimate;variable parameter;random walk;signal processing;simulation numerique;changement variable;estimacion senal;separacion senal;methode differentielle;analyse composante independante;separation aveugle;artificial intelligence;decorrelation;stability criterion;separation source;differential learning;inteligencia artificial;analisis componente independiente;source separation;algoritmo aprendizaje;procesamiento senal;estimation signal;modelo marcha aleatoria;random walk model;maxima verosimilitud;variable transformation;apprentissage hebbien;numerical simulation	Decorrelation and its higher-order generalization, independent component analysis (ICA), are fundamental and important tasks in unsupervised learning, that were studied mainly in the domain of Hebbian learning. In this paper we present a variation of the natural gradient ICA, differential ICA, where the learning relies on the concurrent change of output variables. We interpret the differential learning as the maximum likelihood estimation of parameters with latent variables represented by the random walk model. In such a framework, we derive the differential ICA algorithm and, in addition, we also present the differential decorrelation algorithm that is treated as a special instance of the differential ICA. Algorithm derivation and local stability analysis are given with some numerical experimental results.	algorithm;decorrelation;generalization (psychology);gradient;hebbian theory;independent computing architecture;independent component analysis;information geometry;latent variable;machine learning;numerical analysis;unsupervised learning	Seungjin Choi	2006	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2006.06.002	unsupervised learning;econometrics;computer science;machine learning;signal processing;mathematics;maximum likelihood;random walk;statistics	ML	20.04794700401673	-27.929902080414738	262
1f7f3c40eb9b6e826bfa91bc91977d0da8a35528	diagnosis by volatile organic compounds in exhaled breath from lung cancer patients using support vector machine algorithm	lung cancer;volatile organic compounds vocs;gas chromatography mass spectrometry analysis;support vector machine svm;screening;exhaled air	Monitoring exhaled breath is a very attractive, noninvasive screening technique for early diagnosis of diseases, especially lung cancer. However, the technique provides insufficient accuracy because the exhaled air has many crucial volatile organic compounds (VOCs) at very low concentrations (ppb level). We analyzed the breath exhaled by lung cancer patients and healthy subjects (controls) using gas chromatography/mass spectrometry (GC/MS), and performed a subsequent statistical analysis to diagnose lung cancer based on the combination of multiple lung cancer-related VOCs. We detected 68 VOCs as marker species using GC/MS analysis. We reduced the number of VOCs and used support vector machine (SVM) algorithm to classify the samples. We observed that a combination of five VOCs (CHN, methanol, CH₃CN, isoprene, 1-propanol) is sufficient for 89.0% screening accuracy, and hence, it can be used for the design and development of a desktop GC-sensor analysis system for lung cancer.	1-propanol;analyzer, device;breath tests;carcinoma of lung;class;column (database);congenital hypomyelinating neuropathy;cross reactions;cross-validation (statistics);desktop computer;diagnostic neoplasm staging;early diagnosis;exhaled air (substance);expiration, function;flatulence;gas chromatography-mass spectrometry;halitosis;malignant neoplasm of lung;methanol;neoplasms;non-small cell lung carcinoma;nonlinear system;organic chemicals;organometallic compounds;oversampling;patients;prototype;support vector machine;tnr gene;volatile organic compounds;algorithm;isoprene;mixture;sensor (device)	Yuichi Sakumura;Yutaro Koyama;Hiroaki Tokutake;Toyoaki Hida;Kazuo Sato;Toshio Itoh;Takafumi Akamatsu;Woosuck Shin	2017		10.3390/s17020287	chromatography;chemistry;analytical chemistry;environmental chemistry;physics;quantum mechanics;electric-field screening	HCI	11.992248047795103	-64.42220301666605	263
f05ed3e359d23feeca82a2bedf6e706d347efaac	new algorithm and fortran module to carry out the four-index transformation of atomic and molecular physics wholly in central memory	orbitale;software;atome;calculo vectorial;atomo;etude theorique;logiciel;two electron integral;molecula;structure electronique;relativistic theory;theorie non relativiste;estructura electronica;algorithme;molecules;algorithm;teoria relativista;orbital;atoms;vector calculus;indexation;non relativistic theory;estudio teorico;integrale 2 electrons;logicial;calcul vectoriel;integral 2 electrones;molecule;fortran;theoretical study;theorie relativiste;electronic structure;teoria no relativista;algoritmo	Abstract   This is the first of three companion papers describing a complete and modular library to carry out the four-index transformation of a block [( pq/mrs )] of two-electron integrals involving symmetry-adapted primitive orbitals  p, q, r, s  and a symmetry-related index  m  into a block [( ij/klm )] of integrals over symmetry-adapted orthonormal (orbitals  i, j, k, l . The index  m  allows for the unified treatment of atomic and molecular, relativistic and nonrelativistic calculations.  A new algorithm based on a generalization of a method by Saunders & van Lenthe [ Mol. Phys.  48, 923 (1983)] for non-symmetry orbitals has been developed. It is here implemented by means of a FORTRAN code, controlled by subroutine C4ITD, requiring all computed quantities to be held in central memory. A non-redundant set of integrals is used throughout. All pertinent sums run only over a limited range determined by a given symmetry. The rate determining steps may be carried out in parallel and are vectorizable within each possible concurrent processor. For 20 non-symmetry orbitals and with 64-bit precision on a VAX-11/780 computer, using a working set of 200 kbytes, subroutine C4ITD takes 48 CPU s. Ninety percent of this time is spent in steps dominated by vector-scalar-multiply-and-add (VSMA) operations carried out at 0.170 mega floating point operations per second, which is 91% of the performance achieved on the same computer by an equivalent mix of pure VSMA operations and vector sizes. C4ITD will be useful in atomic and molecular electronic structure calculations beyond Hartree-Fock.	algorithm;fortran	Carlos F. Bunge;Annik Vivier Bunge;Gerardo Cisneros;Jean-Pierre Daudey	1988	Computers & Chemistry	10.1016/0097-8485(88)85010-1	chemistry;molecule;computer science;calculus;computational chemistry;mathematics;physics;algorithm;quantum mechanics	ML	60.550933136467634	28.127339430293095	264
4e62c9ab15645a01cb36d797825256a11a0cfd13	applying a conceptual design framework to study teachers’ use of educational technology		Theoretical outcomes of design-based research (DBR) are often presented in the form of local theory design principles. This article suggests a complementary theoretical construction in DBR, in the form of a design framework at a higher abstract level, to study and inform educational design with ICT in different situated contexts. Laurillard’s Conversational Framework (CF) is used as a conceptual lens to analyse how eight teachers use or envisage using technology to support learning in one-to-one environments. The findings demonstrate how the researcher uses the CF to discern different aspects of the teachers’ situated design practices. In the study, ICT is primarily used to support communication and the exchange of knowledge representations between the teachers and their students. Considerably fewer examples are found where ICT is used to support communication, collaborative creation and modelling between peers. However, the interview analyses reveal that the teachers’ intentions to apply ICT to support learning often include this second type of ICT use. Reasons for this discrepancy between the expressed intentions and de facto use of ICT include limitations in technical know-how and a perceived conflict between collaborative learning, existing school cultures and individual assessment. The findings suggest that in DBR, an analytical design framework could be an important tool for researchers and teachers when analysing and discussing educational uses of ICT. The CF provides a promising basis for a design framework, but should be expanded to include interactions with actors outside the classroom.	design rationale;discrepancy function;distributed bragg reflector;interaction;mike lesser;modeling language;one-to-one (data model);pedagogical agent;principle of locality;proof-carrying code;seamless3d;situated	Jörgen Holmberg	2016	Education and Information Technologies	10.1007/s10639-016-9536-3	simulation;knowledge management;management;pedagogy	HCI	-73.61736488548807	-37.687139747719506	265
b438381be40d2848b1ee3d0974eaf9470c9b8ee4	simulation of position-based routing protocol in wireless mobile ad hoc network	resource utilization;communication system traffic control;topology;routing protocols;protocols;wireless mobile ad hoc network;propagation model;routing;cartesian coordinate system;traffic control;osi;testing;routing protocols mobile ad hoc networks peer to peer computing discrete event simulation testing broadcasting network topology bandwidth communication system traffic control traffic control;data mining;network simulator;position based routing protocol;simulation program development;network topology;routing protocols ad hoc networks mobile radio radiowave propagation;mobile ad hoc networks;mobile radio;quadrant based directional routing protocol;routing algorithm;bandwidth;ad hoc networks;mobile ad hoc network;radiowave propagation;broadcasting;power consumption;peer to peer computing;routing protocol;cartesian coordinate system wireless mobile ad hoc network position based routing protocol power consumption resource utilization osi propagation model simulation program development quadrant based directional routing protocol;tk electrical engineering electronics nuclear engineering;coordinate system;discrete event simulation	New routing algorithms are being developed to reduce power consumption, effective utilization of resources and best route to a specific destination, to name a few, in wireless mobile ad hoc network. To simulate the algorithm, interaction between the different layers of the OSI, the link available and furthermore, employing a suitable propagation model in a wireless environment, the use of network simulators do eliminate irrelevant issues pertaining to the actual work which is testing the algorithm. This paper presents the development of the simulation program to test a new routing protocol called quadrant-based directional routing protocol (Q-DIR). Q-DIR uses exact location of nodes based on Cartesian-coordinate system to restrict the broadcast region to only a quadrant where the destination node and source node are located.	algorithm;broadcast domain;cartesian closed category;hoc (programming language);osi model;relevance;routing;simulation;software propagation	Liza Abdul Latiff;Norsheila Fisal;Sharifah Hafizah Syed Ariffin	2009	2009 Third Asia International Conference on Modelling & Simulation	10.1109/AMS.2009.10	wireless routing protocol;wireless ad hoc network;optimized link state routing protocol;routing;enhanced interior gateway routing protocol;static routing;adaptive quality of service multi-hop routing;mobile ad hoc network;dsrflow;zone routing protocol;telecommunications;computer science;dynamic source routing;destination-sequenced distance vector routing;ad hoc wireless distribution service;distributed computing;routing protocol;link-state routing protocol;triangular routing;path vector protocol;hazy sighted link state routing protocol;geographic routing;routing information protocol;computer network	Mobile	1.0711398020324832	84.58845523991516	266
910107491a5370bba767495de17596327f7d4708	a novel approach to the statistical modeling of wireline channels	modelizacion;pulse response;estimacion canal;power line channels;interferencia intersimbolo;lognormal fading channel modeling power line communications twisted pairs coaxial cables;topology;medium voltage;root mean square delay spread;câble coaxial;evaluation performance;propriete physique;courant porteur;power line;egalisation;performance evaluation;variable aleatoire;transfer functions;intersymbol interference;twisted pairs;estimation canal;aumento potencia;evaluacion prestacion;lognormal distribution;power gain;gain puissance;gain;transmision alta caudal;variable aleatoria;phone lines;equalization;loi lognormale;respuesta impulsion;attenuation;indexing terms;channel estimation;rms ds;statistical model;approche deterministe;equalisers carrier transmission on power lines coaxial cables digital subscriber lines;similitude;cable coaxial;lognormal fading;equalisers;digital subscriber lines;deterministic approach;modelisation;funcion logaritmica;low voltage;ley lognormal;logarithmic function;power line communications;medium power;channel model;baja tension;coaxial cable;carrier current;propiedad fisica;igualacion;ligne abonne numerique;digital subscriber line;power line communications statistical modeling root mean square delay spread channel power gain power line channels coaxial cables phone lines rms ds dsl links impulse response wireline communications equalization schemes inter symbol interference;reponse impulsion;brouillage intersymbole;carrier transmission on power lines;fonction logarithmique;similarity;enfoque determinista;equalization schemes;random variable;basse tension;estimacion parametro;modele statistique;dsl links;potencia media;puissance moyenne;high rate transmission;impulse response;modelo estadistico;temps retard;coaxial cables;root mean square;similitud;delay time;statistical modeling;parameter estimation;estimation parametre;correlation;channel models;gain topology transfer functions correlation channel models attenuation delay	We report here that channel power gain and Root-Mean-Square Delay Spread (RMS-DS) in Low/Medium Voltage power line channels are negatively correlated lognormal random variables. Further analysis of other wireline channels allows us to report a strong similarity between some properties observed in power line channels and the ones observed in other wireline channels, e.g. coaxial cables and phone lines. For example, it is here reported that channel power gain and logarithm of the RMS-DS in DSL links are linearly correlated random variables. Exploiting these results, we here propose a statistical wireline channel model where tap amplitudes and delays are generated in order to reflect these physical properties. Although wireline channels are considered deterministic as their impulse response can be readily calculated once the link topology is known, a statistical wireline channel model is useful because the variability of link topologies and wiring practices give rise to a stochastic aspect of wireline communications that has not been well characterized in the literature. Finally, we also point out that alternative channel models that normalize impulse responses to a common (often unitary) power gain may be misleading when assessing the performance of equalization schemes since this normalization artificially removes the correlation between channel power gain and RMS-DS and, thus, Inter-Symbol Interference (ISI).	catastrophic interference;channel (communications);digital subscriber line;heart rate variability;interference (communication);low-power broadcasting;mathematical optimization;modulation;performance;power gain;qualitative comparative analysis;ray tracing (graphics);simulation;spatial variability;statistical model;telephone line;transfer function;transform, clipping, and lighting;wiring	Stefano Galli	2011	IEEE Transactions on Communications	10.1109/TCOMM.2011.031611.090692	statistical model;electronic engineering;digital subscriber line;telecommunications;engineering;electrical engineering;mathematics;statistics	Metrics	39.99727530832096	77.2059570452185	267
38ccde80274aa11e2f30323e58815c544eab75ab	effiziente mehrkernarchitektur für eingebettete java-bytecode-prozessoren			gesellschaft für informatik;java bytecode	Martin Zabel	2011			java bytecode;programming language;computer science	NLP	-100.48247610701203	30.339493105522433	268
a7e6cc6cf4226053ce7688e738c486978e6f7ac7	improving reliability of safety applications in vehicle ad hoc networks through the implementation of a cognitive network	automotive engineering;data transmission;reliability engineering;protocols;vehicular network;telecommunication network reliability;data contention locations;telecommunication network reliability ad hoc networks cognitive radio mobile radio protocols;delay effects;telecommunication computing;ad hoc network;spectrum;vanets;bit rate;data transmission contention;computer networks;electrical safety;data transmission contention vanets safety applications cognitive networks spectrum sharing;cognitive network;spectrum sharing;cognitive radio;roads;ieee;mobile radio;safety applications;safety;access protocols;data delivery;ad hoc networks;vehicular ad hoc network;vehicle ad hoc networks;data delivery vehicle ad hoc networks safety applications cognitive network wireless access in vehicular environments protocol stack ieee control channel data contention locations;vehicle safety;vehicles;control channel;wireless access in vehicular environments protocol stack;wireless access in vehicular environments;vehicle safety ad hoc networks computer networks automotive engineering telecommunication network reliability telecommunication computing reliability engineering access protocols delay effects electrical safety;vehicular communication;cognitive networks	Researchers have suggested Vehicular Ad hoc Networks as a way to enable car to car communications and to allow for the exchange of safety and other types of information among cars. The Wireless Access in Vehicular Environments (WAVE) protocol stack is standardized by the IEEE, and it allocates spectrum for vehicular communication. In our work we prove that it does not provide sufficient spectrum for reliable exchange of safety information. To alleviate this problem, we present a system that employs cognitive network principles to increase the spectrum allocated to the control channel (CCH) by the WAVE protocols, where all safety information is transmitted. To accomplish this objective, the proposed system relies on sensed data sent by the cars to road side units that in turn forward the aggregated data to a processing unit. The processing unit infers data contention locations and generates spectrum schedules to dispatch to the passing cars. Analysis and simulation results indicate the effectiveness of the system in improving data delivery in vehicular networks and thus increasing the reliability of safety applications.	algorithm;artificial neural network;bus contention;cognitive network;dynamic dispatch;hoc (programming language);left 4 dead 2;protocol stack;simulation	Kassem Fawaz;Ali J. Ghandour;Majd Olleik;Hassan Artail	2010	2010 17th International Conference on Telecommunications	10.1109/ICTEL.2010.5478817	vehicular ad hoc network;wireless ad hoc network;cognitive network;telecommunications;computer science;engineering;computer security;computer network	Mobile	6.714688490403582	87.42215457181723	269
1cf4630d626e34c8ab9d6c547e35f82cbf5bf7d9	an introduction to the trusted platform module and mobile trusted module		The trusted platform module (TPM) is a tamper-resistant component that provides roots of trust in secure computing and remote attestation frameworks. In this chapter, we briefly discuss the TPM architecture, operations and services. The discussion is then extended to the mobile trusted module (MTM)—to contrast and compare different approaches to implement a trusted platform architecture. This illustrates the vital role the ecosystem of a computing platform plays in the architectural design decisions regarding the root of trust in a trusted platforms.	trusted platform module	Raja Naeem Akram;Konstantinos Markantonakis;Keith Mayes	2014		10.1007/978-1-4614-7915-4_4	embedded system;direct anonymous attestation;hengzhi chip;trusted network connect;trusted platform module;computer security;trusted service manager	Security	-47.36478155290088	56.80733850963196	270
56d4f977d017d6485a0b05c8676ec99003ca664a	convexification of polygons by flips and by flipturns	convexification;polygon;flip;flipturn	Abstract Simple polygons can be made convex by a finite number of flips, or of flipturns. These results are extended to very general polygons.	convex function;convex hull	Branko Grünbaum;Joseph Zaks	2001	Discrete Mathematics	10.1016/S0012-365X(01)00152-2	polygon mesh;combinatorics;point in polygon;topology;rectilinear polygon;star-shaped polygon;polygon;mathematics;geometry;regular polygon	Theory	33.86335318027901	19.63997743843802	271
d544f404bda1e27b5dbc7b57804968810959c968	load-balancing query hotspots for next-generation sensornet	network lifetime;routing protocols;point to point;resource allocation;wireless sensor networks quality of service resource allocation routing protocols;wireless sensor network query hotspot detection next generation sensornet point to point routing protocol content based load balancing primitive quality of service;next generation;routing protocols base stations temperature sensors internet query processing computer science computer architecture throughput broadcasting monitoring;load balance;quality of service;routing protocol;wireless sensor networks	The expected architecture of next-generation sensornets raises the need for load-balanced point-to-point routing protocols to cope with different sources of traffic skewness. Query hotspots are one of these skewness sources that highly impact the sensornet performance. In this paper, we present a set of content-based load-balancing primitives to be used on top of any point- to-point routing protocol in order to detect and decompose query hotspots. Our schemes are based on local hotspot detection by the sensors targeted by queries. Hotspots are then decomposed by avoiding duplication when forwarding results to the query issuers. Our simulation results show the high benefit, in terms of network lifetime and throughput, of using our schemes to load-balance query hotspots of different sizes.	global communications conference;hotspot (wi-fi);java hotspot virtual machine;load balancing (computing);point-to-point protocol;routing;sensor;simulation;subject matter expert turing test;throughput	Mohamed Aly;Anandha Gopalan;Adel M. Youssef	2007	IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference	10.1109/GLOCOM.2007.150	policy-based routing;wireless routing protocol;routing domain;routing;enhanced interior gateway routing protocol;static routing;real-time computing;zone routing protocol;computer science;interior gateway protocol;dynamic source routing;distance-vector routing protocol;distributed computing;routing protocol;link-state routing protocol;routing information protocol;computer network	Visualization	5.339363441789506	79.06651196153392	272
beebc287cf5f7c50494dd6c2b396cb187984a088	simultaneously solving swarms of small sparse systems on simd silicon		A number of computational science algorithms lead to discretizations that require a large number of independent small matrix solves. Examples include small non-linear coupled chemistry and flow systems, one-dimensional sub-systems in climate and diffusion simulations and semi-implicit time integrators, among others. We introduce an approach for solving large quantities of independent banded matrix problems on SIMD architectures. Unlike many vectorized or batched approaches that rely on reusing the matrix factorization across multiple solves, our algorithm supports batches of matrices that differ (due to spatial variation or non-linear solvers, for example). We present an implementation of our approach for diagonally-dominant tridiagonal systems that is optimized via compiler directives, tiling, and choice of data layout. Performance is evaluated on three Intel micro-architectures with different cache, vectorization, and threading features: Intel Ivy Bridge, Haswell, and Knight's Landing. Finally, we show that our solver improves on existing approaches and achieves ∼90% of STREAM Triad effective bandwidth on all three platforms.	algorithm;automatic vectorization;cpu cache;compiler;computational science;diagonally dominant matrix;directive (programming);discretization;haswell (microarchitecture);ivy bridge (microarchitecture);nonlinear system;simd;semiconductor industry;simulation;solver;sparse;the matrix;thread (computing);tiling window manager	Bryce Adelstein-Lelbach;Hans Johansen;Samuel Williams	2017	2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2017.114	band matrix;computer science;parallel computing;distributed computing;ivy bridge;theoretical computer science;tridiagonal matrix;vectorization (mathematics);sparse matrix;matrix decomposition;solver;simd	HPC	-4.493525144042308	39.61767543800611	273
4773eed0cbcf0db42346506858b0afbfb50df12b	type-based access control in data-centric systems	programming language;collaborative application;software systems;role based access control;multi user;qa75 electronic computers computer science;social network;access control policy;application profile;security requirements;data access;relational database management system;web based system;access control;type system;data security	Data-centric multi-user systems, such as web applications, require flexible yet finegrained data security mechanisms. Such mechanisms are usually enforced by a specially crafted security layer, which adds extra complexity and often leads to error prone coding, easily causing severe security breaches. In this paper, we introduce a programming language approach for enforcing access control policies to data in data-centric programs by static typing. Our development is based on the general concept of refinement type, but extended so as to address realistic and challenging scenarios of permission-based data security, in which policies dynamically depend on the database state, and flexible combinations of columnand row-level protection of data are necessary. We state and prove soundness and safety of our type system, stating that well-typed programs never break the declared data access control policies.	apl;access control;cognitive dimensions of notations;data access;data security;multi-user;programming language;refinement (computing);type system;web application	Luís Caires;Jorge A. Pérez;João Costa Seco;Hugo Torres Vieira;Lúcio Ferrão	2011		10.1007/978-3-642-19718-5_8	computer security model;data access;relational database management system;type system;computer science;access control;role-based access control;data mining;database;data security;programming language;computer security;social network;software system	Security	-52.212998120362016	52.15235597564672	274
2cc70e20014b2206cc926b042ebd74a94c42b6fe	random instances of w[2]-complete problems: thresholds, complexity, and algorithms	polynomial time algorithm;lower bound	The study of random instances of NP complete and coNP complete problems has had much impact on our understanding of the nature of hard problems as well as the strength and weakness of wellfounded heuristics. This work is part of our effort to extend this line of research to intractable parameterized problems. We consider instances of the threshold dominating clique problem and the weighted satisfiability under some natural instance distribution. We study the threshold behavior of the solution probability and analyze some simple (polynomial-time) algorithms for satisfiable random instances. The behavior of these simple algorithms may help shed light on the observation that small-sized backdoor sets can be effectively exploited by some randomized DPLLstyle solvers. We establish lower bounds for a parameterized version of the ordered DPLL resolution proof procedure for unsatisfiable random instances.	clique problem;closing (morphology);co-np;cobham's thesis;dpll algorithm;heuristic (computer science);np-completeness;parameterized complexity;polynomial;proof calculus;proof complexity;random graph;randomized algorithm;resolution (logic);time complexity;with high probability	Yong Gao	2008		10.1007/978-3-540-79719-7_9	mathematical optimization;combinatorics;computer science;mathematics;upper and lower bounds;algorithm	Theory	11.166700968802546	18.959636900510446	275
db6a4e6b0bbe6d9b38d7c88d2b3bdc51ca509900	multi-dimensional nurbs model for predicting maximum free surface oscillation in swaying rectangular storage tanks		Abstract In this paper, a multi-dimensional non-uniform rational B-spline (NURBS) model is presented aiming at predicting the maximum free surface oscillation (MFSO) in the swaying rectangular storage tanks. The accuracy and clear mathematical meaning of the NURBS formulation make it attractive for the purpose. In this regard, a numerical model is developed to simulate the liquid sloshing phenomenon in a rectangular tank by means of coupling the Reynolds averaged Navier–Stokes (RANS) solver and the volume of fluid (VOF) technique. The RANS equations are discretized and solved using the staggered grid finite difference SMAC method. In order to validate the model, a rectangular tank with specified width and water depth is exposed to a presumed horizontal periodic sway motion and the results of the numerical analysis are compared to a well-established example. In the next step, the numerical model is used to simulate the sloshing phenomenon in the rectangular storage tanks with different widths and water depths under different harmonic excitations. Finally, a multi-dimensional NURBS hyper-surface is employed to fit a multivariate function to the sloshing results based on the numerical data. As a result, a new mathematical relationship is suggested by this article to estimate the MFSO in the swaying rectangular storage tanks, so that practitioners may benefit more when they try to apply the presented methodology to their real-world problems.	non-uniform rational b-spline	Kwabena Nkansah-Amankra;Ali Hashemian	2018	Computers & Mathematics with Applications	10.1016/j.camwa.2018.08.049	finite difference;mathematical optimization;volume of fluid method;numerical analysis;discretization;mathematics;oscillation;reynolds-averaged navier–stokes equations;phenomenon;solver	Theory	92.73345300075376	4.271567991354757	276
97abba518987b3f5797eeede37d0a1222243d629	effective instruction for persisting dyslexia in upper grades: adding hope stories and computer coding to explicit literacy instruction	dyslexia;computerized writing instruction;hope themes;mode of sentence presentation during reading comprehension;computer coding instruction	Children in grades 4 to 6 (N = 14) who despite early intervention had persisting dyslexia (impaired word reading and spelling) were assessed before and after computerized reading and writing instruction aimed at subword, word, and syntax skills shown in four prior studies to be effective for treating dyslexia. During the 12 two-hour sessions once a week after school they first completed HAWK Letters in Motion© for manuscript and cursive handwriting, HAWK Words in Motion© for phonological, orthographic, and morphological coding for word reading and spelling, and HAWK Minds in Motion© for sentence reading comprehension and written sentence composing. A reading comprehension activity in which sentences were presented one word at a time or one added word at a time was introduced. Next, to instill hope they could overcome their struggles with reading and spelling, they read and discussed stories about struggles of Buckminister Fuller who overcame early disabilities to make important contributions to society. Finally, they engaged in the new Kokopelli’s World (KW)©, blocks-based online lessons, to learn computer coding in introductory programming by creating stories in sentence blocks (Thompson and Tanimoto 2016). Participants improved significantly in hallmark word decoding and spelling deficits of dyslexia, three syntax skills (oral construction, listening comprehension, and written composing), reading comprehension (with decoding as covariate), handwriting, orthographic and morphological coding, orthographic loop, and inhibition (focused attention). They answered more reading comprehension questions correctly when they had read sentences presented one word at a time (eliminating both regressions out and regressions in during saccades) than when presented one added word at a time (eliminating only regressions out during saccades). Indicators of improved self-efficacy that they could learn to read and write were observed. Reminders to pay attention and stay on task needed before adding computer coding were not needed after computer coding was added.	auditory perception;computer programming;dyslexia;dyslexia, acquired;grade;manuscripts;orthographic projection;platyrrhini;regression - mental defense mechanism;saccades;self efficacy;substring;word decoding;sentence;spelling	Robert H. Thompson;Steve Tanimoto;Ruby Dawn Lyman;Kira Geselowitz;Kristin Kawena Begay;Kathleen Nielsen;William Nagy;Robert D. Abbott;Marshall Raskind;Virginia W. Berninger	2017	Education and Information Technologies	10.1007/s10639-017-9647-5	syntax;active listening;computer science;natural language processing;handwriting;dyslexia;artificial intelligence;comprehension;spelling;reading comprehension;sentence	HCI	-70.33403862375913	-53.457056298238605	277
23aef7915039b1292c97e5a77ff3522dd6bb6342	optimization algorithms vs. random sampling of entry sources for a deliberate food contamination		We focus on a deliberate scenario, where milk prod ucers are used as entry sources for a contamination and where milk co nsumers are the target of the attack. The aim of this study is to demonstrate how the size of damage differs dependent on the use of an optimization algorithm o r a random selection of entry sources. The results indicate that with a random se lection of entry sources the same results can be provided with respect to the number of consumers reached, as with the application of the greedy algorithm. However, i t should be also noted that with random selection of entry sources there is also a p ossibility of selecting milk producers, which would not reach any consumer with the hypothetical contaminated milk. The résumé is that by using the greedy algorithm always the “best” suited milk producers will be selected for a maximum spread of contaminated milk in our model. Risk managers can us e these results in order to select the sources of entry in a timeand resource efficient manner.	greedy algorithm;mathematical optimization;sampling (signal processing)	Beate Pinior;Thomas Selhorst	2014			sampling (statistics);food contaminant;environmental engineering;environmental science	ML	9.7578014618453	-5.577032747553235	278
c630ab96414b0c5cf5c9ac863964e6eeeadc1687	some design criteria for feistel-cipher key schedules	design criteria	The ANSI Data Encryption Algorithm (DEA) X3.92-1981, is probably a Feistel cipher. In considering how to expand the key of the DEA from 56 to 64 bits two similar ciphers are examined, and several design criteria inferred. As an example of their application, a 64-bit key schedule for the DEA is presented.	feistel cipher	Richard Outerbridge	1986	Cryptologia	10.1080/0161-118691860967	computer science;theoretical computer science;key schedule;mathematics;computer security	Crypto	-37.95602576504049	81.53650619937143	279
998b68f2e31ebfef3cb5c1144f2d5475b0da8d9b	effects of visualizing roles of variables with animation and ide in novice program construction	teaching learning strategies;roles of variables;visualization;solo taxonomy;novice programming	ROV visualization promotes novices SOLO level of program design rather than accuracy.Visualization assists novices to learn the concept of roles of variables.ROV combining with animation and IDE helps novices design program in a holistic view. In this research, the researchers apply the roles of variables visualization to the teaching of novice C language programmers. The results are evaluated using the Structure of Observed Learning Outcomes (SOLO) taxonomy. The participants of the research were fifty-five undergraduates who major in computer science at a polytechnic institute. They were divided into an experimental group and a control group. The students from the control group learned programming in the traditional role-based teaching method. The students in the experimental group learned programming using variables visualization with the support of PlanAni and generic integrated development environment (IDE). For the purposes of determining the effects of the role-based visualization teaching, the SOLO level of the code writing was graded according the SOLO categories for program construction. A course satisfaction questionnaire was conducted. Data analyses show there was a significant improvement of SOLO level of program construction and a higher approval about the roles of variables. These results indicate that visualizing the roles of variables with animations and IDE can provide novices with a new conceptual framework that enables them to design relational program from a holistic point of view and helps them learn the concept of the roles of variables.	integrated development environment	Nianfeng Shi;Zhiyu Min;Ping Zhang	2017	Telematics and Informatics	10.1016/j.tele.2017.02.005	software construction;computer science;animation;human–computer interaction;visualization;teaching method;conceptual framework;program design language;development environment	HCI	-82.25813883326437	-38.898619058804805	280
af8fac5e200143945f53d9feb91fbcef24fc82de	prediction of the collapse index by a mamdani fuzzy inference system	collapse index;collapsible ground;caliche;indexation;fuzzy inference system	The human decision making process has been characterized as relatively sequential, limited by cognitive processes, and influenced by previous experience. Under conditions associated with real-time decisions, humans can experience emotional intensity, ...	inference engine	Kivanc Zorlu;Candan Gokceoglu	2008		10.1007/978-3-540-85563-7_15	artificial intelligence;data mining;control theory;mathematics	NLP	-9.494877496642234	-12.210891385249424	281
b764ab162c2bdffc200e3d33cc3f5e37cf56fa77	layout analysis affecting strategic decisions in artificial container terminals	container terminals;simulation;allocation strategies;logistics	As a result of a major growth in world trade, importance of the container terminals, which are the exit gates of international maritime trade, has been emphasized and the competition between these terminals has increased. In recent years, increasing competition in shallow seas, which have low berth depth and intensive trade, has caused terminal managers to investigate how the strategic decisions affect the future development of terminal operations. Due to their low berth depth, container terminals in the feeder ports of shallow seas are built artificially near coastlines. The most common layouts found in these terminals are P, L, p, or W. In this paper, simulation models were developed for the container terminals to examine the effect of transporter dispatching rules and resource allocation strategies in terms of total annual handling amount. According to the results, terminal performance is significantly affected by terminal layout design under different transporter dispatching rules and allocation strategies. 2014 Elsevier Ltd. All rights reserved.	p (complexity);production (computer science);simulation	Mustafa Egemen Taner;Osman Kulak;Mehmet Ulas Koyuncuoglu	2014	Computers & Industrial Engineering	10.1016/j.cie.2014.05.025	logistics;computer science;engineering;marketing;operations management;transport engineering;operations research	AI	8.323692082101607	-7.089383586425281	282
b85eb312d443835d028763a7200603d2027fb945	wavesync: a low-latency source synchronous bypass network-on-chip architecture	network on chip;source synchronous;low latency communication;synchronization;router design;bypass routing;globally asynchronous locally synchronous;half cycle synchronizer	WaveSync is a network-on-chip architecture for a globally asynchronous locally-synchronous (GALS) design. The WaveSync design facilitates low-latency communication leveraging the source-synchronous clock sent along with the data to time components in the datapath of a downstream router, reducing the number of synchronizations needed. WaveSync accomplishes this by partitioning the router components at each node into different clock domains, each synchronized with one of the orthogonal incoming source-synchronous clocks in a GALS 2D mesh network. The data and clock subsequently propagate through each node/router synchronously until the destination is reached, regardless of the number of hops this may take. As long as the data travels in the path of clock propagation and no congestion is encountered, it will be propagated without latching as if in a long combinatorial path, with both the clock and the data accruing delay at the same rate. The result is that the need for synchronization between the mesochronous nodes and/or the asynchronous control associated with the typical GALS network is completely eliminated. To further reduce the latency overhead of synchronization, for those occasions when synchronization is still required (when a flit takes a turn or arrives at the destination), we propose a novel less-than-one-cycle synchronizer. The proposed WaveSync network outperforms conventional GALS networks by 87--90% in average latency, synthesized using a 45nm CMOS library.	asynchronous circuit;cmos;datapath;downstream (software development);globally asynchronous locally synchronous;mesh networking;mesochronous network;network congestion;network on a chip;overhead (computing);router (computing);software propagation;source-synchronous;synchronizer (algorithm)	Yoon Seok Yang;Reeshav Kumar;Gwan S. Choi;Paul V. Gratz	2012	2012 IEEE 30th International Conference on Computer Design (ICCD)	10.1145/2647950	clock synchronization;embedded system;synchronization;parallel computing;real-time computing;telecommunications;clock domain crossing;computer science;asynchronous communication;distributed computing;network on a chip;synchronous circuit	EDA	13.78103100970996	60.43968524412988	283
68a6e3b26123aa121ff15e60a31891254e6a77c5	the online performance estimation framework: heterogeneous ensemble learning for data streams	data streams;ensembles;meta-learning	Ensembles of classifiers are among the best performing classifiers available in many data mining applications, including the mining of data streams. Rather than training one classifier, multiple classifiers are trained, and their predictions are combined according to a given voting schedule. An important prerequisite for ensembles to be successful is that the individual models are diverse. One way to vastly increase the diversity among the models is to build an heterogeneous ensemble, comprised of fundamentally different model types. However, most ensembles developed specifically for the dynamic data stream setting rely on only one type of base-level classifier, most often Hoeffding Trees. We study the use of heterogeneous ensembles for data streams. We introduce the Online Performance Estimation framework, which dynamically weights the votes of individual classifiers in an ensemble. Using an internal evaluation on recent training data, it measures how well ensemble members performed on this and dynamically updates their weights. Experiments over a wide range of data streams show performance that is competitive with state of the art ensemble techniques, including Online Bagging and Leveraging Bagging, while being significantly faster. All experimental results from this work are easily reproducible and publicly available online.		Jan N. van Rijn;Geoff Holmes;Bernhard Pfahringer;Joaquin Vanschoren	2017	Machine Learning	10.1007/s10994-017-5686-9	pattern recognition;mathematics;artificial intelligence;ensembles of classifiers;machine learning;ensemble learning;data stream mining;dynamic data;training set	ML	13.858795705834341	-38.15902604501906	284
dbe8be0979e9a4c813543490e8a37190ae822741	a theorem for locating eigenvalues	location problem;symmetric positive definite;probleme localisation;eigenvalue problem;search method;probleme valeur propre;eigenvalues;matrice mathematique;symmetric matrix;mathematical matrix;matrix computation;matrice definie positive;positive definite matrix;matrice hermitienne;matrice symetrique;hermitian matrix	The primary conclusion derived in this paper is that the leading principal minors of matrixB-ωA (whereB is a symmetric positive definite matrix andA is Hermite matrix) have properties similar to those of Sturm sequences, which is the theoretical basis of the Determinant Search Method for solving the eigenvalue-problem of damped structural systems [1]–[5], correcting the errors in a statement given by K. K. Gupta in [1]–[5]. Das Hauptergebnis dieser Arbeit ist der Satz, daß die Haupt-Minoren der MatrixB-ωA (B symmetrisch und positiv definit,A Hermitesch) ähnliche, Eigenschaften wie Sturmsche Folgen haben. Dieser Satz ist die theoretische Grundlage der Determinanten-Methode für die Lösung des Eigenwert-Problems von gedämpften Systemen, wobei die Fehler einer Arbeit von K. K. Gupta korrigiert werden.	internet explorer;sturm's theorem	Xiaoshu Pan;Hua Dai	1985	Computing	10.1007/BF02240150	hermitian matrix;combinatorics;eigenvalues and eigenvectors;calculus;mathematics;positive-definite matrix;numerical linear algebra;symmetric matrix;algebra	AI	-96.0972310740938	35.371632447837484	285
47f8f255b95dcd55fbbf5cf0986ace8bc22575fd	an exploratory investigation of word aversion		Why do people self-report an aversion to words like “moist”? The present study represents an initial scientific exploration into the phenomenon of word aversion by investigating its prevalence and cause. We find that as many as 20% of the population equates hearing the word “moist” to the sound of fingernails scratching a chalkboard. This population often speculates that phonological properties of the word are the cause of their displeasure. One tantalizing possibility is that words like “moist” are aversive because speaking them engages facial muscles that correspond to expressions of disgust. However, three experiments suggest that semantic features of the word – namely, associations with disgusting bodily functions – underlie peoples’ unpleasant experience. This finding broadens our understanding of language and contributes to a growing literature on the cognitive processes relating to highly valenced and arousing words.	cognition;experiment;exploratory testing;risk aversion	Paul H. Thibodeau;Christopher Bromberg;Robby Hernandez;Zachary Wilson	2014			onomatopoeia;psychology;referent;social psychology;cognitive psychology;valence (psychology);disgust;lexicon;linguistics;sound symbolism;population;lexical item	NLP	-8.324456690970386	-78.29534985757516	286
43e9c8a722b06f186c38cfa9a8d5e8fe1004b116	polynomial based recursive and non recursive filter design	dissertation		filter design;polynomial;recursion (computer science);recursive filter	Vinay Kumar	2009			control engineering;mathematical optimization;recursive bayesian estimation;control theory;μ operator	EDA	58.849861359674364	16.7663070456547	287
8730428fe984784a7171ad3b85249a0203d23650	decline & fall		"""One's mind and the earth are in a constant state of erosion, mental rivers wear away abstract banks, brain waves undermine cliffs of thought, ideas decompose into stones of unknowing. """""""	neural oscillation	Greg Garvey	2004		10.1145/1185884.1185914	computer graphics (images);computer vision;artificial intelligence;computer science	Robotics	-56.856594278845414	-25.831655911991536	288
9b96e51980a7cb13848c0d1c4b6a1af36aaf48e8	ada: first users-pleased; prospective users-still hesitant	computer languages;uncertainty;embedded system;programming profession;scheduling;production;program processors scheduling programming profession computer languages productivity production buildings uncertainty costs embedded system;productivity;program processors;buildings	"""compilers and environments and favorable early experience presage an Ada boom, but DoD-mandated users are still reluctant. """"Programmers who are familiar with other programming languages will achieve a level of proficiency in Ada in a matter of weeks,"""" according to Jean D. Ichbiah, leader of the team that developed Ada and now president of Alsys, one of the vendors of compilers, productivity tools, and educationa programs for Ada. T he first returns from the actual use of the Department of Defense's T Ada language are just coming in and they are predominantly favorable. Early users believe that they are getting substantial productivity gains from Ada. They report that the language is minimizing errors and greatly reducing the time it takes to integrate modules into a system. Their programmers learn Ada with two or three weeks of instruction, become adequate in about two months, and reach a high level of capability in about six months. However, given the complexities of the language, further learning continues indefinitely. Most of the initial applications, however, have been in building tools for Ada in Ada or in small research projects. No large embedded systems-the area to which the design of the Ada language was especially directed-have yet been completed in Ada, although numerous applications of Ada to these systems are getting under way. Because of this lack of directly relevant experience and the consequent uncertainties as to how the use of Ada will impact costs and schedules, many members of the embedded systems community are still reluctant to employ it."""	ada;alsys;compiler;embedded system;high-level programming language;jean;modular programming;programmer;prospective search	Ware Myers	1987	Computer	10.1109/MC.1987.1663509	productivity;real-time computing;uncertainty;computer science;operating system;software engineering;programming language;scheduling;statistics	SE	-68.53810184427356	28.335837541912102	289
1e8dc31555fcccd32a92578dcaca5b6af0f28420	supporting data consistency in concurrent process execution with assurance points and invariants	invariants;concurrent data access;web services;data consistency;data monitoring	....................................................................................................................IV LIST OF FIGURES ........................................................................................................... V   I. INTRODUCTION........................................................................................................... 1    II. RELATED WORK....................................................................................................... 5  2.1 Transactional Issues for Web Services ................................................................ 5  2.2 Transactional Workflows..................................................................................... 6  2.3 Promises ............................................................................................................... 8  2.4 Reservation-Based Techniques .......................................................................... 10  2.5 Transactional Attitudes ...................................................................................... 11  2.6 Tentative Holding .............................................................................................. 12  2.7 Monitoring Extensions to BPEL ........................................................................ 13  2.8 Aspect-Oriented Workflows .............................................................................. 14  2.9 Summary ............................................................................................................ 16    III. BACKGROUND RESEARCH FOR THE USE OF INVARIANTS.................................... 18  3.1 Delta-Enabled Grid Services.............................................................................. 18  3.2 Service Composition and Recovery with Assurance Points .............................. 20    IV. OVERVIEW OF THE INVARIANT MONITORING SYSTEM ....................................... 23  4.1 The Invariant Monitoring System ...................................................................... 23  4.2 Invariant Specification ....................................................................................... 25  4.3 Hotel Room Monitoring Example...................................................................... 26  4.4 Bank Loan Application Monitoring Example.................................................... 28  4.5 Summary ............................................................................................................ 30    V. A PROTOTYPE OF THE INVARIANT MONITORING SYSTEM ................................... 31  5.1 Monitored Objects.............................................................................................. 31  5.2 XML Representation of Invariants .................................................................... 33  5.3 Registration of Invariants................................................................................... 37  5.4 Invariant Evaluation Web Service ..................................................................... 38  5.5 Extensions to DEGS........................................................................................... 40  5.6 The Delta Analysis Process................................................................................ 40  5.6.1 Invariant Storage Container ................................................................................... 41  5.6.2 Overview of the Delta Process Filtering ................................................................. 45  5.6.3 Delta Filtering Algorithms....................................................................................... 47    VI. TESTING AND EVALUATION OF THE INVARIANT MONITORING SYSTEM............. 57  6.1 Testing Environment Setup................................................................................ 57  Texas Tech University, Andrew Courter, December 2010 iii 6.2 Test Cases .......................................................................................................... 58  6.3 Performance of Invariant Evaluation ................................................................. 58    VII. SUMMARY AND FUTURE RESEARCH.................................................................... 62 REFERENCES........................................................................................................... 64  Texas Tech University, Andrew Courter, December 2010	algorithm;aspect-oriented software development;business process execution language;dirac delta function;parallel computing;surround sound;vii;web service;xml	Susan Darling Urban;Andrew Courter;Le Gao;Mary Shuman	2011		10.1007/978-3-642-24908-2_18	web service;real-time computing;computer science;invariant;data mining;database;invariant;data consistency	SE	-45.90235957876388	-4.993913140315772	290
aadb61dbae401c45492d9b6e9265561b16bb79fc	connecting with students through inventive marketing: five (or six) questions for ... keith bourne	marketing strategy	Lisa Neal Gualtieri interviewed Keith Bourne about marketing online programs. Given the current economic state, what are the primary advantages and disadvantages of a solely online marketing strategy?	bourne shell;online advertising;stephen r. bourne	Lisa Gualtieri	2009	eLearn Magazine	10.1145/1595385.1555399	computer science;marketing;operations management;marketing strategy;management	HCI	-69.79753046359197	-1.7062906762753236	291
05cf2327181a28adec0f957a23348a0012de94a7	vigilante: end-to-end containment of internet worms	sistema operativo;systeme confinement;analisis datos;alert type;flux donnee;flujo datos;vulnerability;worm containment;data analysis;vulnerabilite;vulnerabilidad;internet;operating system;self certifying alerts;flot commande;control flow;data flow analysis;control flow analysis;analyse donnee;systeme exploitation;internet worms;flujo control;data flow;self certification;autocertification;containment systems	Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work has proposed network-level techniques to automate worm containment; these techniques have limitations because there is no information about the vulnerabilities exploited by worms at the network level. We propose Vigilante, a new end-to-end approach to contain worms automatically that addresses these limitations. Vigilante relies on collaborative worm detection at end hosts, but does not require hosts to trust each other. Hosts run instrumented software to detect worms and broadcast self-certifying alerts (SCAs) upon worm detection. SCAs are proofs of vulnerability that can be inexpensively verified by any vulnerable host. When hosts receive an SCA, they generate filters that block infection by analysing the SCA-guided execution of the vulnerable software. We show that Vigilante can automatically contain fast-spreading worms that exploit unknown vulnerabilities without blocking innocuous traffic.	blocking (computing);end-to-end encryption;end-to-end principle;vigilante	Manuel João Costa;Jon A Crowcroft;Miguel Castro;Antony I. T. Rowstron;Lidong Zhou;Lintao Zhang;Paul Barham	2005		10.1145/1095810.1095824	data flow diagram;the internet;telecommunications;vulnerability;computer science;data-flow analysis;data analysis;programming language;control flow;world wide web;computer security;control flow analysis	OS	-62.72814095016435	69.62354105312002	292
a40d7dbc1ccc15a6057cb7e914443ecc71a45018	the structural synthesis of tendon-driven manipulators having a pseudotriangular structure matrix	engineering;equipment;analisis mecanismo;manipulators;synthese structure;laboratory equipment;mechanism analysis;degree of freedom;manipulateur;cinematica;robotics;power transmission;kinematics;degrees of freedom;analyse mecanisme;materials handling equipment;manipulador;remote handling equipment 420200 engineering facilities equipment techniques;research programs;robots;cinematique;robotica;robotique;technical report;remote handled;kinetics;mechanical systems;manipulator;tendon;systeme anthropomorphe	Tendons have been widely used for power transmission in the field of anthropomorphic manipulating systems. This article deals with the identification and enumeration of the kinematic structure of tendon-driven robotic mechanisms. The structural isomorphism of tendon-driven manipulators is defined, and the structural characteristics of such mechanical systems are described. Applying these structural characteristics, a methodology for the enumeration of tendon-driven robotic mechanisms is developed. Mechanism structures with up to six degrees of freedom are enumerated.		Jyh-Jone Lee;Lung-Wen Tsai	1991	I. J. Robotics Res.	10.1177/027836499101000306	control engineering;simulation;computer science;engineering;artificial intelligence;degrees of freedom;robotics;mechanical engineering	Robotics	70.03577189684802	-18.519836088151724	293
934f1c71f7396e6c02b1484ebd927b2509a8ef16	parameter-controlled volume thinning	algorithme rapide;estructura 3 dimensiones;aplicacion medical;visualizacion;esqueleto;volume;transformacion;skeleton;structure 3 dimensions;medical visualization;computational fluid dynamics;navigation chirurgicale;reconstruction image;afinamiento;visualization;volumen;compact representation;visualisation;reconstruccion imagen;image reconstruction;fast algorithm;distancia;squelette;amincissement;medical application;transformation;three dimensional structure;mecanique fluide numerique;mecanica fluido numerica;algoritmo rapido;thinning;distance;application medicale	The availability of large 3D datasets has made volume thinning essential for compact representation of shapes. The density of the skeletal structure resulting from the thinning process depends on the application. Current thinning techniques do not allow control over the density and can therefore address only specific applications. In this paper, we describe an algorithm which uses a thinness parameter to control the thinning process and thus the density of the skeletal structure. We present applications from CFD and medical visualization and show how the skeletal structure can be used in these domains. We also illustrate a technique to construct a centerline for surgical navigation.	algorithm;distance transform;medical imaging;reconstruction filter;thinning;time complexity;voxel	Nikhil Gagvani;Deborah Silver	1999	Graphical Models and Image Processing	10.1006/gmip.1999.0495	simulation;visualization;computer science;mathematics;geometry;algorithm	Visualization	70.70807912051681	-50.540202058680464	294
1917c98cb03048c5c8a3d2dcd784c2e3d6db7259	using cache line coloring to perform aggressive procedure inlining	instruction cache;optimizing compiler;dual path execution;cache memory;program optimization;computer architecture;speculation;memory systems;eager execution;memory hierarchy;instruction scheduling;multithreading	Memory hierarchy performance has always been an important issue in computer architecture design. The likelihood of a bottleneck in the memory hierarchy is increasing, as improvements in microprocessor performance continue to outpace those made in the memory system. As a result, effective utilization of cache memories is essential in today's architectures.The nature of procedural software poses visibility problems when attempting to perform program optimization. One approach to increasing visibility in procedural design is to perform procedure inlining. The main downside of using inlining is that inlined procedures can place excess pressure on the instruction cache.To address this issue we attempt to perform code reordering. By combining reordering with aggressive inlining, a larger executable image produced through inlining can be effectively remapped onto the cache address space, while not noticeably increasing the instruction cache miss rate.In this paper, we evaluate our ability to perform aggressive inlining by employing cache line coloring. We have implemented three variations of our coloring algorithm in the Alto toolset and compare them against Alto's aggressive basic block reordering algorithms. Alto allows us to generate optimized executables, that can be run on hardware to generate results. We find that by using our algorithms, we can achieve up a 21% reduction is execution runtime over the base Compaq optimizing compiler, and a 6.4% reduction when compared to Alto's interprocedural basic block reordering algorithm.	address space;algorithm;basic block;cpu cache;computer architecture;executable;graph coloring;inline expansion;mathematical optimization;memory hierarchy;microprocessor;offset binary;optimizing compiler;procedural design;program optimization;visibility (geometry)	Hakan Aydin;David R. Kaeli	2000	SIGARCH Computer Architecture News	10.1145/346023.346046	computer architecture;cache-oblivious algorithm;speculation;parallel computing;real-time computing;cache coloring;multithreading;cpu cache;computer science;operating system;program optimization;optimizing compiler;instruction scheduling;programming language;cache algorithms;cache pollution	Arch	-3.3086622981916833	52.56012249431866	295
ad825f2c9adb92dc12e47d0123e787633b1e2b69	automatic traffic detection system		This article presents the concept of automatic traffic detection system. Idea of the system is based on image processing and distributed database. This work focuses on presentation of test installation set-up.	distributed database;image processing	Michal Lyczek;Lukasz Kaminski;Agata Chrobak;Anna Kulka	2009		10.7148/2009-0660-0663	image processing;computer vision;distributed database;artificial intelligence;computer science	EDA	-35.00870864498109	-13.844877866376674	296
90e1022ecc2c640078fa6fe60d932aa2ae639de8	using a reverse engineering type paradigm in clustering. an evolutionary programming based approach		The aim of this work is to propose a novel view on the well-known clustering approach that is here dealt with from a different perspective. We consider a kind of a reverse engineering related approach, which basically consists in discovering the broadly meant values of the parameters of the clustering algorithm, including the choice of the algorithm itself, or even – more generally – its class, and some other parameters, that have possibly led to a given partition of data, known a priori. We discuss the motivation and possible interpretations related to such a novel reversed process. In fact the main motivation is gaining insight into the structure of the given data set or even a family of data sets. The use of the evolutionary strategies is proposed to computationally implement such a reverse analysis. The idea and feasibility of the proposed computational approach is illustrated on two benchmark type data sets. The preliminary results obtained are promising in terms of a balance between analytic and computational effectiveness and efficiency, quality of results obtained and their comprehensiveness and intuitive appeal, a high application potential, as well as possibilities for further extensions.	evolutionary programming;programming paradigm;reverse engineering	Jan W. Owsinski;Janusz Kacprzyk;Karol R. Opara;Jaroslaw Stanczak;Slawomir Zadrozny	2017		10.1007/978-3-319-47557-8_9	evolutionary programming	DB	5.915564755758238	-43.68658538999426	297
6f37cdd49ce1fb703e192c74fe8ba3a749cd43fe	arabic quranic search tool based on ontology		This paper reviews and classifies most of the common types of search techniques that have been applied on the Holy Quran. Then, it addresses the limitations of these methods. Additionally, this paper surveys most existing Quranic ontologies and what are their deficiencies. Finally, it explains a new search tool called: a semantic search tool for Al-Quran based on Qur’anic on-tologies. This tool will overcome all limitations in the existing Quranic search applications.		Mohammad M. Alqahtani;Eric Atwell	2016		10.1007/978-3-319-41754-7_52	data science;data mining;information retrieval	Logic	-31.965729677052774	-59.10593312091635	298
3fbe0199dc192a9d0f176ad6f8897cec770d4f03	logik 2 - vollständige logik der endlich und unendlich großen größen: heft 1, einleitung			heterogeneous earliest finish time	Lothar Seidel	1994				NLP	-97.17590263541878	24.916196912774147	299
899c747853cb833c043673529164097cb06ccfdc	a note on the construction of a multivariate normal sample	multivariate normal;covariance matrix gaussian distribution vectors random variables stochastic processes;random variables;vectors;stochastic processes;gaussian distribution;covariance matrix	s TEIN and Store? have recently discussed the problem of constructing samples having a specified multivariate normal distribution. They are apparently unaware of simple facts concerning the behavior of the covariance matrix under a linear transformation. If [ =(x1, . . . , x,) is a 1 X n vector whose components are random variables having covariance matrix C, and if M is an m X n matrix of constants, then the covariance matrix of the components of .$M is M’CM. Thus, if x1, . . . , x, are independent, normally distributed, with variances 1, the components of [M will be jointly normally distributed with a specified covariance matrix S if, and only if,		George Marsaglia	1957	IRE Trans. Information Theory	10.1109/TIT.1957.1057410	normal distribution;multivariate analysis of variance;covariance mapping;scatter matrix;estimation of covariance matrices;matrix t-distribution;random variable;stochastic process;econometrics;covariance matrix;multivariate normal distribution;combinatorics;multivariate random variable;normal-wishart distribution;cma-es;inverse-wishart distribution;covariance;gaussian process;mathematics;wishart distribution;quadratic form;multivariate stable distribution;hotelling's t-squared distribution;matrix normal distribution;statistics;covariance function;multivariate t-distribution	ML	32.340800374334336	-25.108636714391448	300
c745c6bfde2cb3e249833a23a1250e2532912909	algebraic approaches for the elliptic curve discrete logarithm problem over prime fields		The elliptic curve discrete logarithm problem is one of the most important problems in cryptography. In recent years, several index calculus algorithms have been introduced for elliptic curves defined over extension fields, but the most important curves in practice, defined over prime fields, have so far appeared immune to these attacks. In this paper we formally generalize previous attacks from binary curves to prime curves. We study the efficiency of our algorithms with computer experiments and we discuss their current and potential impact on elliptic curve standards. Our algorithms are only practical for small parameters at the moment and their asymptotic analysis is limited by our understanding of Gröbner basis algorithms. Nevertheless, they highlight a potential vulnerability on prime curves which our community needs to explore further.	algorithm;discrete logarithm;elliptic curve cryptography;experiment;gröbner basis	Christophe Petit;Michiel Kosters;Ange Messeng	2016		10.1007/978-3-662-49387-8_1	supersingular elliptic curve;mathematical analysis;discrete mathematics;jacobian curve;tripling-oriented doche–icart–kohel curve;counting points on elliptic curves;post-quantum cryptography;strong prime;mathematics;elliptic curve cryptography;algorithm;schoof's algorithm;algebra	Crypto	-39.25902897757578	81.37578057446164	301
c4e7f8f6b5dce021a8d88ab060ac2489855f4cc1	multiscale derivative transform and its application to image watermarking	watermarking;multi scale transform;derivative transform;wavelet transform;angle quantization index modulation aqim	Abstract A transform that estimates the first and higher-order derivatives of images at multiple scales is proposed. The proposed transform, called Multi-Scale Derivative Transform (MSDT), is specially designed for image watermarking applications. To calculate the first and higher-order image derivatives, MSDT uses the detail wavelet coefficients of the image. Unlike traditional wavelet-based image derivative estimators that use only the horizontal and vertical wavelet coefficients, the proposed transform maps the diagonal as well as the horizontal and vertical wavelet coefficients to the horizontal and vertical derivatives of the image. The inverse transform is designed such that any change in the image derivative domain results in the minimum possible change in the wavelet coefficients. This renders a watermark, that is embedded in the derivative domain, less visible in the image domain. The application of this transform to image watermarking is discussed, and the results are compared with those obtained using traditional wavelet-based image derivative estimators.	digital watermarking	Ehsan Nezhadarya;Rabab Kreidieh Ward	2014	Digital Signal Processing	10.1016/j.dsp.2014.06.011	wavelet;computer vision;constant q transform;mathematical optimization;mathematical analysis;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;digital watermarking;fractional fourier transform;discrete fourier transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;top-hat transform;wavelet transform	Graphics	56.421568200813276	-67.78994538011003	302
6271451601dca336900601a4e0453d14a147f1c3	grid computing for detailed hemodynamics-simulation-based planning of endovascular interventions		"""Detailed numerical simulations of blood flow in arteries with various malformations and its conjugate loads on the vessel walls have been a research topic for specialized medical and engineering communities over decades. The present state of computing resources and software allows access to these elaborate diagnostic and research tools to a broad user circle and even to integrate them into clinical workflows. To tap the full potential of hemodynamic simulations, a Grid-based """"virtual vessel surgery"""" application has been developed and deployed as part of the image processing module of the MediGRID project of the German Federal Ministry of Education and Science (BMBF). The very resource-intensive parts of that application, including not only the numerical simulation itself, but also automated data pre- and post-processing and visualization are implemented and are made available through the MediGRID portal. Some highly interactive workflow steps of the application are left at the user site but automated - via a specially developed Web interface - to a degree allowing intuitive, fast interaction and seamless integration with the Grid components. Standard middleware provided by D-Grid is used throughout. The complete workflow is implemented into the grid and can in principle be carried out using no external software. It was applied to real vessel data with a stenosis and an aneurysm, respectively."""	aneurysm;blood vessel tissue;community;computation (action);computer simulation;congenital abnormality;d-grid;grid computing;hemodynamics;image processing;immunostimulating conjugate (antigen);interface device component;middleware;numerical analysis;part dosing unit;seamless3d;stenosis;video post-processing;walls of a building	Kamen Beronov;Olga Dzhimova;Thomas Tolxdorff;Michal Vossberg;Dagmar Krefting	2009	Studies in health technology and informatics	10.3233/978-1-60750-027-8-82	risk analysis (engineering);knowledge management;endovascular interventions;grid computing;medicine	HPC	37.044803724558726	-88.18778854965316	303
d0149ff9b00be7a58c7d39caedadae1e81d4aefe	image enhancement and space-variant color reproduction method for endoscopic images using adaptive sigmoid function	endoscopic image image enhancement color reproduction adaptive sigmoid function;visual representation image enhancement space variant color reproduction method endoscopic images adaptive sigmoid function ycbcr conversion matrix luminance components chrominance components uniformly distributed luminance pixels old chrominance texture information rgb color image tissue characteristics vascular characteristics pit patterns lesion polyp image quality focus value;image color analysis image enhancement visualization endoscopes color image quality imaging;medical image processing biological tissues endoscopes image colour analysis image enhancement image representation image texture	This paper presents an image enhancement and space-variant color reproduction method based on adaptive sigmoid function for endoscopic image. At first, using YCBCR conversion matrix, the color image is separated into luminance and chrominance components. The adaptive sigmoid function with two controlling parameters is applied on the uniformly distributed luminance pixels. The space-variant color reproduction generates new chrominance components by transferring and modifying old chrominance based on texture information. Finally, new luminance and chrominance components are converted into RGB color image. The proposed method highlights some of the tissue and vascular characteristics as well as pit patterns in lesion and polyp. The performance of the proposed scheme is compared with other related methods in terms of image quality, focus value, efficiency of color reproduction and statistic of visual representation.	color image;image editing;image quality;mucous membrane;pixel;sigmoid colon;sigmoid function;statistic (data);polyps	Mohammad Shamim Imtiaz;Khan A. Wahid	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944477	demosaicing;color histogram;image texture;rgb color model;computer vision;hsl and hsv;color image;image gradient;binary image;image processing;mathematics;color balance;optics;histogram equalization;computer graphics (images)	Robotics	58.087892369364724	-64.10526024674441	304
44fa855818aa82315c236f48fc63f037b3d4f404	an improved self-organizing cpn-based fuzzy system with adaptive back-propagation algorithm	esquema;structure learning;neuro fuzzy systems;gradient descent method;mimo system;systeme neuronal flou;schema apprentissage;learning;counterpropagation network;gradient method;logique floue;logica difusa;cpn;dynamic system;backpropagation;schema;aprendizaje;fuzzy logic;methode gradient;back propagation learning scheme;retropropagation;apprentissage;metodo gradiente;multi input multi output system;adaptive learning rate;neurofuzzy system;nearest neighbor;neuro fuzzy system;hybrid learning;back propagation algorithm;autoorganizacion;self organization;sistema difuso;systeme flou;reseau neuronal;retropropagacion;scheme;back propagation;red neuronal;learning scheme;autoorganisation;counter propagation network;fuzzy system;neural network	This paper describes an improved self-organizing CPN-based (Counter-Propagation Network) fuzzy system. Two self-organizing algorithms IUSOCPN and ISSOCPN, being unsupervised and supervised respectively, are introduced. The idea is to construct the neural-fuzzy system with a two-phase hybrid learning algorithm, which utilizes a CPN-based nearest-neighbor clustering scheme for both structure learning and initial parameters setting, and a gradient descent method with adaptive learning rate for fine tuning the parameters. The obtained network can be used in the same way as a CPN to model and control dynamic systems, while it has a faster learning speed than the original back-propagation algorithm. The comparative results on the examples suggest that the method is fairly efficient in terms of simple structure, fast learning speed, and relatively high modeling accuracy.	algorithm;backpropagation;coloured petri net;fuzzy control system;organizing (structure);self-organization;software propagation	Zhiming Zhang;Yue Wang;Ran Tao;Siyong Zhou	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00170-1	unsupervised learning;wake-sleep algorithm;computer science;artificial intelligence;backpropagation;machine learning;artificial neural network;algorithm;fuzzy control system	ML	11.197933441822709	-29.65626026267989	305
062a90d9a75416b3f1ec8e7ff36113f356a5ad0a	using the perseus system for modelling epistemic interactions	multi agent systems;dynamic epistemic logic;knowledge representation	The aim of the paper is to apply the software tool Perseus to modelling epistemic interactions. We focus on the issue of agents' knowledge acquisition, using a logical puzzle in which agents increase their knowledge about the hats they wear. In the paper, first we present a model of epistemic interactions, which allows us to resolve the hats puzzle. Then, the model is used to build the problem's specification for the Perseus system. Finally, we show how the hats puzzle can be solved and analysed in a detailed way with the use of a parametric verification method executed by Perseus.	interaction	Magdalena Kacprzak;Piotr Kulicki;Robert Trypuz;Katarzyna Budzynska;Pawel Garbacz;Marek Lechniak;Pawel Rembelski	2011	Trans. Computational Collective Intelligence	10.1007/978-3-642-24016-4_6	simulation;engineering;artificial intelligence;theoretical physics	AI	-26.31075868495669	-8.79752845664493	306
c203f8f85c2011ae18cec523ba2d561caec60fe4	lessons learned from virtual humans	animacion por computador;realite virtuelle;realidad virtual;virtual reality;virtual human;intelligence artificielle;lessons learned;artificial intelligence;inteligencia artificial;computer animation;virtual humans;natural language processing;multiagent systems;animation par ordinateur	decade ago to bring together researchers working at the cutting edge of simulation technologies, such as computer graphics, artificial intelligence, and virtual reality to work with people from the entertainment industry who know how to create characters that are compelling and stories that are engaging to work toward the goal of creating the next generation of simulation and training systems. Early on, we decided to focus on training human-oriented skills, such as leadership, negotiation, and cultural awareness. These skills are based on what is sometimes called tacit knowledge (Sternberg 2000), that is, knowledge that is not easily explicated or taught in a classroom setting but instead is best learned through experience. Currently, these training experiences are usually delivered through various human-to-human role-playing exercises. We sought to replace the human role players with virtual humans, which are computer-generated interactive characters that look and act like people but exist in virtual environments. There are several benefits to taking such an approach. Human-based role playing is costly in terms of personnel requirements and is often done at training centers that may be far away from the student’s location. In contrast, virtual exercises can be delivered on a laptop, making them available to a student whenever and wherever they are needed, without the need to tie up additional personnel resources. Articles	artificial intelligence;computer graphics;computer-generated holography;experience;humans;knowledge management;laptop;requirement;simulation;virtual reality	William R. Swartout	2010	AI Magazine		simulation;computer science;engineering;artificial intelligence;virtual reality;computer animation	AI	-57.447854203621304	-30.775193951472755	307
ea1c778b97a0430085f859b3a4f4d6d0c13941bd	guiding term reduction through a neural network: some prelimanary results for the group theory	rewrite rule;group theory;normal form;neural network	[2] Krogan NJ, Cagney G, Yu H, Zhong G, Guo X, Ignatchenko A, Li J, Pu S, Datta N, Tikuisis AP, Punna T, Peregrin-Alvarez JM, Shales M, Zhang X, Davey M, Robinson MD, Paccanaro A, Bray JE, Sheung A, Beattie B, Richards DP, Canadien V, Lalev A, Mena F, Wong P, Starostine A, Canete MM, Vlasblom J, Wu S, Orsi C, Collins SR, Chandran S, Haw R, Rilstone JJ, Gandi K, Thompson NJ, Musso G, St Onge P, Ghanny S, Lam MH, Butland G, Altaf-Ul AM, Kanaya S, Shilatifard A, O’Shea E, Weissman JS, Ingles CJ, Hughes TR, Parkinson J, Gerstein M, Wodak SJ, Emili A, Greenblatt JF.(2006) Global landscape of protein complexes in the yeast Saccharomyces cerevisiae. Nature, Mar 30;440(7084):637-43	ap computer science principles;artificial neural network;javascript;lam/mpi;mass effect trilogy;modified huffman coding;scala;shoshana wodak;the 3-d battles of worldrunner	Alberto Paccanaro	1995		10.1007/3-540-59200-8_80	combinatorics;discrete mathematics;computer science;mathematics;normal-form game;group theory;algorithm	ML	-45.554927726977546	-11.285123285277955	308
07b8a9a225b738c4074a50cf80ee5fe516878421	convolutional simplex projection network for weakly supervised semantic segmentation		Weakly supervised semantic segmentation has been a subject of increased interest due to the scarcity of fully annotated images. We introduce a new approach for solving weakly supervised semantic segmentation with deep Convolutional Neural Networks (CNNs). The method introduces a novel layer which applies simplex projection on the output of a neural network using area constraints of class objects. The proposed method is general and can be seamlessly integrated into any CNN architecture. Moreover, the projection layer allows strongly supervised models to be adapted to weakly supervised models effortlessly by substituting ground truth labels. Our experiments have shown that applying such an operation on the output of a CNN improves the accuracy of semantic segmentation in a weakly supervised setting with image-level labels.	artificial neural network;constraint (mathematics);convolutional neural network;experiment;ground truth;supervised learning	Rania Briq;Michael Moeller;Juergen Gall	2018			computer vision;computer science;pattern recognition;artificial intelligence;simplex;segmentation	Vision	27.16328765330612	-49.96822018736905	309
eb5b884dc256d907b0e2999b5cc3b80b104cc5ba	approaching the rate-distortion limit by spatial coupling with belief propagation and decimation	lossy source coding;rate distortion bound lossy source coding spatial coupling ldgm belief propagation guided decimation;source coding bandwidth compression;bandwidth compression;ldgm;spatial coupling;bpgd algorithm spatial coupling encoding scheme lossy compression spatially coupled low density generatormatrix codes poisson code bit side low complexity belief propagation guided decimation algorithm shannon rate distortion limit random gibbs measure dynamical thresholds condensation thresholds cavity method condensation threshold approaches information theoretic test channel parameter rate distortion theory;rate distortion bound;complexity theory cavity resonators rate distortion equations belief propagation channel coding;source coding;belief propagation guided decimation	We investigate an encoding scheme for lossy compression based on spatially coupled Low-Density GeneratorMatrix codes. The degree distributions are regular, or are Poisson on the code-bit side and check-regular which allows use for any compression rate. The performance of a low complexity Belief Propagation Guided Decimation algorithm is excellent, and for large check degrees it gets close to Shannon's rate-distortion limit. We investigate links between the algorithmic performance and the phase diagram of a relevant random Gibbs measure. The associated dynamical and condensation thresholds are computed within the framework of the cavity method. We observe that: (i) the dynamical threshold of the spatially coupled construction saturates towards the condensation threshold; (ii) for large degrees the condensation threshold approaches the information theoretic test-channel parameter of rate-distortion theory. This provides heuristic insight into the excellent performance of the BPGD algorithm.	algorithm;belief propagation;decimation (signal processing);degree distribution;distortion;dynamical system;heuristic;line code;lossy compression;phase diagram;rate–distortion theory;shannon (unit);software propagation	Vahid Aref;Nicolas Macris;Marc Vuffray	2013	2013 IEEE International Symposium on Information Theory	10.1109/ISIT.2013.6620412	discrete mathematics;computer science;theoretical computer science;mathematics;source code	Vision	39.92852423991521	62.8235176966851	310
1a59f4cadb7b1efe3947323b1ce80e0da9eb6625	why synchronous tree substitution grammars?	synchronous tree substitution grammar;regular restriction;natural language processing;essential additional feature;translation model;extended multi bottom-up tree;bottom-up analogue;syntax-based machine translation;formal setting;particular attention	Synchronous tree substitution grammars are a translation model that is used in syntax-based machine translation. They are investigated in a formal setting and compared to a competitor that is at least as expressive. The competitor is the extended multi bottom-up tree transducer, which is the bottom-up analogue with one essential additional feature. This model has been investigated in theoretical computer science, but seems widely unknown in natural language processing. The two models are compared with respect to standard algorithms (binarization, regular restriction, composition, application). Particular attention is paid to the complexity of the algorithms.	algorithm;binary image;bottom-up parsing;computable function;computation;machine translation;natural language processing;serial digital video out;theoretical computer science;top-down and bottom-up design;transducer	Andreas Maletti	2010				NLP	-0.8626796801662179	18.73283355897299	311
69c1179325fd924f82b05342f40e7fad78b0580c	the known stranger: supporting conversations between strangers with personalized topic suggestions	conversations;topic suggestion;personalization;strangers	Striking up a good conversation with new acquaintances is often a difficult problem. In this paper we report on the perceptions of wearable device users who were given real-time personalized topic suggestions during a conversation with a person they just met. Suggestions were generated using a ranking recommendation algorithm, and were delivered via Google Glasses. We conducted a study with 38 pairs of strangers, who received such suggestions while conversing for the first time. Participants found the suggestions to be helpful, but only at the right moments, and for certain types of speakers. Our results contribute to the understanding of how communication interventions influence people's experience and behaviors, and enhance interpersonal interactions. Our study also presents design implications for applications on wearable devices to facilitate conversations between strangers.	algorithm;glass;interaction;personalization;real-time transcription;wearable technology	Tien T. Nguyen;Duyen T. Nguyen;Shamsi T. Iqbal;Eyal Ofek	2015		10.1145/2702123.2702411	personalization;multimedia;world wide web	HCI	-56.96645984778806	-47.9764381785569	312
3b8852cd0dc3ec7404b0c48769ba858ffcd56d88	a topological data model for spatial databases	simplicial complex;geographic information system;spatial data;spatial database;data model;object oriented database management system;affine transformation;closed world assumption	There is a growing demand for engineering applications which need a sophisticated treatment of geometric properties. Implementations of Euclidian geometry, commonly used in current commercial Geographic Information Systems and CAD/CAM, are impeded by the finiteness of computers and their numbering systems. To overcome these deficiencies a spatial data model is proposed which is based upon the mathematical theory of simplices and simplicial complexes from combinatorial topology and introduces and as an extension to the closed world assumption. It guarantees the preservation of topology under affine transformations. This model leads to straightforward algorithms which are described. The implementation as a general spatial framework on top of an object-oriented database management system is discussed.	algorithm;closed-world assumption;computer;computer-aided design;data model;database;geographic information system;simplicial complex	Max J. Egenhofer;Andrew U. Frank;Jeffrey P. Jackson	1989		10.1007/3-540-52208-5_32	database theory;closed-world assumption;object-based spatial database;discrete mathematics;semi-structured model;data model;computer science;database model;data mining;affine transformation;database;mathematics;spatial analysis;geographic information system;simplicial complex;spatial database;database design	DB	-28.390349262636466	8.320580831930268	313
db4e1d4e83f3984f0080267898ac32bb4ffdafcd	optimized design method of three-axis force sensor for robot fingers	stress;response surface method;robot sensing systems;quadratic programming;response surface methodology;structure optimization;sequential quadratic programming optimized design method three axis force sensor robot finger structural optimization dexterous hand strain gauge design of experiment finite element analysis response surface method;force sensors;response surface methodology design of experiments dexterous manipulators finite element analysis force control force sensors manipulator dynamics quadratic programming;manipulator dynamics;force;sequential quadratic programming;objective function;dexterous manipulators;design of experiments;optimal design;finite element analysis;strain;leg;force sensor;strain gauge;design optimization design methodology force sensors robot sensing systems fingers optimization methods humans force measurement humanoid robots capacitive sensors;design of experiment;force control	A structural optimization method of a three-axis force sensor for robot fingers is proposed. To achieve dexterous hands like human hands, it is important to measure the forces loaded on robot fingers. We have been developing three-axis force sensors that can detect three-axis forces simultaneously with strain gauges. This sensor is small and can be produced inexpensively. However, because the proposed sensor has been newly devised, there is neither a design basis for it nor an accumulation of know-how. Therefore, we suggest a method to optimize the sensor structure using design of experiments and finite element analysis. In the proposed method, the approximated expressions of the objective function and the constraints are generated by the response surface method and FEA. The sequential quadratic programming method is used to optimize the design variables. As a result, a force sensor is designed the robot fingers that is more effective than the prototype sensor, and the effectiveness of the proposed method is verified.	apache axis;approximation algorithm;design of experiments;experiment;federal enterprise architecture;finite element method;loss function;mathematical optimization;optic axis of a crystal;optimization problem;prototype;response surface methodology;robot;sensor;sequential quadratic programming;shape optimization;tree accumulation	Hiroko Oshima;Nobutaka Tsujiuchi;Takayuki Koizumi;Yuichiro Hayashi;Akihito Ito;Yotaro Tsuchiya	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420587	control engineering;simulation;engineering;control theory;sequential quadratic programming;design of experiments;quadratic programming	Robotics	69.01176036205084	-22.283145478088453	314
30749d8edec3d1a20cba5d5ae8df63841c15b743	high-frequency low-power multirate sc realizations for ntsc/pal digital video filtering	digital video broadcasting;passband;linear phase;image filtering;filtering;interpolation;digital filters finite impulse response filter sampling methods filtering passband capacitance clocks switching circuits band pass filters bandwidth;cmos technology;3 v high frequency sc realizations low power sc realizations hf multirate sc realizations ntsc pal digital video filtering switched capacitor structure sampled data anti imaging filter fir polyphase structure double sampling techniques linear phase filtering cmos technology 27 to 108 mhz 0 35 micron 28 mw;band pass filters;video signal processing;switching circuits;clocks;finite impulse response filter;circuit design;vhf filters;low power sc realizations;fir polyphase structure;high frequency sc realizations;3 v;28 mw;switched capacitor filters;switched capacitor structure;low power;cmos analogue integrated circuits;double sampling techniques;double sampling;digital filters;0 35 micron;low power electronics;hf multirate sc realizations;digital video broadcasting cmos analogue integrated circuits low power electronics switched capacitor filters video signal processing vhf filters fir filters interpolation;cost efficiency;bandwidth;sampled data anti imaging filter;27 to 108 mhz;capacitance;digital video;fir filters;linear phase filtering;sampling methods;high frequency;ntsc pal digital video filtering;switched capacitor	This paper proposes a cost-efficient multirate Switched-Capacitor structure and its realization for a high-frequency low-power sampled-data anti-imaging filter with 108 MHz output sampling rate for CCIR-601 NTSC/PAL digital video. The filter employs a 2-stage (8-tap + 6-tap) improved multirate FIR polyphase structure with double-sampling techniques to achieve the desired linear-phase filtering with 5 MHz-corner passband ( 40 dB attenuation) as well as an embedded sampling rate increase from 27 MHz to 108 MHz. The circuit, designed with 0.35 /spl mu/m CMOS technology, is expected to consume only about 28 mW for the analog part at 3.0 V supply.	digital video;low-power broadcasting;ntsc;pal	U Seng-Pan;Rui Paulo Martins;José E. Franca	2001		10.1109/ISCAS.2001.921826	electronic engineering;telecommunications;computer science;electrical engineering;finite impulse response;control theory	Arch	61.02594482470585	50.992742511600724	315
55d8d1ae3afcf9f2e6a7306408bd92eb5573773b	forecasting expectations of insured depository default and catastrophic losses	insurance banking us government business data engineering aggregates data security statistics costs condition monitoring;banking;banking industry;losses;history;deposit insurance;structural impact insured depository default losses insured catastrophic losses expectation forecasting historical data us annual aggregate losses insured bank deposits property insurance casualty insurance us tax paying public federal government federal deposit insurance corporation fdic bank insurance fund macroeconomic assessment depository banking industry securities transactions deposit closings statistics public funds residual net exposure general public guarantor solvency cost capital basis regulatory impact functional impact;history insurance forecasting theory banking economic cybernetics disasters losses statistics;federal government;fdic;forecasting theory;statistics;economic cybernetics;insurance;historical data;disasters	Examines the historical data on US annual aggregate losses of both the insured bank deposits and the property and casualty (P&C) insurers due to catastrophic occurrences. These two topics are considered separately and also in tandem, since the high-end, final guarantor on these losses has become the American tax-paying public, through the auspices of the Federal Government and the Federal Deposit Insurance Corporation (FDIC). The paper develops a macro-economic assessment of both the depository banking industry and the P&C insurance industry. The paper continues by examining the empirical data, identifying structural concerns. A few mathematical approaches are outlined, useful to value or craft securities or transactions designed on the underlying variables of deposit closings and catastrophe losses. The paper shows statistics on the variables. The paper concludes by suggesting approaches, some utilizing a portion of the existing public funds in the FDIC's Bank Insurance Fund, that reduce the residual net exposure to the general public guarantor, while also enhancing the solvency, cost and capital basis of the two industries. Further considerations as towards regulatory, functional and structural impact are briefly commented on for each of the methods included.		David Andrew D'Zmura	1998		10.1109/CIFER.1998.690003	actuarial science;finance;business	Vision	-10.250299197507879	-18.91659201465919	316
931bcb9695cbdd53fc31afd92f770cf0b09afcad	breaking the search space symmetry in partitioning problems: an application to the graph coloring problem	assignment;objet;asignacion;optimisation;coloracion grafo;splitting;theoretical framework;relation equivalence;aplicacion;optimizacion;voisinage;linear assignment problem;search space;optimal method;equality test;brisure symetrie;65kxx;assignation;object;optimization method;partitioning problems;calculo automatico;equivalence;metodo optimizacion;carta de datos;68wxx;computing;red;symetrie;symmetry;calcul automatique;49xx;equivalence relation;test egalite;coloration graphe;graph coloring problem;numerotation;test igualdad;informatique theorique;mappage;reseau arrangement;68r10;symmetry breaking;methode optimisation;distance metric;distancia;ruptura simetria;array;algorithme evolutionniste;numbering;algoritmo evolucionista;optimization;mapping;numerotacion;evolutionary algorithm;landscape analysis;simetria;application;relacion equivalencia;objeto;search space symmetry;equivalencia;distance;graph colouring;computer theory;graphe colore;informatica teorica;05c15	Many problems consist in splitting a set of objects into different groups so that each group verifies some properties. In practice, a partitioning is often encoded by an array mapping each object to its group numbering. In fact, the group number of an object does not really matter, and one can simply rename each group to obtain a new encoding. That is what we call the symmetry of the search space in a partitioning problem. This property may be prejudicial for optimization methods such as evolutionary algorithms (EA) which require some diversity during the search. This paper aims at providing a theoretical framework for breaking this symmetry. We define an equivalence relation on the encoding space. This leads us to define a non-trivial search space which eliminates symmetry. We define polynomially computable tools such as equality test, a neighborhood operator and a distance metric applied on the set of partitionings. This work has been applied to the graph coloring problem (GCP). A new distance has been proposed, which is quicker to compute and closer to the problem structure. Computing this distance has been reduced to the linear assignment problem which can be solved polynomially. Using this distance, the analysis of the landscape of the GCP has been carried out.	graph coloring	El-Ghazali Talbi;Benjamin Weinberg	2007	Theor. Comput. Sci.	10.1016/j.tcs.2007.01.023	equivalence;symmetry breaking;combinatorics;computing;computer science;object;evolutionary algorithm;assignment;mathematics;geometry;symmetry;equivalence relation;programming language;distance;numbering;algorithm;splitting	Theory	22.0475809085094	30.661738553361527	317
a404f3cd1524734bd1cf50bc711615ad3a34a8da	a logarithmic approximation for polymatroid congestion games	matroid;approximation algorithm;polymatroid;congestion game	We study the problem of computing a social optimum (minimum cost solution) in polymatroid congestion games, where the strategy space of every player consists of the set of vectors in a playerspecific integral polymatroid base polyhedron defined on the ground set of resources. For general non-decreasing cost functions we devise an Hrk-approximation algorithm, where rk is the sum of the ranks of the player-specific polymatroids and Hrk denotes the rk-th harmonic number. The main idea of our algorithm is to iteratively increase resource utilization in a greedy fashion and to invoke a polynomial covering oracle that checks feasibility of every computed resource utilization. The approximation guarantee is best possible up to a constant factor. As a special case, our result (partially) settles an open problem of Ackermann et al. (H. Ackermann, H. Röglin, and B. Vöcking. On the impact of combinatorial structure on congestion games. J. ACM, 55(6):1–22, 2008. Section 2.2) where the complexity of computing a socially optimal solution for matroid congestion games with non-decreasing cost functions is considered. Here, the approximation guarantee is best possible up to a constant factor if the number of resources is polynomially bounded in the number of players.		Tobias Harks;Tim Oosterwijk;Tjark Vredeveld	2016	Oper. Res. Lett.	10.1016/j.orl.2016.09.001	matroid;mathematical optimization;combinatorics;discrete mathematics;mathematics;approximation algorithm	Theory	17.88208914317969	15.664371863145686	318
d1b156184b9ef036c3e5bcca706accb3d3a67525	accelerating the kalman filter on a gpu	parallel computing;kalman filters graphics processing unit covariance matrix instruction sets symmetric matrices noise acceleration;kalman filters;graphics processor unit;kalman filter;gpu;matrix algebra;cuda;parallel computing kalman filter gpu cuda;large scale;large scale time critical application kalman filter gpu accelerated filter linear dynamic system error covariance matrix operation graphic processor unit;graphics processing units;linear dynamical system;matrix algebra graphics processing units kalman filters;parallel computer	For linear dynamic systems with hidden states, the Kalman filter can estimate the system state and its error covariance considering the uncertainties in transition and observation models. In each iteration of applying the Kalman filter, the two phases of predict and update contain a total of 18 matrix operations which include addition, subtraction, multiplication and inversion. As recent graphic processor units (GPU) have shown to provide high speedup in matrix operations, we implemented a GPU accelerated Kalman filter in this work. For general reference purposes, we tested the filter on typical large-scale over-determined systems with thousands of components in states and measurements. For the various combinations of configurations in our test, the GPU accelerated filter shows a scalable speedup as either the state or the measurement dimension increases. The obtained 2 to 3 orders of magnitude speedup over its single-threaded CPU counterpart shows a promising direction of using the GPU-based Kalman filter in large-scale time-critical applications.	central processing unit;dynamical system;graphics processing unit;iteration;kalman filter;scalability;speedup;thread (computing);window of opportunity	Min-Yu Huang;Shih-Chieh Wei;Bormin Huang;Yang-Lang Chang	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.153	kalman filter;invariant extended kalman filter;ensemble kalman filter;parallel computing;digital filter;fast kalman filter;computer hardware;computer science;theoretical computer science;extended kalman filter;moving horizon estimation;filter design;alpha beta filter;simultaneous localization and mapping	Robotics	-1.6428672480761197	40.6788336486741	319
7ef56977016a62f6ba8b32bf934c035c6b927101	a teaching model exploiting cognitive conflict driven by a bayesian network	bayes estimation;teoria cognitiva;task performance;metodo adaptativo;evaluation performance;bayesian network;interfase usuario;sequencage;performance evaluation;conflict;user interface;evaluacion prestacion;educational software program;diagnostico;cognitive theory;methode adaptative;intelligence artificielle;didacticiel;field trial;long terme;systeme adaptatif;long term;theorie cognitive;user assistance;reseau bayes;sequencing;estimacion bayes;largo plazo;assistance utilisateur;red bayes;conflicto;adaptive method;asistencia usuario;adaptive system;bayes network;sistema adaptativo;artificial intelligence;interface utilisateur;performance prediction;enseignement;programa didactico;inteligencia artificial;conflit;diagnosis;tutoring system;teaching;estimation bayes;diagnostic;ensenanza	This paper describes the design and construction of a teaching model in an adaptive tutoring system designed to supplement normal instruction and aimed at changing students’ conceptions of decimal numbers. The teaching model exploits cognitive conflict, incorporating a model of student misconceptions and task performance, represented by a Bayesian network. Preliminary evaluation of the implemented system shows that the misconception diagnosis and performance prediction performed by the BN reasoning engine supports the item sequencing and help presentation strategies required for teaching based on cognitive conflict. Field trials indicate the system provokes good long term learning in students who would otherwise be likely to retain misconceptions.	bayesian network;performance prediction;semantic reasoner	Kaye Stacey;Liz Sonenberg;Ann E. Nicholson;Tal Boneh;Vicki Steinle	2003		10.1007/3-540-44963-9_48	simulation;computer science;artificial intelligence;adaptive system;machine learning;bayesian network;operations research	AI	-73.84090127510679	-49.44420984465407	320
970817ac76b6c709a6e2501fe18e707ba7d795c2	"""discussion on: """"identification of arx and ararx models in the presence of input and output noises"""""""			arx	Juan C. Agüero;Juan I. Yuz;Graham C. Goodwin	2010	Eur. J. Control	10.1016/S0947-3580(10)70649-X	control theory	HCI	69.72800556724025	-7.83074182393738	321
b474e0b8054712cfaa58a5f9f07a5c397ee89efc	developing the designer's toolkit with software comprehension models	program understanding;design ideas development;software comprehension models;electrical capacitance tomography;software designer s toolkit;software testing;design strategies;design goals generation;computer aided software engineering software tools reverse engineering psychology;software maintenance;perforation;vocabulary;tool feature value;software design software tools software maintenance cognition usability software testing electrical capacitance tomography programming profession vocabulary;psychology;cognitive models;computer aided software engineering;programming profession;cognition;task performance improvements;software tools;program understanding software designer s toolkit software comprehension models cognitive models design ideas development design goals generation design strategies software development cycle tool feature value task performance improvements;software design;usability;software development cycle;cognitive model;reverse engineering	Cognitive models of software comprehension are potential sources of theoretical knowledge for tool designers. Although their use in analysis of existing tools is fairly wellestablished, the literature has shown only limited use of such models for directly developing design ideas. This paper suggests a way of utilizing existing cognitive models of software comprehension to generate design goals and suggest design strategies early in the development cycle. A crucial part of our method is a scheme for explaining the value of tool features by describing the mechanisms that are presumed to underly the expected improvements in task performance.	cognition;cognitive model;list comprehension;taxonomy (general)	Andrew Walenstein	1998		10.1109/ASE.1998.732687	cognitive model;cognition;usability;computer science;systems engineering;software design;software engineering;design strategy;software testing;programming language;software maintenance;computer-aided software engineering;software development process;reverse engineering;computer engineering	HCI	-59.318235113860005	30.79164262063402	322
0b6ce997df68a2656032424db82005d37266cca8	faster 64-bit universal hashing using carry-less multiplications		Intel and AMD support the carry-less multiplication (CLMUL) instruction set in their x64 processors. We use CLMUL to implement an almost universal 64-bit hash family (CLHASH). We compare this new family with what might be the fastest almost universal family on x64 processors (VHASH). We find that CLHASH is at least 60 % faster. We also compare CLHASH with a popular hash function designed for speed (Google’s CityHash). We find that CLHASH is 40 % faster than CityHash on inputs larger than 64 bytes and just as fast otherwise.	64-bit computing;byte;clmul instruction set;central processing unit;cryptography;exclusive or;fastest;hash function;message authentication;microprocessor;polynomial ring;toeplitz hash algorithm;universal hashing;universality probability;vmac;x86-64	Daniel Lemire;Owen Kaser	2015	Journal of Cryptographic Engineering	10.1007/s13389-015-0110-5	parallel computing;dynamic perfect hashing;theoretical computer science;universal hashing;mathematics;k-independent hashing;distributed computing	Crypto	8.487663715542077	43.9533722717516	323
a7db3e513d0668a8f569342b001105f53e03d33d	efficient revocable hierarchical identity-based encryption using cryptographic accumulators	revocation;cryptographic accumulator;private key update;dual system encryption;hierarchical identity-based encryption	Hierarchical identity-based encryption is an important extension from IBE and has found many applications in the network world. Private key revocation is a crucial requirement for any public key system. In this paper, we propose a novel revocation method for the hierarchical identity-based encryption. Existing revocable hierarchical identity-based encryption schemes have several disadvantages: the key update size increases logarithmically with the number of users in the system, the public information of key update received by each user is different and always related to the level of the identity hierarchy and the security proof of the revocable scheme is very complex. In our scheme, cryptographic accumulators are used to compress hierarchical levels and revoked users’ information into constant values. So we achieve almost constant size of private key update which is irrelevant with the user number in the system. Because of the compression of hierarchical information we can use simple dual system encryption techniques to prove our scheme to be fully secure under several common assumptions without resorting to complex nested dual system encryption techniques.	accumulator (computing);attribute-based encryption;id-based encryption;key;overhead (computing);provable security;public-key cryptography;relevance;requirement	Hongyong Jia;Yue Chen;Julong Lan;Kaixiang Huang	2017	International Journal of Information Security	10.1007/s10207-017-0387-8	56-bit encryption;computer security;40-bit encryption;keyfile;theoretical computer science;filesystem-level encryption;client-side encryption;computer science;encryption;cryptographic key types;probabilistic encryption	Security	-40.68421016781687	69.07172567745064	324
b6c34424345482255da373923e9a41a2a272c034	parallel deformation of heterogeneous chainmail models: application to interactive deformation of large medical volumes	volume rendering;chainmail;gpu;soft tissue deformation	In this work we present a new solution for correctly handling heterogeneous materials in ChainMail models, which are widely used in medical applications. Our core method relies on two main components: (1) a novel timestamp-based propagation scheme that tracks the propagation speed of a deformation through the model and allows to correct ambiguous configurations, and (2) a novel relaxation stage that performs an energy minimization process taking into account the heterogeneity of the model. In addition, our approach extends the SP-ChainMail algorithm by supporting interactive topology changes and handling multiple concurrent deformations, increasing its range of applicability. Finally, we present an improved blocking scheme that efficiently handles the sparse computation, greatly increasing the performance of our algorithm. Our proposed solution has been applied to interactive deformation of large medical datasets. The simulation model is directly generated from the input dataset and a user defined material transfer function, while the visualization of the deformations is performed by rendering the resampled deformed model using direct volume rendering techniques. In our results, we show that our parallel pipeline is capable of interactively deforming models with several million elements. A comparison is finally discussed, analyzing the properties of our approach with respect to previous work. The results show that our algorithm correctly handles very large heterogeneous ChainMail models in an interactive manner, increasing the applicability of the ChainMail approach for more demanding scenarios both in response time and material modeling.		Alejandro Rodríguez Aguilera;Alejandro León;Germán Arroyo	2016	Computers in biology and medicine	10.1016/j.compbiomed.2016.10.012	simulation;radiology;computer science;volume rendering;computer graphics (images)	Visualization	69.76657282801227	-49.816263071786786	325
1bb2c692b17d9a211a12f8ec9cfcbef7ec6b0149	fast, fixed-order, least-squares algorithms for adaptive filtering	eigenvalues and eigenfunctions;filtering algorithms adaptive filters lattices sampling methods computer errors tin vectors time factors interference eigenvalues and eigenfunctions;lattices;tapped delay line;interference;adaptive algorithm;adaptive filters;time factors;vectors;filtering algorithms;computational complexity;least square;stochastic gradient;recursive algorithm;sampling methods;tin;adaptive filter;computer errors;transient behavior	Fast, fixed-order, exact-least-squares algorithms for tapped-delay-line adaptive-filtering applications are presented in this paper. These new recursive algorithms require fewer operations per iteration and exhibit better numerical properties than the so-called Fast-Kalman algorithm of Ljung and Falconer [1978] and the unnormalized, least-squares, joint-process-lattice algorithms of Morf and Lee [1978]. In comparison with the currently used stochastic-gradient or LMS adaptive algorithm of Widrow and Hoff, the new, fixed-order, least-squares algorithms yield substantial improvements in transient behavior at a modest increase in computational complexity. Additionally, over a wide range of practical applications, the new algorithms demonstrate numerical properties comparable to those of the normalized lattice introduced by Lee, Morf, and Friedlander [1981], but at a considerable reduction in complexity.	adaptive filter;algorithm;least squares	John M. Cioffi;Thomas Kailath	1983		10.1109/ICASSP.1983.1172033	adaptive filter;computer vision;mathematical optimization;probabilistic analysis of algorithms;computer science;theoretical computer science;machine learning;mathematics;statistics	Vision	60.0706135996727	12.958445197489326	326
2139c25553efc916c57fa98f9204105b56ece75d	color image segmentation - an innovative approach	cluster algorithm;fuzzy membership function;image segmentation;color space;region segmentation;fuzzy logic;objective function;fuzzy clustering;color segmentation;pattern recognition;satellite image;color image;color image segmentation	"""In this paper we describe a color image segmentation system that performs color clustering in a color space and then color region segmentation in the image domain. For color segmentation, we developed a fuzzy clustering algorithm that iteratively generates color clusters using a uniquely de""""ned fuzzy membership function and an objective function for clustering optimization. The fuzzy membership function represents belief value of a color belonging to a color cluster and the mutual interference of neighboring clusters. The region segmentation algorithm merges clusters in the image domain based on color similarity and spatial adjacency. We developed three di!erent methods for merging regions in the image domain. Unlike many existing clustering algorithms, the image segmentation system does not require the knowledge about the number of the color clusters to be generated at each stage and the resolution of the color regions can be controlled by one single parameter, the radius of a cluster. The color image segmentation system has been implemented and tested on a variety of color images including satellite images, car and face images. The experiment results are presented and the performance of each algorithm in the segmentation system is analyzed. The system has shown to be both e!ective and e$cient. 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	algorithm;cluster analysis;color image;color space;fuzzy clustering;image segmentation;interference (communication);loss function;mathematical optimization;optimization problem;pattern recognition	Tie Qi Chen;Yi Lu	2002	Pattern Recognition	10.1016/S0031-3203(01)00050-4	fuzzy logic;color histogram;image texture;computer vision;color quantization;hsl and hsv;color normalization;color image;binary image;fuzzy clustering;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;color space;scale-space segmentation	Vision	44.496442094712016	-67.21788842372933	327
2a1a5cd06589b6e47dbe11530d9a7dab2ff188e8	serving embedded content via web applications: model, design and experimentation	model design;smart card;embedded web server;closed system;embedded system;applicative model;tcp performance;modeling;use case	Embedded systems such as smart cards or sensors are now widespread, but are often closed systems, only accessed via dedicated terminals. A new trend consists in embedding Web servers in small devices, making both access and application development easier. In this paper, we propose a TCP performance model in the context of embedded Web servers, and we introduce a taxonomy of the contents possibly served by Web applications. The main idea of this paper is to adapt the communication stack behavior to application contents properties. We propose a strategies set fitting with each type of content. The model allows to evaluate the benefits of our strategies in terms of time and memory charge. By implementing a real use case on a smart card, we measure the benefits of our proposals and validate our model. Our prototype, called Smews, makes a gap with state of the art solutions both in terms of performance and memory charge.	closed system;embedded system;prototype;sensor;smart card;web application;web server	Simon Duquennoy;Gilles Grimaud;Jean-Jacques Vandewalle	2009		10.1145/1629335.1629352	use case;embedded system;smart card;web modeling;real-time computing;systems modeling;computer science;operating system;closed system;world wide web	Mobile	-27.199686661723952	80.48681185360513	328
9aed78cb68494e412271285172db7c08a30e88e3	invariant components of synergy, redundancy, and unique information among three variables		In a system of three stochastic variables, the Partial Information Decomposition (PID) of Williams and Beer dissects the information that two variables (sources) carry about a third variable (target) into nonnegative information atoms that describe redundant, unique, and synergistic modes of dependencies among the variables. However, the classification of the three variables into two sources and one target limits the dependency modes that can be quantitatively resolved, and does not naturally suit all systems. Here, we extend the PID to describe trivariate modes of dependencies in full generality, without introducing additional decomposition axioms or making assumptions about the target/source nature of the variables. By comparing different PID lattices of the same system, we unveil a finer PID structure made of seven nonnegative information subatoms that are invariant to different target/source classifications and that are sufficient to construct any PID lattice. This finer structure naturally splits redundant information into two nonnegative components: the source redundancy, which arises from the pairwise correlations between the source variables, and the non-source redundancy, which does not, and relates to the synergistic information the sources carry about the target. The invariant structure is also sufficient to construct the system’s entropy, hence it characterizes completely all the interdependencies in the system.	interdependence;pid;synergy	Giuseppe Pica;Eugenio Piasini;Daniel Chicharro;Stefano Panzeri	2017	Entropy	10.3390/e19090451	information theory;mathematics;mathematical optimization;statistics;redundancy (engineering);pid controller;lattice (order);generality;axiom;pairwise comparison;invariant (mathematics)	AI	48.56011217784971	20.729444827234992	329
b239118fd8dbd240fa9fe4c4a747d08161e2e475	evolutionary computation for resource leveling optimization in project management			evolutionary computation;mathematical optimization	Christos Kyriklidis;Georgios Dounias	2016	Integrated Computer-Aided Engineering	10.3233/ICA-150508	resource leveling;management science	EDA	19.036636153595776	-1.2634870434168541	330
86ee9f123889bda2eed580086ac49ebcbe6521d4	mapping the history of environmental impacts of land-falling hurricanes in the southeastern united states - a demonstration for isabel	vegetation mapping;southeastern united states;flooding;environmental impacts;drought relief environmental impacts land falling hurricanes southeastern united states land use land cover change historical record tropical storms phenological disturbance filter modis vegetation index hurricane isabel outer banks north carolina ad 2003 09 18 woody wetland areas phenological activity flooding erosion wind damage vegetation stress coastal plain;environmental impact;vegetation index;wind damage;phenological disturbance filter;land use land cover;wind disasters erosion floods geochronology storms vegetation vegetation mapping;vegetation indices;vegetation stress;tropical storms;drought relief;tropical storm;regional scale;phenological activity;vegetation;ad 2003 09 18;modis vegetation index;storms;tropical cyclones;erosion;land falling hurricanes;land use;woody wetland areas;drought hurricane modis vegetation index land cover;satellites;hurricane isabel;coastal plain;hurricanes;drought;outer banks;modis;history hurricanes tropical cyclones vegetation mapping performance analysis artificial satellites filters modis floods stress;earth observation;geochronology;floods;land cover change;wind;land cover;hurricane;disasters;historical record;sea measurements;north carolina	The objective of our research is to develop a framework to perform a systematic and comprehensive analysis of land use, land cover (LULC) change along the historical record of the terrestrial tracks of hurricanes and tropical storms since the beginning of the earth observation satellite era. Here we present a phenological disturbance filter based on MODIS vegetation indices to detect and characterize the impact of Hurricane Isabel, which made landfall on the Outer Banks in North Carolina on 18th September 2003. The results show that woody wetland areas have a pronounced and localized decrease in phenological activity in the two following years, likely due to the disturbance created by flooding, erosion and wind damage. At the regional scale, we identify a relationship between vegetation stress measured by the persistence of below average EVI anomalies, and the frequency of hurricane and tropical storm in the coastal plain of North Carolina. This analysis also shows a direct link between hurricanes and tropical storms (TS) and drought relief in this region.	inline linking;persistence (computer science);terrestrial television;tropical cyclone track forecasting	Julien Brun;Ana P. Barros	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779731	meteorology;atlantic hurricane;tropical cyclone;geology;hydrology;climatology;geochronology	Embedded	83.09323676161895	-57.312332995377425	331
b36ee0a7807c792ba5d7261a1c540843f144f63b	when expectation fails: towards a self-correcting inference system		"""Contextual understanding depends on a reader's ability to correctly infer a context within which to interpret the events in a story. This """"context-selection problem"""" has traditionally been expressed in terms of heuristics for making the correct initial selection of a story context. This paper presents a view of context selection as an ongoing process spread throuqhout the understanding process. This view requires that the understander be capable of recognizing and correcting erroneous initial context inferences. A computer program called ARTHUR is described, which selects the correct context for a story by dynamically re-evaluating its own initial inferences in light of subsequent information in a story."""	computer program;heuristic (computer science);inference engine;selection algorithm	Richard Granger	1980			computer science;artificial intelligence;machine learning;data mining	PL	-17.398037756848453	0.44486451088050866	332
189b3b772cf55840bad01da4b6f5fc44937bbff6	gene clusters as intersections of powers of paths	simulation and modeling;data structures;computer science general;computer system implementation;operating systems	There are various definitions of a gene cluster determined by two genomes and methods for finding these clusters. However, there is little work on characterizing configurations of genes that are eligible to be a cluster according to a given definition. For example, given a set of genes in a genome, is it always possible to find two genomes such that their intersection is exactly this cluster? In one version of this problem, we make use of the graph theory to reformulated it as follows: Given a graph G with n vertices, do there exist two θ-powers of paths G S =(V S ,E S ) and G T =(V T ,E T ) such that G S ∩G T contains G as an induced subgraph? In this work, we divide the problem in two cases, depending on whether or not G is an induced subgraph of G S or G T . We show an $\mathcal{O}(n^{2})$ time algorithm that generates the smallest θ-powers of paths G S and G T (with respect to and the number of vertices) that contains G as an induced subgraph. Finally, we discuss the problem when G is an induced subgraph neither of G S nor of G T and we present a method of finding the smallest power of a path when graph G is a cycle C n .	algorithm;carrier-to-noise ratio;clique (graph theory);computer cluster;consistent pricing process;emoticon;existential quantification;forbidden graph characterization;graph (discrete mathematics);graph theory;induced subgraph;pro tools;ps (unix);roland gs;vertex (geometry);vertex (graph theory)	Vítor Costa;Simone Dantas;David Sankoff;Ximing Xu	2012	Journal of the Brazilian Computer Society	10.1007/s13173-012-0064-8	data structure;computer science;operating system;subgraph isomorphism problem;induced subgraph isomorphism problem;programming language;induced path;algorithm	Theory	24.838602934370876	27.97523559507892	333
a95696c1f602e3c173ec908a6b1c2219c6ec09f7	kcf-s: kegg chemical function and substructure for improved interpretability and prediction in chemical bioinformatics	simulation and modeling;metabolic networks and pathways;systems biology;physiological cellular and medical topics;computational biology bioinformatics;cluster analysis;structure activity relationship;enzymes;reproducibility of results;algorithms;computational biology;databases chemical;bioinformatics	In order to develop hypothesis on unknown metabolic pathways, biochemists frequently rely on literature that uses a free-text format to describe functional groups or substructures. In computational chemistry or cheminformatics, molecules are typically represented by chemical descriptors, i.e., vectors that summarize information on its various properties. However, it is difficult to interpret these chemical descriptors since they are not directly linked to the terminology of functional groups or substructures that the biochemists use. In this study, we used KEGG Chemical Function (KCF) format to computationally describe biochemical substructures in seven attributes that resemble biochemists' way of dealing with substructures. We established KCF-S (KCF-and-Substructures) format as an additional structural information of KCF. Applying KCF-S revealed the specific appearance of substructures from various datasets of molecules that describes the characteristics of the respective datasets. Structure-based clustering of molecules using KCF-S resulted the clusters in which molecular weights and structures were less diverse than those obtained by conventional chemical fingerprints. We further applied KCF-S to find the pairs of molecules that are possibly converted to each other in enzymatic reactions, and KCF-S clearly improved predictive performance than that presented previously. KCF-S defines biochemical substructures with keeping interpretability, suggesting the potential to apply more studies on chemical bioinformatics. KCF and KCF-S can be automatically converted from Molfile format, enabling to deal with molecules from any data sources.	bioinformatics;chemical database;chemical space;chemical table file;cheminformatics;cluster analysis;computational chemistry;data sources;fingerprint;kegg;molecular weight;nomenclature;statistical cluster	Masaaki Kotera;Yasuo Tabei;Yoshihiro Yamanishi;Yuki Moriya;Toshiaki Tokimatsu;Minoru Kanehisa;Susumu Goto	2013		10.1186/1752-0509-7-S6-S2	computational biology;biology;enzyme;structure–activity relationship;computer science;bioinformatics;data mining;cluster analysis;systems biology	Comp.	2.712314516973276	-57.04106327637009	334
3979183351209157f3876202c3019b52c30b2ca7	pathgen: a transitive gene pathway generator	gen;gene	SUMMARY Many online sources of gene interaction networks supply rich visual data regarding gene pathways that can aid in the study of biological processes, disease research and drug discovery. PathGen incorporates data from several sources to create transitive connections that span multiple gene interaction databases. Results are displayed in a comprehensible graphical format, showing gene interaction type and strength, database source and microarray expression data. These features make PathGen a valuable tool for in silico discovery of novel gene interaction pathways, which can be experimentally tested and verified. The usefulness of PathGen interaction analyses was validated using genes connected to the altered facial development related to Down syndrome.   AVAILABILITY http://dna.cs.byu.edu/pathgen.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online. Further information is available at http://dna.cs.byu.edu/pathgen/PathGenSupplemental.pdf.	bioinformatics;down syndrome;drug discovery;experiment;gene regulatory network;graphical user interface;interaction network;microarray;published database;span distance	M Kendell Clement;Nathaniel Gustafson;Amanda Berbert;Hyrum Carroll;Christopher Merris;Ammon Olsen;Mark J. Clement;Quinn Snell;Jared Allen;Randall J. Roper	2010	Bioinformatics	10.1093/bioinformatics/btp661	biology;bioinformatics;gene;data mining;genetics	Comp.	-0.2036421209540185	-59.36687523672335	335
e9efa58d58203f28c3048022d3aecf9f6f7ff82f	a statistical model for the influence of body dynamics on the gain pattern of wearable antennas in off-body radio channels	body dynamics;statistical radiation pattern;channel models;body area networks	The goal of this paper is to address a statistical approach for modelling the influence of body dynamics on the gain pattern of wearable antennas in Body Area Networks, particularly in off-body radio channels. A dynamic model was developed based on Motion Capture data, describing a realistic human body movement. Antennas are located on 4 typical positions (i.e., Head, Chest, Arm and Leg), for which statistics of antenna orientation (i.e., average and standard deviation of elevation and azimuth angles) were calculated for 2 dynamic scenarios, i.e., Walk and Run. Based on the rotation of the antenna, the statistics of gain patterns of a wearable patch antenna operating at 2.45 GHz were calculated. The standard deviation of the change in the antenna orientation is the highest for the Arm location, reaching $$19^{\circ }$$ 19 ? and $$37^{\circ }$$ 37 ? for the Run scenario, for elevation and azimuth angles, respectively. For most of the scenarios, the distribution of the change in antenna orientation fits well to a Kumaraswamy distribution (using the $$\chi ^2_{95\,\%}$$ ? 95 % 2 test). For all antenna positions and the Walk scenario, the standard deviation is $$<4^{\circ }$$ < 4 ? .	statistical model;wearable computer	Michal Mackowiak;Luís M. Correia	2013	Wireless Personal Communications	10.1007/s11277-013-1193-x	simulation;telecommunications	Mobile	24.88852925869457	77.18045664035984	336
a13203fc67842d5c6812c59860b55c123fa055a1	a nomadicity-driven negotiation protocol, tactics and strategies for interacting software agents		The rising integration of pocket computing devices in our daily life duties has taken the attention of researchers from different scientific backgrounds. Today’s amount of software applications that bring together advanced mobile services and literature of Artificial Intelligence (AI) is quite remarkable and worth investigating. Cooperation, coordination and negotiation are some of AI’s focal points wherein many of its related research efforts are strengthening the join between sophisticated research outcomes and modern life requirements, such as serviceability on the move. In Distributed Artificial Intelligence (DAI), several of the research conducted in Multi-Agent Systems (MASs) addresses the mutually beneficial agreements that a group of interacting autonomous agents are expected to reach. In our research, we look at agents as the transportable software packets that each represents a set of needs a user of a pocket computing device demands from a remote service acquisition platform. However, when a set of software agents attempt to reach an agreement, a certain level of cooperation must be reached first, then, a negotiation process is carried out. Depending on each agent’s negotiation skills and considerations, the returns of each accomplished agreement can either be maximized or minimized. In this thesis, we introduce a new negotiation model, (i.e., protocol, set of tactics, strategy), for software agents to employ while attempting to acquire a service on behalf of users of pocket computing devices. The purpose of our model is to maximize the benefits of the interacting agents while considering the limitations of the communication technologies involved and, the nomadic nature of the users they represent. We show how our model can be generically implemented. Then, we introduce two case-studies that we have been working on with our industrial partner and, we demonstrate these cases’ experimental results before and after applying our negotiation model.	autonomous robot;computer;dfa minimization;distributed artificial intelligence;focal (programming language);interaction;multi-agent system;requirement;software agent	Sameh Abdel-Naby	2010			autonomous agent;management science;simulation;software;serviceability (structure);software agent;network packet;negotiation;engineering	AI	-19.68591445872641	-12.470641985247763	337
47b1bf376049b26d43815496ced225a055bc389d	a 201.4gops 496mw real-time multi-object recognition processor with bio-inspired neural perception engine	databases;traitement pipeline;processing element;cmos integrated circuits;simd processors;object recognition;arquitectura red;engines visual perception pipeline processing object recognition energy management image recognition neural networks fuzzy logic circuits hardware;human performance;video surveillance;static random access memory;object recognition chip;multimedia;integrated circuit;execution time;circuito multipiso;autonomous vehicle;on chip sram;network on chip;clocks;intelligent robots;energy efficient;conference;real time;logique floue;video surveillance computerised navigation intelligent robots object recognition;rendement energetique;low power object recognition;gaussian;logica difusa;multiobject recognition algorithm;logic circuits;circuito integrado;memoire acces direct statique;three stage pipelined architecture;reconnaissance objet;engines switches clocks circuits network on a chip energy consumption delay neural networks energy management cmos technology;neural perception engine;circuito logico;cmos process;calculateur simd;communication architecture;tecnologia mos complementario;architecture reseau;intelligent workload estimations;fuzzy set theory;system on a chip;interconnection network;multistage circuit;hardware architecture;chip;algorithme;fuzzy logic;human like multiobject perception;etat actuel;algorithm;task pipelining;neural chips;visualization;consumo electricidad;three stage pipelined architecture multi casting network on chip multimedia processor multi object recognition neural perception engine visual perception workload aware dynamic power management;multiple objectives;low power;multi casting network on chip;fuzzy logic circuits;engines;percepcion visual;circuit logique;sistema sobre pastilla;simd computer;feature extraction	The visual attention mechanism, which is the way humans perform object recognition [1], was applied to the implementation of a high performance object recognition chip [2]. Even though the previous chip achieved 50% gain of computational cost [2], it could recognize only one object in a frame so that it is not suitable for advanced multi-object recognition applications such as video surveillance, intelligent robots, and autonomous vehicle navigation [3].	algorithmic efficiency;autonomous robot;british informatics olympiad;closed-circuit television;computation;outline of object recognition;real-time clock;real-time computing	Joo-Young Kim;Minsu Kim;Seungjin Lee;Jinwook Oh;Kwanho Kim;Sejong Oh;Jeong-Ho Woo;Donghyun Kim;Hoi-Jun Yoo	2009	2009 IEEE International Solid-State Circuits Conference - Digest of Technical Papers	10.1109/ISSCC.2009.4977352	embedded system;parallel processing;electronic engineering;real-time computing;logic gate;telecommunications;computer science;electrical engineering;operating system;hardware architecture	Robotics	45.199335426339104	-34.53532737817184	338
00f7e3689d5bc68188b201836d7812d7491e05ff	a compact, effective descriptor for video copy detection	internal structure;large scale;video copy detection;computer experiment;indexation;graph representation;computational efficiency;frame descriptor	Large scale video copy detection tasks require a compact and computational-efficient descriptor that is robust to various transformations that are typically applied to generate copies. In this paper, we propose a new frame-level descriptor for such a task. The descriptor encodes the internal structure of a video frame by computing the pair-wise correlations between geometrically pre-indexed blocks. It is conceptually simple, small in size, and fast to compute. Experiments using the MUSCLE VCD benchmark show its superior performance compared to existing approaches.	benchmark (computing);experiment;video copy detection	Mei-Chen Yeh;Kwang-Ting Cheng	2009		10.1145/1631272.1631375	computer vision;computer experiment;gloh;computer science;theoretical computer science;graph	Vision	36.71134105585573	-54.84719461800898	339
a009a3b784b6921588472dc800494d84b4473e1f	damasio, descartes, alarms and meta-management	dynamic change;evolutionary history;engineering design;cognitive systems;cognitive psychology;theoretical biology;philosophy of mind;animal cognition;psychology;developmental psychology;multi agent systems;computer architecture control systems design engineering process control psychology computer science world wide web intelligent agent systems engineering and theory history;emotions control architecture intelligent human like agent partly predictable world meta management mechanisms deliberative mechanisms reactive mechanisms global alarm systems engineering design requirements brain function intelligence;machine learning;control architecture;evolutionary psychology;multi agent systems cognitive systems psychology;artificial intelligence;brain function;evolution	"""This paper discusses some of the requirements for the control architecture of an intelligent human-like agent with multiple independent dynamically changing motives in a dynamically changing only partly predictable world. The architecture proposed includes a combination of reactive, deliberative and meta-management mechanisms along with one or more global \alarm"""" systems. The engineering design requirements are discussed in relation our evolutionary history, evidence of brain function and recent theories of Damasio and others about the relationships between intelligence and emotions."""	engineering design process;requirement;theory	Aaron Sloman	1998		10.1109/ICSMC.1998.725060	philosophy of mind;computer science;knowledge management;artificial intelligence;evolutionary psychology;machine learning;evolution;animal cognition;cognitive science	Robotics	-25.8193531841839	-15.60054848337372	340
00686d63bf0cb319fad4af67449c75519be6b1bc	developing efficient algorithms for data mining large scale high dimensional data	riverside eamonn keogh zakaria;computer science developing efficient algorithms for data mining large scale high dimensional data university of california;time series;data mining;scalable;clustering;high dimensional data;jesin;computer science	Data mining and knowledge discovery has attracted a great deal of attention in information technology in recent years. The rapid progress of computer hardware technology in the past three decades provides a great enhancement to the database and information industry. The size and complexity of real world data is dramatically increasing with the growth of hardware technology. Although new efficient algorithms to deal with such data are constantly being proposed, the mining of large scale high dimensional data still presents a lot of challenges. In this dissertation, several novel algorithms are proposed to handle such challenges. These algorithms are applied to domains as diverse as electrocardiography (ECG), stock market data, geospatial data, power supply data, audio data, image data, etc. This dissertation contributes to the data mining community in the following three ways:Firstly, we propose a novel algorithm for clustering time series data efficiently in the presence of noise or extraneous data. Most existing methods for time series clustering rely on distances calculated from the entire raw data. As a consequence, most work on time series clustering only considers the clustering of individual time series behaviors, e.g., individual heart beats and contrives the time series in some way to make them all equal in length. However, for any real world problem, formatting the data in such a way is often a harder task than the clustering itself. In order to remove these unrealistic assumptions, we have developed a new primitive called unsupervised shapelet or u-shapelet and shown its utility for clustering time series.Secondly, in order to speed up the discovery of u-shapelet and make it scalable we have proposed two optimization techniques which can speed up the unsupervised shapelet discovery independently of each other. Moreover, if we combine the two optimization procedures, it results in a super linear speedup. In addition to the above, we can also cast our u-shapelet discovery algorithm as an anytime algorithm. In my final contribution, we have developed a novel and robust algorithm for mining mice vocalizations with symbolized representation. Our algorithm processes large scale, high dimensional, noisy mice vocalization by dimensionality reduction and cardinality reduction and make it suitable for knowledge discovery like classification, clustering, similarity search, motif discovery, contrast set mining etc.	algorithm;data mining	Jesin Zakaria	2013			constrained clustering;data stream clustering;computer science;data science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;data stream mining;cluster analysis;biclustering;affinity propagation;clustering high-dimensional data	ML	-2.719691410256273	-37.629077088235356	341
aaea7e5f08254996d3aa20b98d193c55c647c8a9	random walks, markov processes and the multiscale modular organization of complex networks	institutional repositories;graph theory;community detection;cs si;time scale;complex networks;random walks stochastic dynamics potts model stationary distribution dynamic based community detection statistical property time parametrized function markov stability random graph model edge counting quality function combinatorial graph complex network multiscale modular organization markov process;fedora;cond mat stat mech;complex network;dynamical processes;optimization community detection partition stability multiscale structure random walks graph theory centrality;random walks;journal article;vital;statistical properties;centrality;community structure;null model;optimization;random processes complex networks graph theory markov processes network theory graphs;communities;partition stability;multi resolution;vtls;multiscale structure;physics soc ph;computational efficiency;ils;multiscale structures;structural properties;communities optimization graph theory multiscale structures complex networks	Most methods proposed to uncover communities in complex networks rely on combinatorial graph properties. Usually an edge-counting quality function, such as modularity, is optimized over all partitions of the graph compared against a null random graph model. Here we introduce a systematic dynamical framework to design and analyze a wide variety of quality functions for community detection. The quality of a partition is measured by its Markov Stability, a time-parametrized function defined in terms of the statistical properties of a Markov process taking place on the graph. The Markov process provides a dynamical sweeping across all scales in the graph, and the time scale is an intrinsic parameter that uncovers communities at different resolutions. This dynamic-based community detection leads to a compound optimization, which favours communities of comparable centrality (as defined by the stationary distribution), and provides a unifying framework for spectral algorithms, as well as different heuristics for community detection, including versions of modularity and Potts model. Our dynamic framework creates a systematic link between different stochastic dynamics and their corresponding notions of optimal communities under distinct (node and edge) centralities. We show that the Markov Stability can be computed efficiently to find multi-scale community structure in large networks.	algorithm;centrality;complex network;graph property;heuristic (computer science);markov chain;markov property;mathematical optimization;potts model;random graph;stationary process;stochastic process	Renaud Lambiotte;Jean-Charles Delvenne;Mauricio Barahona	2014	IEEE Transactions on Network Science and Engineering	10.1109/TNSE.2015.2391998	conductance;markov chain;mathematical optimization;combinatorics;discrete mathematics;null model;graph partition;graph theory;machine learning;mathematics;markov process;markov model;complex network;statistics;variable-order markov model	ML	42.01900253682471	8.476693281705627	342
85ae4bb02e4804ec751df050632d1164b0d6b4de	toward on-chip functional neuronal networks: computational study on the effect of synaptic connectivity on neural activity	neurophysiology bioelectric potentials biological tissues biomedical measurement brain models medical computing neural nets;specific spike train on chip functional neuronal networks computational study synaptic connectivity neural activity unified computational experimental approach synaptic activity neuron activity neuronal tissue organ action potential recording neuron population small cortical neuronal network dynamics constrained connectivity multicompartmental hodgkin huxley model neuron software experimental data synthesized nn network response;biological neural networks computational modeling system on chip data models artificial neural networks nerve fibers	This paper presents a new unified computational-experimental approach to study the role of the synaptic activity on the activity of neurons in the small neuronal networks (NNs). In a neuronal tissue/organ, this question is investigated with higher complexities by recording action potentials from population of neurons in order to find the relationship between connectivity and the recorded activities. In this approach, we study the dynamics of very small cortical neuronal networks, which can be experimentally synthesized on chip with constrained connectivity. Multi-compartmental Hodgkin-Huxley model is used in NEURON software to reproduce cells by extracting the experimental data from the synthesized NNs. We thereafter demonstrate how the type of synaptic activity affects the network response to specific spike train using the simulation results.	action potentials;action potential;anatomy, regional;artificial neural network;computation;computation (action);experiment;hodgkin–huxley model;huxley: the dystopia;neural network simulation;neuron;neuronal ceroid-lipofuscinoses;simulation;synapses;synaptic package manager;synaptic connectivity;synaptic weight;synthetic intelligence	Armin Najarpour Foroushani;Ebrahim Ghafar-Zadeh	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6943899	neuroscience;computer science;artificial intelligence;machine learning;spiking neural network	ML	17.046835207162957	-69.10991672139323	343
060fb58c595197a4acc345961ef3cb3f772eee49	an fpga memcached appliance	energy efficiency;fpga;memcached appliance;low power;data centers	Providing low-latency access to large amounts of data is one of the foremost requirements for many web services. To address these needs, systems such as Memcached have been created which provide a distributed, all in-memory key-value store. These systems are critical and often deployed across hundreds or thousands of servers. However, these systems are not well matched for commodity servers, as they require significant CPU resources to achieve reasonable network bandwidth, yet the core Memcached functions do not benefit from the high performance of standard server CPUs. In this paper, we demonstrate the design of an FPGA-based Memcached appliance. We take Memcached, a complex software system, and implement its core functionality on an FPGA. By leveraging the FPGA's design and utilizing its customizable logic to create a specialized appliance we are able to tightly integrate networking, compute, and memory. This integration allows us to overcome many of the bottlenecks found in standard servers. Our design provides performance on-par with baseline servers, but consumes only 9% of the power of the baseline. Scaled out, we see benefits at the data center level, substantially improving the performance-per-dollar while improving energy efficiency by 3.2X to 10.9X.	algorithm;baseline (configuration management);bottleneck (software);central processing unit;data access;data center;dynamic random-access memory;field-programmable gate array;foremost;general-purpose modeling;in-memory database;input/output;key-value database;memcached;memory management;microprocessor development board;requirement;server (computing);software system;total cost of ownership;usb flash drive;web service	Sai Rahul Chalamalasetti;Kevin T. Lim;Mitch Wright;Alvin AuYoung;Parthasarathy Ranganathan;Martin Margala	2013		10.1145/2435264.2435306	embedded system;data center;parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use;field-programmable gate array	OS	-14.582646004887808	53.33041114897199	344
6f6ce00bac10e99034bfef53da5de484e4805db9	audio classification based on adaptive partitioning	databases;audio segmentation;signal classification audio signal processing;audio signal processing;audio databases information retrieval ontologies spatial databases feature extraction computer science robustness tv web page design books;homogeneous segment;prototypes;audio classification;training;indexing terms;data mining;classification;new class detection;accuracy;feature extraction;homogeneous segment content based audio classification method adaptive partitioning audio segmentation composite sound plausible audio matching class;signal classification;classification algorithms;composite sound;plausible audio matching class;content based audio classification method;adaptive partitioning;new class detection audio segmentation classification	This paper presents an audio classification system that provides improved accuracy, robustness and flexibility over reported content-based audio classification methods. The system reads an input audio file, performs segmentation and classification of the composite sounds contained within the file and, for each sound clip, determines the most plausible matching class of audio in the database. Improvements in the accuracy of audio classification are largely due to the partitioning of the input audio file into homogeneous segments while the incorporation of new class detection offers greater flexibility of use.		Jessie Xin Zhang;Stephen Brooks;Jacqueline L. Whalley	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202541	statistical classification;computer vision;audio mining;speech recognition;index term;audio signal processing;feature extraction;biological classification;computer science;machine learning;speech coding;pattern recognition;accuracy and precision;prototype	Robotics	-8.58553665665601	-92.17027209510978	345
71cf374856d34ed5aaf02ca2d806b9d2a154d169	qualifizierung als beschleuniger für die plm-einführung		Copolyester-carbonate resins, and articles molded therefrom, exhibiting improved processability comprising the reaction product of; (i) at least one dihydric phenol; (ii) a carbonyl halide carbonate precursor; (iii) at least one difunctional carboxylic acid or a reactive derivative thereof; and (iv) at least one bishaloformate represented by the general formula wherein X represents a halogen radical and R represents an alkylene radical, said bishaloformate being present in an amount effective to improve the processability of said resin.		Rainer Pusch	2006	HMD - Praxis Wirtschaftsinform.		knowledge management;carboxylic acid;engineering;phenol;halogen;carbonate;polymer chemistry;halide	Vision	97.78058660997299	-12.038407366700737	346
9bcc59080ef091af56598d07c11bcfe05f6a4508	cloud-based bp system integrated with cpoe improves self-management of the hypertensive patients: a randomized controlled trial	randomized controlled trial;hypertension;cloud based bp system;cpoe;self management	BACKGROUND Less than 50% of patients with hypertensive disease manage to maintain their blood pressure (BP) within normal levels.   OBJECTIVE The aim of this study is to evaluate whether cloud BP system integrated with computerized physician order entry (CPOE) can improve BP management as compared with traditional care.   METHODS A randomized controlled trial done on a random sample of 382 adults recruited from 786 patients who had been diagnosed with hypertension and receiving treatment for hypertension in two district hospitals in the north of Taiwan. Physicians had access to cloud BP data from CPOE. Neither patients nor physicians were blinded to group assignment. The study was conducted over a period of seven months.   RESULTS At baseline, the enrollees were 50% male with a mean (SD) age of 58.18 (10.83) years. The mean sitting BP of both arms was no different. The proportion of patients with BP control at two, four and six months was significantly greater in the intervention group than in the control group. The average capture rates of blood pressure in the intervention group were also significantly higher than the control group in all three check-points.   CONCLUSIONS Cloud-based BP system integrated with CPOE at the point of care achieved better BP control compared to traditional care. This system does not require any technical skills and is therefore suitable for every age group. The praise and assurance to the patients from the physicians after reviewing the Cloud BP records positively reinforced both BP measuring and medication adherence behaviors.		Peisan Lee;Ju-Chi Liu;Ming-Hsiung Hsieh;Wen-Rui Hao;Yuan-Teng Tseng;Shuen-Hsin Liu;Yung-Kuo Lin;Li-Chin Sung;Jen-Hung Huang;Hung-Yu Yang;Jong-Shiuan Ye;He-Shun Zheng;Min-Huei Hsu;Syed Abdul Shabbir;Richard Lu;Phung Anh Nguyen;Usman Iqbal;Chih-Wei Huang;Wen-Shan Jian;Yu-Chuan Li	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.04.003	intensive care medicine;medicine;pathology;surgery;randomized controlled trial	HCI	-61.389725613064016	-64.84540330290767	347
e9a634facf1ed117ccdddd1a633645ee1a026e9c	hmdd v2.0: a database for experimentally supported human microrna and disease associations	genes;databases nucleic acid;disease;datasets;journal;science technology;internet;micro rna;disease association;life sciences biomedicine;human cancers;epigenesis genetic;similarity;biochemistry molecular biology;humans;micrornas;biogenesis	The Human microRNA Disease Database (HMDD; available via the Web site at http://cmbi.bjmu.edu.cn/hmdd and http://202.38.126.151/hmdd/tools/hmdd2.html) is a collection of experimentally supported human microRNA (miRNA) and disease associations. Here, we describe the HMDD v2.0 update that presented several novel options for users to facilitate exploration of the data in the database. In the updated database, miRNA-disease association data were annotated in more details. For example, miRNA-disease association data from genetics, epigenetics, circulating miRNAs and miRNA-target interactions were integrated into the database. In addition, HMDD v2.0 presented more data that were generated based on concepts derived from the miRNA-disease association data, including disease spectrum width of miRNAs and miRNA spectrum width of human diseases. Moreover, we provided users a link to download all the data in the HMDD v2.0 and a link to submit novel data into the database. Meanwhile, we also maintained the old version of HMDD. By keeping data sets up-to-date, HMDD should continue to serve as a valuable resource for investigating the roles of miRNAs in human disease.	database;download;experiment;interaction;mental association;micrornas;oldversion.com;type iii site-specific deoxyribonuclease;world wide web;study of epigenetics;width	Yang Li;Chengxiang Qiu;Jian Tu;Bin Geng;Jichun Yang;Tianzi Jiang;Qinghua Cui	2014		10.1093/nar/gkt1023	biology;bioinformatics;genetics;microrna	DB	-1.16311809247668	-61.00104452788254	348
3126ed74bf6b28b150f68d07cc1f39f4f71e0b63	increasing error tolerance in biometric systems	biometric authentication;authentication;mean error;error correcting codes;error correction code;success rate;biometric system	Using biometrics for authentication or cryptosystems would require tolerance of errors of the order of 40%. Most of the proposed techniques either use the reliable bits of a biometric or good quality biometric which does not have such a large variation. However, biometric authentication can be viewed as a design of error correcting codes wherein the redundancy provides the tolerance to errors. This paper explores addition of redundancy to support variations as high as 40% in biometrics. We investigated the error characteristics of biometric templates and found that some blocks within the template tend to produce errors higher than the mean error. To handle this behavior we propose a two level correction. We extensively tested our proposed method using iris images from the commercial Bath dataset [13] and found that the proposed method can tolerate the errors with a success rate of 99.07%, while not accepting any imposters.	authentication;biometrics;code;cryptosystem;error detection and correction;error-tolerant design;experiment;forward error correction	Parman Sukarno;Mieng Phu;Nandita Bhattacharjee;Bala Srinivasan	2010		10.1145/1971519.1971531	error detection and correction;computer science;theoretical computer science;authentication;mean squared error;internet privacy;computer security;biometrics	HCI	14.588373735526016	64.72672588507619	349
63c0b5eeab0637dc6f056f747fdfbc4c93d75f33	test-driven fault navigation for debugging reproducible failures		Michael Perscheid, Michael Haupt, Robert Hirschfeld and Hidehiko Masuhara Debugging activities, particularly those for searching for failure causes, are often laborious and timeconsuming. Techniques such as spectrum-based fault localization or back-in-time debugging help programmers to reduce development cost. However, such approaches are often limited to a single point of view, ignoring the need for combined perspectives. We present test-driven fault navigation as an interconnected guide to failure causes. Based on failurereproducing unit tests, we introduce a novel systematic top-down debugging process with corresponding tool support. With spectrum-based fault localization, we offer navigation to suspicious system parts and erroneous behavior in the execution history and rank developers most qualified for addressing the faults localized. Our evaluation illustrates the practicability of this approach, its high accuracy of developer recommendation, and the fast response times of its corresponding tool suite.	breadth-first search;debugging;failure cause;programmer;software bug;top-down and bottom-up design;unit testing;usability testing;version control	Michael Perscheid	2013			embedded system;real-time computing;operating system;algorithmic program debugging;computer security;algorithm	HCI	-59.98194343044381	37.11908324363045	350
f0abf3385e4a13bd88780fe2a6c818907f59fefc	multi-modal convergence maps: from body schema and self-representation to mental imagery	multi-modal convergence map;exploratory use;body schema;arm proprioception;multiple modality;multiple dimension;multimodal convergence;mental imagery;multiple cortical area;multimodal contingency;early cortical amodal computation;artificial neural network model	Understanding the world involves extracting the regularities that define the interaction of the behaving organism within this world, and computing the statistical structure characterizing these regularities. This can be based on contingencies of phenomena at various scales ranging from correlations between sensory signals (e.g., motor-proprioceptive loops) to high-level conceptual links (e.g., vocabulary grounding). Multiple cortical areas contain neurons whose receptive fields are tuned for signals co-occurring in multiple modalities. Moreover, the hierarchical organization of the cortex, described within the Convergence Divergence Zone framework, defines an ideal architecture to extract and make use of contingency at increasing levels of complexity. We present an artificial neural network model of the early cortical amodal computations, which we have demonstrated on the humanoid robot iCub. This model explains and predicts findings in neurophysiology and neuropsychology along with being an efficient tool to control the robot. In particular, through exploratory use of the body, the system learns a form of body schema in terms of specific modalities (e.g., arm proprioception, gaze proprioception, vision) and their multimodal contingencies. Once multimodal contingencies have been learned, the system is capable of generating and exploiting internal representations or mental images based on inputs in one of these multiple dimensions. The system thus provides insight on a possible neural substrate for mental imagery within the context of multimodal convergence.	map;modal logic	Stéphane Lallée;Peter Ford Dominey	2013	Adaptive Behaviour	10.1177/1059712313488423	computer vision;artificial intelligence;communication	Vision	18.998633560766923	-67.745012767317	351
99e2c1bee954d36567c454eca998777122286626	mapping contexts to vocabularies to represent intentions		In the framework of multi-target use of a given ontology, thi s paper proposes a representation of vocabularies based on the iden tification of elementary vocabularies, which can be equivalently defin ed using specializations of the “kind of” relation. It defines a way of c mbining contexts and vocabularies that allows context-specific querying.	conceptual graph;definition;vocabulary	Rallou Thomopoulos;Marie-Laure Mugnier;Michel Leclère	2006			knowledge management;data mining;unified medical language system	NLP	-37.09373812550257	7.626497087772221	352
23ddeeb6e3697ca049cef6901375113d2d0afc8d	promoting the effective use of ict for enhancing education in the arab world	teaching cloud computing computer aided instruction cultural aspects mobile computing natural language processing;educational mobile apps ict usage education enhancement arab world arab league educational cultural and scientific organization alecso educational activities cultural activities scientific activities arab countries plan for the development of education in the arab countries arab educational system educational improvement learning teaching practice strategic ict projects educational arab system open educational resources oer k 12 education mooc arabic language learning cloud computing technologies educational mobile content;education mobile communication cloud computing cultural differences organizations buildings guidelines	"""Summary form only given. The Arab League Educational, Cultural and Scientific Organization (ALECSO) has, as its primary responsibility, the promotion and coordination of educational, cultural and scientific activities for Arab countries. In this perspective, a """"Plan for the Development of Education in the Arab Countries"""" (2008-2018) is being implemented by ALECSO. The implementation plan aims at systematically developing the Arab educational system. One of its fundamental dimensions, to be taken into consideration for Educational Improvement, deals with the effective use of ICTs in education to enhance learning/teaching practices. In this talk, we will explore a set of ongoing strategic ICT projects aiming to contribute in the innovation and the development of the educational Arab system: Promoting Open Educational Resources (OERs) for K-12 education in Arab countries, Creating MOOCs for Arabic language learning for native and non-native speakers, Promoting cloud computing technologies for education and research in the Arab world, and the ALECSO Apps store for promoting educational mobile content and apps."""	cloud computing;massive open online course	Mohamed Jemni	2014	2014 IEEE 14th International Conference on Advanced Learning Technologies	10.1109/ICALT.2014.10	knowledge management;multimedia;pedagogy	HCI	-73.57036286570649	-33.01580536770785	353
45b5d7aa2b5f042bf3f4047423ca24e9957ab9fc	an optimization perspective of the superiority of noma compared to conventional oma		Existing work regarding the performance comparison between nonorthogonal multiple access (NOMA) and orthogonal multiple access (OMA) can be generally divided into two categories. The work in the first category aims to develop analytical results for the comparison, often with fixed system parameters. The work in the second category aims to propose efficient algorithms for optimizing these parameters, and compares NOMA with OMA by computer simulations. However, when these parameters are optimized, the theoretical superiority of NOMA over OMA is still not clear. Therefore, in this paper, the theoretical performance comparison between NOMA and conventional OMA systems is investigated, from an optimization point of view. First, sum rate maximizing problems considering user fairness in both NOMA and various OMA systems are formulated. Then, by using the method of power splitting, a closed-form expression for the optimum sum rate of NOMA systems is derived. Moreover, the fact that NOMA can always outperform any conventional OMA systems, when both are equipped with the optimum resource allocation policies, is validated with rigorous mathematical proofs. Finally, computer simulations are conducted to validate the correctness of the analytical results.	algorithm;computer simulation;correctness (computer science);fairness measure;mathematical optimization;oma;point of view (computer hardware company)	Zhiyong Chen;Zhiguo Ding;Xuchu Dai;Rui Zhang	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2725223	mathematical optimization;noma;resource management;mathematics;telecommunications link;correctness;mathematical proof;orthogonal frequency-division multiplexing;resource allocation	EDA	34.28137748958309	90.18597460095911	354
c749b45a9ae5d55bfa14c940244cd0cdd32cd348	the theoretical and experimental analysis of ultrasound contrast agent micro bubbles: the sonovue	phantoms;lipid ultrasound contrast agent microbubbles sonovue acoustic characteristics ultrasound diagnosis ultrasound therapy second harmonic wave high quality image numerical simulation fundamental wave bubble oscillation;ultrasonic imaging;acoustics;会议论文;liquids;mathematical model harmonic analysis numerical models acoustics liquids phantoms ultrasonic imaging;mathematical model;patient treatment biomedical ultrasonics;2nd harmonic wave sonovue uca;numerical models;harmonic analysis	Sonovue is a kind of ultrasound contrast agent (UCA) with a stable shell made by lipid, within which is SF6. Due to the specific acoustic characteristics, UCA has been widely used in ultrasound diagnosis and therapy. In some cases, the 2nd harmonic wave of the scattered signal should be strong enough to help form a high quality image. To better utilize the bubbles to get enough intensity of the 2nd harmonic wave of the scattered signal, numerical simulation and corresponding experiments based on the several models proposed before have been analyzed to study the behavior of the bubbles. In this paper, we use the ratio of the second harmonic wave and fundamental wave to evaluate the oscillation of the bubbles. Besides, we have compared different simulation models and designed corresponding platform of phantom study. Finally, we find that the second harmonic wave will get a remarkable growth when the driving pressure is under 150KPa in the simulation or under 200KPa in the experiment.	acoustic cryptanalysis;computer simulation;display resolution;emoticon;experiment;imaging phantom;medical ultrasound;phantom reference;unicode collation algorithm	Shuiying Li;Yurong Huang;Yi Cheng;Jinhua Yu;Yuanyuan Wang	2015	2015 8th International Conference on Biomedical Engineering and Informatics (BMEI)	10.1109/BMEI.2015.7401521	acoustics;harmonic analysis;mathematical model;mathematics;optics;statistics	Robotics	92.81419468098095	-24.584556631342473	355
2bb813318c3ab99ca132974a2a9117cf87582779	single stage high-frequency non-isolated step-up sinusoidal inverter with three ground-side power switches			network switch;power inverter	Liang Hu;Xueye Wei;Jianguang Ma;Junhong Zhang	2018	IEICE Electronic Express	10.1587/elex.15.20180335	electronic engineering;inverter;computer science	HCI	63.72423068994555	42.495382399550444	356
27c5c9ede9be80e458baaff417f86fec95c456fb	modelling mailing list behaviour in open source projects: the case of arm embedded linux	information systems;collaborative work;open source projects;embedded systems;social networks;web based communities;virtual communities;open source	One of the benefits firms can derive from using Open Source Software (OSS) is informal development collaboration, and the primary tool for collaboration and coordination are group mailing lists. The purpose of the paper is modelling mailing lists behaviour in OSS projects, using a set of descriptors that could inform about their quality and their evolution. As a case study, a mailing list focused on ARM embedded Linux has been selected. Messages posted to this list from 2001 to 2006 have been extracted, and factor analysis has been applied to obtain the underlying patterns of behaviours. Theory about communities of practice has been used to understand the meaning of the extracted patterns. Their time distribution is finally described. The paper provides new insights into the behaviour of mailing list as a source of support for OSS projects and highlights the importance of an involved core of individuals inside the community.	arm architecture;embedded system;factor analysis;linux on embedded systems;open sound system;open-source software;tanenbaum–torvalds debate;thread (computing)	Sergio L. Toral Marín;M. Rocío Martínez-Torres;Federico Barrero	2009	J. UCS	10.3217/jucs-015-03-0648	computer science;data mining;database;world wide web;information system;social network	SE	-81.88582844456135	-20.268715826438992	357
bdabf16611798c5267e7d1c8287dc8b03bcb41bc	event management for uncertainties in collaborative production scheduling and transportation planning: a review		This paper presents a review of using event management to deal with the uncertainties in production scheduling and transportation planning processes at the operational level. Moreover, it argues the importance of considering uncer- tainties and the application of event management in a collaborative production and transportation planning process at the operational level.		Bernd Scholz-Reiter;Yi Tan;Nagham M. El-Berishy;José B. S. Santos	2012		10.1007/978-3-642-35966-8_16	simulation;systems engineering;operations management;business;transportation planning	HCI	-61.948877217937955	8.376228159755188	358
beb8ae5d9cac84ca8247335a3614f4dae1aae04c	a real-time personal authentication system based on incremental feature extraction and classification of audiovisual information		We propose a new approach to a real-time personal authentication system based on incrementally updated visual (face) and audio (voice) features of persons. The proposed system consists of real-time face detection, incremental audiovisual feature extraction, and incremental neural classifier model with long-term memory. The face detection part, a biologically motivated face-color preferable selective attention model first localizes face candidate regions in natural scenes, and then the Adaboost-based face detection identifies human faces from the localized face-candidate regions. The mel-frequency cepstral coefficient is used for vocal feature extraction of speakers. Moreover, incremental principal component analysis (IPCA) is used to reduce the dimensions of audiovisual features and to update them incrementally. The features extracted by IPCA is fed to the resource allocating network with long-term memory which learns facial and vocal features incrementally and recognizes faces in real time. Experimental results show that the proposed system can enhance the test performance incrementally without serious forgetting. In addition, a multi-modal (facial and vocal) feature effectively increases the robustness of the personal authentication system in noisy environments.		Young-Min Jang;Minho Lee;Seiichi Ozawa	2011	Evolving Systems	10.1007/s12530-011-9033-2	face detection;speech recognition;computer science;machine learning;pattern recognition	DB	-2.3177181353673144	-85.66422189166956	359
45f7d02d5c855dd342b52c866d3562ba9f9d5858	minimum convex piecewise linear cost tension problem on quasi-k series-parallel graphs	piecewise linear;series parallel graph;series parallel;graph decomposition	"""This article proposes an extension, combined with the out-o f-kilter technique, of the aggregation method (that solves the minimum convex piecewise linear cos t tension problem, or CPLCT, on series-parallel graphs) to solve CPLCT on quasi series-par allel graphs. To make this algorithm efficient, the key point is to find a """"good"""" way of decomposing t he graph into series-parallel subgraphs. Decomposition techniques, based on the recogni tion of series-parallel graphs, are thoroughly discussed."""	algorithm;heuristic;piecewise linear continuation;series-parallel graph	Bruno Bachelet;Philippe Mahey	2004	4OR	10.1007/s10288-004-0049-3	1-planar graph;block graph;pathwidth;topological graph theory;mathematical optimization;split graph;combinatorics;series and parallel circuits;discrete mathematics;cograph;interval graph;graph product;piecewise linear function;longest path problem;dense graph;pancyclic graph;comparability graph;mathematics;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph;book embedding;line graph	ML	24.061838594383204	24.764669364670095	360
1d2cb970f0b9f5e1dcbd33363398a9391c0d56ea	dependence, correlation and gaussianity in independent component analysis	linear component;minimum entropy;larger framework;independent component analysis;arbitrary decorrelation constraint;mutual information;gram-charlier expansion;non-gaussianity;source separation;local geometry;ideal measure;cumulant expansions;key result;information geometry;linear transformation;second order;cumulant;non gaussianity	Independent component analysis (ICA) is the decomposition of a random vector in linear components which are ‘as independent as possible’. Here, ‘independence’ should be understood in its strong statistical sense: it goes beyond (second-order) decorrelation and thus involves the non Gaussianity of the data. The ideal measure of independence is the ‘mutual information’ and is known to be related to the entropy of the components when the search for components is restricted to uncorrelated components. This paper explores the connections between mutual information, entropy and non Gaussianity in a larger framework, without resorting to a somewhat arbitrary decorrelation constraint. A key result is that the mutual information can be decomposed, under linear transforms, as the sum of two terms: one term expressing the decorrelation of the components and one expressing their non Gaussianity. Our results extend the previous understanding of these connections and explain them in the light of information geometry. We also describe the ‘local geometry’ of ICA by reexpressing all our results via a Gram-Charlier expansion by which all quantities of interest are obtained in terms of cumulants.	decorrelation;entropy (information theory);independent computing architecture;independent component analysis;information geometry;mutual information	Jean-François Cardoso	2003	Journal of Machine Learning Research		econometrics;mathematical optimization;mathematics;statistics	ML	46.83191114213252	15.044772171360764	361
d9f7e941c3020f7de184e0ada89f196a95a4d1e3	resource sharing and remote utilization in communication servers	input output;information integration;resource dispatch;resource sharing;cooperative server cluster;concurrent processing;real time systems	The communication cluster servers require many key technologies in an information integration platform. in order to improve dependability, scalability, and other QoS features, we design a system structure to meet these requirements. The resource sharing and remote utilizing are applied to resolving the dynamical resource dispatch and the tasks distribution. The thread model and mechanism of the thread pool in the Non-blocking input/Output (NiO) are created which include a case trigger mechanism. The system successfully resolves thousands of terminals connected to the information integrated platform. Messages, files, data, and other information can transport among the platform, the clients and the terminals.	avaya unified communications management	Guofeng Qin;Qiyan Li;Xiuying Deng	2007		10.1007/978-3-540-74780-2_43	shared resource;input/output;real-time computing;simulation;computer science;information integration;operating system;database;distributed computing	Arch	-28.187264383120613	48.20871047547688	362
2b03e01e9f7caaacc0b3a6e2c343cef2c9e4e686	grouping customers for better allocation of resources to serve correlated demands	heuristique;decentralization;optimisation;minimal model;service level;optimizacion;equipement collectif;resource allocation;standard deviation;groupement consomateur;service management;customer grouping;decentralisation;prise decision;service operation;equipamiento colectivo;linearisation;linearizacion;facility;operation service;linearization;job shop;optimization;safety factor;decentralization of facilities;heuristics;asignacion recurso;allocation ressource;toma decision;nonlinear optimization;service operations;descentralizacion	In this paper, we discuss a common decision-making problem arising in the allocation and decentralization of resources under uncertain demand. The total resource requirements for a given service level equals the sum of mean demands plus a safety factor multiplied by the standard deviations of demands. Since the demand means are una!ected by any customer groupings, we attempt to exploit demand correlations for developing customer groups such that the sum of the standard deviations over all groups is minimized. A concave minimization model with binary variables is developed for this purpose and a heuristic partitioning method is proposed to e$ciently solve the model. The model is appropriate for both manufacturing and service management with potential applications in salesforce allocation, grouping of machines in job shops, and allocation of plant capacities.	concave function;heuristic;operating system service management;requirement	Rajesh Tyagi;Chandrasekhar Das	1999	Computers & OR	10.1016/S0305-0548(99)00023-4	mathematical optimization;nonlinear programming;mathematics;management science;operations research;decentralization	Metrics	9.643053454184221	-1.6653450007307855	363
ba3fbe7ab55c531b1041330817e6bc2daca41a42	sparse gaussian process regression for compliant, real-time robot control	training;approximation methods training ground penetrating radar computational modeling data models robots accuracy;accuracy;computational modeling;ground penetrating radar;robots;approximation methods;robot dynamics approximation theory gaussian processes learning artificial intelligence regression analysis;training point selection sparse gaussian process regression compliant real time robot control large data sets sparse gp model approximation learning performance learning inverse dynamics models;data models	Sparse Gaussian process (GP) models provide an efficient way to perform regression on large data sets. The key idea is to select a representative subset of the available training data, which induces the sparse GP model approximation. In the past, a variety of selection criteria for GP approximation have been proposed, but they either lack accuracy or suffer from high computational costs. In this paper, we introduce a novel and straightforward criterion for successive selection of training points used for GP model approximation. The proposed algorithm allows a fast and efficient selection of training points, while being competitive in learning performance. As evaluation, we employ our approach in learning inverse dynamics models for robot control using very large data sets (e.g. 500.000 samples). It is demonstrated in experiments that our approximated GP model is sufficiently fast for real-time prediction in robot control. Comparisons with other state-of-the-art approximation techniques show that our proposed approach is significantly faster, while being competitive to generalization accuracy.	approximation algorithm;computation;expectation–maximization algorithm;experiment;gaussian process;greedy algorithm;inverse dynamics;kriging;model selection;real-time clock;real-time computing;robot control;sparse matrix	Jens Schreiter;Peter Englert;Duy Nguyen-Tuong;Marc Toussaint	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139547	robot;data modeling;mathematical optimization;simulation;ground-penetrating radar;computer science;artificial intelligence;machine learning;sparse approximation;accuracy and precision;computational model;statistics	Robotics	49.55137405921002	-26.23681317894821	364
bdd7575a26e0432a009b47308a7bd433c8d6a159	audio-visual robot command recognition: d-meta'12 grand challenge	audio visual categorization;multimodal learning	"""This paper addresses the problem of audio-visual command recognition in the framework of the D-META Grand Challenge1. Temporal and non-temporal learning models are trained on visual and auditory descriptors. In order to set a proper baseline, the methods are tested on the """"Robot Gestures"""" scenario of the publicly available RAVEL data set, following the leave-one-out cross-validation strategy. The classification-level audio-visual fusion strategy allows for compensating the errors of the unimodal (audio or vision) classifiers. The obtained results (an average audio-visual recognition rate of almost 80%) encourage us to investigate on how to further develop and improve the methodology described in this paper."""	baseline (configuration management);cross-validation (statistics);grand challenges;robot;temporal logic	Jordi Sanchez-Riera;Xavier Alameda-Pineda;Radu Horaud	2012		10.1145/2388676.2388760	computer vision;speech recognition;computer science;artificial intelligence;machine learning	Robotics	25.932483696005793	-57.228148170252375	365
c156750015319a456fbe5e1b3a247b4bed6f27e1	expected hypervolume improvement with constraints		Bayesian optimisation has become a powerful framework for global optimisation of black-box functions that are expensive to evaluate and possibly noisy. In addition to expensive evaluation of objective functions, many real-world optimisation problems deal with similarly expensive black-box constraints. However, there are few studies regarding the role of constraints in multi-objective Bayesian optimisation. In this paper, we extend the Expected Hypervolume Improvement by introducing expectation of constraints satisfaction and merging them into a new acquisition function called Expected Hypervolume Improvement with Constraints (EHVIC). We analyse the performance of our algorithm by estimating the feasible region dominated by Pareto front using 4 benchmark functions. The proposed method is also evaluated on a realworld problem of Alloy Design. We demonstrate that EHVIC is an effective algorithm that provides a promising performance by comparing to a well-known related method.		Majid Abdolshah;Alistair Shilton;Santu Rana;Sunil Kumar Gupta;Svetha Venkatesh	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545387	evolutionary computation;multi-objective optimization;mathematical optimization;artificial intelligence;merge (version control);linear programming;benchmark (computing);feasible region;gaussian process;pattern recognition;bayesian probability;computer science	AI	19.26979637528756	-9.31549775682824	366
11c90411b5fe0ddf4fd691594d70800c8e942c29	implicit schemes for the fokker-planck-landau equation	metodo directo;calcul scientifique;linear systems;82c80;schema conservatif;kinetic equation;metodo subespacio krylov;schema implicite;krylov subspace method;conservative schemes;methode sousespace krylov;implicit schemes;krylov methods;matrix inversion;conjugate gradient method;linear system;analyse numerique;kinetic equations;computacion cientifica;numerical analysis;fokker planck;entropie;algebra lineal numerica;algebre lineaire numerique;delai d execution;82d10;inversion matrice;65m06;plazo ejecucion;entropy;fokker planck landau equation;numerical linear algebra;methode gradient conjugue;82c40;equation fokker planck landau;systeme lineaire;krylov method;scientific computation;conjugate gradient methods;65y20;equation cinetique;methode directe;65f10;direct method;time allowed;implicit scheme	We propose time implicit schemes to solve the homogeneous Fokker-Planck-Landau equation in both the isotropic and 3D geometries. These schemes have properties of conservation and entropy. Moreover, they allow for large time steps, making them faster than the usual explicit schemes. To solve the involved linear systems, we prove that the use of Krylov-like solvers preserves the conservation properties. We show in particular that the Conjugate Gradient method can be used. Numerical tests are performed for the isotropic case and demonstrate an important gain in terms of CPU time, with the same accuracy as explicit schemes. This work is a first step to the development of fast implicit schemes to solve a class of inhomogeneous kinetic equations.	central processing unit;conjugate gradient method;entropy (information theory);functional programming;iteration;krylov subspace;krylov–bogolyubov theorem;linear system;matrix multiplication;numerical analysis;numerical method;preconditioner;quantum hall effect;solver;wavelet	Mohammed Lemou;Luc Mieussens	2005	SIAM J. Scientific Computing	10.1137/040609422	entropy;mathematical optimization;mathematical analysis;calculus;control theory;mathematics;linear system;flux limiter;algebra	Theory	88.01436197605985	16.497172154017964	367
ac94e5a19bf7b41ad67ece371f8f4a07d1874033	non-clairvoyant reduction algorithms for heterogeneous platforms	approximation algorithms;reduction;non clairvoyant algorithms;scheduling	Summary#R##N#We revisit the classical problem of the reduction collective operation in a heterogeneous environment. We discuss and evaluate four algorithms that are non-clairvoyant, that is, they do not know in advance the computation and communication costs. On the one hand, Binomial-stat and Fibonacci-stat are static algorithms that decide in advance which operations will be reduced, without adapting to the environment; they were originally defined for homogeneous settings. On the other hand, Tree-dyn and Non-Commut-Tree-dyn are fully dynamic algorithms, for commutative or non-commutative reductions. We show that these algorithms are approximation algorithms with constant or asymptotic ratios. We assess the relative performance of all four non-clairvoyant algorithms with heterogeneous costs through a set of simulations. Our conclusions hold for a variety of distributions. Copyright © 2014 John Wiley & Sons, Ltd.	algorithm	Anne Benoit;Louis-Claude Canon;Loris Marchal	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3347	randomized algorithms as zero-sum games;mathematical optimization;parallel computing;probabilistic analysis of algorithms;reduction;computer science;theoretical computer science;operating system;analysis of parallel algorithms;distributed computing;scheduling;approximation algorithm	PL	14.702815755373278	12.160207734061608	368
14604460b550cf7f2973ffa5fd6d038ca6957990	automatic detection of student mental models based on natural language student input during metacognitive skill training	science instruction;mental model detection;mathematics;comparative analysis;semantics;college students;prior learning;latent semantic indexing;intelligent tutoring systems;metacognition;prediction;natural language processing;self management;textual similarity	This article describes the problem of detecting the student mental models, i.e. students’ knowledge states, during the self-regulatory activity of prior knowledge activation in MetaTutor, an intelligent tutoring system that teaches students self-regulation skills while learning complex science topics. The article presents several approaches to automatically detecting students' mental models in MetaTutor based on paragraphs generated by students during prior knowledge activation. Three major categories of methods (content-based overlap methods; cohesion analysis of text; and tf-idf based weighted representations) were developed and combined with machine learning algorithms in order to automatically infer the underlying parameters. A detailed comparison among the methods and across all machine learning algorithms is provided. The evaluation of the proposed methods is performed by comparing the methods’ predictions with human judgments on a set of 309 prior knowledge activation paragraphs collected from experiments with the MetaTutor system on college students. According to the experiments, a word-weighting method, which uses tf-idf values calculated from the corpus, combined with a Bayes Nets machine learning algorithm, offers the most accurate results. Second best performance is given by a Latent Semantic Analysis-based approach enhanced with lexical features and combined with the machine learning algorithm of Logistic Regression.	algorithm;bayesian network;cohesion (computer science);embedded system;experiment;kappa calculus;latent semantic analysis;logistic regression;machine learning;mental model;natural language;performance;sensor;supervised learning;tf–idf	Mihai C. Lintean;Vasile Rus;Roger Azevedo	2012	I. J. Artificial Intelligence in Education	10.3233/JAI-2012-022	natural language processing;metacognition;qualitative comparative analysis;latent semantic indexing;prediction;computer science;artificial intelligence;machine learning;semantics	AI	-77.07716542524888	-49.25437545197403	369
8d91d6d903a8826854f757d7252820f145ffee4a	security issues on cloud computing		Cloud computing is a framework for providing various computing and storage services on the on-demand basis via the internet. It provides access to a user pool of shared network and storage resources using the server of the service provider without materially acquiring these resources. Hence, it saves managing cost and time for various organizations as well as individual users. Many industries, such as education, banking, healthcare and manufacturing are widely adapting cloud services due to their efficiency, flexibility and reduction of costs. Since, cloud services are universally accessible; it makes accessing data process a lot easier than traditional storage methods. Some popular cloud providers where client data is stored and maintained are Google, Amazon, SalesForce, Microsoft, etc. However, cloud technology is completely internet dependent and hence, faces as many threats as that are existing in the networks such as intranets. These threats can occur in various forms such as traffic hijacking, insecure interface and APIs, malicious insiders, abuse of cloud services, shared technology vulnerabilities, data breaches, perimeter security model broken or unknown risk profile. The primary objective of this paper is to acknowledge the major issues of security and provide a	cloud computing;computer accessibility;data breach;internet;intranet;perimeter;server (computing)	Harit Shah;Sharma Shankar Anandane;Shrikanth	2013	CoRR		computer security model;cloud computing security;security service;internet privacy;world wide web;computer security	Security	-50.84619841394978	61.072071531209644	370
2a226bf2c55b715a54aab56c2ff3358b35495519	extracting conceptual relations from persian resources	statistical approach;information resources;pragmatics;web sites knowledge acquisition natural language processing ontologies artificial intelligence pattern matching statistical analysis;wikipedia;information extraction;persian language;information technology;data mining;ontologies artificial intelligence;relation extraction;structure based approach relation extraction system statistical approach wikipedia articles information extraction persian language pattern matching ontology learning;ontology learning;internet;statistical analysis;system design;pattern matching;knowledge acquisition;wikipedia data mining ontologies statistical analysis pattern matching information technology natural language processing information resources clustering methods;web sites;structure based approach;wikipedia relation extraction ontology learning pattern matching information extraction;relation extraction system;ontologies;electronic publishing;clustering methods;encyclopedias;relational learning;natural language processing;wikipedia articles	In this paper we present a relation extraction system which uses a combination of pattern based, structure based and statistical approaches. This system uses raw texts and Wikipedia articles to learn conceptual relations. Wikipedia structures are rich source of information in relation extraction and are well used in this system. A set of patterns are extracted for Persian language and are used to learn both taxonomic and non-taxonomic relations. This system is one of the few relation extraction systems designed for Persian language and is the first system among them which uses Wikipedia structures in the process of relation learning.	heuristic (computer science);information source;pattern matching;relationship extraction;wikipedia	Hakimeh Fadaei;Mehrnoush Shamsfard	2010	2010 Seventh International Conference on Information Technology: New Generations	10.1109/ITNG.2010.191	natural language processing;the internet;computer science;ontology;pattern matching;data mining;brand;electronic publishing;information technology;information extraction;information retrieval;encyclopedia;pragmatics;systems design	NLP	-24.78904140502267	-64.32851698156874	371
093239229cca33c131db3decd96130ff0410242b	the power of well-structured systems	classic decision algorithm;well-structured system;wsts algorithm;exhaustive search;complexity analysis;basic theory;aka wsts;computational model;complexity-theoretical sense;important wsts model	Well-structured systems, aka WSTS, are computational models where the set of possible configurations is equipped with a well-quasiordering which is compatible with the transition relation between configurations. This structure supports generic decidability results that are important in verification and several other fields. This paper recalls the basic theory underlying well-structured systems and shows how two classic decision algorithms can be formulated as an exhaustive search for some “bad” sequences. This lets us describe new powerful techniques for the complexity analysis of WSTS algorithms. Recently, these techniques have been successful in precisely characterizing the power, in a complexity-theoretical sense, of several important WSTS models like unreliable channel systems, monotonic counter machines, or networks of timed systems.	algorithm;analysis of algorithms;brute-force search;computation;computational model	Sylvain Schmitz;Philippe Schnoebelen	2013		10.1007/978-3-642-40184-8_2	discrete mathematics;computer science;artificial intelligence;theoretical computer science;mathematics;algorithm	Logic	0.9385243458407909	22.42521109515199	372
1e37c42ce9e4cc50b7026b3a26aa129208431da4	characterizing incidents in cloud-based iot data analytics		Systems for big Internet of Things (IoT) data analytics are extremely complex. Different software components at different software stacks from different infrastructures and providers are involved in handling different types of data. Various types of incidents may occur during execution of such systems due to problems in software stacks, the data itself, and processing algorithms. Here incidents reflect unexpected context-specific situations that might happen within data themselves, machine learning algorithms, data analytics pipelines, and underlying big data services and computing platforms. It is important to address any incident that prevents the pipeline running correctly or producing the expected quality of analytics. In this paper, we show the need to characterize incidents for IoT data analytics in the cloud with real-world examples. We characterize incidents based on various aspects in IoT data analytics, including analytics phases, status of data, software services, and stakeholders. We introduce a meta-model for capturing knowledge about incidents.	algorithm;big data;cloud computing;component-based software engineering;internet of things;machine learning;metamodeling;pipeline (computing);statistical classification	Hong-Linh Truong;Manfred Halper	2018	2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2018.00068	data mining;data type;component-based software engineering;systems engineering;task analysis;big data;software;data analysis;cloud computing;computer science;analytics	DB	-51.92909647879361	41.857657523608125	373
9f5fa692566066e21ae3a174841e4722fbe4b2c0	photo data retrieval via p300 evoked potentials	evaluation performance;tecnologia electronica telecomunicaciones;recording;non invasive method;performance evaluation;electroencefalografia;evaluacion prestacion;potencial evocado;methode non invasive;electroencephalographie;life log;potentiel evoque;enregistrement;evoked potential;photo retrieval;methode moyenne;p300 evoked potentials;electroencephalography;tecnologias;grupo a;electroencephalogram;data retrieval;averaging method;human brain;registro;metodo medio;metodo no invasivo	In this letter, a new concept of life log retrieval using human brain activities is presented. The non-invasive electroencephalogram (EEG) recording was applied to have P300 evoked potentials during the photo retrieving tasks. Three subjects tried to select the photo images that interest them among nine according to their mental states. It was found that with four times EEG averaging, the performances of target photo selections could reach 90% for two subjects. This concept would be applicable in future to achieve intuitive retrieval of life log with large quantities of data.		Hideaki Touyama	2008	IEICE Transactions	10.1093/ietisy/e91-d.8.2212	recording;electroencephalography;computer science;artificial intelligence;data retrieval;lifelog	Vision	22.582595384533363	-92.65689990970924	374
733ba397de97c2c830eee38e47140777eb37c704	enabling responsible online gambling by real-time persuasive technologies		Online gambling, unlike other offline addiction forms, provides unprecedented opportunities for monitoring users’ behaviour in real-time, along with the ability to adapt persuasive interactions and messages that would match the gamblers usage and personal context. Online gambling industry usually offers Application Programming Interfaces (APIs) that are mainly intended to allow third-party applications to interact with their services and enhance user’s experience. In this article, we claim that such API’s can also be utilised to retrieve gamblers’ online data, such as browsing and betting history and other available offers, and use it to build more proactive and intelligent responsible gambling systems. We report on our experience in this field and make the argument that the available data for persuasive marketing and usability should, under certain usage conditions, also be made available for responsible online gambling services. We discuss the psychological foundations of our proposed approach and the risks and challenges typically identified when building such a software-assisted intervention, persuasion and emotion regulation technology. We also explain the potential impact of corporate social responsibility and data protection prospects. Furthermore, we explore the required principles that should be followed by the gambling industry for enabling responsible online gambling. We finally propose a conceptual architecture to show our vision and explain how it can be implemented. In the broader context, the article is intended to provide insights on building behavioural awareness and regulation information systems related to problematic digital media usage.		George Drosatos;Fotis Nalbadis;Emily Arden-Close;Victoria Baines;Elvira Bolat;Laura Vuillier;Theodoros Kostoulas;Marcin Budka;Sonia Wasowska;Maris Bonello;Jamie Brown;Tessa Corner;John McAlaney;Keith Phalp;Raian Ali	2018	CSIMQ	10.7250/csimq.2018-17.03	persuasion;application programming interface;digital media;internet privacy;corporate social responsibility;information system;usability;data protection act 1998;conceptual architecture;computer science	HCI	-83.43191245883189	-12.108034174293282	375
cd11d88f3c61e83a0b919d81c6396dad69bfd5dc	when mimo control meets mimo communication: a majorization condition for networked stabilizability		In this paper, we initiate the study of networked stabilization via a MIMO communication scheme between the controller and the plant. Specifically, the communication system is modeled as a MIMO transceiver, which consists of three par ts: an encoder, a MIMO channel, and a decoder. In the spirit of MIMO communication, the number of SISO subchannels in the transceiver is often greater than the number of data streamsto be transmitted. Moreover, the subchannel capacities are as sumed to be fixed a priori. In this case, the encoder/decoder pair gi ves an additional design freedom on top of the controller, leadi ng to a stabilization problem via coding/control co-design. It turns out that how to take the best advantage of the coding mechanism is quite crucial. From a demand/supply perspective, the desig n of the coding mechanism boils down to reshaping the demands for communication resource from different control inputs to match the given supplies. We study the problem for the case of AWGN subchannels and fading subchannels, respectively. In both cases, we arrive at a unified necessary and sufficient condition on th e capacities of the subchannels under which the coding/contr ol codesign problem is solvable. The condition is given in terms o f a majorization type relation. As we go along, systematic proc edures are also put forward to implement the coding/control co-des ign. A numerical example is presented to illustrate our results.	additive white gaussian noise;decision problem;digital subchannel;encoder;mimo;numerical analysis;simulation interoperability standards organization;transceiver	Wei Chen;Songbai Wang;Li Qiu	2014	CoRR		control engineering;electronic engineering;control theory;mathematics	Theory	36.12107821688147	72.81338007526138	376
7c967f4a7a625bb31a02682f1a38af123865bb32	premises of an algebra of japanese characters	calculus;script;model;language;chinese;characters;kanji	The Japanese language is made mostly of three characters sets: hiragana, katakana and kanji characters. Kanji characters are inherited from Chinese, and include thousands of glyphs. Characters, especially kanji, memorisation is thus an extremely challenging task for learners of the Japanese language, and even for native speakers who tend to forget the meaning or writing of uncommon characters. In this paper, we address this problem by proposing a novel memorisation technique based on an algebra defined for these kanji characters. By reusing classic algebra notations, the learner is able to rely on acquired knowledge (numbers algebra) to support his/her memorisation of Japanese characters. In addition, we consider automatic processing and application of this algebra to low-spec systems, for instance embedded systems.	embedded system;glyph;spec#	Antoine Bossard	2015		10.1145/2790798.2790800	arithmetic;natural language processing;furigana;kanji;computer science;kana;language;chinese;japanese writing system	NLP	-32.41688584893327	-84.84447442036785	377
c2889bdeb5b4d09f63dba7a59c47efb1c642febc	editorial note on the processing, storage, transmission, acquisition, and retrieval (p-star) of bio, medical, and health information	health information			Yuan-Ting Zhang;Carmen C. Y. Poon	2010	IEEE transactions on information technology in biomedicine : a publication of the IEEE Engineering in Medicine and Biology Society	10.1109/TITB.2010.2051834	medicine;computer science;data science;information retrieval	Visualization	-60.71627703023065	-8.676181957700997	378
661895e5b6edeab87f80c2839810d8a78c169c8f	a note on integral properties of periodic orbits	34c15;plan phase;funcion periodica;equation differentielle;34a34;sistema hamiltoniano;differential equation;hamiltonian systems;periodic function;propriete integrale;systeme hamiltonien;ecuacion diferencial;34c25;orbita periodica;hamiltonian system;fonction periodique;phase plane;orbite periodique;periodic orbit;periodic orbits	For the class of periodic functions u(x) that can be associated with closed orbits in the (u, phase-plane described by a differential equation of the form u 2 p F(u), around some local minimum of F(u), a useful integral expression is obtained for any derivative with respect to the parameter p of any suitably differentiable function of u and p integrated over one period in x. That is, an expression is found for (d/dp) k f:(P) f(u, p) dx, where )(p) is the period. Some illustrative examples are presented concerning relationships between moments of u(x) and some forms of the function F(u). Key words, periodic orbits, phase plane, Hamiltonian systems AMS(MOS) subject classifications. 34A34, 34C15, 34C25 1. Motivation. This article presents some useful formulae for studying integral properties of periodic functions u(z) which satisfy an equation of the form	maxima and minima	John Dold	1993	SIAM Review	10.1137/1035007	hamiltonian system;mathematical analysis;calculus;mathematics;geometry;quantum mechanics	Theory	80.1882402816425	8.597358675541702	379
51b5971f21acdc918a009a97dfdcfe24afcbe22c	approximation of conp sets by np-complete sets	combinatorial algorithm	Abs t rac t . It is said that a set L1 in a class C1 approximates a set L2 in a class C2 if L~ is a subset of L2. Approximation L1 is said to be optimal if there is no approximation L~ such that L~ D L1 and L~ L1 is infinite. When CI=P and C2=NP, it is known that there is no optimal approximation under a quite general condition unless P=NP. In this paper we discuss the case where Cl=the class of NP-complete sets and C2---coNP. A similar result as above that shows the difficulty of the optimal approximation is obtained. Approximating coNP sets by NP-complete sets play an important role in the efficient generation of test instances for combinatorial algorithms.	algorithm;approximation;co-np;np-completeness;p versus np problem;trusted computer system evaluation criteria	Kazuo Iwama;Shuichi Miyazaki	1995		10.1007/BFb0030815	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	16.624792375249932	19.43870127808084	380
1d5ca6733a74bdfb56c95c01572c0e2a18387c79	simple and fast inverse alignment	supervised learning;protein sequence;substitution score matrices;cutting plane algorithm;parametric sequence alignment;polynomial time algorithm;cutting plane algorithms;linear programming;linear program;sequence analysis;sequence alignment;affine gap penalties	For as long as biologists have been computing alignments of sequences, the question of what values to use for scoring substitutions and gaps has persisted. While some choices for substitution scores are now common, largely due to convention, there is no standard for choosing gap penalties. An objective way to resolve this question is to learn the appropriate values by solving the Inverse String Alignment Problem: given examples of correct alignments, find parameter values that make the examples be optimal-scoring alignments of their strings. We present a new polynomial-time algorithm for Inverse String Alignment that is simple to implement, fast in practice, and for the first time can learn hundreds of parameters simultaneously. The approach is also flexible: minor modifications allow us to solve inverse unique alignment (find parameter values that make the examples be the unique optimal alignments of their strings), and inverse near-optimal alignment (find parameter values that make the example alignments be as close to optimal as possible). Computational results with an implementation for global alignment show that, for the first time, we can find best-possible values for all 212 parameters of the standard protein-sequence scoring-model from hundreds of alignments in a few minutes of computation.	algorithm;benchmark (computing);computation;gap penalty;loss function;mathematical optimization;optimization problem;polynomial;protein family;sequence alignment;time complexity	John D. Kececioglu;Eagu Kim	2006		10.1007/11732990_37	biology;mathematical optimization;combinatorics;computer science;linear programming;machine learning;protein sequencing;sequence analysis;sequence alignment;mathematics	Comp.	26.04656126581448	8.012513279243565	381
3ba19720d5a39cad77b50aabb6f2649300c41f46	a 36-gb/s 1.3-mw/gb/s duobinary-signal transmitter exploiting power-efficient cross-quadrature clocking multiplexers with maximized timing margin		For wireline transmitters delivering a high-speed multi-level signal, such as pulse-amplitude-modulation-4 or duobinary, a high-performance multiplexer (MUX) is critical to serialize the low-speed parallel data into one full-speed output. To enhance the power efficiency and data eye’s opening, this paper proposes a universal 2-to-1 MUX, featuring a cross-quadrature clocking technique to enlarge the timing margin, and a simplified three-latch topology without delay buffers to boost the internal bandwidth (BW). The MUX ratios are extendable to 4-to-2 and 4-to-1, and their benefits are exemplified via a duobinary-signal transmitter. It further includes an output driver unifying the MUX-and-SUM operation, a BW-extended single-to-differential converter, and an active-inductor-embedded clock buffer for swing enhancement. Also, a predictive method for estimating the duobinary-signal data-dependent jitter according to the load capacitance of the output driver is developed. Fabricated in 65-nm CMOS, the transmitter exhibits a figure-of-merit of 1.3 mW/Gb/s at 36 Gb/s, while occupying a compact die area of 0.037 mm2.	bandwidth (signal processing);cmos;clock rate;data dependency;embedded system;extensibility;gigabyte;multiplexer;multiplexing;performance per watt;serialization;transmitter;yang	Yong Chen;Pui-in Mak;Chirn Chye Boon;Rui Paulo Martins	2018	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2018.2829725	multiplexer;electronic engineering;transmitter;electrical efficiency;timing margin;jitter;mathematics;cmos;bandwidth (signal processing);multiplexing	EDA	59.20765871311593	52.5139989611556	382
fce46d90a6717781c9a143a74de0477a66e0e7e8	optimization matching algorithm based on improved harris and sift	optimisation computational geometry edge detection feature extraction image matching;epipolar constraint image matching feature matching sift descriptor merging algorithm;image features;detectors;machine learning algorithms;optimisation;epipolar constraint;analytic geometry optimization matching algorithm sift feature detector local matching algorithms harris corner detector;local matching algorithms;edge detection;image matching;computational geometry;feature matching;feature extraction detectors algorithm design and analysis merging accuracy machine learning algorithms image matching;harris corner detector;accuracy;sift;analytic geometry;merging algorithm;feature extraction;nearest neighbor;merging;feature detector;algorithm design and analysis;optimization matching algorithm;sift descriptor	In order to restrain the problem of low automation level of feature detector and high matching consuming as for conventional local matching algorithms, the paper proposes an optimization matching algorithm based on Harris and SIFT algorithm. In this algorithm, Harris corner detector based on the ideology image break raises automation level of feature detector, and then merging algorithm optimizes the initial feature points in order to increase matching speed firstly. It constructs the SIFT descriptor for image feature description secondly. The matching result is finally obtained by the nearest neighbor matching algorithm on the condition that feature points are well-proportioned distributing. In addition, it applies the knowledge of analytic geometry to calculate the distance between matching point and epipolar line to reduce the error matching. The experimental results prove that the combination of those algorithms is effective. This algorithm wins high matching accuracy and matching time-consuming cuts down.	corner detection;epipolar geometry;experiment;feature (computer vision);feature extraction;feature model;genetic algorithm;harris affine region detector;k-nearest neighbors algorithm;mathematical optimization;nearest neighbor search;scale-invariant feature transform;stereopsis	Jie Zhao;Li-Juan Xue;Guo-Zun Men	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5581057	corner detection;algorithm design;mathematical optimization;detector;template matching;edge detection;feature extraction;computational geometry;computer science;3-dimensional matching;machine learning;optimal matching;pattern recognition;feature detection;scale-invariant feature transform;mathematics;accuracy and precision;analytic geometry;k-nearest neighbors algorithm;feature	Robotics	40.97714164386109	-57.20160194566057	383
d7448769fa3630eb4c41c6f344f4f5647fcb5859	feature extraction for smart sensing using multi-perspectives transformation		Air quality sensing systems, such as e-nose, are one of the complex dynamic systems; due to their sensitivity to electromagnetic interference, humidity, temperature, pressure and airflow. This yield to a Multi-Dependency effect over the output signal. To address the Multi-Dependency effect, we propose a multi-dimensional signal transformation for feature extraction. Our idea is analogous to viewing one huge object from different angles and arriving at different perspectives. Every perspective is partially true, but the final picture can be inferred by combining all perspectives. We evaluated our method extensively on two data sets including a publicly available e-nose dataset generated over a three-year period. Our results show higher performance in term of accuracies, F-measure, and stability when compared to standard methods.	feature extraction	Sanad Al-Maskari;Ibrahim A. Ibrahim;Xue Li;Eimad Abusham;Abdulqader Almars	2018		10.1007/978-3-319-92013-9_19	computer science;data mining;computer vision;feature extraction;airflow;electromagnetic interference;artificial intelligence;data set	HCI	35.7670906196753	-34.94820203829891	384
490c449e299fe419c1cff5484be1d3cb7850bd8d	ethical modernization: research misconduct and research ethics reforms in korea following the hwang affair	stem cell research;hwang affair;research ethics;regulation;scientific misconduct;ethical modernization	The Hwang affair, a dramatic and far reaching instance of scientific fraud, shocked the world. This collective national failure prompted various organizations in Korea, including universities, regulatory agencies, and research associations, to engage in self-criticism and research ethics reforms. This paper aims, first, to document and review research misconduct perpetrated by Hwang and members of his research team, with particular attention to the agencies that failed to regulate and then supervise Hwang's research. The paper then examines the research ethics reforms introduced in the wake of this international scandal. After reviewing American and European research governance structures and policies, policy makers developed a mixed model mindful of its Korean context. The third part of the paper examines how research ethics reform is proactive (a response to shocking scientific misconduct and ensuing external criticism from the press and society) as well as reactive (identification of and adherence to national or international ethics standards). The last part deals with Korean society's response to the Hwang affair, which had the effect of a moral atomic bomb and has led to broad ethical reform in Korean society. We conceptualize this change as ethical modernization, through which the Korean public corrects the failures of a growth-oriented economic model for social progress, and attempts to create a more trustworthy and ethical society.		Jongyoung Kim;Kibeom Park	2013	Science and engineering ethics	10.1007/s11948-011-9341-8	psychology;public relations;regulation;engineering ethics;research ethics;scientific misconduct;medicine;engineering;sociology;law	NLP	-73.4942280232459	-13.448721433141255	385
19d482fa3e724e2fc34f07f96e9d4ce7e2eb94f9	a brief fuzzy controller for an intelligent tracking system	fuzzy control;wireless control;path tracking	This paper introduces the design of an intelligent tracking and drawing system based on the Freescale MCU MC9S12DG128. By using infrared photoelectric sensors, the system can detect real-time route information. Then system steering angle and running speed can be effectively adjusted by combination of proportional control and fuzzy control methods, which realises the dynamical control of the whole system. Based on the communication between PC and RF wireless modules, remote control is effectively realised and accurate drawing is obtained. Finally, experiment results are given to show the effectiveness of the proposed system.	tracking system	Shaocheng Qu;Xiaobang Liu;Qinqin Wang;Zhiqiu Lei;Yuewei Che	2013	IJMIC	10.1504/IJMIC.2013.054321	control engineering;embedded system;computer science;engineering;artificial intelligence;control theory;fuzzy control system	Robotics	59.04373995162373	-28.84405687936314	386
9f4a7f413e1666e6ca77bd0996d150213bf9f038	turning ideas into products: the guide system	personal computer	The Guide system is a successful commercial product that originally came out of some ideas of a research project. Unlike many other hypertext systems, Guide is aimed at naive users and authors in the personal computer market. This paper evaluates the basic principles of Guide, and describes the interplay between the product and the continuing hypertext research programme.	hypertext;personal computer	Peter J. Brown	1987		10.1145/317426.317430	simulation;human–computer interaction;computer science;world wide web	DB	-64.73588099447106	-25.476416074346655	387
1c4c945375b525d47840df618be76b994bd88e51	brzozowski's and up-to algorithms for must testing	part of book or chapter of book	Checking language equivalence (or inclusion) of finite automata is a classical problem in Computer Science, which has recently received a renewed interest and found novel and more effective solutions, such as approaches based on antichains or bisimulations up-to. Several notions of equivalence (or preorder) have been proposed for the analysis of concurrent systems. Usually, the problem of checking these equivalences is reduced to checking bisimilarity. In this paper, we take a different approach and propose to adapt algorithms for language equivalence to check one prime equivalence in concurrency theory, must testing semantics. To achieve this transfer of technology from language to must semantics, we take a coalgebraic outlook at the problem.	algorithm;applet;automata theory;bisimulation;computer science;concurrency (computer science);congruence of squares;correctness (computer science);finite-state machine;microsoft outlook for mac;powerset construction;time complexity;turing completeness;universal instantiation	Filippo Bonchi;Georgiana Caltais;Damien Pous;Alexandra Silva	2013		10.1007/978-3-319-03542-0_1	logical equivalence;computer science;algorithm	Logic	-6.685822619479562	22.390536882012057	388
69173c31d05265b5e2361bae5cccf5a0ef809054	self-triggered rendezvous of gossiping second-order agents	institutional repositories;graph theory;convergence;fedora;time varying systems;time varying systems continuous time systems convergence graph theory multi robot systems;vital;algorithm design and analysis convergence heuristic algorithms multi agent systems synchronization position measurement;continuous time systems;relative measurement exchange self triggered rendezvous gossiping second order agents first order continuous time systems self triggered gossiping coordination algorithm time varying communication graph convergence properties coordination task double integrator agent formation pair wise communication;multi robot systems;vtls;ils	A recent paper by some of the authors introduced several self-triggered coordination algorithms for first-order continuous-time systems. The extension of these algorithms to second-order agents is relevant in many practical applications but presents some challenges that are tackled in this contribution and that require to depart from the analysis that was carried out before. We design a self-triggered gossiping coordination algorithm that induces a time-varying communication graph, which is enough connected to guarantee useful convergence properties, and allows us to achieve the desired coordination task in a formation of double-integrator agents that (i) establish pair-wise communication at suitably designed times and (ii) exchange relative measurements while reducing the sensing and communication effort.	algorithm;first-order predicate;reo coordination language	Claudio De Persis;Paolo Frasca;Julien M. Hendrickx	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6761064	real-time computing;simulation;convergence;computer science;graph theory;control theory;mathematics;distributed computing	Embedded	63.973219344900095	6.07278910052351	389
bb56531075a11d0215714c7083069ec42946bdc1	a secure and efficient framework to read isolated smart grid devices	data security smart grid power system security authentication communication systems;smart grids smart meters protocols security data communication monitoring	With increasing deployments of smart grid systems, a large quantity of energy usage and grid status data have been collected by smart grid devices like smart meters. To secure these critical and sensitive data, it is crucial to prevent unauthorized readings from these devices. Many authentication protocols have been proposed to control access to smart grid devices that are a part of the smart grid data communication network; however, authentication protocols to control readings from the isolated smart grid devices are mostly ignored. In this paper, we propose a secure and efficient framework to enable secure data readings from the isolated smart grid devices based on a two-phase authentication protocol. The framework not only makes use of the smart reader as a bridge to connect the isolated smart grid device and the smart grid cloud, but also considers the physical constraints of all the devices in the system. Security analysis shows that our framework is efficient and secure under most typical attacks, meanwhile it satisfies the hardware constraints of smart grid devices. Comprehensive performance evaluation also validates the efficiency of the proposed framework.	algorithm;authentication protocol;authorization;communications protocol;imperative programming;performance evaluation;prototype;smart card;smart device;smart meter;telecommunications network;two-phase commit protocol;usage data	Kewei Sha;Naif Alatrash;Zhiwei Wang	2017	IEEE Transactions on Smart Grid	10.1109/TSG.2016.2526045	computer science;security service;smart grid;internet privacy;computer security;internet of things;computer network	HPC	-45.47454809157362	66.35718637744121	390
8e73514e5e44be457640988d3cac3a3c35573abc	international conference on reconfigurable computing and fpgas, reconfig 2015, riviera maya, mexico, december 7-9, 2015			autodesk maya;field-programmable gate array;reconfigurable computing		2015				EDA	-51.095142258108424	-4.136307819253621	391
12648f44c7c4a7a8f80d973ff55394ab87a6a9f5	optimal distribution of oppositely charged phases: perfect screening and other properties	obstacle problem;charge distribution;screening;nonlocal coulomb interaction;35r35;49q10;49s05;charge neutrality;49k10;31b35	We study the minimum energy configuration of a uniform distribution of negative charge subject to Coulomb repulsive self-interaction and attractive interaction with a fixed positively charged domain. After having established existence and uniqueness of a minimizing configuration, we prove charge neutrality and the complete screening of the Coulomb potential exerted by the positive charge, and we discuss the regularity properties of the solution. We also determine, in the variational sense of Γ-convergence, the limit model when the charge density of the negative phase is much higher than the positive one.	calculus of variations;γ-convergence	Marco Bonacini;Hans Knüpfer;Matthias Röger	2016	SIAM J. Math. Analysis	10.1137/15M1020927	classical mechanics;mathematical optimization;partial charge;charge density;mathematics;obstacle problem;physics;quantum mechanics;electric-field screening;charge conservation	Metrics	86.64567075780047	3.1164075248165712	392
6e6150214699bcfcfca25ec5b91b994816c52cb9	wanderer wins breakthrough tournament			world wide web wanderer	Richard Lorentz	2017	ICGA Journal	10.3233/ICG-170028	simulation;artificial intelligence;tournament;computer science	Vision	-55.75000641210909	-17.55745251964444	393
61d40b869e99ff64c5a51ce5096a112826e5ea8e	optimal production run time for a deteriorating production system under an extended inspection policy	deteriorating production system;manufacturing defect;modelizacion;product inspection;control de calidad;inspection policy;politique inspection;gestion production;production system;politica inspeccion;temps minimal;deterioracion;production management;modelisation;product cycle;monitoring;gestion produccion;defaut fabrication;minimum time;controle qualite;deterioration;deteriorating production system product inspection production run time;monitorage;monitoreo;quality control;tiempo minimo;modeling;production run time;defecto fabricacion	A deteriorating production system is subjected to random deterioration from an in-control state to an out-of-control state with a general shift distribution. In order to reduce the defective items, part inspection policy, under which production inspections are performed only at the end of the production run, and full inspection policy are both considered in the literature. Moreover, the former dominates the latter. Since the product produced towards the end of a production cycle are more likely to be defective, it can further economize the inspection costs that they are directly reworked without inspection. In this paper, we propose an extended product inspection policy for a deteriorating production system. Product inspections are performed in the middle of a production cycle, and after the inspection, all products produced until the end of the production run are fully reworked. Based on the model, we show that there exists a production run time and a corresponding unique inspection policy such that the expected total cost per item per cycle is minimized. Finally, numerical examples are provided to illustrate our extended inspection policy, and indicate that such product inspection model will reduce the quality-related cost than part inspection does.	production system (computer science);run time (program lifecycle phase)	Fei Hu;Qun Zong	2009	European Journal of Operational Research	10.1016/j.ejor.2008.05.008	quality control;systems modeling;artificial intelligence;operations management;product lifecycle;production system	Robotics	6.580219000235372	-2.072421842626451	394
06151e335485234a3fa83784b0e127ba38c4de35	piaf: a knowledge-based/algorithmic top-down floorplanning system	design automation;design engineering;routing;top down;circuit topology;systems engineering and theory;integrated circuit interconnections design automation partitioning algorithms permission circuit topology design engineering systems engineering and theory laboratories australia routing;permission;integrated circuit interconnections;australia;partitioning algorithms;knowledge base	Over the last few years, techniques have been developed to automate the process of placing predefined circuit blocks and the routing of their interconnections. Few efforts have been directed at the more complex problem of floorplanning which involves not only placement but also specification of undefined cells to allow chip assembly in a way that minim&es area and satisfies performance constraints. Top-down floorplanning considers that block details may not be known and predictions or estimations have to be carried out. The infinite solution space of this design problem makes the es.timation process tricky, in that certain design attributes mu.st be instantiated without committing other important attributes. The classification of the attributes and the priority scheme used in their instantiation determine the floorplanning strat-	feasible region;floorplan (microelectronics);routing;timation;top-down and bottom-up design;undefined behavior;universal instantiation	Marwan A. Jabri;David J. Skellern	1989	26th ACM/IEEE Design Automation Conference	10.1145/74382.74480	topology;physical design;knowledge base;routing;electronic engineering;electronic design automation;computer science;systems engineering;computer-automated design;circuit design;top-down and bottom-up design;computer network;computer engineering	EDA	14.280466757307476	51.0562485186832	395
de3f5195594646fc09a46f9bf147c4272a51109b	data fusion of surface meshes and volumetric representations		The term Data Fusion refers to integrating knowledge from at least two independent sources of information such that the result is more than merely the sum of all inputs. In our project, the knowledge about a given specimen comprises its acquisitions from optical 3D scans and Computed Tomography with a special focus on limited-angle artifacts. In industrial quality inspection those imaging techniques are commonly used for non-destructive testing. Additional sources of information are digital descriptions for manufacturing, or tactile measurements of the specimen. Hence, we have several representations comprising the object as a whole, each with certain shortcomings and unique insights. We strive for combining all their strengths and compensating their weaknesses in order to create an enhanced representation of the acquired object. To achieve this, the identification of correspondences in the representations is the first task. We extract a subset with prominent exterior features from each input because all acquisitions include these features. To this end, regional queries from random seeds on an enclosing hull are employed. Subsequently, the relative orientation of the original data sets is calculated based on their subsets, as those comprise the—potentially defective—areas of overlap. We consider global features such as principal components and barycenters for the alignment, since in this specific case classical point-to-point comparisons are prone to error. Our alignment scheme outperforms traditional approaches and can even be enhanced by considering limited-angle artifacts in the reconstruction process of Computed Tomography. An analysis of local gradients in the resulting volumetric representation allows to distinguish between reliable observations and defects. Lastly, tactile measurements are extremely accurate but lack a suitable 3D representation. Thus, we also present an approach for converting them in a 3D surface suiting our work flow. As a result, the respective inputs are now aligned with each other, indicate the quality of the included information, and are in compatible format to be combined in a subsequent step. The data fusion result permits more accurate metrological tasks and increases the precision of detecting flaws in production or indications of wear-out. The final step of combining the data sets is briefly presented here along with the resulting augmented representation, but in its entirety and details subject to another PhD thesis within our joint project.		Andreas Beyer	2016			computed tomography;polygon mesh;principal component analysis;data set;artificial intelligence;computer vision;computer science;sensor fusion	Vision	48.81816673430754	-86.34034944780566	396
fa58089a1bd1bf91ae6f0b354b63a9200bfb0dd1	graphical t-designs via polynomial kramer-mesner matrices	polynomial kramer-mesner matrix;graphical t-designs;symmetric group;indexation;graphic design;maximal subgroup	Kramer-Mesner matrices have been used as a powerful tool to construct t-designs. In this paper we construct Kramer-Mesner matrices for xed values of k and t in which the entries are polynomials in n the number of vertices of the underlying graph. From this we obtain an elementary proof that with a few exceptions S 2] n is a maximal subgroup of S (n 2) or A (n 2). We also show that there are only nitely many graphical incomplete t-(v; k;) designs for xed values of 2 t and k at least in the cases k = t + 1, t = 2, and 2 t < k 6. All graphical t-designs are determined by the program DISCRETA for various small parameters. Most parameter sets are new for graphical designs, some also for general simple t-designs. The largest value of t for which graphical designs were found is t = 5. Some of the smaller designs which are block transitive are drawn as graphs.	directed graph;graphical user interface;kramer graph;maximal set;polynomial	Anton Betten;Mikhail H. Klin;Reinhard Laue;Alfred Wassermann	1999	Discrete Mathematics	10.1016/S0012-365X(99)90045-6		Theory	36.00647138725189	32.489830768170684	397
77d22d1c4ed79751e384b0d1ff75497a7a885afc	free-riding in bittorrent networks with the large view exploit	bittorrent;free riding	This paper presents an experimental study on the behavior of BitTorrent networks when selfish peers attempt to maintain high download rates without uploading. We modified a BitTorrent client so that it acquires a larger than normal view of a BitTorrent swarm and connects to all peers in its view. At the same time, the modified client does not upload any data to its peers. Our experimental results show that: a) our modified freerider client can achieve better download rates than a compliant client in most common-case public torrents; b) when the percentage of our modified free-rider clients in PlanetLab-residing torrents with ∼300 leechers is less than 40%, free-riders on average outperform compliant clients; and c) as the number of free-riders increases, both free-riders and compliant clients incur substantial performance degradation. These results suggest that the large view exploit is effective, and it has the potential for wide adoption.	bittorrent;download;elegant degradation;emoticon;experiment;ibm notes;leech (computing);planetlab;swarm;upload;vii	Michael Sirivianos;Jong Han Park;Rex Chen;Xiaowei Yang	2007			computer network;distributed computing;free riding;exploit;computer science;bittorrent;upload;download	Networks	-15.214864130266177	74.91569577393597	398
2ec389919c3b96655e267860fd7dab5215bc8ef5	towards generalization and simplicity in continuous control		This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of widely studied continuous control tasks, including the gym-v1 benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, the standard training and testing scenarios for these tasks are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution induces more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.	artificial neural network;online and offline;overfitting;radial basis function network	Aravind Rajeswaran;Kendall Lowrey;Emanuel Todorov;Sham M. Kakade	2017			simulation;artificial intelligence;machine learning;mathematics	ML	19.18882380094559	-32.2687777033214	399
e676ecf305cad2dcbae7ab703d99eb799ebadce5	compensation of loudspeaker nonlinearity in acoustic echo cancellation using raised-cosine function	power amplifier nonlinearity;echo cancellation;raised cosine function acoustic echo cancellation aec adaptive filters nonlinearity compensation;least mean squares methods;acoustic distortion;acoustic echo cancellation;power amplifier;acoustic signal processing;power amplifiers acoustic distortion acoustic signal processing adaptive filters echo suppression least mean squares methods loudspeakers nonlinear distortion;nonlinear echo component;nonlinear distortion;power amplifiers;adaptive filters;loudspeakers;nonlinear transformation;computational complexity;linear adaptive filters;loudspeakers nonlinear acoustics echo cancellers adaptive filters power amplifiers signal processing algorithms nonlinear distortion computational complexity polynomials computational modeling;echo suppression;acoustic signal;normalized least mean square;raised cosine function;computer simulation;adaptive filter;loudspeaker nonlinearity compensation;computational complexity loudspeaker nonlinearity compensation acoustic echo cancellation raised cosine function power amplifier nonlinearity nonlinear distortion acoustic signal linear adaptive filters nonlinear echo component nonlinear transformation normalized least mean square algorithm;acoustic echo cancellation aec;nonlinearity compensation;normalized least mean square algorithm;acoustic echo canceller	The nonlinearity of a power amplifier or loudspeaker in a large-signal situation gives rise to a nonlinear distortion of acoustic signal. A conventional acoustic echo canceller using linear adaptive filters is not able to eliminate the nonlinear echo component. In this brief, a novel nonlinear echo cancellation technique is presented by using a nonlinear transformation in conjunction with a conventional linear adaptive filter. The nonlinear transformation is derived from a raised-cosine function and is exploited to compensate for the nonlinearity of a loudspeaker. The transformation parameters are updated using the normalized least mean square algorithm according to the unknown nonlinear characteristic of the loudspeaker. Computer simulations show that the proposed method yields, in general, a satisfactory cancellation performance while having a very low computational complexity	acoustic cryptanalysis;adaptive filter;approximation algorithm;audio power amplifier;computational complexity theory;computer simulation;distortion;echo suppression and cancellation;gain compression;large-signal model;loudspeaker;mean squared error;nonlinear system	Hongyun Dai;Jun Yan	2006	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2006.882344	computer simulation;adaptive filter;computer vision;electronic engineering;speech recognition;acoustics;computer science;electrical engineering;mathematics	Vision	60.90671015387176	14.130246589055973	400
b76b94e17b7797e2a38013c93c3e6806eef880a8	on the bayesian calibration of computer model mixtures through experimental data, and the design of predictive models	uncertainty quantification;computer experiments;multinomial logistic model;mcmc;gaussian process;polynomial bases	For many real systems, several computer models may exist with different physics and predictive abilities. To achieve more accurate simulations/predictions, it is desirable for these models to be properly combined and calibrated. We propose the Bayesian calibration of computer model mixture method which relies on the idea of representing the real system output as a mixture of the available computer model outputs with unknown input dependent weight functions. The method builds a fully Bayesian predictive model as an emulator for the real system output by combining, weighting, and calibrating the available models in the Bayesian framework. Moreover, it fits a mixture of calibrated computer models that can be used by the domain scientist as a mean to combine the available computer models, in a flexible and principled manner, and perform reliable simulations. It can address realistic cases where one model may be more accurate than the others at different input values because the mixture weights, indicating the contribution of each model, are functions of the input. Inference on the calibration parameters can consider multiple computer models associated with different physics. The method does not require knowledge of the fidelity order of the models. We provide a technique able to mitigate the computational overhead due to the consideration of multiple computer models that is suitable to the mixture model framework. We implement the proposed method in a real world application involving the Weather Research and Forecasting large-scale climate model.		Georgios Karagiannis;Guang Lin	2017	J. Comput. Physics	10.1016/j.jcp.2017.04.003	overhead (computing);experimental data;mixture model;uncertainty quantification;computer experiment;variable-order bayesian network;markov chain monte carlo;artificial intelligence;gaussian process;machine learning;computer science	ML	29.615226131574556	-17.293685532398033	401
9b24c02731ba12194ad4d6006c21c68f3d1ff977	codit: an integrated business partner discovery tool over snss		The success of open innovation alliances depends on the right selection of business partners. Given the vast representation of business organizations on SNSs and the resulting availability of significant amount of information about their products and services, the SNSs seem to be a promising opportunity for the businesses to look for the potential partners. In this regard, this paper reviews the potential of SNSs for supporting business partner discovery. In order to address the identified inadequacies of SNSs for supporting business partner search, we present a web based search tool: CoDiT. The CoDiT system seamlessly integrates the company pages hosted on multiple SNSs by leveraging the potential of existing social networking APIs. The main premise of CoDiT system is to support open innovation process – in the ‘Find’ phase - by assisting organizations in locating complementary assets through the aggregation and exchange of the information about potential partners available on SNSs.		Atia Bano Memon;Kyrill Meyer	2015		10.1007/978-3-319-24141-8_59	web application;open innovation;knowledge management;social media;social network;computer science;complementary assets	DB	-75.6253889851197	5.475831903861398	402
213a055c0373e06b5a389f1f29647a40752c0fb3	a meshless method for asian style options pricing under the merton jump-diffusion model	levy processes;jump diffusion models;65n35;radial basis;differentia lquadrature;91g20;asian options;62p05;strang splitting;exponential time integration	In this paper, we consider the partial integro-differential equation arising when a stock follows a Poisson distributed jump process, for the pricing of Asian options. We make use of the meshless radial basis functions with differential quadrature for approximating the spatial derivatives and demonstrate that the algorithm performs effectively well as compared to the commonly employed finite difference approximations. We also employ Strang splitting with the exponential time integration technique to improve temporal efficiency. Throughout the numerical experiments covered in the paper, we show how the proposed scheme can be efficiently employed for the pricing of American style Asian options under both the Black–Scholes and the Merton jump-diffusion models.	meshfree methods	Aslam Aly El-Faïdal Saib;Mohammad Sameer Sunhaloo;Muddun Bhuruth	2015	Int. J. Comput. Math.	10.1080/00207160.2015.1061125	mathematical optimization;asian option;lévy process;mathematics;mathematical economics;statistics	Theory	85.81464831421862	16.020948350844982	403
bdba5cae67e0a49c23544b19300d7a3365d36a03	comparative performance of advanced microcomputer lp systems	estudio comparativo;performance;operations research;microordinateur;microcomputer;etude comparative;programacion lineal;microcomputadora;mathematical programming;recherche operationnelle;comparative study;linear programming;programmation lineaire;rendimiento;programmation mathematique;programacion matematica;investigacion operacional	Linear Programming (LP) software on microcomputers has been introduced by a number of companies. The software ranges from simple educational programs to programs with the ability to solve problems with hundreds of rows and columns and thousands of non-zeroes. This study provides a list of recently released LP software packages. Then it compares four major LP packages which are aimed at advanced users, accept MPSX formatted input, include mixed integer programming options and use the 8087 math coprocessor for speed improvement. A comparative summary of input, data management, user interface and technical features is provided. We then report the results of our tests on all four packages. We attempted to solve fourteen problems on all of the systems. The problems ranged from (8 × 20) to (661 × 2362). While no LP system was able to solve all of the problems, each was able to solve many of them. The speed was acceptable. Comparison with MPSX solutions indicated that the accuracy was also good overall. LINDO was able to solve most of the problems; LP88 solved eight of the problems; LP83 and MPS-PC solved half of the test problems. The packages vary in other features. While no ranking is provided, the comparison along many dimensions should permit one to select a package based on performance and user environment criteria.	microcomputer	Ramesh Sharda;C. Somarajan	1986	Computers & OR	10.1016/0305-0548(86)90004-3	mathematical optimization;performance;computer science;linear programming;artificial intelligence;comparative research;mathematics;microcomputer;algorithm	HPC	18.080097787159165	7.5685447912042125	404
231a6966a63e80dce9df42e5cb6237ab7e5dfa37	in-cycle myocardium tissue electrical impedance monitoring using broadband impedance spectroscopy	myocardium;time varying;biological tissues;impedance;electric impedance measurement;impedance myocardium impedance measurement mathematical model frequency measurement heart equations;heart;electrical impedance spectroscopy;frequency 1 khz to 1 mhz in cycle myocardium tissue electrical impedance monitoring broadband impedance spectroscopy cardiac cycle myocardium cell cell membranes extracellular spaces impedance cardiac signal myocardium tissue activity frequency sweep electrical impedance spectroscopy eis dynamic behavior in vivo healthy myocardium tissue impedance multisine excitation cole model diastolic function systolic function myocardium impedance time variation biomedical application electrochemical application intracellular spaces;system dynamics;dielectric spectroscopy electric impedance heart humans;diastolic function;cardiology;electric impedance measurement biological tissues biomembranes cardiology cellular biophysics;frequency measurement;biomembranes;impedance spectroscopy;impedance measurement;mathematical model;cell membrane;electrical impedance;cellular biophysics;dynamic behavior	Measurements of myocardium tissue impedance during the cardiac cycle have information about the morphology of myocardium cells as well as cell membranes and intra/extra cellular spaces. Although the variation with time of the impedance cardiac signal has information about the myocardium tissue activity during the cardiac cycle, this information has been usually underestimated in the studies based on frequency-sweep Electrical Impedance Spectroscopy (EIS) technique. In these cases, the dynamic behavior was removed from the impedance by means of averaging. The originality of this research is to show the time evolution of in-vivo healthy myocardium tissue impedance during the cardiac cycle, being measured with a multisine excitation at 26 frequencies (1 kHz–1 MHz). The obtained parameters from fitting data to a Cole model are valid indicators to explain the time relation of the systolic and diastolic function with respect to the myocardium impedance time variation. This paper presents a successful application of broadband Impedance Spectroscopy for time-varying impedance monitoring. Furthermore, it can be extended to understand various unsolved problems in a wide range of biomedical and electrochemical applications, where the system dynamics are intended to be studied.	anterior descending branch of left coronary artery;cardiography, impedance;characteristic impedance;diastole;dielectric spectroscopy;excitation;heart ventricle;least absolute deviations;linear programming relaxation;mathematical morphology;megahertz;muscle cells;myocardium;nominal impedance;plasma membrane;quantitative impedance;system dynamics;tissue fiber;video-in video-out;electric impedance	Benjamin Sanchez;Gerd Vandersteen;Javier Rosell-Ferrer;Juan Cinca;Ramon Bragós	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090697	electronic engineering;electrical engineering;electrical impedance;biological engineering	Robotics	26.78467370603017	-84.70169335597205	405
d45636f2aad7216b3ec233377dea9ebbef6884a0	fuzzy predictive control in czochralski crystal growth with power input as the unique manipulation	predictive control;single crystal;transfer functions;predictive control crystal growth from melt fuzzy control inertial systems;fuzzy control;niobium;crystal growth;inertia system fuzzy predictive control czochralski crystal growth hurle model time lagging system;heating;data mining;crystals;equilibrium point;czochralski crystal growth;time lagging system;fuzzy predictive control;inertia system;hurle model;time lag;process control;predictive models;inertial systems;growth process;fuzzy control predictive control temperature fuzzy systems automation transfer functions predictive models production power system modeling three term control;crystal growth from melt	This paper presents a study to Czochralski crystal growth process based on Hurle’s model linearized at the equilibrium points. In this study, we do the attempt to get single crystal with constant diameter using the power input as the unique manipulation by removing pull rate manipulation which was suspected to be contributing to structure defects in the crystal from the system. Compared with the traditional PID, fuzzy predictive control algorithm is more effective to handle the Czochralski growth process which enhances the tracking capability and system robustness. Simulation results indicate that we can get the constant diameter of crystal by manipulating the power input only and fuzzy predictive control is more effective to the time-lagging and inertia system.	algorithm;pid;simulation	Yongling Fu;Kai Zhang;Xiaoye Qi	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.314	equilibrium point;niobium;simulation;computer science;process control;control theory;crystal;predictive modelling;transfer function;crystal growth;single crystal;model predictive control;fuzzy control system	Robotics	58.24829248790234	-8.564455122608052	406
08eedeea7e1c67c660a7de199d576013ed03b110	on the estimation of reliability of a software system using reliabilities of its components	software testing;system reliability;errors;computer languages;path reliability estimates;software testing software reliability estimation software component reliability experiment component based reliability estimation path reliability estimates test input unix utility errors faulty components frequency count approach;frequency count approach;resource management;errors software reliability program testing unix;software systems;frequency estimation;software systems system testing frequency estimation software engineering yield estimation software testing software reliability programming computer languages resource management;yield estimation;indexing terms;software engineering;experience report;unix utility;program testing;software component reliability;faulty components;component based reliability estimation;software reliability estimation;system testing;experiment;software reliability;programming;unix;test input	"""We report an experiment to evaluate a method, known as Component Based Reliability Estimation (CBRE), for the estimation of reliability of a software system using reliabili-ties of its components. CBRE involves computing path reliability estimates based on the sequence of components executed for each test input. Path reliability estimates are averaged over all test runs to obtain an estimate of the system reliability. In the experiment reported, three components of a Unix utility were seeded with errors and the reliability of each component was measured. The faulty components were then introduced systematically into the utility, in various combinations, to produce several faulty versions of the utility. For each faulty version, test cases were drawn from an operational proole to measure the \component-based reliability"""". The \true reliability"""" of the faulty version was estimated using the frequency count approach. The goodness of CBRE was assessed in terms of the accuracy and eeciency of the estimates with respect to the \true reliability."""" Results from this experiment suggest that CBRE yields reasonably accurate results at an eecient rate. However, the accuracy and eeciency of CBRE is sensitive to the dependency among successive calls to a component."""	certified broadcast radio engineer;list of unix commands;software system;test case	Saileshwar Krishnamurthy;Aditya P. Mathur	1997		10.1109/ISSRE.1997.630860	experiment;reliability engineering;programming;real-time computing;index term;computer science;engineering;resource management;software engineering;software testing;unix;system testing;software quality;statistics;software system	SE	-61.7982078849346	32.768760017762844	407
7c70de3754af49c18f1154106d39d77ddbae1d65	研究速報 : カルボニル安定化スルフィルイミン(s-nイリド)の塩基性と反応性	departmental bulletin paper	Dr. Wamocho F. I. Dr. Karugu G. K. Dr. Nwoye A. 2008 A Framework of Guidance and Counseling Programme for Students with Special Educational Needs 2. Dr. Otube N. W. Nomadic Culture and its Impact on Children with Disabilities: A Case Study of the Samburu 3. Dr. Mugo J. K. & Andang’o E. 2007 Early Childhood Music Education in Kenya: Between Broad National Policies and Local Realities. International Arts Review		秀夫 木瀬;杉山 由己男;学 妹尾	1977		10.1007/978-0-387-39940-9_3504		Web+IR	-62.16122754701533	-14.312615460722448	408
87a458977ae41f5069de1f8db347383e7693ece3	proximity-based rocchio's model for pseudo relevance	relevance model;pseudo relevance feedback;information retrieval;term frequency;rocchio s model;proximity based term frequency;query expansion;relevance feedback	Rocchio's relevance feedback model is a classic query expansion method and it has been shown to be effective in boosting information retrieval performance. The selection of expansion terms in this method, however, does not take into account the relationship between the candidate terms and the query terms (e.g., term proximity). Intuitively, the proximity between candidate expansion terms and query terms can be exploited in the process of query expansion, since terms closer to query terms are more likely to be related to the query topic. In this paper, we study how to incorporate proximity information into the Rocchio's model, and propose a proximity-based Rocchio's model, called PRoc, with three variants. In our PRoc models, a new concept (proximity-based term frequency, ptf) is introduced to model the proximity information in the pseudo relevant documents, which is then used in three kinds of proximity measures. Experimental results on TREC collections show that our proposed PRoc models are effective and generally superior to the state-of-the-art relevance feedback models with optimal parameters.A direct comparison with positional relevance model (PRM) on the GOV2 collection also indicates our proposed model is at least competitive to the most recent progress.	information retrieval;query expansion;relevance feedback;statistical relational learning;tf–idf	Jun Miao;Xiangji Huang;Zheng Ye	2012		10.1145/2348283.2348356	natural language processing;query expansion;computer science;data mining;tf–idf;information retrieval	Web+IR	-28.154149223056734	-61.97802463229361	409
8d9ceca40c0067b6e8eedb7bbad3143a1cfbea2d	john von neumann and the evolutionary growth of complexity: looking backward, looking forward	growth of complexity;john von neumann artificial life machine self reproduction evolvability growth of complexity;machine self reproduction;evolvability;artificial life;john von neumann	In the late 1940s John von Neumann began to work on what he intended as a comprehensive theory of [complex] automata. He started to develop a book length manuscript on the subject in 1952. However, he put it aside in 1953, apparently due to pressure of other work. Due to his tragically early death in 1957, he was never to return to it. The draft manuscript was eventually edited, and combined for publication with some related lecture transcripts, by Burks in 1966. It is clear from the time and effort that von Neumann invested in it that he considered this to be a very significant and substantial piece of work. However, subsequent commentators (beginning even with Burks) have found it surprisingly difficult to articulate this substance. Indeed, it has since been suggested that von Neumann's results in this area either are trivial, or, at the very least, could have been achieved by much simpler means. It is an enigma. In this paper I review the history of this debate (briefly) and then present my own attempt at resolving the issue by focusing on an analysis of von Neumann's problem situation. I claim that this reveals the true depth of von Neumann's achievement and influence on the subsequent development of this field, and further that it generates a whole family of new consequent problems, which can still serve to informif not actually definethe field of artificial life for many years to come.	arthur burks;artificial life;automata theory;cessation of life;enigma machine;manuscripts	Barry McMullin	2000	Artificial Life	10.1162/106454600300103674	biology;computer science;artificial intelligence;operations research;genetics;evolvability;artificial life	AI	-58.5506249469219	-22.80455597554945	410
f04f3fe09903e0a44ba06d37ea927aa4e3b76424	variability bugs in highly configurable systems: a qualitative analysis		Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs.	acm transactions on software engineering and methodology;benchmark (computing);bitbucket;browsing;bug tracking system;busybox;c++;c99;cognitive dimensions of notations;compile time;compiler;database;experiment;heart rate variability;ieee transactions on software engineering;interaction;linux;maximal set;patch (computing);preprocessor;program analysis;programmer;prototype;sampling (signal processing);software bug;spatial variability;time complexity;user interface;web application;menuconfig	Iago Abal;Jean Melo;Stefan Stanciulescu;Claus Brabrand;Márcio Ribeiro;Andrzej Wasowski	2018	ACM Trans. Softw. Eng. Methodol.	10.1145/3149119	data mining;software bug;computer science;implementation;theoretical computer science;comprehension;user interface	SE	-57.08126132405534	38.136872389652076	411
4501c0326ddaabf1b38d1fbfbdb9f72230228faf	an analysis of region disparities between land productivities and land use comprehensive conditions in china	vegetation mapping;regional disparities;water resources;land use;humidity;environmental economics;land surface temperature;cities and towns;agriculture;productivity;meteorology;temperature distribution;productivity land surface temperature environmental economics cities and towns water resources temperature distribution vegetation mapping humidity meteorology agriculture	To analyze the region disparities between land productivities and land use comprehensive conditions helps to disclose the impact of land use conditions on land productivities, and to provide decision foundation about how to improve the land potential productivities. Establishing a 500m×500m grid digital ground model of China's land productivities and land use comprehensive conditions, the article puts forward the main ideas, content, and methods of analyzing the region disparities between land productivities and land use comprehensive conditions. Choosing a region of the worst grade of the land productivities, the article contrasts its relevant natural conditions, socio-economic conditions and comprehensive conditions of land use with its land productivities. The conclusions are as the following: in the region of the worst land productivities, its land use socio-economic conditions are mainly among the second or third grade, and its natural conditions of land use are mainly among the third grade, but not among the worst grade of the two aspects. It indicates that in the region of the worst grade of land productivities, there is a great potential to improve its land productivity, according to its current comprehensive conditions of land use. I. CONSTRUCTION OF LAND USE COMPREHENSIVE DIGITAL GROUND MODEL Digital ground model, a digital description of one or varied ground feature of space distribution, which adding one-dimensional or multi-dimensional ground feature of vector space to two-dimensional geographical space, is a kind of substance of geographical systematic space data bank or the sum of all, and the essential generality of digital ground model is two-dimensional geographical space location and digital description. For what it is, the digital description of reflecting all features of spatial distribution of land use could be named land use comprehensive digital ground model. The spatial distribution of the LUCDGM is described by x, y level coordinate, while the features of it mainly contain land use natural condition (terrain and landforms, water temperature condition, vegetation etc.)、 land resources survey data and other related social economic factors. The basic data of land use natural condition are established according to the environment factors data of national space distinguishing rate 500m*500m provided in China “the Ninth Five-Year Plan” key item. Among them, the temperature and humidity data are established by collecting the meteorological data of 1915 stations in whole country. II. THE TOTAL THOUGHT OF ANALYZING THE DIFFERENCE BETWEEM LAND PRODUCTIVITY AND LAND USE COMPREHENSIVE CONDITIONS The main aim of analyzing the difference between land productivity and land use comprehensive conditions is to disclose the different influences of condition factors on land productivity by comprehensively analyzing their differences, and to provide decision foundation about how to improve the land potential productivity. The Fig.1. is a total thought chart about the analyzing process. The calculation of land productivity, using the main components analyzing comprehensive evaluation method, synthesizes the two indexes of agriculture total output value in unit land area and grain yield in unit planting area into one. 2381 0-7803-9050-4/05/$20.00 ©2005 IEEE. 2381 General land use degree is comprehensively reflected by varied indexes, such as land use rate, reclaiming wasteland rate, cultivated land multiple crop index, woodland index, grassland use index, water surface use rate, town land use volume rate, building density, etc. According to land use classification in “the Ninth Five-Year Plan” item and other available materials, the article chooses the indexes of land use rate, reclaiming wasteland rate, woodland index, grassland index, water area index and town land use index to calculate the land productivity by adopting the main components analyzing comprehensive evaluation method. The social economic condition of land use contains lots of factors. Because of the obtaining data at present, including chemical fertilizer usage, agricultural machinery force, electricity usage, population, village population and village labor force indexes etc of all counties, the preliminary construction of digital ground model of social economic conditions can be set up only on these indexes. Agriculture input condition is a total index reflecting agriculture input intensity. To some extent, the differences of agriculture input conditions comprehensively reflect the difference of agriculture input intensity. The article chooses three indexes, chemical fertilizer usage, agricultural machinery force, electricity usage, in unit land area to comprehensively reflect agriculture input conditions. III. EXAMPLE ANALYSIS According to the total thought of the analyzing, multilevel difference analysis between land productivity and land use comprehensive conditions can be done. Land productivity usually accords with land use degree, land use natural conditions and social economic conditions, but it is not difficult to understand from determining the nature that land use degree, land use natural conditions and social economic condition etc may be different even if in the region of the same land productivity grade. There are many reasons for disappointing land productivity in some regions, maybe such as the poor degree and natural conditions of land use and the social economic conditions etc; or some bad unknown conditions among them, or only one poor condition; even all of these conditions are good, other social economic conditions could influence the land productivity comprehensively. The understanding of these concrete differences can help to state related policy-making body to make scientific decision. On one hand, we should know the difference of comprehensive conditions of the same land productivity grade on the macro-level; on the other hand, we can improve some land use conditions to increase the land productivity on the micro-level. Tab. I. Land use degree classification in the first grade region of land productivity Grade Grid number Area (km) Percentage (%) 2 295962 73990.5 0.07 3 949714 237428.5 0.23 4 1636691 409172.75 0.40 5 386222 96555.5 0.09 6 459741 114935.25 0.11 7 356937 89234.25 0.09 8 40633 10158.25 0.01	british undergraduate degree classification;geographic coordinate system;land;parameter (computer programming);race condition;wasteland 2	Yujie Huang;Jie Chang;Jiyuan Liu;Zhixiang Chen;Haihong Li	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525456	meteorology;water resources;agriculture;productivity;land use;land development;hydrology;humidity	ML	83.049739745362	-54.291248492734326	412
dadfdb1320327d3e4b88ba39a2bf95efb9f70b8b	mathematical modeling of vehicle frontal crash by a double spring-mass-damper model	damping;least squares approximations;curve fitting vehicle frontal crash double spring mass damper model mathematical model stiffness nonlinear least square method levenberg marquart algorithm;shock absorbers;impact mechanical;mathematical analysis;acceleration;vehicle crash testing vehicles mathematical model vehicle dynamics damping acceleration equations;parameters estimation;vibration control;vehicle crash testing;mathematical model;nonlinear equations;vehicles;vehicle frontal crash;curve fitting modeling vehicle frontal crash parameters estimation;vibration control impact mechanical least squares approximations mathematical analysis nonlinear equations shock absorbers vehicle dynamics;curve fitting;modeling;vehicle dynamics	This paper presents development of a mathematical model to represent the real vehicle frontal crash scenario. The vehicle is modeled by a double spring-mass-damper system. The front mass m1 represents the chassi of the vehicle and rear mass m2 represents the passenger compartment. The physical parameters of the model (Stiffness and dampers) are estimated using Nonlinear least square method (Levenberg-Marquart algorithm) by curve fitting the response of a double spring-mass-damper system to the experimental displacement data from the real vehicle crash. The model is validated by comparing the results from the model with the experimental results from real crash tests available.	algorithm;chassis;curve fitting;displacement mapping;mathematical model;multi-compartment model;relevance;stiffness	Bernard B. Munyazikwiye;Hamid Reza Karimi;Kjell G. Robbersmyr	2013	2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)	10.1109/ICAT.2013.6684071	control engineering;simulation;engineering;automotive engineering	Robotics	72.11902939830965	-15.761929367887307	413
e4e65887fcd68733733740ec53c4ac3ff048ed7e	a framework for graded beliefs, goals and intentions	engineering and technology;teknik och teknologier	In natural language we often use graded concepts, reflecting different intensity degrees of certain features. Whenever such concepts appear in a give n real-life context, they need to be appropriately expressed in its models. In this paper, we pro vide a framework which allows for extending the BGI model of agency by grading beliefs, goals a nd intentions. We concentrate on TEAMLOG [6, 7, 8, 9, 12] and provide a complexity-optimal decision me thod for its graded version TEAMLOG by translating it intoCPDLreg (propositional dynamic logic with converse and “inclusion axioms” characterized by regular languages). We also d evelop a tableau calculus which leads to the first EXPTIME (optimal) tableau decision procedure for CPDLreg. As CPDLreg is suitable for expressing complex properties of graded operators, the procedure can also be used as a decision tool for other multiagent formalisms. ∗Address for correspondence: Institute of Informatics, War saw University, Warsaw, Poland 54 B. Dunin-Kȩplicz et al. / A Framework for Graded Beliefs, Go als and Intentions 1. Paper Contribution and Structure BGI, standing for Beliefs, Goals and Intentions, is a well-r cognized intensional model of agency. 1 Intuitively, an agent’s beliefs correspond to information it has about the environment, including other agents. An agent’s goals or desires represent states of affa irs (options) that it would choose. In practical human reasoning, intentions are first class citizens, as the y are not reducible to beliefs and desires. They form a consistent subset of an agent’s goals that it chooses t focus on for the time being. In this way, they create a screen of admissibility for the agent’s furthe r, possibly long-term, decision process, known as deliberation. The paper is devoted to extending the BGI model of agency by as suming that beliefs, goals and intentions may be graded. We use grades similarly to degrees of beliefs employed in some epistemic logics studied in [23, 15]. Our notion of grades is inclusion -based. As an analogy, consider the grading of concepts in natural language, where, e.g., linguistic mo difiers like “very”, “slightly”, etc., are used for that purpose. From the semantical point of view, such mod ifiers can be understood as operations on sets. For example, consider a set of fast cars, consisting of cars with maximum speed exceeding a certain threshold. Some of the fast cars are very fast, some very fast ones are very very fast, and so on. We then have the following chain of inclusions, where FC denotes the set of fast cars: . . . ⊆ very(FC) ⊆ . . . ⊆ very(very(FC)) ⊆ very(FC) ⊆ FC. Of course, such inclusions may reflect other linguistic modi fiers as well as constructs reflecting, e.g., the strength of beliefs, the importance of goals or degrees o f commitment to intentions. It should be emphasized that we restrict neither the ways of grading BGIs nor the intuitions behind them. Therefore, the grading may express a number of possible interpretation s, varying from subjective to objective ones, from static to dynamic ones, from rigid to flexible ones, etc. The only requirement is that the considered gradings can be represented by inclusions. Such a notion of grading differs from that used in graded moda l logics [13, 19], where grades are obtained by restricting the number of possible worlds acces sible from the current world. It also differs from [1, 2], where a kind of fuzzy calculi are used to model pre fe nces and graded attitudes, as well as from [5], where doxastic epistemic models, constructed fro m preference relations for agents, are used to represent both knowledge and degrees of belief. The main motivation for this research comes from T EAMLOG [6, 7, 8, 9, 12], a multi-modal formalism for specifying cooperating BGI agents and reasoning about teamwork. Even though T EAMLOG expresses many subtle aspects of informational and motivat i nal stances of agents, to adjust it better to the richness of dynamic multi-agent environments, more fine -grained notions are introduced here, leading to a graded version of that formalism: T EAMLOG. Therefore, the main contribution of our research consists in: • introducing and motivating graded beliefs, graded goals an d graded intentions • extending TEAMLOG to TEAMLOGwith operators reflecting graded BGIs • introducingCPDLreg as a logical formalism suitable for expressing complex prop e ties of graded modalities and reasoning about them The BGI model is often called BDI (Beliefs, Desires and Inten tio s). B. Dunin-Kȩplicz et al. / A Framework for Graded Beliefs, Go als and Intentions 55 • providing a translation of T EAMLOG and TEAMLOG into CPDLreg • developing a sound and complete tableau calculus for CPDLreg which leads to the first E XPTIME (optimal) tableau decision procedure for CPDLreg and TEAMLOG. To the best of our knowledge, these are the first tableau calculus and the first decision pro cedure forCPDLreg. The proposed formalism is based on CPDLreg, which extends propositional dynamic logic with converse (CPDL) with “inclusion axioms” characterized by regu lar languages. CPDLreg is very expressive but still decidable (EXPTIME-complete). It can then serve as a framework for introducing graded modalities into logical BGI formalisms. As already indicated, we concentrate on T EAMLOG, extending it to a graded version — T EAMLOG. This move provides a powerful formalism which can be used in such complex and diverse applications as multi-agent environme nts. Thus, it can also be seen as a case study showing how to approach graded BGIs in formalisms based on lo gics other than TEAMLOG. The rest of the paper is structured as follows. In Section 2, w e discuss and motivate graded beliefs, goals and intentions. In Section 3, we recall the T EAMLOG formalism. Section 4 is devoted to TEAMLOG which allows BGIs to be graded. Then in Section 5 we define CPDLreg as the technical framework and continue, in Sections 6 and 7, with transla tions of TEAMLOG and TEAMLOG into CPDLreg. In Section 8, we provide a tableau calculus for CPDLreg. Finally, Section 9 concludes the paper.	admissible heuristic;agent-based model;borland graphics interface;controller–pilot data link communications;decision problem;doxastic logic;dynamic logic (modal logic);exptime;field electron emission;first-class function;formal system;informatics;intensional logic;long division;method of analytic tableaux;mod database;modal logic;multi-agent system;natural language;norm (social);possible world;real life;regular language;semantics (computer science)	Barbara Dunin-Keplicz;Linh Anh Nguyen;Andrzej Szalas	2010	Fundam. Inform.	10.3233/FI-2010-263	computer science;artificial intelligence;mathematics;algorithm	AI	-14.386441921755155	8.961340576521842	414
ab27f14f47b305f4164a9da6d5a48bc79db08349	gallai-milgram properties for infinite graphs	finite group;graphe infini;linearity;camino grafo;independance;grupo acabado;graph path;relacion orden;linearite;ordering;linearidad;connected graph;independence;relation ordre;infinite graph;independencia;cycle graphe;chemin graphe;groupe fini;grafo infinito;cycle graph;grafo completo;complete graph;graphe complet;graphe connexe;grafo connexo;ciclo diagrama	We discuss extensions of the Gallai-Milgram theorem to infinite graphs. We define a path to be a directed graph whose transitive closure is a linear ordering. We show that an undirected graph with no infinite independent set is covered by finitely many pairwise disjoint paths; moreover for a given integer k this graph is covered by ?k (resp. ?k pairwise disjoint) paths if each finite set of vertices is contained in the union of ?k (resp. ?k pairwise disjoint) paths. Hence an undirected graph with no independent set of size k + 1 is covered by ?k pairwise disjoint paths. We prove also that if to each edge (x, y) of a countable path is associated an element ?(x, y) of a finite group, then some edges can be deleted so that the new graph is still a path and the product ?(x0, x1)??(x1, x2)????(xn?1, xn) of the elem ents of the group along any finite path (x0, x1,?,xn) of the new graph depends only upon the extremities x0, xn of the path.	milgram experiment	J.-M. Brochet;Maurice Pouzet	1991	Discrete Mathematics	10.1016/0012-365X(91)90328-Y	gallai–hasse–roy–vitaver theorem;independence;combinatorics;clique graph;discrete mathematics;cograph;topology;graph property;order theory;connectivity;comparability graph;cycle graph;path graph;mathematics;linearity;path;complete graph;complement graph;tree;algebra	Theory	25.787495078194148	31.78566879358978	415
cbb4cb1a3191e4b139f43fadee25e27557b416c8	reconocimiento de patrones numéricos para vuelo controlado de un ar drone utilizando redes neuronales artificiales		In this paper the letter segmentation of photographs was used, taken from a Parrot AR Drone’s camera with the aim of establishing a stimulusresponse, where the original picture formed by Red, Green and Blue (RGB) colors was segmented by color (choosing the red channel). Once the character is recognized, the Drone executes the corresponding action. Noise-free number patterns were initially used and then some pixels were added in the image in order to make a set of patterns more robust, which provided the training set for neural network and thus are able to interpolate new patterns. Edge techniques detection were used for image segmentation including Sobel filter and filters for noise removal based on the median filtering, that is a low pass filter. All this took place in a closed environment, expecting to extend this to different environments. 61 Research in Computing Science 107 (2015) pp. 61–71; rec. 2015-08-04; acc. 2015-10-19	artificial neural network;channel (digital image);color;image segmentation;interpolation;low-pass filter;median filter;pixel;sobel operator;test set;unmanned aerial vehicle	Juan Carlos Rodríguez-Sánchez;Victor M. Landassuri-Moreno;José Martín Flores Albino	2015	Research in Computing Science		art;performance art	Vision	38.26342129087147	-65.22051882314454	416
0335cdcd623f0cef222a651a8a3168e7322c5081	closing statement: reflections on a singularity symposium: the technological singularity (ubiquity symposium)		The debate about computers and intelligence must go on - we have more to learn, and more people need to convert their strong opinions to measured arguments. There is no reason to panic, however.	amiga reflections;closing (morphology);computer;technological singularity	Espen Andersen	2014	Ubiquity	10.1145/2668390	panic;artificial intelligence;computer science;technological singularity;singularity	Logic	-53.82043125399375	-19.61101078402084	417
b2b8eb164b040acf9247d15f4949a65aa89726eb	the wiseskin artificial skin for tactile prosthetics: a power budget investigation	sensors;wireless sensor networks power consumption prosthetic power supplies skin tactile sensors;skin;actuators;prosthetics;thumb;medicinteknik;artificial skin smart skin wireless ultra low power;sensors power demand prosthetics thumb skin actuators;wiseskin artificial skin power consumption ultralow power wireless sensor nodes wiseskin project targets adequate sensors sensory feedback provision prosthetic hands tactile prosthetics;power demand	The use of prosthetic hands are limited partly due to the fact that they lack the provision of sensory feedback to the user. A first step to providing sensory feedback is having adequate sensors in the prosthesis. The WiseSkin project targets the use of artificial skin embedding ultra-low power wireless sensor nodes. This paper provides an overview of the WiseSkin artificial skin for tactile prosthetics and specifically addresses the power consumption by the sensor nodes.	artificial skin;feedback;sensor	Christian Antfolk;Vladimir Kopta;John R. Farserotu;Jean-Dominique Decotignie;Christian C. Enz	2014	2014 8th International Symposium on Medical Information and Communication Technology (ISMICT)	10.1109/ISMICT.2014.6825229	electronic engineering;engineering;electrical engineering;biological engineering	Embedded	56.59621751445149	44.7009021992229	418
bfa1b8b1dba95759103f19bbee91fac6564993e4	morphological segmentation and opus for finnish-english machine translation		This paper describes baseline systems for Finnish-English and English-Finnish machine translation using standard phrasebased and factored models including morphological features. We experiment with compound splitting and morphological segmentation and study the effect of adding noisy out-of-domain data to the parallel and the monolingual training data. Our results stress the importance of training data and demonstrate the effectiveness of morphological pre-processing of Finnish.	baseline (configuration management);machine translation;preprocessor;statistical machine translation;libopus	Jörg Tiedemann;Filip Ginter;Jenna Kanerva	2015			natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.741073781865847	-76.8343417533325	419
309f2246183c859d96d61d4264fbd1ddadef1904	event extraction from cancer genetics literature	natural language cancer genetics literature pattern classification technique biological literature syntactic patterns cancer genetics data bionlp 13 shared task conditional probability scores linguistic features svm bioevent extraction;cancer;training;genetics;syntactics;feature extraction;tumors;event extraction syntactic patterns bioinformatics;support vector machines biology cancer data handling pattern classification;context;feature extraction training cancer tumors genetics context syntactics	This paper attempts to employ learning based pattern classification technique to extract events from biological literature. Although various approaches to extract events have been explored, none is suitable for designing a practical system of event extraction. Extracting events more precisely is still an ongoing process. In this paper, new features that seem to be relevant for the given task are investigated. Two syntactic patterns namely phrase structure and dependency structure are explored to produce improved results with respect to the Cancer Genetics Data provided in the BioNLP'13 Shared Task. A stacked model based on conditional probability scores are also considered as features. The patterns and the probability scores along with some other linguistic features are fed to SVMs to train it for the task of bio-event extraction from natural language articles. The results are compared with the performance of the best extraction system in Cancer Genetics Task.	british informatics olympiad;dependency grammar;natural language;parsing;phrase structure rules	Debajyoti Sinha;Utpal Garain;Sanghamitra Bandyopadhyay	2015	2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR)	10.1109/ICAPR.2015.7050697	natural language processing;computer science;pattern recognition;data mining	NLP	-19.733905404473152	-70.05243297914369	420
dceec330b8f969382e2674cf390ac2d73b7f3588	multi-objective optimization for virtual machine allocation and replica placement in virtualized hadoop		Resource management is a key factor in the performance and efficient utilization of cloud systems, and many research works have proposed efficient policies to optimize such systems. However, these policies have traditionally managed the resources individually, neglecting the complexity of cloud systems and the interrelation between their elements. To illustrate this situation, we present an approach focused on virtualized Hadoop for a simultaneous and coordinated management of virtual machines and file replicas. Specifically, we propose determining the virtual machine allocation, virtual machine template selection, and file replica placement with the objective of minimizing the power consumption, physical resource waste, and file unavailability. We implemented our solution using the non-dominated sorting genetic algorithm-II, which is a multi-objective optimization algorithm. Our approach obtained important benefits in terms of file unavailability and resource waste, with overall improvements of approximately 400 and 170 percent compared to three other optimization strategies. The benefits for the power consumption were smaller, with an improvement of approximately 1.9 percent.	apache hadoop;cloud computing;decision theory;genetic algorithm;mathematical optimization;microservices;multi-objective optimization;openvms;optimization problem;sorting;unavailability;virtual machine	C. V. Serey Guerrero;Isaac Lera;Belén Bermejo;Carlos Juiz	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2018.2837743	genetic algorithm;replica;resource management;real-time computing;cloud computing;multi-objective optimization;unavailability;computer science;virtual machine;distributed computing;sorting	HPC	-19.548937033656777	62.97897648567275	421
4388be23471f6526d87b70d706131cd39c36f082	developing iot applications: challenges and frameworks		Internet of things (IoT) is creating new opportunities for developing innovative applications by leveraging on existing and new technologies. In recent years, a variety of consumer and industrial IoT applications have been developed and deployed. Despite much progress, developing IoT applications is still a complex, time-consuming and challenging activity. This is because IoT systems involve a wide range of hardware and software components, depending on a variety of communication and distributed system technologies. Many IoT application frameworks of varying approaches have been developed to manage the complexities of developing IoT applications. However, there remains a paucity of surveys on these IoT application development frameworks. This study presents a comprehensive review and a comparative analysis of existing IoT application development frameworks and toolkits, illustrating their strengths and weaknesses. This study will assist in finding the most appropriate IoT application development paradigm for the desired IoT application. Finally, future research directions are highlighted to improve existing and future frameworks and toolkits for IoT applications.		Rafal Bobihski;Gerald Kotonya	2018	IET Cyper-Phys. Syst.: Theory & Appl.		component-based software engineering;systems engineering;emerging technologies;strengths and weaknesses;internet of things;computer science	Mobile	-45.794642508910655	39.345840236676	422
6c69d2c361ed34294e20cd1deec702889c7fc4ae	high-speed match algorithm of a production system	algorithme rapide;systeme commande;sistema control;sistema experto;red semantica;production system;semantic network;systeme production;sistema produccion;condition lelment;working memory element;reseau semantique;control system;condition match algorithm;fast algorithm;condition element;working memory;systeme expert;rete network;high speed;algoritmo rapido;expert system	Abstract#R##N##R##N#The production system is suited to representing the miscellaneous pieces of human knowledge, and is used most widely in constructing the expert system. A problem is that a tremendous amount of time is required for the condition match, and an efficient condition match algorithm is desired.#R##N##R##N##R##N##R##N#This paper considers the expert system which has been designed for the inference based on the semantic network, and aims at the realization of the high-speed condition match in the expert system rewritten by OPS5. The working memory elements which are the objects of investigation are mostly the knowledge of two-term relations, and the number of attributes usually is 2.#R##N##R##N##R##N##R##N#This paper also proposes a data structure for the working memory, which can derive directly all working memory elements satisfying the condition, from the condition elements with 2 or less attributes. Using the proposed data structure, an algorithm is shown that can realize a high-speed condition match. It is shown for the considered expert system that the proposed algorithm can realize a higher speed than the conventional condition match algorithm.	algorithm;production system (computer science)	Haruhiko Kimura;Shin-ya Kobayashi;Tsuyoshi Takebe;Kazuyuki Sumiyoshi	1995	Systems and Computers in Japan	10.1002/scj.4690260103	computer science;control system;artificial intelligence;working memory;production system;semantic network;expert system;algorithm	NLP	-23.359359087743698	-4.525453317580329	423
335a3921e2690de1f8c64615915ef0f5d66f10b4	design of heterogeneous embedded systems using dfcharts model of computation	data flow graphs;operational semantics;process network;embedded system computational modeling embedded computing control systems synchronization automatic control signal processing automata control system synthesis electronic mail;embedded system;boolean automata heterogeneous embedded system dfcharts model dataflow process networks signal processing system synchronous dataflow hierarchical concurrent finite state machines data dominated systems control dominated systems argos semantics operational semantics;embedded systems;finite state machines;signal processing;synchronous dataflow;model of computation;embedded systems finite state machines data flow graphs;finite state machine	Dataflow process networks have been successfully used for modeling signal processing systems which are data-dominated. In this family of models, the most popular one is synchronous dataflow (SDF). On the other hand, hierarchical concurrent finite state machines (HCFSM) have been successfully employed for control-dominated systems. Most complex embedded systems are heterogeneous, consisting of both control-dominated and data-dominated parts. In this paper, we introduce a new model of computation, called DFCharts, which targets heterogeneous embedded systems. It combines the HCFSM (with Argos semantics) and SDF models. It has a formal, operational semantics based on Boolean automata with variables.	automata theory;consistency model;dataflow architecture;embedded system;finite-state machine;kahn process networks;model of computation;operational semantics;signal processing	Ivan Radojevic;Zoran A. Salcic;Partha S. Roop	2006	19th International Conference on VLSI Design held jointly with 5th International Conference on Embedded Systems Design (VLSID'06)	10.1109/VLSID.2006.67	model of computation;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;finite-state machine;programming language;operational semantics;algorithm	Embedded	-33.05723516856588	33.48243036588117	424
e4b411bcd4d8836ef0fca9d4733f9bb28a306fdb	cloud security challenges: investigating policies, standards, and guidelines in a fortune 500 organization	cloud computing	Cloud computing is quickly becoming pervasive in today’s globally integrated networks. The cloud offers organizations opportunities to potentially deploy software and data solutions that are accessible through numerous mechanisms, in a multitude of settings, at a reduced cost with increased reliability and scalability. The increasingly pervasive and ubiquitous nature of the cloud creates an environment that is potentially conducive to security risks. While previous discussions have focused on security and privacy issues in the cloud from the end-users perspective, minimal empirical research has been conducted from the perspective of a corporate environment case study. This paper presents the results of an initial case study identifying real-world information security documentation issues for a Global Fortune 500 organization, should the organization decide to implement cloud computing services in the future. The paper demonstrates the importance of auditing policies, standards and guidelines applicable to cloud computing environments along with highlighting potential corporate concerns. The results from this case study has revealed that from the 1123 ‘relevant’ statements found in the organization’s security documentation, 175 statements were considered to be ‘inadequate’ for cloud computing. Furthermore, the paper provides a foundation for future analysis and research regarding implementation concerns for corporate cloud computing applications and services.	cloud computing security;documentation;information security;pervasive informatics;privacy;reduced cost;scalability	George Grispos;William Bradley Glisson;Tim Storer	2013			cloud computing security;cloud computing;computer science;management science;computer security	SE	-49.32532374573849	56.26402456298405	425
18eed721845dd876c769c1fd2d967c04f3a6eeaa	learning blocking schemes for record linkage	learning effectiveness;machine learning;record linkage	Record linkageis the process of matching records across data sets that refer to the same entity. One issue within record linkage is determining which record pairs to consider, since a detailed comparison between all of the records is impractical. Blockingaddresses this issue by generating candidate matches as a preprocessing step for record linkage. For example, in a person matching problem, blocking might return all people with the same last name as candidate matches. Two main problems in blocking are the selection of attributes for generating the candidate matches and deciding which methods to use to compare the selected attributes. These attribute and method choices constitute a blocking scheme . Previous approaches to record linkage address the blocking issue in a largely ad-hoc fashion. This paper presents a machine learning approach to automatically learn effective blocking schemes. We validate our approach with experiments that show our learned blocking schemes outperform the ad-hoc blocking schemes of non-experts and perform comparably to those manually built by a domain expert.	blocking (computing);experiment;hoc (programming language);linkage (software);machine learning;preprocessor;subject-matter expert	Matthew Michelson;Craig A. Knoblock	2006			record linkage;computer science;artificial intelligence;machine learning;data mining	Web+IR	-28.839851415581816	-64.52255257298883	426
10b9491ead8a6ea5b21d45d0d70cf583a38aba7e	community detection in sparse networks via grothendieck's inequality		We present a simple and flexible method to prove consistency of semidefinite optimization problems on random graphs. The method is based on Grothendieck’s inequality. Unlike the previous uses of this inequality that lead to constant relative accuracy, we achieve arbitrary relative accuracy by leveraging randomness. We illustrate the method with the problem of community detection in sparse networks. Despite much progress in the recent years, almost no rigorous results have been known for totally sparse networks – those with bounded average degrees. We demonstrate that even in this regime, various natural semidefinite programs can be used to recover the community structure up to an arbitrarily small fraction of misclassified vertices. The method is general; it can be applied to a variety of stochastic models of networks and semidefinite programs.	apollonian network;mathematical optimization;random graph;randomness;semidefinite programming;social inequality;sparse matrix;stochastic process	Olivier Guédon;Roman Vershynin	2014	CoRR			Theory	24.925977351507818	-33.26567197849956	427
052462bb0155980defb4e8dd38f7bb6fa00c412a	nonlinear prediction by kriging, with application to noise cancellation	volterra series;bayes estimation;traitement signal;acoustique sous marine;non linear filtering;methode parametrique;krigeage;maximum likelihood;forme onde;metodo parametrico;parametric method;simulacion numerica;serie volterra;filtrado no lineal;maximum vraisemblance;arma model;non linear model;modele non lineaire;modelo arma;experimental result;reduccion ruido;identificacion sistema;estimacion bayes;modelo no lineal;forma onda;system identification;signal processing;noise reduction;simulation numerique;prediccion lineal;systeme non lineaire;reduction bruit;estimacion parametro;resultado experimental;modele arma;linear prediction;waveform;parameter estimation;estimation parametre;resultat experimental;sistema no lineal;kriging;procesamiento senal;underwater acoustics;non linear system;identification systeme;maxima verosimilitud;prediction lineaire;filtrage non lineaire;estimation bayes;numerical simulation;acustica submarina	A semi-parametric approach based on kriging is suggested for nonlinear prediction. It does not rely on any specific model structure, which makes the approach much more flexible than those based on parametric behavioural models. At the same time, accurate predictions are obtained for extremely short-training sequences. Various examples are presented to illustrate the robustness of the method, with a discussion of the prior choices concerning the parametric part of the model. Application on real data is considered in the context of a noise-cancellation problem in underwater acoustics.	kriging	Jean-Pierre Costa;Luc Pronzato;Eric Thierry	2000	Signal Processing	10.1016/S0165-1684(99)00153-X	autoregressive–moving-average model;econometrics;underwater acoustics;waveform;system identification;linear prediction;computer science;calculus;signal processing;noise reduction;mathematics;maximum likelihood;estimation theory;kriging;statistics	Robotics	56.27461090684131	10.896923640692677	428
2d7b8e546cd1d30137105897e7c8c89c3d8db6c5	compact integrated motion sensor with three-pixel interaction	image features;monolithic integration;cmos integrated circuits;integrated circuit;edge detection;motion estimation;motion estimation data mining optical sensors image motion analysis very large scale integration analog computers circuit noise biology computing correlation gradient methods;velocity field;chip;local interaction compact integrated motion sensor three pixel interaction integrated circuit on chip photoreceptors bi directional velocity visual stimulus motion sensing elements velocity field estimation higher level image features global interaction;robot vision;velocity sensor;indexation;analog vlsi;optical flow;optical sensors;optical sensors motion estimation image sequences edge detection cmos integrated circuits;image sequences	An integrated circuit with on-chip photoreceptors is described, that computes the bi-directional velocity of a visual stimulus moving along a given axis in the focal plane by measuring the time delay of its detection at two positions. Due to the compactness of the circuit, a dense array of such motion-sensing elements can be monolithically integrated to estimate the velocity field of an image and to extract higher-level image features through local or global interaction.	motion detector;pixel	Jörg Kramer	1996	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.491628	chip;computer vision;vector field;edge detection;computer science;integrated circuit;motion estimation;optical flow;cmos;feature	Vision	45.88907705483395	-33.57163071985752	429
21c4121192b4baca86b1cb5973e90bc0c2c1d708	virusseq: software to identify viruses and their integration sites using next-generation sequencing of human cancer tissue	software;high throughput nucleotide sequencing;virus integration;viruses;genome human;algorithms;humans;sequence analysis rna;neoplasms	SUMMARY We developed a new algorithmic method, VirusSeq, for detecting known viruses and their integration sites in the human genome using next-generation sequencing data. We evaluated VirusSeq on whole-transcriptome sequencing (RNA-Seq) data of 256 human cancer samples from The Cancer Genome Atlas. Using these data, we showed that VirusSeq accurately detects the known viruses and their integration sites with high sensitivity and specificity. VirusSeq can also perform this function using whole-genome sequencing data of human tissue.   AVAILABILITY VirusSeq has been implemented in PERL and is available at http://odin.mdacc.tmc.edu/∼xsu1/VirusSeq.html.   CONTACT xsu1@mdanderson.org   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithm;bioinformatics;biopolymer sequencing;dna integration;massively-parallel sequencing;neoplasms;non-small cell lung carcinoma;perl;rna;sensitivity and specificity;sensor;whole transcriptome sequencing;whole genome sequencing	Yunxin Chen;Hui Yao;Erika J. Thompson;Nizar M. Tannir;John N. Weinstein;Xiaoping Su	2013	Bioinformatics	10.1093/bioinformatics/bts665	biology;molecular biology;cancer genome sequencing;computer science;bioinformatics;genetics	Comp.	-0.35243855168900606	-57.55099394866218	430
8eaadad8ac3d18ac63d189069df704e106e25c07	optimal repair resource assignment strategy for partial failed systems	maintenance service optimal repair resource assignment strategy partial failed system repair capacity maintenance policy failed component repairman problem partial failure optimum repairman assignment policy;resource allocation;maintenance engineering;system performance;service model;partial failure resource allocation optimal assignment policy maintenance service modeling;maintenance engineering servers vehicles equations hafnium operations research resource management	The situation that arises in the maintenance of systems which operate continuously and possess limited repair capacity can be modeled. Since the number of available repairmen is less than the number of components, the performance of the system depends on the maintenance policy employed, whenever the number of failed components is greater than the number of available repairmen. Thus it is important to have maintenance policies that yield a maximum value to some relevant measure of system performance. The components in most classic repairman problems can be only in two states, functioning or failed, while failed components are not always totally unusable in practice. The concept of extra cost is introduced to describe the damage caused by this partial failure. In this study, it come up with general rules about how to decide the optimum repairman assignment policy for two types of maintenance service, namely, preemptive and nonpreemptive, in the term of minimizing the average extra cost.	non-functional requirement;preemption (computing);usability	Zhengjun Wang	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023321	maintenance engineering;reliability engineering;resource allocation;computer science;systems engineering;engineering;service-oriented modeling	SE	7.112725164578891	-1.0544073903965179	431
6eee64cef88a87fdda5c3b4708da3f89323bb8f0	removing linear phase mismatches in concatenative speech synthesis	analyse parole;linear phase;centro gravitacional;speech intelligibility;centre gravite;speech synthesis;etude theorique;analisis palabra;speech processing;center of mass;speech analysis;tratamiento palabra;traitement parole;acoustic signal processing;concatenacion;delay system;at t labs concatenative speech synthesis linear phase mismatch removal text to speech systems acoustic units concatenation recorded speech speech intelligibility synchronization signal processing concatenated speech frames phase spectra processing output speech quality center of gravity differentiated phase data speech database harmonic plus noise model hnm tts system;concatenation;speech synthesis interpolation frequency synchronization concatenated codes speech processing gravity smoothing methods signal synthesis acoustic signal processing databases;spectral analysis speech synthesis speech intelligibility acoustic signal processing hidden markov models noise;hidden markov models;phase lineaire;systeme a retard;harmonic plus noise model;signal processing;center of gravity;estudio teorico;text to speech;sintesis palabra;sistema con retardo;fase lineal;theoretical study;spectral analysis;synthese parole;noise	Many current text-to-speech (TTS) systems are based on the concatenation of acoustic units of recorded speech. While this approach is believed to lead to higher intelligibility and naturalness than synthesis-by-rule, it has to cope with the issues of concatenating acoustic units that have been recorded at different times and in a different order. One important issue related to the concatenation of these acoustic units is their synchronization. In terms of signal processing this means removing linear phase mismatches between concatenated speech frames. This paper presents two novel approaches to the problem of synchronization of speech frames with an application to concatenative speech synthesis. Both methods are based on the processing of phase spectra without, however, decreasing the quality of the output speech, in contrast to previously proposed methods. The first method is based on the notion of center of gravity and the second on differentiated phase data. They are applied off-line, during the preparation of the speech database without, therefore, any computational burden on synthesis. The proposed methods have been tested with the harmonic plus noise model, HNM, and the TTS system of AT&T Labs. The resulting synthetic speech is free of linear phase mismatches.	acoustic cryptanalysis;concatenation;database;intelligibility (philosophy);linear phase;online and offline;signal processing;speech corpus;speech synthesis;synthetic intelligence	Yannis Stylianou	2001	IEEE Trans. Speech and Audio Processing	10.1109/89.905997	voice activity detection;concatenation;center of mass;linear phase;speech recognition;acoustics;computer science;noise;speech processing;center of gravity;speech synthesis;intelligibility	ML	81.50893294532513	-33.36995441655209	432
1bb77ed96df2baf472c7e97a4a7ac86ec65911d6	mastermind: a predictor of computer programming aptitude	predict;computer program;aptitude;mastermind;game;ability	For two semesters, the authors have tested CS1 introductory students on their ability to play the MasterMind© game at the beginning of the semester and compared those scores with in class programming test scores. The resulting correlations suggest that this game can be used as part of a computer programming aptitude test. This aptitude test could be used to advise potential students or employees about their probable success as programmers. Our survey of the literature yielded many programming aptitude tests with correlations lower than ours of 0.6; we are unique in using a game as an aptitude test.	computer programming;kerrison predictor;programmer;aptitude	Torben Lorenzen;Hang-Ling Chang	2006	SIGCSE Bulletin	10.1145/1138403.1138436	games;simulation;aptitude;computer science;artificial intelligence;achievement test	HCI	-81.26368503361451	-39.044268014967564	433
9edba5001868b52df52da42ac86e30e5e8fa0c87	on the design of fir filters by complex chebyshev approximation	finite impulse response filter chebyshev approximation frequency response delay algorithm design and analysis signal processing digital systems digital filters speech;linear phase;filtre reponse impulsion finie;error function;helium;finite impulse response filter;equalization;speech;digital filter;tchebychev filter;remez exchange algorithm;frequency response;filtro respuesta impulsion acabada;finite impulse response;filtro numerico;signal processing;fir filter;digital systems;digital filters;real valued selective systems;complex chebyshev approximation;digital filters chebyshev approximation;design;filtro tchebychev;fir filters;complex error function;chebyshev approximation;filtre tchebychev;complex valued filters;algorithm design and analysis;filtre numerique;linear phase design fir filters complex chebyshev approximation finite impulse response equalization complex valued filters complex error function error function remez exchange algorithm real valued selective systems	The long-standing problem of approximating a complex-valued desired function with a finite impulse-response (FIR) filter is considered. It is formulated as an equalization to be solved using complex-valued filters. The proposed algorithm deals directly with the complex error function, which depends linearly on the coefficients of the filter to be designed. The magnitude of this error function is minimized in the Chebshev sense using a generalization of the Remez exchange algorithm. The method can be used to design complex- or real-valued-selective systems as well. The well-known design of optimal FIR filters with linear phase is included here as a special case. >	approximation theory;finite impulse response	Klaus Preuss	1989	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/29.17562	adaptive filter;network synthesis filters;computer vision;mathematical optimization;electronic engineering;parks–mcclellan filter design algorithm;electrical engineering;finite impulse response;signal processing;control theory;mathematics;filter design	Visualization	60.71168295077408	16.589831986287667	434
20706fcee57b496dc49b7bbbe48887660278c5c1	energy–performance tunable logic	logic family;internally dynamic topology;tunable circuits and devices circuit testing circuit topology power supplies logic circuits integrated circuit interconnections power system interconnection circuit optimization field programmable gate arrays;wires;power supply;chip;size 90 nm;energy performance;size 90 nm internally dynamic topology logic family logic gates transistor thresholds programmable interconnect circuits field programmable gate arrays;logic testing field programmable gate arrays logic gates;logic gates;energy consumption;integrated circuit interconnections;transistors;programmable interconnect circuits;logic testing;transistor thresholds;field programmable gate arrays	We propose a new logic family that enables the user to tune the transistor's effective threshold voltage after fabrication for higher speed or lower power. This technique along with dynamic voltage scaling allows simultaneous optimization of static and dynamic power based on the application/workload requirements. Programmable interconnect from an FPGA was implemented using this logic family on a 90 nm CMOS test chip. Measurements show that this topology provides twice the tuning range in the energy-performance space compared to a conventional interconnect utilizing only supply voltage scaling. For most of the performance range, this circuit consumes 35% less energy compared to a state-of-the-art design. The circuit is an externally static, internally pulse-mode topology which can replace static circuits without requiring significant changes to the system.	cmos;dynamic voltage scaling;field-programmable gate array;image scaling;logic family;mathematical optimization;requirement;transistor	Bita Nezamfar;Mark Horowitz	2009	2009 IEEE Custom Integrated Circuits Conference	10.1109/CICC.2009.5280874	chip;electronic engineering;logic optimization;logic gate;logic family;telecommunications;programmable logic array;engineering;electrical engineering;pass transistor logic;transistor;field-programmable gate array;computer engineering	EDA	16.69599597244214	57.17553500375697	435
7b1a96feaa854f725395ffcaef1a5603b4903396	testing implication of probabilistic dependencies	intriguing connection;logical implication;chase computation;exponential time;probabilistic reasoning system;powerful tool;non-axiomatic method;probabilistic dependency;testing implication;nontrivial theoretical result;new dependency	Axiomatization has been widely used for test­ ing logical implications. This paper suggests a non-axiomatic method, the chase, to test if a new dependency follows from a given set of probabilistic dependencies. Although the chase computation may require exponential time in some cases, this technique is a pow­ erful tool for establishing nontrivial theoreti­ cal results. More importantly, this approach provides valuable insight into the intriguing connection between relational databases and probabilistic reasoning systems. 1 INTRODUCTION In probability theory, the notion of dependencies (in­ dependencies) play an important role. The knowledge of conditional independencies, in particular, is essen­ tial for developing a viable probabilistic reasoning sys­ tem [9, 12, 13]. Given a set of probabilistic dependencies, there are ad­ ditional dependencies implied by this set in the sense that any joint probability distribution that satisfies the original set must also satisfy the additional de­ pendencies. Developing a qualitative method for test­ ing logical implication of dependencies is important for many reasons. First, it enables us to derive interesting and powerful theorems that may or may not be obvi­ ous from the numerical representation of probabilities. Second, in the design of a probabilistic inference sys­ tem, we often need to know whether one dependency is implied by a given set of dependencies. Axiomatization has been widely used for determining logical implications [2, 14, 15, 16, 17, 19]. In this ap­ proach, a finite set of complete inference rules are in­ troduced for a particular class of dependencies. These rules are used to generate symbolic proofs for new dependencies in a manner analogous to proofs in math­ ematical logic. In this paper, we adopt an alterna­ tive method from relational database theory [7, 11] for testing logical implication of probabilistic dependen­ cies. We use tableaux and an operation on tableaux, the chase, to test if a new dependency follows from the initial set of dependencies. Our study will focus on the generalized acyclic join dependency (GAJD) [20, 21]. Probabilistic conditional independencies are a subclass of this dependency. The chase computation may require exponential time in some cases [11]. However, this approach provides valuable insight into the intriguing connection between relational database and probabilistic reasoning sys­ tems. On the practical side, the chase technique is a powerful tool for establishing some important theoret­ ical results. For example, based on this technique, one can show that a GAJD is equivalent to a set of …	axiomatic system;best, worst and average case;chase (algorithm);computation;conditional entropy;database theory;directed acyclic graph;exptime;join dependency;mathematical optimization;need to know;numerical analysis;probabilistic database;relational database;time complexity;ical	S. K. Michael Wong	1996			dependency theory;theoretical computer science;machine learning;mathematics;algorithm	AI	-9.16237996006334	19.00589111348045	436
7e7752098fb05c02e5ca27f2d68684117be83302	tutoring an entire game with dynamic strategy graphs: the mixed-initiative sudoku tutor	student model;intelligent tutoring system;student modeling;mixed initiative;strategy games;indexing terms;mixed initiative interaction;mixed initiative systems;intelligent tutoring systems	In this paper, we develop a mixed-initiative intelligent tutor for the game of Sudoku called MITS. We begin by developing a characterization of the strategies used in Sudoku as the basis for teaching the student how to play. In order to reason about interaction with the student, we also introduce a student modeling component motivated by the mixed-initiative model of Fleming and Cohen that tracks what the student knows and understands. In contrast to other systems for tutoring games, we are able to interact with students to complete an entire game. This is achieved by retaining a model of acceptable next moves (called a strategy graph) and dynamically adjusting this model as the student plays the game. We present the overall architecture of the system followed by an explanation of the modules that encapsulate the rules of Sudoku. We also outline formulae for reasoning about interaction with the student that support mixed-initiative where either the system or the student can elect to direct the playing of the game. An implementation of the system is discussed, including examples of MITS interacting with students in order to tutor the game. To conclude, we discuss how this research is useful not only to gain insight into how to tutor students about strategy games but also to understand how to support mixed-initiative interaction in tutorial settings.	algorithm;deadlock;decision theory;interaction;level of detail;micro instrumentation and telemetry systems;semantic reasoner;sudoku;tutor;yet another	Allan Caine;Robin Cohen	2007	JCP	10.4304/jcp.2.1.20-32	simulation;index term;computer science;artificial intelligence;machine learning;multimedia;world wide web	HCI	-74.64677783230951	-50.32282984073794	437
ec38648ad96190f603e7e4168d284360e4ff93a7	using nightlight remote sensing imagery and twitter data to study power outages	disaster management;hurricane sandy;remote sensing;power outages;social media	"""Hurricane Sandy made landfall in one of the most populated areas of the United States, and affected almost 8 million people. The event provides a unique opportunity to study power outages because of the data available and the large impact to a densely populated area. Satellite nightlight imagery of """"before"""" and """"after"""" the landfall of the hurricane is used to quantify the light dimming caused by power outages. Geolocated tweets filtered by keywords provide valuable information on human activity at a high temporal and spatial resolution during the event. Analysis of brightness change in the satellite data and the density of power related tweets points to a spatial relationship that identifies severely impacted areas with human presence. Classification of tweets through text analysis serves to further narrow the information search to find the most relevant and reliable content. Twitter data fused with satellite imagery identifies power outage information at a street-level resolution that is not achievable with satellite imagery alone."""	population;sun outage;text mining;toshiba satellite;tropical cyclone forecasting	Carolynne Hultquist;Mark Simpson;Guido Cervone;Qunying Huang	2015		10.1145/2835596.2835601	geography;telecommunications;cartography;remote sensing	HCI	-21.20394003155735	-33.76647229920526	438
d564a1985b96fe6416b247777ad42546dbc0bc81	ic failure analysis: the importance of test and diagnostics	fault localization;customer satisfaction;failure analysis;continuous improvement;scan;integrated circuit testing;vlsi;failure analysis integrated circuit testing circuit faults chemical analysis hardware dry etching inspection wiring manufacturing processes chemical processes;time to market;vlsi failure analysis integrated circuit testing;software based diagnostics;fault localization ic failure analysis test diagnostics yield customer satisfaction quick corrective action root cause failure analysis;diagnostic method	failures are unfortunately an inherent part of the microelectronics business, where complexity is growing rapidly. Failures can occur during several points of a product’s life cycle, such as technology or product development and qualification, yield learning, reliability improvement, system manufacture, and field application. The impact of such failures ranges from consequential to catastrophic. While we expect failures during reliability stressing or yield learning on a new technology, mature programs and parts qualified for sale and field application demand competitive quality and reliability levels. Failures during these later phases of production need immediate analysis and corrective action. Whether anticipated or sudden, failures can have a severe business impact. Because narrow market opportunities often drive shortened product cycles, companies need to understand failures and take corrective actions quickly. Electrical characterization, statistical analysis, signature analysis, and process experiments can provide important clues that allow us to infer the cause of failure. But only full root-cause physical failure analysis can provide the incriminating evidence necessary to correct problems with confidence: the picture worth a thousand words. The crucial element of failure analysis is fault localization, a task for which both hardware and software techniques exist (see the adjacent box). Trends toward denser circuits and more sophisticated packaging, however, are limiting physical access to internal chip circuitry and thus diminishing the effectiveness of hardware-based diagnostics. This article reviews hardware and software options for fault localization and shows why software diagnostics must become a key focus within the design, test, and failure analysis communities.	electronic circuit;enterprise life cycle;experiment;failure analysis;new product development;physical access	David P. Vallett	1997	IEEE Design & Test of Computers	10.1109/54.606001	reliability engineering;failure analysis;electronic engineering;engineering;very-large-scale integration;forensic engineering;customer satisfaction	Arch	23.648485531536412	54.03445662433165	439
0f64c91929abf28460aff369c3b193bf3518bcfb	relational learning as search in a critical region	inductive logic programming;probability of failure;phase transition;machine learning;learning problems;relational learning	Machine learning strongly relies on the covering test to ass es whether a candidate hypothesis covers training examples. The present paper investigates lear ning elational concepts from examples, termedrelational learningor inductive logic programming . In particular, it investigates the chances of success and the computational cost of relational learnin g, which appears to be severely affected by the presence of a phase transition in the covering test. To his aim, three up-to-date relational learners have been applied to a wide range of artificial, full y re ational learning problems. A first experimental observation is that the phase transition beha ves s an attractor for relational learning; no matter which region the learning problem belongs to, all three learners produce hypotheses lying within or close to the phase transition region. Second , a failure regionappears. All three learners fail to learn any accurate hypothesis in this regio n. Quite surprisingly, the probability of failure does not systematically increase with the size of th e underlying target concept: under some circumstances, longer concepts may be easier to accurately approximate than shorter ones. Some interpretations for these findings are proposed and discuss ed.	algorithmic efficiency;approximation algorithm;failure cause;inductive logic programming;machine learning	Marco Botta;Attilio Giordana;Lorenza Saitta;Michèle Sebag	2003	Journal of Machine Learning Research		phase transition;multi-task learning;instance-based learning;error-driven learning;algorithmic learning theory;statistical relational learning;computer science;artificial intelligence;machine learning;data mining;inductive transfer;mathematics;stability;computational learning theory;active learning	ML	17.80678158849591	-34.00291654310992	440
bcb0f69277bcdebcfe88d65982174cf244351c88	derivation of the bias of the normalized sample covariance matrix in a heterogeneous noise with application to low rank stap filter	eigenvalues and eigenfunctions;gaussian noise;clutter;low rank clutter;white noise adaptive filters covariance matrices eigenvalues and eigenfunctions gaussian noise radar clutter radar detection space time adaptive processing spatiotemporal phenomena;space time adaptive processing stap bias study consistency low rank clutter normalized sample covariance matrix sirv orthogonal projector;space time adaptive processing stap;eigenvalues;orthogonal projector;adaptive filters;consistent estimator;clutter covariance matrix eigenvalues and eigenfunctions gaussian noise context radar;covariance matrices;lr stap normalized sample covariance matrix heterogeneous noise spatiotemporal adaptive processing spherically invariant random vector white gaussian noise data contamination nscm eigenvectors eigenvalues clutter subspace projector;spatiotemporal phenomena;radar detection;white gaussian noise;radar clutter;bias study;sirv;data validation;context;white noise;consistency;radar;eigenvectors;covariance matrix;normalized sample covariance matrix;space time adaptive processing	In a previous work, we have developed a low-rank (LR) spatio-temporal adaptive processing (STAP) filter when the disturbance is modeled as the sum of a low-rank spherically invariant random vector (SIRV) clutter and a zero-mean white Gaussian noise. This LR-STAP filter is built from the normalized sample covariance matrix (NSCM) and exhibits good robustness properties to secondary data contamination by target components. In this correspondence, we derive the bias of the NSCM with this noise model. We show that the eigenvectors estimated from the NSCM are unbiased. The new expressions of the expectation of NSCM eigenvalues are also given. From these results, we also show that the estimate of the clutter subspace projector based on the NSCM used in our LR-STAP is a consistent estimate of the true one. Results on numerical data validates the theoretical approach.	clutter;lr parser;level of measurement;low-rank approximation;numerical analysis;space-time adaptive processing;video projector	Guillaume Ginolhac;Philippe Forster;Frédéric Pascal;Jean Philippe Ovarlez	2012	IEEE Transactions on Signal Processing	10.1109/TSP.2011.2169063	econometrics;eigenvalues and eigenvectors;pattern recognition;mathematics;statistics	ML	51.53845061760803	9.74668759226485	441
062b12b2bfeb224ae0457abd81a91bde3649ebd7	new birthday attacks on some macs based on block ciphers	forgery attack;distinguishing attack;block cipher;impossible differential cryptanalysis;aes;birthday attack;differential cryptanalysis;mac	This paper develops several new techniques of cryptanalyzing MACs based on block ciphers, and is divided into two parts. The first part presents new distinguishers of the MAC construction Alred and its specific instance Alpha-MAC based on AES. For the Alred construction, we first describe a general distinguishing attack which leads to a forgery attack directly with the complexity of the birthday attack. A 2round collision differential path of Alpha-MAC is adopted to construct a new distinguisher with about 2 chosen messages and 2 queries. One of the most important results is to use this new distinguisher to recover the internal state, which is an equivalent subkey of Alpha-MAC. Moreover, our distinguisher on Alred construction can be applied to the MACs based on CBC and CFB encryption modes. The second part describes the first impossible differential attack on MACs-Pelican, MT-MAC-AES and PC-MAC-AES. Using the birthday attack, enough message pairs that produce the inner near-collision with some specific differences are detected, then the impossible differential attack on 4-round AES to the above mentioned MACs is performed. For Pelican, our attack recovers its internal state, which is an equivalent subkey. For MT-MAC-AES, the attack turns out to be a subkey recovery attack directly. The complexity of the two attacks is 2 chosen messages and 2 queries. For PC-MAC-AES, we recover its 256-bit key with 2 chosen messages and 2 queries.	birthday attack;block cipher mode of operation;digital signature forgery;distinguishing attack;encryption;impossible differential cryptanalysis;iteration;route distinguisher;sensor;time complexity;web design	Zheng Yuan;Wei Wang;Keting Jia;Guangwu Xu;Xiaoyun Wang	2009		10.1007/978-3-642-03356-8_13	advanced encryption standard;block cipher;chosen-ciphertext attack;differential cryptanalysis;watermarking attack;interpolation attack;pre-play attack;computer science;birthday attack;ciphertext-only attack;boomerang attack;mathematics;impossible differential cryptanalysis;internet privacy;world wide web;slide attack;computer security;statistics	Crypto	-37.11111060721514	80.0440324284157	442
0c55bd59780dbad4a5ba19f4c70dfcdd037a2069	monitoring prayer using mobile phone accelerometer		The ever-increasing ability of smartphones for sensing paves the way for a new form of human-computer interaction which wasn’t possible before. One of these possibilities is monitoring Muslims’ prayers which consist of a set of physical activities that must be conducted in a correct way, i.e. ordered and complete. In this paper, we introduce a novel method to monitor and detect prayer activities using mobile phone accelerometers in order to evaluate the correctness of prayer. The method involves four stages: data collection, signal pre-processing, features extraction and classification.		Reem Al-Ghannam;Eiman Kanjo;Hmood Al-Dossari	2015		10.1007/978-3-319-38904-2_17	real-time computing;computer science;data collection;activity recognition;correctness;mobile phone;accelerometer;prayer	HCI	5.051227723081259	-86.2210991830875	443
351d5bb09a250d6fc58226f86877825742f607b7	metadata standard interoperability: application in the geographic information domain	geographic metadata;ontologie;systeme information geographique;standards;aplicacion;concordance;geographic information system;metadata;geographic information;congres international;interoperabilite;interoperabilidad;congreso internacional;semantics;concordancia;data exchange;semantica;semantique;international conference;carta de datos;harmonization;crosswalks;mappage;norma;metadonnee;ontologia;mapping;metadata interoperability;metadatos;interoperability;armonizacion;application;ontology;norme;sistema informacion geografica;harmonisation	The use of metadata expands on the opportunities for interoperability. By interoperability, it is meant the ability to develop conventions that enable data exchange and integration. Metadata descriptions from different domains are not semantically distinct but overlap and relate to each other in complex ways. As the number, size and complexity of the metadata standards grow, the task of facilitating metadata in different standards becomes more difficult and tedious. One possible solution for this problem is the creation of mechanisms that enable the translation of this information in order to make it conform to the different standards. These mechanisms are denominated ”crosswalks” and the objective of this work is to present the process of ”crosswalk-creation”, which has been used by a research team at the University of Zaragoza in order to translate information among some of the most extended standards for geographic information metadata.	comit;conformity;dublin core;geographic information system;interoperability;interoperation;requirement;style sheet (web development);web resource;xml	Javier Nogueras-Iso;F. Javier Zarazaga-Soria;Javier Lacasta;Rubén Béjar;Pedro R. Muro-Medrano	2004	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2003.12.004	data exchange;interoperability;geospatial metadata;computer science;marker interface pattern;ontology;data mining;database;semantics;geographic information system;metadata;world wide web;data element;concordance;metadata repository	HPC	-38.233733145577155	11.158302314904489	444
7979149b3eb038e1b3974ac5ac644ec2f7372469	an empirical quality model for web service ontologies to support mobile devices	web services;graph theory;mobile computing;ontologies (artificial intelligence);semantic web;web service ontologies;empirical quality model;empirical software engineering approach;ontology subgraph;semantic web	As Web Services and the Semantic Web become more important, enabling technologies such as web service ontologies will grow larger. The ability of mobile devices, such as cell phones and PDAs, to download and reason across them will be severely limited. Given that an agent on a mobile device only needs a subset of what is described in a web service ontology, an ontology sub-graph can be created. In this paper, we develop a empirical software engineering approach to build a prediction model to measure the quality in terms of correct query handling of ontology sub-graphs relative to the original ontology — mean average recall of the sub-graph compared to the original ontology is used as the quality standard. Our metrics allow speedy selection of a sub-graph for use by a mobile device.	download;experimental software engineering;mobile device;mobile phone;ontology (information science);personal digital assistant;semantic web;web service	Dan Schrimpsher;Letha H. Etzkorn	2009	2009 3rd International Symposium on Empirical Software Engineering and Measurement	10.1145/1671248.1671304	web service;web modeling;mobile search;bibliographic ontology;web standards;computer science;graph theory;social semantic web;data mining;database;predictive modelling;ontology-based data integration;mobile computing;world wide web;owl-s;process ontology	Web+IR	-42.561156686720125	11.625036717587841	445
bba66701b388046999c37dc11a7c2531ac155326	trustworthy service discovery for dynamic web service composition	qos evaluation;trust management;trustworthy service discovery;dynamic service composition	As the number of services available on the Web increases, it is vital to be able to identify which services can be trusted. Since there can be an extremely large number of potential services that offer similar functionality, it is challenging to select the right ones. Service requestors have to decide which services closelysatisfy their needs, and theymust worry about the reliability of the service provider. Although an individual service can be trusted, a composed service is not guaranteed to be trustworthy. In this paper, we present a trust model that supports service discovery and composition based on trustworthiness. We define a method to evaluate trust in order to discover trustworthy services. We also provide a method to perform trust estimation for dynamic service composition, and we present results of two experiments. The proposed model allows for service requestors to obtain the most trustworthy services possible. Our mechanism uses direct and indirect user experience to discover the trustworthiness of the services and service providers. Moreover, composing services based on quantitative trust measurements will allow for consumers to acquire a highly reliable service that meet their quality and functional requirements.	service discovery;trustworthy computing;web service	Yukyong Kim;Jong-Seok Choi;Yongtae Shin	2015	TIIS	10.3837/tiis.2015.03.024	service provider;service level requirement;service level objective;mobile qos;service product management;differentiated service;service delivery framework;service design;service guarantee;service discovery;internet privacy;world wide web;computer security;service system	Web+IR	-45.468485244709775	15.803728260231246	446
3d31fe2646e77a3391647b04aec365abb2a39715	acceleration feedback control for nonlinear teleoperation systems with time delays	reglerteknik;iss;acceleration feedback;delays;teleoperation systems	Acceleration feedback control for nonlinear teleoperation systems with time delays Yuling Li, Rolf Johansson & Yixin Yin a School of Automation and Electrical Engineering, University of Science and Technology Beijing, P.O. Box 136, 100083 Beijing, P.R. China b Department of Automatic Control, Lund University, P.O. Box 118, 22100 Lund, Sweden Accepted author version posted online: 10 Sep 2014.Published online: 15 Oct 2014.	automatic control;automation;electrical engineering;feedback;nonlinear system	Yuling Li;Rolf Johansson;Yixin Yin	2015	Int. J. Control	10.1080/00207179.2014.963829	control engineering;simulation;international space station;engineering;control theory	Robotics	58.203790573402244	-0.7139970224206824	447
e3336f80d057805e3f06a28415d3b790933f6c31	the tale of two localization technologies: enabling accurate low-overhead wifi-based localization for low-end phones		WiFi fingerprinting is one of the mainstream technologies for indoor localization. However, it requires an initial calibration phase during which the fingerprint database is built manually by site surveyors. This process is labour intensive, tedious, and needs to be repeated with any change in the environment. While a number of recent systems have been introduced to reduce the calibration effort through RF propagation models and/or crowdsourcing, these still have some limitations. Other approaches use the recently developed iBeacon technology as an alternative to WiFi for indoor localization. However, these beacon-based solutions are limited to a small subset of high-end phones.  In this paper, we present HybridLoc: an accurate low-overhead indoor localization system. The basic idea HybridLoc builds on is to leverage the sensors of high-end phones to enable localization of lower-end phones. Specifically, the WiFi fingerprint is crowdsourced by opportunistically collecting WiFi-scans labeled with location data obtained from BLE-enabled high-end smart phones. These scans are used to automatically construct the WiFi-fingerprint, that is used later to localize any lower-end cell phone with the ubiquitous WiFi technology. HybridLoc also has provisions for handling the inherent error in the estimated BLE locations used in constructing the fingerprint as well as to handle practical deployment issues including the noisy wireless environment, heterogeneous devices, among others.  Evaluation of HybridLoc using Android phones shows that it can provide accurate localization in the same range as manual fingerprinting techniques under the same deployment conditions. Moreover, the localization accuracy on low-end phones supporting only WiFi is comparable to that achieved with high-end phones supporting BLE. This accuracy is achieved with no training overhead, is robust to the different user devices, and is consistent under environment changes.	android;crowdsourcing;fingerprint (computing);ibeacon;indoor positioning system;mobile phone;overhead (computing);radio frequency;sensor;smartphone;software deployment;software propagation	Ahmed Shokry;Moustafa Elhamshary;Moustafa Youssef	2017		10.1145/3139958.3139989	data mining;phone;wireless;embedded system;android (operating system);software deployment;computer science;ibeacon;crowdsourcing	Mobile	19.10092410561159	72.67053689787649	448
d668747ba46a0d52220ba5940ad5414eba25ca32	designing database operators for flash-enabled memory hierarchies		Flash memory affects not only storage options but also query processing. In this paper, we analyze the use of flash memory for database query processing, including algorithms that combine flash memory and traditional disk drives. We first focus on flash-resident databases and present data structures and algorithms that leverage the fast random reads of flash to speed up selection, projection, and join operations. FlashScan and FlashJoin are two such algorithms that leverage a column-based layout to significantly reduce memory and I/O requirements. Experiments with Postgres and an enterprise SSD drive show improved query runtimes by up to 6x for queries ranging from simple relational scans and joins to full TPC-H queries. In the second part of the paper, we use external merge sort as a prototypical query execution algorithm to demonstrate that the most advantageous external sort algorithms combine flash memory and traditional disk, exploiting the fast access latency of flash memory as well as the fast transfer bandwidth and inexpensive capacity of traditional disks. Looking forward, database query processing in a three-level memory hierarchy of RAM, flash memory, and traditional disk can be generalized to any number of levels that future hardware may feature.	data structure;database;external sorting;flash memory;ibm tivoli storage productivity center;input/output;memory hierarchy;merge sort;postgresql;random-access memory;requirement;solid-state drive;sorting algorithm	Goetz Graefe;Stavros Harizopoulos;Harumi A. Kuno;Mehul A. Shah;Dimitris Tsirogiannis;Janet L. Wiener	2010	IEEE Data Eng. Bull.		parallel computing;database;external sorting;memory map;distributed shared memory;merge sort;data structure;computer science;ranging;memory hierarchy;flash memory	DB	-14.032403964348253	53.890250874866844	449
5c0ccf73858cd10cbb62deb7bd3e95156d537164	an internet oriented custom-fit production approach for apparel industry		In the apparel industry, the new trend is to switch from mass production to custom-fit (personal) production. Using this method, manufacturers can rapidly produce models with the customeru0027s body measurements. In this study, we developed a software tool to help customers and manufacturers for custom-fit production. Customers can generate different models using reusable components that are provided from related apparel manufacturer via the Internet. Customers can also easily take their body measurements with this tool. The idea is to prepare user-friendly software for having customeru0027s own designs with proper body measurements in their houses via personal computers and other communication appliances. To create a cost-effective solution, Internet infrastructure is chosen.		Ender Yazgan Bulgun;Alp Kut	2005	J. Org. Computing and E. Commerce	10.1207/s15327744joce1504_2	marketing;advertising;commerce	DB	-62.82265382206236	5.082712336618732	450
d7f326a940838505a61637cdf36c99d3cbaa4ae0	knowledge bases for visual dynamic scene understanding		In conventional computer vision the actual 3-D state of objects is of primary interest; it is embedded in a temporal sequence analyzed in consecutive pairs. In contrast, in the 4-D approach to machine vision the primary interest is in temporal processes with objects and subjects (defined as objects with the capability of sensing and acting). All perception of 4-D processes is achieved through feedback of prediction errors according to spatiotemporal dynamical models constraining evolution over time. Early jumps to object/subject-hypotheses including capabilities of acting embed the challenge of dynamic scene understanding into a richer environment, especially when competing alternatives are pursued in parallel from beginning. Typical action sequences (maneuvers) form an essential part of the knowledge base of subjects. Expectation-based Multi-focal Saccadic (EMS-) vision has been developed in the late 1990s to demonstrate the advantages and flexibility of this approach. Based on this experience, the paper advocates knowledge elements integrating action processes of subjects as general elements for perception and control of temporal changes, dubbed ‘maneuvers’ here. − As recently discussed in philosophy, emphasizing individual subjects and temporal processes may avoid the separation into a material and a mental world; EMS-vision quite naturally leads to such a monistic view.	artificial intelligence;code;coefficient;computer vision;control engineering;dynamical system;embedded system;focal (programming language);knowledge base;machine vision;mental world;real-time transcription	Ernst D. Dickmanns	2015		10.5220/0005340802090215	computer vision;knowledge management	AI	19.922602123615707	-63.96739327819238	451
86cd7175bb7de944ad107529e1ffa95466f08b59	a porous thermoelastic problem: an a priori error analysis and computational experiments		In this paper, a porous thermoelastic problem is numerically considered. The variational formulation is written as a coupled system of two hyperbolic equations for the displacement and the porosity fields and a parabolic equation for the temperature field. An existence and uniqueness result as well as an energy decay property are recalled. Then, fully discrete approximations are introduced by using the finite element method to approximate the spatial variable and the backward Euler scheme to discretize the first-order time derivatives. A priori error estimates are proved, from which the linear convergence is deduced under some additional regularity conditions. Finally, some one- and two-dimensional numerical simulations are presented to show the accuracy of the approximation and the behavior of the solution.	computation;error analysis (mathematics);experiment	José R. Fernández;M. Masid	2017	Applied Mathematics and Computation	10.1016/j.amc.2017.01.070	mathematical optimization;mathematical analysis;calculus;mathematics	Theory	89.66361155997681	10.21924121862403	452
05cf7256ed751b5ec04e8ad7ef311254c1bb63fe	domain-specific language for hw/sw co-design for fpgas	hw sw co design;system design;domain specific language;hardware design;communication channels;embedded processor;finite state machine	This article describes FSMLanguage, a domain-specific language for HW/SW co-design targeting platform FPGAs. Modern platform FPGAs provide a wealth of configurable logic in addition to embedded processors, distributed RAM blocks, and DSP slices in order to help facilitate building HW/SW codesigned systems. A technical challenge in building such systems is that the practice of designing software and hardware requires different areas of expertise and different description domains, i.e. languages and vocabulary. FSMLanguage attempts to unify these domains by defining a way to describe HW/SW co-designed systems in terms of sets of finite-state machines – a concept that is reasonably familiar to both software programmers and hardware designers. FSMLanguage is a domain-specific language for describing the functionality of a finite-state machine in such a way that its implementation can be re-targeted to software or hardware in an efficient manner. The efficiency is achieved by exploiting the resources found within modern platform FPGAs – namely the distributed RAM blocks, soft-core processors, and the ability to construct dedicated communication channels between FSMs in the reconfigurable fabric. The language and its compiler promote uniformity in the description of a HW/SW co-designed system, which allows a system designer to make partitioning and implementation strategy decisions later in the design cycle.	central processing unit;circuit complexity;compiler;domain-specific language;embedded system;field-programmable gate array;finite-state machine;programmer;random-access memory;shattered world;systems design;transform, clipping, and lighting;vocabulary	Jason Agron	2009		10.1007/978-3-642-03034-5_13	computer architecture;parallel computing;real-time computing;computer science	EDA	1.5802115558165044	49.90589787290798	453
a41be269b7b72ff7a03601024e4f12b2ae6da907	comparación experimental de controladores pid clásico, pid no lineal y pid difuso para el caso de regulación		This article presents the experimental results when comparing the Controllers PID Classic, Nonlinear PID and Fuzzy Gain Scheduling PID used in position controllers, for the tuning of the Classical PID was used the second method	controller (control theory);pid	Luis Fidel Cerecero Natale;Eduardo Campos-Mercado;Julio César Ramos Fernández;Marco Antonio Márquez-Vera;Irvin Arlin Chan Ac	2017	Research in Computing Science		pid controller;control theory;mathematics	Robotics	69.89535175059494	-8.453461372860996	454
53d7427e3bc20dd19bf3f4b5e075e7d0268eef76	stake: a coupled simulation environment for risc-v memory experiments		The recent emergence of open hardware platforms has drastically reduced the cost required to develop special purpose device architectures. The RISC-V instruction set and associated ecosystem is one such architecture that has become widely popular as the basis for the next generation embedded control systems and IoT devices.  Despite the recent popularity of the RISC-V architecture, there is currently very little support for performing architectural experiments of high performance RISC-V devices designed to target high performance computing and high performance analytics applications. Further, there are no general purpose simulation infrastructures that support developing architectural experiments using RISC-V cores coupled to emerging memory devices such as HBM and HMC. As a result, there is currently a significant gap for those seeking to develop high performance architectural models using emerging memory devices and the RISC-V instruction set.  The Stake infrastructure is designed to provide integrated simulation capabilities that supports experimentation using RISC-V processing cores as well as emerging memory devices. Stake is constructed using a fusion of the existing RISC-V Spike functional simulation infrastructure coupled to a custom memory generation component within the Sandia Structural Simulation (SST) framework. The result being an integrated simulation infrastructure that has the ability to simulate heterogeneous mixtures of hierarchical caches, traditional DRAM memories and emerging memory devices. We demonstrate this infrastructure with a design study to elicit the efficacy of 64 bit RISC-V cores on executing graph applications using different mixtures of cache, DRAM memories and HMC memories. In this study, we utilize the Graph Analytics Benchmark Suite to drive simulated memory traffic into mixtures of single level and multi-level caches coupled to each of the aforementioned backend memory devices. From this, we determine that in our simulated architecture, the HMC memory configuration with a single (L1) caching layer provides the most advantageous system configuration for executing graph algorithms and applications.		John D. Leidel	2018		10.1145/3240302.3240307	discrete event simulation;cache;computer architecture;risc-v;instruction set;dram;supercomputer;control system;analytics;computer science	Arch	-4.049865123187477	45.82494072785232	455
71ae8605af898c0606fbdaf74345b69e1afb5242	performance analysis of a parallel banyan atm switch	performance analysis;queueing analysis;banyan networks;atm	Asynchronous transfer mode (ATM) switches can be constructed by connecting multiple banyan networks in parallel. To utilize the capacity of the parallel banyan networks fully, it is crucial to allow up to L cells from each input to be switched, and up to L cells to be received by each output simultaneously, whereL is the total number of parallel banyan networks. This is possible if the switch operates inL overlapping phases and one banyan network is used to switch cells in each phase. Although a couple of such designs have been proposed and simulated, there is a lack of suitable models for such switches to be analysed mathematically. In this paper, two approximate analyses of a parallel banyan ATM switch are described. A comparison of the analytical and simulation results show that the analyses give reasonably accurate results.  1997 by John Wiley & Sons, Ltd.	atm turbo;apollonian network;approximation algorithm;john d. wiley;network switch;profiling (computer programming);simulation	T. H. Cheng;Y. Shen	1997	Int. J. Communication Systems	10.1002/(SICI)1099-1131(199701)10:1%3C43::AID-DAC324%3E3.0.CO;2-5	crossover switch;parallel computing;real-time computing;telecommunications;computer science;atmosphere	HPC	0.5900606497221744	93.8990092624718	456
172d64e57ca6e8f02730d8ce9819a17cab1c5ea2	efficient similarity search in large databases of tree structured objects	content based filtering similarity search large databases tree structured object tree feature structural filtering;internal structure;tree data structures;feature vector;semi structured data;tree structure;xml document;information filters very large databases tree data structures content based retrieval;very large databases;information filters;content based retrieval;similarity search;filters spatial databases filtering histograms computer science image databases application software chemical compounds xml computational complexity	We implemented our new approach for efficient similarity search in large databases of tree structures. Our experiments show that filtering significantly accelerates the complex task of similarity search for tree-structured objects. Moreover, they show that no single feature of a tree is sufficient for effective filtering, but only the combination of structural and content-based filters yields good results.	database;experiment;similarity search	Karin Murthy;Hans-Peter Kriegel;Stefan Schönauer;Thomas Seidl	2004	Proceedings. 20th International Conference on Data Engineering	10.1109/ICDE.2004.1320066	semi-structured data;xml;feature vector;computer science;data mining;database;fractal tree index;tree structure;tree;information retrieval	DB	-6.25737286662445	-40.20395516790323	457
0b828dc7f238fdfedbce099de15934328f3f7d5e	evaluating the accuracy of defect estimation models based on inspection data from two inspection cycles	quality assurance;animals;defect data;project management;biological system modeling;software requirements defect estimation models inspection data inspection cycles defect content estimation techniques defect data university environment;inspection yield estimation quality assurance biological system modeling context modeling software quality chromium animals statistics decision support systems;yield estimation;development process;inspection;software requirements;inspection cycles;code inspection;code review;object oriented;inspection data;chromium;decision support systems;statistics;project management inspection software development management;university environment;context modeling;defect estimation models;software quality;software development management;defect content estimation techniques;empirical methods	Defect content estimation techniques (DCETs), based on defect data from inspection, estimate the total number of defects in a document to evaluate the development process. For inspections that yield few data points DCETs reportedly underestimate the number of defects. If there is a second inspection cycle, the additional defect data is expected to increase estimation accuracy. In this paper we consider 3 scenarios to combine data sets from the inspection-reinspection process. We evaluate these approaches with data from an experiment in a university environment where 31 teams inspected and reinspected a software requirements document. Main findings of the experiment were that reinspection data improved estimation accuracy. With the best combination approach all examined estimators yielded on average estimates within 20% around the true value, all estimates stayed within 40% around the true value.	data point;requirement;software bug;software requirements	Stefan Biffl;Wilfried Grossmann	2001		10.1109/ICSE.2001.919089	project management;reliability engineering;quality assurance;chromium;code review;inspection;computer science;systems engineering;engineering;software engineering;data mining;software requirements;software quality	SE	-65.11527220263473	30.903180456749354	458
d6645ad3cea8f254755b31b4213393738476f13b	regional variation in temporal organization in american english	rhythm;dialect variation;temporal organization;american english	The goal of the current study was to explore the temporal organization of six regional dialects of American English to gain a better understanding of the perceptual impressions of speaking rate variation. The study further examines whether regional dialects form different groupings based on their segmental vs. global temporal characteristics. Acoustic measures included articulation rate, pause frequency and duration, and vowel and consonant duration variability. The results revealed that Southern American English is characterized by a slow overall articulation rate, long pauses, and highly variable syllable-to-syllable vowel durations, whereas the New England dialect is characterized by a fast overall articulation rate, short pauses, and highly variable syllable-tosyllable consonant durations. The patterns for the other dialects are more mixed: the Northern and Western dialects are characterized by low variability vowel durations, the Midland dialect shares a slow articulation rate with the Southern dialect, and the Mid-Atlantic dialect exhibits no unique temporal properties among those examined. Thus, temporal variation in regional dialects of American English is orthogonal to vowel variation, in which New England, Midland, and Western dialects are often characterized together as “General American”. Taken together, the results are consistent with the stereotype that Southerners talk slowly and Northerners talk quickly and suggest that pausing and segmental duration variability may contribute to the perceived speaking rate differences. & 2014 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;backdrop cms;biconnected component;coefficient;heart rate variability;spatial variability;syllable;triune continuum paradigm	Cynthia G. Clopper;Rajka Smiljanic	2015	J. Phonetics	10.1016/j.wocn.2014.10.002	speech recognition;rhythm;linguistics;communication	NLP	-10.973173537304335	-81.78192397833692	459
099acb165c533b40623ccbac885c153b4230380a	on the probability of independent sets in random graphs	random graph;independent set;independence number;satisfiability	Let k be the asymptotic value of the independence number of the random graph G(n, p). We prove that if the edge probability p(n) satisfies p(n) À n−2/5 ln n then the probability that G(n, p) does not contain an independent set set of size k−c, for some absolute constant c > 0, is at most exp{−cn2/(k4p)}. We also show that the obtained exponent is tight up to logarithmic factors, and apply our result to obtain new bounds on the choice number of random graphs. We also discuss a general setting where our approach can be applied to provide an exponential bound on the probability of certain events in product probability spaces.	independent set (graph theory);rado graph;random graph;time complexity	Michael Krivelevich;Benny Sudakov;Van H. Vu;Nicholas C. Wormald	2003	Random Struct. Algorithms	10.1002/rsa.10063	probability distribution;independence;random regular graph;random graph;random variable;combinatorics;probability mass function;discrete mathematics;illustration of the central limit theorem;multivariate random variable;independent set;random element;convolution of probability distributions;convergence of random variables;cumulative distribution function;symmetric probability distribution;regular conditional probability;mathematics;algebra of random variables;joint probability distribution;moment-generating function;satisfiability	Theory	39.17873976510642	15.405043431953267	460
a310fa766688283182ea8541c07a76cb219843a7	fault detection of rolling element bearings using the frequency shift and envelope based compressive sensing		Rolling element bearings are the essential components of rotating machines, faults of which can cause serious failures or even major breakdowns of a machine. Fault diagnosis deliveries significant benefits to machines with rolling element bearings by finding the faults at early period and taking corrective actions to enhance safe and high performance operations. However, multiple sensor usages and high rate data acquisition involved in a monitoring system have considerable drawbacks of high system cost involved in purchasing hardware for data transfer, storage and processing. To reduce these shortages, this paper investigates compressive sensing (CS) techniques for the fault detection of rolling element bearings. Based on the frequency shift and envelope analysis, a CS scheme is developed for monitoring the bearing. The number of data transmitted and stored can be reduced by several thousands of times. The simulation and the experimental results demonstrate that the compressed vibration signals of rolling element bearings are effective to detect bearing faults at the total compressing ratio up to several thousand with the corresponding maximum compression ratio (CR) of CS process at nearly 100. In addition, several performance measures are applied to evaluate the reconstructed signals and show approximately the information about the noise level of the system.	compressed sensing;data acquisition;data compression ratio;data point;fault detection and isolation;modulation;noise (electronics);purchasing;simulation	Xiaoli Tang;Yuandong Xu;Fengshou Gu;Andrew D. Ball;Guangbin Wang	2017	2017 23rd International Conference on Automation and Computing (ICAC)	10.23919/IConAC.2017.8082063	engineering;compressed sensing;control engineering;noise measurement;fault detection and isolation;data transmission;data acquisition;bearing (mechanical);electronic engineering;vibration;signal-to-noise ratio	HPC	38.91569731393904	-30.459484587149472	461
54e924a3f28db78a1e87aabed9809ed385085ad2	quality adaptive trained filters for compression artifacts removal	quality metric;least mean square;video coding adaptive filters data compression least mean squares methods;data compression;least mean squares methods;video frame filtering;indexing terms;image decompression;artifact reduction coding quality adaptive trained filters compression artifacts removal input video signal artifact visibility video frame filtering least mean square mechanism image decompression;video coding;adaptive filters;quality adaptive trained filters;least mean square optimization quality metric adaptive filters compression artifacts removal;artifact visibility;least mean square mechanism;adaptive filters image coding degradation video compression transform coding testing pixel discrete cosine transforms filtering automatic voltage control;least mean square optimization;compression artifacts removal;artifact reduction coding;adaptive filter;input video signal	A compression artifacts removal algorithm that is adaptive to the artifact visibility level of the input video signal is proposed. The artifact visibility is determined per frame by the ratio of the accumulated gradient on the block edges to that of the remaining area. The filtering of each video frame is optimized using a least mean square mechanism which trains on pairs of target images and decompressed images of similar quality as the input frame. Experimental results show that the proposed approach outperforms several methods in coding artifact reduction.	adaptive algorithm;coefficient;compression artifact;elegant degradation;gradient;mean squared error;online and offline;pixel;reduction (complexity)	Ling Shao;Jingnan Wang;Ihor O. Kirenko;Gerard de Haan	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517755	adaptive filter;residual frame;computer vision;computer science;theoretical computer science	Robotics	45.980759122146054	-17.2330533653817	462
01867be2fa0ecb4db47da23b48a5ccd261dc63e1	encoding qualitative three dimensional shape features	three dimensional			James E. Gary	2003			encoding (memory);mathematics;artificial intelligence;pattern recognition	Vision	60.35616784904482	-46.83178090785561	463
ca7e0780f74ab18e1ac18bde0fac36bbe73560e7	an extension of monte carlo hypothesis tests	mixtures;monotonicity;barnard s test;point mass;power	AbstractThere are many hypothesis testing settings in which one can calculate a “reasonable” test statistic, but in which the null distribution of the statistic is unknown or completely intractable. Fortunately, in many such situations, it is possible to simulate values of the test statistic under the null hypothesis, in which case one can conduct a Monte Carlo test. A difficulty however arises in that Monte Carlo tests, as they are currently structured, are applicable only if ties cannot occur amongst the values of the test statistics. There is a frequently occurring scenario in which there are lots of ties, namely that in which the null distribution of the test statistic has a (single) point mass. It turns out that one can modify the current form of Monte Carlo tests so as to accommodate such settings. Developing this modification leads to an intriguing identity involving the binomial probability function and its derivatives. In this paper we will briefly explain the modified procedure, discuss simulati...	monte carlo method	Rolf Turner;Celeste Jeffs	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1208232	econometrics;p-value;test statistic;one- and two-tailed tests;chi-square test;monotonic function;likelihood-ratio test;power;mathematics;exact test;fisher's exact test;point particle;mixture;statistics;z-test	ML	30.41454041115934	-20.933656538827464	464
70295aa21a3d0dc9ffdf382db733fc9d1fcbbe62	time frequency analysis and parametric approximation of room impulse responses	subband signal approximation;reverberation;parametric model;time frequency analysis fourier transforms reflection time invariant systems parametric statistics signal analysis finite impulse response filter filter bank performance evaluation transfer functions;performance evaluation;filter bank;parametric statistics;transfer functions;decay rate;signal analysis;time frequency;finite impulse response filter;acoustic signal processing time frequency analysis transient response architectural acoustics fourier transforms poles and zeros signal reconstruction reverberation;acoustic transmission characteristics;recursive realization;architectural acoustics;acoustic signal processing;source receiver pair;short time fourier transform;poles and zeros;linear time invariant systems;model evaluation;room impulse response;early reflections;transient response;parametric models time frequency analysis parametric approximation room impulse responses linear time invariant systems acoustic transmission characteristics source receiver pair recursive realization short time fourier transform subband signal approximation pole zero models steiglitz mcbride iteration perfect reconstruction early reflections reverberation decay rate;fourier transforms;parametric models;linear time invariant;signal reconstruction;pole zero models;parametric approximation;room impulse responses;steiglitz mcbride iteration;perfect reconstruction;time invariant systems;time frequency analysis;reflection;steiglitz mcbride	Rooms are modeled as linear time invariant systems, where the acoustic transmission characteristics between a specific source receiver pair are described by the room impulse response (RIR). A recursive realization of the short time Fourier transform is employed to decompose the RIR in the time frequency domain, where the different subband signals are approximated by parametric models. The pole-zero models, evaluated by the Steiglitz-McBride iteration, perform perfect reconstruction of the early reflections, whereas the decay rate of the reverberant part is sufficiently approximated. The RIR of a real room is studied as an example case.	approximation;frequency analysis	John C. Sarris;George Cambourakis	2003		10.1109/ICASSP.2003.1201677	speech recognition;parametric model;time–frequency analysis;signal processing;control theory;mathematics;statistics	NLP	56.218622348809866	15.666536807422512	465
ceea3770688cb7efea705d3117551638f70572f5	rudimentary beth models and conditionally rudimentary kripke models for the heyting propositional calculus	kripke model	This paper continues the investigation of rudimentary Kripke models, i.e. non-quasiordered Kripke-style models for the Heyting propositional calculus H. Three topics, which make three companion pieces of [3], are pursued. The first topic is Beth-style models for H called rudimentary Beth models. Rudimentary Kripke models may be conceived as a particular type of these models, which are analogous to rudimentary Kripke models in not assuming quasi-ordering for the underlying frames. The second topic is a first step into the correspondence theory for rudimentary Kripke models. It is shown what conditions on frames are now defined by the characteristic schemata of Dummettu0027s logic, the logic of weak excluded middle and classical propositional logic. The third topic is a generalization of rudimentary Kripke models that yields models called conditionally rudimentary Kripke models. It is shown that, if we donu0027t want to change the usual semantic clauses for the connectives, conditionally rudimentary Kripke models make the largest class of Kripkestyle models with respect to which we can demonstrate the ordinary soundness and completeness of H.	evert willem beth;kripke semantics;propositional calculus	Kosta Dosen	1991	J. Log. Comput.	10.1093/logcom/1.5.613	discrete mathematics;intuitionistic logic;computer science;mathematics;kripke semantics;algorithm	Logic	-11.721688195781011	7.834834395110833	466
385fc2ad0b535e06d42e99352790ff88f2dd2411	type-checking software product lines - a formal approach	formal approach;software;feature annotation;type system type checking software product lines formal approach program variants type safety featherweight java calculus feature annotation;pediatrics;frequency modulation;featherweight java calculus;color;program variants;java color calculus software cognition frequency modulation pediatrics;type checking;calculus;type theory;cognition;type theory java process algebra;process algebra;software product line;software product lines;type safety;type system;java	A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test all variants and ensure properties like type-safety for the entire SPL. While first steps to type-check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java (FJ) calculus with feature annotations to be used for SPLs. By extending FJ's type system, we guarantee that - given a well-typed SPL - all possible program variants are well- typed as well. We show how results from this formalization reflect and help implementing our own language-independent SPL tool CIDE.	interaction;java compiler;language-independent specification;sensor;software product line;type rule;type safety;type system	Christian Kästner;Sven Apel	2008	2008 23rd IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2008.36	frequency modulation;process calculus;cognition;type system;type safety;computer science;theoretical computer science;programming language;java;type theory;algorithm	SE	-22.52709793336443	22.88002354195271	467
67356af13f35729497e94c74a65240f71d1c8565	efficient multiplierless channel filters for multi-standard sdr	software radio fir filters;linear phase;complexity theory;multistandard decimating filter channel filters linear phase fir filters software defined radios hardware complexity;design reuse;dynamic reconfiguration;software defined radio;hardware complexity;channel filters;finite impulse response filter;low complexity;fir digital filter;multistandard decimating filter;finite impulse response filter adders hardware gsm digital filters optimization complexity theory;linear phase fir filters;software radio;software defined radios;adders;fir filter;digital filters;optimization;fir filters;gsm;hardware	This paper investigates the design of very low complexity multiplierless linear phase FIR filters for use in the channelizer of multi-standard software defined radios. A technique for reducing the hardware complexity of linear phase FIR digital filters which minimizes the adder depth and the number of adders in the multiplier block is introduced and is used to implement a multistage, multi-standard decimating filter. The design reuses components for different communications standards and is thus ideal for use in systems which support dynamic reconfiguration.	adder (electronics);digital filter;dynamic programming;etsi satellite digital radio;finite impulse response;linear phase;multistage amplifier	Douglas L. Maskell;A. Prasad Vinod	2008	2008 8th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2008.4594680	real-time computing;telecommunications;computer science;finite impulse response;software-defined radio	EDA	31.783291320381778	57.79460323631571	468
b1463ca61cbfd3a82257aa9202667e19dca7c3d9	social aspects of digital information in perspective: introduction to a special issue				Roberta Lamb;Susan L Johnson	2004	J. Digit. Inf.		modeling perspective	AI	-65.44170140806818	-3.7913303049468765	469
bc53845deb61eeee42bcc20dee0a4362703dbaf9	dynamic global pid sliding mode control for mems gyroscope using adaptive neural controller	mems gyroscope;rbf neural networks;dynamic global pid sliding control	This paper derived a dynamic global proportional integral derivative (PID) sliding mode control based on adaptive radial basis function (RBF) neural controller for a micro electromechanical systems (MEMS) gyroscope. This approach gives a new dynamic global PID sliding mode manifold, which not only enables system trajectory to run on the global sliding mode surface at the start point more quickly and eliminate the reaching phase of the conventional sliding mode control, but also restrains the steady-state error and reduces the chattering via a dynamic PID sliding surface. Meanwhile, a RBF neural network (NN) system is employed to estimate the lumped uncertainty and eliminate the chattering phenomenon. Additionally, adaptive laws and dynamic global PID sliding control gains that ensure system stability in a Lyapunov sense are proposed, together with the techniques for deciding which basis function should be selected. Finally, the effectiveness of RBFNN dynamic global PID sliding mode control method is demonstrated.	artificial neural network;gyroscope;lyapunov fractal;microelectromechanical systems;pid;radial (radio);radial basis function;simulation;steady state	Juntao Fei;Yundi Chu	2016	2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2016.0018	sliding mode control;vibrating structure gyroscope;control theory	Robotics	64.87699174134161	-10.53578801756728	470
9284a6d5f32568d19e28dab4eb7639566ac768e5	utilizing misleading information for cooperative spectrum sensing in cognitive radio networks	wireless channels;vectors sensors cognitive radio silicon fuses measurement transmitters;cognitive radio;wireless channels cognitive radio radio spectrum management telecommunication security;telecommunication security;radio spectrum management;trust based fusion misleading information cooperative spectrum sensing cognitive radio networks radio spectrum scanning spectrum usage report channel uncertainty malicious nodes trust model trustworthiness evaluation log weighted metric threshold based selective inversion fusion si fusion complete inversion fusion ci fusion inversion based fusion schemes blind based fusion	In cognitive radio networks, the radios continuously scan the radio spectrum and create a spectrum usage report. Due to channel uncertainty, there are inaccuracies in these reports. Oftentimes, the radios share and fuse the observed data in order to increase the accuracy of the spectrum usage. However, malicious nodes tend to send false information (i.e., attack) in order to mislead the construction of the spectrum usage report. In this paper, we use a trust model to evaluate the trustworthiness of every node and use the trust values to effectively fuse the information from all nodes. A node compares the information sent by a neighboring node with the predicted information. Based on the ratio of matches (or mismatches), the neighboring node is assigned a trust value. Then, we propose a log-weighted metric utilizing trust values to distinguish malicious nodes from others. Subsequently, we propose threshold based Selective Inversion (SI) fusion and Complete Inversion (CI) fusion to effectively combine not only the information sent by honest nodes but also utilize misleading information sent by malicious nodes. We also propose a combination of the two inversion schemes. We compare the performance of the inversion based fusion schemes with blind and trust-based fusions. Results reveal better performance for inversion based fusion schemes for various intensities of attack. We also conduct simulations to evaluate the optimal thresholds that are used for invoking the inversion based fusion schemes.	cognitive radio;malware;simulation;trust (emotion)	Shameek Bhattacharjee;Saptarshi Debroy;Mainak Chatterjee;Kevin A. Kwiat	2013	2013 IEEE International Conference on Communications (ICC)	10.1109/ICC.2013.6654929	cognitive radio;telecommunications;computer science;computer security;computer network	Mobile	-53.776880315665494	77.49907783256477	471
21b1c42994242f8d619bac6557b232001da37ef6	identification of immunity-related genes in arabidopsis and cassava using genomic data	gene expression regulation plant;kernel canonical correlation analysis;genomics;functional gene prediction;genome plant;gene regulatory networks;genomic data;manihot;arabidopsis;cassava;plant immunity	Recent advances in genomic and post-genomic technologies have provided the opportunity to generate a previously unimaginable amount of information. However, biological knowledge is still needed to improve the understanding of complex mechanisms such as plant immune responses. Better knowledge of this process could improve crop production and management. Here, we used holistic analysis to combine our own microarray and RNA-seq data with public genomic data from Arabidopsis and cassava in order to acquire biological knowledge about the relationships between proteins encoded by immunity-related genes (IRGs) and other genes. This approach was based on a kernel method adapted for the construction of gene networks. The obtained results allowed us to propose a list of new IRGs. A putative function in the immunity pathway was predicted for the new IRGs. The analysis of networks revealed that our predicted IRGs are either well documented or recognized in previous co-expression studies. In addition to robust relationships between IRGs, there is evidence suggesting that other cellular processes may be also strongly related to immunity.	arabidopsis;document completion status - documented;gene regulatory network;holism;kernel method;microarray;plant immune response;rna;sequence number	Luis Guillermo Leal;Álvaro L. Pérez-Quintero;Andrés Quintero;Ángela Bayona;Juan Felipe Ortiz;Anju Gangadharan;David Mackey;Camilo López;Liliana López Kleine	2013		10.1016/j.gpb.2013.09.010	biology;gene regulatory network;genomics;botany;biotechnology;bioinformatics;genetics	Comp.	3.986585485027728	-59.21053379430641	472
43f88c6c4ab200aebb4e483320439ccb0eaa30df	using pentangular factorizations for the reduction to banded form	calcul matriciel;linear algebra;algoritmo paralelo;decomposition valeur singuliere;parallel algorithm;singular value decomposition;parallel blas;algorithme parallele;factorization;factorizacion;algebre lineaire;factorisation;algebra lineal;decomposicion valor singular;matrix calculus;bidiagonal reduction;calculo de matrices	Most methods for computing the singular value decomposition (SVD) first bidiagonalize the matrix. The ScaLAPACK implementation of the blocked reduction of a general dense matrix to bidiagonal form performs about one half of the operations with BLAS3. If we subdivide the task into two stages dense ? banded and banded ? bidiagonal, we can increase the portion of matrix-matrix operations and expect higher performance. We give an overview of different techniques for the first stage.		B. Großer;Bruno Lang	1999		10.1007/3-540-48311-X_152	combinatorics;discrete mathematics;linear algebra;mathematics;factorization;algebra	Logic	-2.076720222987116	37.2966253742231	473
7878ea17e97897fbee53df352293e7d767130142	molecular electrostatic potentials as input for the alignment of hiv-1 integrase inhibitors in 3d qsar	physicochemical properties;quantitative structure activity relationship;partial least square;human immunodeficiency virus;three dimensional;electrostatic potential;in vitro activity	Comparative molecular similarity indices analysis (CoMSIA), a three-dimensional quantitative structure activity relationship (3D QSAR) paradigm, was used to examine the correlations between the calculated physicochemical properties and the in vitro activities (3'-processing and 3'-strand transfer inhibition) of a series of human immunodeficiency virus type 1 (HIV-1) integrase inhibitors. The training set consisted of 34 molecules from five structurally diverse classes: salicylpyrazolinones, dioxepinones, coumarins, quinones, and benzoic hydrazides. The data set was aligned using extrema of molecular electrostatic potentials (MEPs). The predictive ability of the resultant model was evaluated using a test set comprised of 7 molecules belonging to a different structural class of thiazepinediones. A CoMSIA model using an MEP-based alignment showed considerable internal as well external predictive ability (r2(cv) = 0.821, r2(pred) = 0.608 for 3'-processing; and r2(cv) = 0.759, r2(pred.) = 0.660 for 3'-strand transfer).	alignment;benzoic acid;cathepsin l;chemical similarity;class;cytogenetic analysis;hiv infections;hiv-1;immunologic deficiency syndromes;integrase inhibitors;programming paradigm;quantitative structure-activity relationship;quantitative structure–activity relationship;quinones;resultant;serotonin uptake inhibitors;test set	Mahindra T. Makhija;Vithal M. Kulkarni	2001	Journal of computer-aided molecular design	10.1023/A:1014888730876	three-dimensional space;stereochemistry;chemistry;toxicology;computational chemistry;quantitative structure–activity relationship;electric potential;quantum mechanics	Comp.	11.348717265422184	-58.977011093086595	474
9f063f30fb9ecfb591c91597e011062d65adea6d	application of distributed ga-based rbf neural network in fire detection	pattern clustering;adaptive genetic algorithm;training;network adaptability;fuzzy nearest clustering;fuzzy set theory;radial basis function networks;data analysis;artificial neural networks;subnets neural network;adaptation model;fire environment;fire detection;rbf neural network;distributed neural network;distributed ga based rbf neural network;improved genetic algorithm;autoadaptive crossover probability;fire detection model;genetic algorithm;genetic algorithms;neurons;learning artificial intelligence;fires;rbf network;neural network	Distributed neural network based on RBF network is used to establish the model of fire detection in view of complexity of fire process and multiplicity of fire environment. For guaranteeing network adaptability, the number of subnets neural network and degree of sample belong to subnet are determined by fuzzy nearest clustering, RBF network is optimized though a improved genetic algorithm by adaptive genetic algorithm is improved by auto-adaptive crossover probability and the mutation probability that determined the adaptation value. The feasibility of adaptability of the model is proved through training and testing in fire detection model.	artificial neural network;cluster analysis;dynamic linker;fuzzy clustering;genetic algorithm;radial basis function network;rate of convergence;sensor;simulation;software release life cycle;subnetwork	Hairong Wang;Weiguo Yang;Huiling Jiang;Yun Wang	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.851	probabilistic neural network;engineering;artificial intelligence;machine learning;data mining	Robotics	12.610971034850682	-31.357119757760238	475
3ca4840c3ca37bf1acbbe67f10bdfac3ba2e939b	towards a curated collection of code clones	code analysis;code clones;corpus;empirical studies	In order to do research on code clones, it is necessary to have information about code clones. For example, if the research is to improve clone detection, this information would be used to validate the detectors or provide a benchmark to compare different detectors. Or if the research is on techniques for managing clones, then the information would be used as input to such techniques. Typically, researchers have to develop clone information themselves, even if doing so is not the main focus of their research. If such information could be made available, they would be able to use their time more efficiently. If such information was usefully organised and its quality clearly identified, that is, the information is curated, then the quality of the research would be improved as well. In this paper, I describe the beginnings of a curated source of information about a collection of code clones from the Qualitas Corpus. I describe how this information is currently organised, discuss how it might be used, and proposed directions it might take in the future. The collection currently includes 1.3M method-level clone-pairs from 109 different open source Java Systems, applying to approximately 5.6M lines of code.	benchmark (computing);digital curation;duplicate code;information source;java;open-source software;qr code;sensor;source lines of code	Ewan D. Tempero	2013	2013 7th International Workshop on Software Clones (IWSC)		computer science;bioinformatics;database;programming language	SE	-60.66670046384375	39.56457035582503	476
a869b12f5767cb9d546f09bbe71fabaa8c5a5b67	supercomputer development in europe	software directed cache coherence;parallel task execution;version control	For the sake of clarification the paper first discusses some major aspects of supercomputer technology and architecture. On this basis an outline is given of major European developments in the realm of supercomputers that happened in the second half of the eighties or are planned for the first half of the nineties. The projects discussed are: Supernode, ISIS, SUPRENUM, and GENESIS.	.sch;dalton pritchard;design rationale;flops;genesis;general material designation;isis;interconnection;mimd;message passing;needham–schroeder protocol;operating system;parallel computing;parallel processing (dsp implementation);simd;supercomputer;supercomputer architecture;supernode (circuit);transputer;vim	Wolfgang K. Giloi	1989		10.1145/318789.318833	parallel computing;real-time computing;computer science;revision control;operating system;programming language	HPC	-9.962698434834232	41.55538830871643	477
6b020162250ebcfa0f6ea9802a53f838c9b25909	wireless capsule endoscopy video segmentation using an unsupervised learning approach based on probabilistic latent semantic analysis with scale invariant features	unsupervised learning;image features;image segmentation;video signal processing;small intestine;video signal processing endoscopes learning artificial intelligence medical signal processing;training;supervised classification;semantics;video segmentation;classification;data clustering;content analysis;wireless capsule endoscopy;scale invariant feature transform;image color analysis;feature extraction;endoscopes;intestines;digestive tract;learning artificial intelligence;classification accuracy;feature extraction image color analysis semantics endoscopes training intestines image segmentation;algorithms artificial intelligence capsule endoscopy cluster analysis humans image processing computer assisted semantics videotape recording;medical signal processing;probabilistic latent semantic analysis;invariant feature;plsa model wireless capsule endoscopy wce video segmentation unsupervised learning probabilistic latent semantic analysis digestive tract stomach small intestine large intestine scale invariant feature transform local image feature extraction;wireless capsule endoscopy classification probabilistic latent semantic analysis scale invariant feature transform video segmentation	Since wireless capsule endoscopy (WCE) is a novel technology for recording the videos of the digestive tract of a patient, the problem of segmenting the WCE video of the digestive tract into subvideos corresponding to the entrance, stomach, small intestine, and large intestine regions is not well addressed in the literature. A selected few papers addressing this problem follow supervised leaning approaches that presume availability of a large database of correctly labeled training samples. Considering the difficulties in procuring sizable WCE training data sets needed for achieving high classification accuracy, we introduce in this paper an unsupervised learning approach that employs Scale Invariant Feature Transform (SIFT) for extraction of local image features and the probabilistic latent semantic analysis (pLSA) model used in the linguistic content analysis for data clustering. Results of experimentation indicate that this method compares well in classification accuracy with the state-of-the-art supervised classification approaches to WCE video segmentation.	algorithm;block cipher mode of operation;cpu (central processing unit of computer system);capsule endoscopy;central processing unit;cluster analysis;cognitive dimensions of notations;color space;computation;feature vector;gastric tissue;gastrointestinal system;gastrointestinal tract structure;hardware acceleration;laptop;large intestinal wall tissue;linguistics;paper;patients;pentium 4;probabilistic latent semantic analysis;scale-invariant feature transform;simulation;small intestinal wall tissue;supervised learning;time complexity;tracer;tract (literature);unsupervised learning;vocabulary;biologic segmentation;statistical cluster	Yao Shen;Parthasarathy Guturu;Bill P. Buckles	2012	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2011.2171977	unsupervised learning;computer vision;content analysis;feature extraction;biological classification;computer science;machine learning;pattern recognition;scale-invariant feature transform;semantics;image segmentation;cluster analysis;probabilistic latent semantic analysis;feature	Vision	32.39298604569055	-77.79075733421153	478
8c0fe85ef472a505910b6b62163db4f7f75240e1	how to predict very large and complex crystal structures	degree of freedom;genetic drift;high energy;development tool;complex system;ground state;crystal structure prediction;evolutionary algorithms;fingerprint function;genetic algorithm;order parameter;genetic algorithms;global optimization;crystal structure;evolutionary algorithm;energy landscape;eigenvectors	"""a r t i c l e i n f o a b s t r a c t Evolutionary crystal structure prediction proved to be a powerful approach in discovering new materials. Certain limitations are encountered for systems with a large number of degrees of freedom ("""" large systems """") and complex energy landscapes ("""" complex systems """"). We explore the nature of these limitations and address them with a number of newly developed tools. For large systems a major problem is the lack of diversity: any randomly produced population consists predominantly of high-energy disordered structures, offering virtually no routes toward the ordered ground state. We offer two solutions: first, modified variation operators that favor atoms with higher local order (a function we introduce here), and, second, construction of the first generation non-randomly, using pseudo-subcells with, in general, fractional atomic occupancies. This enhances order and diversity and improves energies of the structures. We introduce an additional variation operator, coordinate mutation, which applies preferentially to low-order ("""" badly placed """") atoms. Biasing other variation operators by local order is also found to produce improved results. One promising version of coordinate mutation, explored here, displaces atoms along the eigenvector of the lowest-frequency vibrational mode. For complex energy landscapes, the key problem is the possible existence of several energy funnels – in this situation it is possible to get trapped in one funnel (not necessarily containing the ground state). To address this problem, we develop an algorithm incorporating the ideas of abstract """" distance """" between structures. These new ingredients improve the performance of the evolutionary algorithm USPEX, in terms of efficiency and reliability, for large and complex systems. Published by Elsevier B.V."""	academy;biasing;blue gene;brookgpu;cell signaling;complex systems;computation;computational science;crystal structure prediction;evolutionary algorithm;fingerprint;ground state;msu lossless video codec;normal mode;randomness;selection rule;supercomputer	Andriy O. Lyakhov;Artem R. Oganov;Mario Valle	2010	Computer Physics Communications	10.1016/j.cpc.2010.06.007	mathematical optimization;genetic algorithm;artificial intelligence;evolutionary algorithm;mathematics;quantum mechanics	AI	98.20210426592286	-2.5369053071095755	479
e4aad050fc2a4423a0fa84756d4439c807657526	behavior of micro-polar flow due to linear stretching of porous sheet with injection and suction	numerical solution;homotopy analysis method;micro rotation;micro polar flow;velocity field;shear stress;stretching sheet;boundary layer;suction and injection;homotopy analysis method ham	The boundary layer flow of a micro-polar fluid due to a linearly stretching sheet is investigated. The influence of various flow parameters like 'suction and injection velocity through the porous surface', 'viscosity parameter causing the coupling of the micro-rotation field and the velocity field' and 'vortex viscosity parameter' on 'shear stress at the surface', 'fluid velocity' and 'micro-rotation' are studied. The governing equations of the transformed boundary layer are solved analytically using homotopy analysis method (HAM). The convergence of the obtained series solutions is explicitly studied and a proper discussion is given for the obtained results. Comparison between the HAM and numerical solutions showed excellent agreement.		H. Bararnia;E. Ghasemi;G. Domairry;Soheil Soleimani	2010	Advances in Engineering Software	10.1016/j.advengsoft.2009.12.007	classical mechanics;mathematical optimization;homotopy analysis method;vector field;shear stress;boundary layer;calculus;mathematics;thermodynamics	SE	89.54427285749289	4.357036327594715	480
4771fd0317b5ef34204c8b0f856ba5ea85275644	a collaborative task experiment by multiple robots in a human environment using the kukanchi system	collaboration;kukanchi;intelligent space;rt middleware;robot	We have developed a collaborative robot system to deliver services to people. The Kukanchi system has been used as an intelligent space that comprises sensors, robots, and management servers. Specifically, we expanded and improved Kukanchi's middleware functionality by adding sensors and robots outside the Kukanchi space. Furthermore, we verified the effectiveness of the system through a guidance service task using the task scheduler in the Kukanchi system. The task scheduler manages the service task by disassembling it and selects and sends requests to robots. In this study, we introduce the system and present the experimental results.	robot	Kazuma Fujimoto;Takeshi Sasaki;Midori Sugaya;Takashi Yoshimi;Makoto Mizukawa;Nobuto Matsuhira	2016		10.1007/978-3-319-43518-3_27	robot;embedded system;real-time computing;simulation;computer science;engineering;artificial intelligence;collaboration	Robotics	-38.11010326315868	44.40315969021651	481
99a06a6fe5558e58742cd85b1063d1329e1d1388	the use of a virtual environment for fe analysis of vehicle crash worthiness	mechanical engineering computing;automobiles;interaction analysis;engineering graphics;simulation;virtual reality;virtual environment iron vehicle crash testing computational modeling analytical models computer simulation computer graphics finite element methods data visualization automotive engineering;finite element;computer graphic;virtual crash finite element simulations computer graphics technology vehicle crash worthiness virtual environment car body crash behavior visualization techniques engineering computations vr system vtcrash computer human interface techniques intuitive analysis interactive analysis crash simulation data geometry data physical properties data;data visualisation;data analysis;human interface;visualization technique;accidents;vehicle design;crashes;traffic engineering computing;finite element analysis;traffic engineering computing virtual reality data visualisation accidents automobiles digital simulation finite element analysis engineering graphics mechanical engineering computing data analysis user interfaces;virtual environment;user interfaces;vehicle dynamics;digital simulation;physical properties	1. Description of the application area The advances in computer graphics technology plus the increased complexity of nite element (FE) simulations of the crash behavior of a car body have resulted in the need for new visualization techniques to facilitate the analysis of such engineering computations. Our VR system VtCrash provides novel computer-human interface techniques for intuitive and interactive analysis of large amounts of crash simulation data. VtCrash takes geometry and physical properties data as input and enables the user to enter a virtual crash and to interact with any part of the vehicle. The system is designed in an object oriented fashion. The data is structured into a class hierarchy derived partly from the element structure the FE models are built upon. Geometric data comprises labelled nodes with global coordinates for each time step of the simulation and labelled elements which reference the components they belong to as well as their nodes. VtCrash employs eecient data sorting methods to generate new local polygon lists with bidirectional pointers between nodes and polygons, creating a data structure suitable for the animation of all time steps of a crash test. A hierarchically built scene graph encapsulates the graphics and visual simulation features. The tree is made up of a root-node and environment-control nodes which control the animation. Finally, geometry nodes contain the topological information of the vertices as well as graphic attributes of the polygons like color, transparency and lighting. Geometry nodes can be manipulated interactively at runtime. In order to meet memory requirements and to maintain high frame rates, the polygon mesh of the model needs to be simpliied. Since it is necessary to keep the shape of the model consistent during the animation , the simpliication algorithm is applied to all time steps, identifying and preserving those vertices relevant for the animation of the deformation and eliminating the rest. The polygon decimation criteria is geometric in nature and is based on general ideas of 2] and 1]. Our virtual crash test environment is immersive and creates an actual sense of presence within the crash for the user. This is achieved through head-coupled stereo displays and gestural input techniques. Alternatively, the system can be used non-immersive with a combination of spacemouse and 2D mouse as well as stereo projection technology. The time evolution of the vehicle deformation can be controlled and manipulated in real time. Structural parts of the vehicle can …	algorithm;anomalous experiences;augmented reality;class hierarchy;computation;computer graphics;data structure;decimation (signal processing);deployment environment;human–computer interaction;interactivity;polygon mesh;real-time computing;requirement;run time (program lifecycle phase);scene graph;simulation;sorting;transparency (graphic);user interface;virtual reality	Sven Kuschfeldt;Martin Schulz;Thomas Ertl;Thomas Reuding;Michael Holzner	1997		10.1109/VRAIS.1997.583073	simulation;human–computer interaction;computer science;operating system;finite element method;virtual reality;computer graphics (images)	Graphics	-36.71490996022046	-31.932386452146158	482
c620474ffb957df3a640810a65e7a84ea3be5944	towards time-varying music auto-tagging based on cal500 expansion	time varying;temporal context;instruments;training;semantics;cal500 dataset time varying music auto tagging text based music retrieval semantic labels cal500exp dataset track level tagging time varying tag labels user interface subject annotation effort musical semantics;multiple signal classification;vectors;dataset construction music auto tagging temporal context time varying annotation interface;instruments vectors semantics training multiple signal classification tagging user interfaces;music auto tagging;user interfaces;annotation interface;user interfaces information retrieval music musical instruments semantic web text analysis;tagging;dataset construction	Music auto-tagging refers to automatically assigning semantic labels (tags) such as genre, mood and instrument to music so as to facilitate text-based music retrieval. Although significant progress has been made in recent years, relatively little research has focused on semantic labels that are time-varying within a track. Existing approaches and datasets usually assume that different fragments of a track share the same tag labels, disregarding the tags that are time-varying (e.g., mood) or local in time (e.g., instrument solo). In this paper, we present a new dataset dedicated to time-varying music auto-tagging. The dataset, called CAL500exp, is an enriched version of the well-known CAL500 dataset used for conventional track-level tagging. Given the tag set of CAL500, eleven subjects with strong music background were recruited to annotate the time-varying tag labels. A new user interface for annotation is developed to reduce the subject's annotation effort yet increase the quality of labels. Moreover, we present an empirical evaluation that demonstrates the performance improvement CAL500exp brings about for time-varying music auto-tagging. By providing more accurate and consistent descriptions of music content in a finer granularity, CAL500exp may open new opportunities to understand and to model the temporal context of musical semantics.	structure of observed learning outcome;tag (metadata);text-based (computing);user interface;whole earth 'lectronic link	Shuo-Yang Wang;Ju-Chiang Wang;Yi-Hsuan Yang;Hsin-Min Wang	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890290	computer science;multiple signal classification;semantics;multimedia;user interface;world wide web;information retrieval	DB	-17.976853210383165	-58.816467090619945	483
642269333896b9a0305c88b7a2c4aa892a8c4d76	point-based planning for multi-objective pomdps		Many sequential decision-making problems require an agent to reason about both multiple objectives and uncertainty regarding the environment’s state. Such problems can be naturally modelled as multi-objective partially observable Markov decision processes (MOPOMDPs). We propose optimistic linear support with alpha reuse (OLSAR), which computes a bounded approximation of the optimal solution set for all possible weightings of the objectives. The main idea is to solve a series of scalarized single-objective POMDPs, each corresponding to a different weighting of the objectives. A key insight underlying OLSAR is that the policies and value functions produced when solving scalarized POMDPs in earlier iterations can be reused to more quickly solve scalarized POMDPs in later iterations. We show experimentally that OLSAR outperforms, both in terms of runtime and approximation quality, alternative methods and a variant of OLSAR that does not leverage reuse.	algorithm;approximation;experiment;iteration;markov chain;ordinary least squares;partially observable markov decision process;partially observable system;quadratically constrained quadratic program;runtime library;solver	Diederik M. Roijers;Shimon Whiteson;Frans A. Oliehoek	2015			mathematical optimization;computer science;artificial intelligence;machine learning;mathematics	AI	19.00813899313494	-9.751631217439177	484
d9156aa91ae73121c30f88cfd9f6b3b54627030b	a pilot biomedical engineering course in rapid prototyping for mobile health	software prototyping;three dimensional displays google pain prototypes solid modeling diseases graphics;training;unlimited hackability pilot biomedical engineering course rapid prototyping medical assistive mobile devices fuel innovation hands on engineering training mobile health design process storyboarding nonfunctional prototypes integrated circuit programming 3d modeling 3d printing cloud computing database programming patient engagement animated videos open source;training biomedical engineering cloud computing medical computing mobile computing public domain software software prototyping;medical computing;public domain software;biomedical engineering;mobile computing;cloud computing	Rapid prototyping of medically assistive mobile devices promises to fuel innovation and provides opportunity for hands-on engineering training in biomedical engineering curricula. This paper presents the design and outcomes of a course offered during a 16-week semester in Fall 2011 with 11 students enrolled. The syllabus covered a mobile health design process from end-to-end, including storyboarding, non-functional prototypes, integrated circuit programming, 3D modeling, 3D printing, cloud computing database programming, and developing patient engagement through animated videos describing the benefits of a new device. Most technologies presented in this class are open source and thus provide unlimited “hackability”. They are also cost-effective and easily transferrable to other departments.	3d modeling;3d printing;cloud computing;database;educational curriculum;end-to-end principle;feedback;hands-on computing;instrumentation (attribute);integrated circuit device component;medical devices;mobile health;mobile device;new product development;open-source software;personnameuse - assigned;rapid prototyping;storyboard;benefit;biomedical engineering field;teams	Todd H. Stokes;Janani Venugopalan;Elena N. Hubbard;May D. Wang	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610051	simulation;cloud computing;computer science;engineering;electrical engineering;multimedia;mobile computing;public domain software;computer engineering	SE	-64.84063292315373	-53.47427747027974	485
b9d18b08ef2ab782e745834f5efa8c5428b5c7e5	multiple motion object segmentation based on homogenous region merging	object recognition;image segmentation;motion estimation;iterative region merging process multiple motion object segmentation homogenous region merging robust multiresolution motion estimator irregularly shaped regions region merging criterion;iterative methods;object segmentation;object segmentation merging motion estimation testing robustness image segmentation motion segmentation computer vision motion measurement data mining;motion estimation object recognition image segmentation iterative methods;region merging	In this paper, a novel multiple motion object segmentation scheme based on homogenous region merging is presented. This scheme is based on utilization of a robust multiresolution motion estimator that can provide good motion estimates from irregularly-shaped regions. In addition, a novel region-merging criterion and an iterative region-merging process have been proposed. Experimental results demonstrate the effectiveness of the proposed method.	iterative method	Hong Li;Bee June Tye;Ee Ping Ong;Weisi Lin;Chi Chung Ko	2001		10.1109/ISCAS.2001.922013	computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;segmentation-based object categorization;pattern recognition;motion estimation;iterative method;image segmentation;scale-space segmentation	Vision	44.62068940676775	-50.589801800635	486
0a4d452334eb302e55d010f079eb00dc8e3e8b5b	high quality video acquisition and segmentation using alternate flashing system	motion estimation;video enhancement;object segmentation;video object segmentation;flashing system;natural scenes	A high quality video acquisition algorithm is proposed in this work. We construct a flashing system to capture lit and unlit frames alternately. We develop a reliable motion estimation scheme, which matches correspondences between an unlit frame and a lit frame. Then, we construct a high quality frame, which combines natural scene mood in the unlit frame and textural details in the lit frame. Furthermore, we propose an object segmentation algorithm based on the observation that foreground objects are more sensitive to flash lights than backgrounds. Simulation results demonstrate that the proposed algorithm can acquire high quality video sequences and segment foreground objects from the sequences efficiently.	algorithm;bios;display resolution;interpolation;motion estimation;noise reduction;simulation	Dae-Youn Lee;Jae-Kyun Ahn;Chul Lee;Chang-Su Kim	2010		10.1007/978-3-642-15696-0_42	computer vision;computer science;motion estimation;block-matching algorithm;multimedia;computer graphics (images)	Vision	57.5034450036041	-55.18258029699413	487
c6194d14abc3b92912c5fb7ef4f31d9485c572e5	soil moisture retrieval with backscatter modeling and satellite datasets in zoige wetland, china		Soil moisture of Zoige wetland, China from 2007 to 2009 was estimated using PALSAR radiometrically terrain-corrected (RTC) and Landsat-5 surface reflectance data coupled with water cloud model (WCM) radar backscattering model. Soil moisture derived from L-HH polarization RTC data was suitable for assessing the soil moisture in Zoige wetland. In 2007, the soil moisture in the growing season had similar spatiotemporal patterns and trends, and the intra-annual difference of soil moisture was very small. However, the inter-annual soil moisture increased from 2007 to 2009 substantially. Variations of temperature and precipitation were attributed to the causes.	polarization (waves);spatiotemporal pattern;terrain rendering;web content management system	Yuanyuan Yang;Yong Wang;Xinyi Miao;Hong Li	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518748	growing season;soil science;remote sensing;backscatter;computer science;radar;wetland;reflectivity;precipitation;water content	Embedded	84.44763684351038	-60.521919009970574	488
84252aecdbe68b4c029fd8dbb703422ce761f803	on computing hilbert class fields of prime degree	prime degree;computing hilbert class fields	In the sequel of our recent work on relative extensions of algebraic number fields [DaPo95] we extend the methods presented there for computing Hilbert class fields of degree three over totally real cubic fields. This is the first progress in arithmetic class field computations since Hasse's paper [Ha].		Mario Daberkow;Michael E. Pohst	1996		10.1007/3-540-61581-4_42	hilbert's twelfth problem;discrete mathematics;topology;prime element;principal ideal theorem;mathematics;algebra	Crypto	42.91539223629621	34.59766140711066	489
1687c4ca83b539d0839044bb8efcce009e3267f8	modelling and analysis of the geometrical errors of a parallel manipulator micro-cmm	micro cmm;parallel manipulator;micro measurement;conference paper;error model;proceedings international;monte carlo simulation;covariance matrix	A micro coordinate measurement machine (micro-CMM) with high precision and high accuracy is introduced for the measurement of part dimensions in micro scale. This design is intended to achieve submicron resolution for a work envelop of at least (100x100x100) mm. In this study, a mathematical measuring model to explicitly define the coordinate of the probe in x, y and z directions have been represented. An algorithm to find the workspace was implemented. The error model of the machine was created and the effect of structural errors on probe position was studied analytically. The significance of each geometric parameter was studied in order to minimize the measuring error and achieve the best machine design. Finally, the results of the analytical error model were confirmed through a Monte Carlo analysis.	algorithm;capability maturity model;monte carlo method;parallel manipulator;workspace	Ali Rugbani;Kristiaan Schreve	2012		10.1007/978-3-642-28163-1_14	simulation;computer science;operations management;engineering drawing	Robotics	77.87890177326624	-18.81673555850525	490
4cf9b525a0d49dc526763142113acf15525193b1	using conceptual graphs to represent agent semantic constituents	semantica operacional;representacion conocimientos;multiagent system;belief;software agent;operational semantics;conceptual analysis;base connaissance;attitude change;agent logiciel;analisis conceptual;software agents;semantique operationnelle;croyance;conceptual graph;communicative action;grafo conceptual;base conocimiento;actitud;analyse conceptuelle;creencia;knowledge representation;sistema multiagente;representation connaissances;attitude;graphe conceptuel;systeme multiagent;knowledge base	This paper develops two agent knowledge bases in conceptual graph form, one using the KD45 underlying logical model for belief and one without any underlying logical model for belief. Action-attitudes in the knowledge bases provide contexts that represent the agents’ mental attitude towards, and willingness to act upon information in the knowledge bases. Preconditions for communication acts are also represented in the knowledge bases as well as mental attitude changes following communications. Conceptual graphs are a flexible and extendable form of knowledge representation that is used to capture and represent semantic constituents of communications in a form that may be used by software agents. The knowledge base representations in this paper provide software agents a perspective from which they may reason about the communicating agent’s beliefs and communication actions.	conceptual graph;extensibility;intelligent agent;knowledge base;knowledge representation and reasoning;modal logic;possible world;precondition;software agent;software developer;vocabulary	Lois W. Harper;Harry S. Delugach	2004		10.1007/978-3-540-27769-9_22	knowledge representation and reasoning;knowledge base;computer science;knowledge management;artificial intelligence;software agent	AI	-24.374732579048587	-8.1521244151442	491
ddd2a012614a0ec9505312dc78ab8be5f6d90efe	planar spline trajectory following for an autonomous helicopter	stability helicopters aircraft control mobile robots splines mathematics;uav;control systems;aircraft control;spline;spline helicopters robots global positioning system aircraft navigation application software fuels computer architecture reconnaissance control systems;application software;path planning;robotic helicopter;behavior based control;stabilization;mobile robots;environmental disturbances;autonomous aerial robot;1 8 m planar spline trajectory following autonomous helicopter behavior based control system stabilization autonomous aerial robot gps positioning error environmental disturbances wind;splines mathematics;stability;autonomous helicopter;computer architecture;planar spline trajectory following;fuels;global positioning system;robots;gps positioning error;1 8 m;behavior based control system;trajectory tracking;reconnaissance;wind;trajectory following;helicopters;aircraft navigation	This paper proposes a technique for planar trajectory following for an autonomous aerial robot. A trajectory is modeled as a planar spline. A behavior-based control system. which stabilizes the robot and enforces trajectory following, has been implemented and tested on an autonomous helicopter. Results from two flight, experiments are presented. The trajectory tracking error is on the order of the size of the robot (1.8 m). Given the inherent error in GPS positioning, and environmental disturbances (wind), this is quite reasonable.	autonomous robot;spline (mathematics)	Kale Harbick;James F. Montgomery;Gaurav S. Sukhatme	2001		10.1109/CIRA.2001.1013235	robot;spline;mobile robot;application software;simulation;stability;global positioning system;computer science;control system;artificial intelligence;motion planning;wind	Robotics	62.69694036917015	-18.435709361572055	492
a143062664206d5ae2289b279436b23ae124210f	chew lips 'slick'	inevitable metaphysical cycle;wonderful odyssey;fantastical landscape	A little girl wakes up to find herself in a strange and fantastical landscape. As she grows up, she experiences a wonderful odyssey realizing her dreams and the inevitable metaphysical cycle of life and death.	chew's second algorithm;dreams;experience;life & death	Gregory de Maria;Resident Creative Studio	2010		10.1145/1900264.1900286	computer graphics (images);aesthetics;girl;computer science	HCI	-58.164988389553606	-25.370955840682463	493
67229e1658124c8f6f41b306481ce7bfd64de11a	task scheduling and file replication for data-intensive jobs with batch-shared i/o	bi level hypergraph partitioning;processor scheduling data analysis subcontracting computer science biomedical engineering data engineering biomedical informatics biomedical computing linear programming degradation;bipartition scheme task scheduling file replication data intensive job batch shared i o behavior integer programming bi level hypergraph partitioning heuristic approach;processor scheduling;storage management;data replication;data intensive job;batch shared i o behavior;bipartition scheme;integer programming;storage management integer programming processor scheduling;file replication;heuristic approach;task scheduling;integer program	This paper addresses the problem of efficient execution of a batch of data-intensive tasks with batch-shared I/O behavior, on coupled storage and compute clusters. Two scheduling schemes are proposed: 1) a 0-1 integer programming (IP) based approach, which couples task scheduling and data replication, and 2) a bi-level hypergraph partitioning based heuristic approach (BiPartition), which decouples task scheduling and data replication. The experimental results show that: 1) the IP scheme achieves the best batch execution time, but has significant scheduling overhead, thereby restricting its application to small scale workloads, and 2) the BiPartition scheme is a better fit for larger workloads and systems - it has very low scheduling overhead and no more than 5-10% degradation in solution quality, when compared with the IP based approach	black and burst;computer cluster;data-intensive computing;disk storage;elegant degradation;file sharing;graph partition;greedy algorithm;heuristic (computer science);input/output;integer programming;linear programming;overhead (computing);referring expression generation;replication (computing);run time (program lifecycle phase);schedule (project management);scheduling (computing)	Gaurav Khanna;Nagavijayalakshmi Vydyanathan;Ümit V. Çatalyürek;Tahsin M. Kurç;Sriram Krishnamoorthy;P. Sadayappan;Joel H. Saltz	2006	2006 15th IEEE International Conference on High Performance Distributed Computing	10.1109/HPDC.2006.1652155	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;integer programming;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;scheduling;round-robin scheduling;i/o scheduling;replication	HPC	-16.962869817890002	60.43059425366149	494
ebc532daa422c19500a522adcbdb6e5e64c4c96e	a novel mobile device user interface with integrated social networking services	social networking services;hypertext navigation;automatic filtering;mobile web;user experience	Modern mobile devices support accessing Web-based social networking services from the user interface (UI) of Web browsers, applications, and mobile widgets. While effectively accessing these services, people may find it tedious to switch between multiple user interfaces in order to be aware of the latest content. Aiming for an improved user experience, we experimented with integration of these services into mobile devices' main user interface. The integrated content is presented beyond application silos and automatically filtered to highlight the relevant elements. A mobile system called LinkedUI was developed and deployed in one lab test and one field study. Three findings emerge from these studies. Firstly, it is feasible to construct an alternative device UI that supports integration of Web content across applications and services via hyperlinking. Time, publisher (e.g., contacts), content types, and geographical locations are key dimensions for association of content. Secondly, the alternative device UI enables better usability of accessing social networking services than accessing them from individual Web sites on mobile devices. It helps people to be aware of the latest content during microbreaks. Thirdly, automatic filtering, on the basis of one user's data, is one promising approach to identifying relevant content. Given filtered content, most people using the automatic filtering approved the functionality and experienced a better sense of control that is arguably due to the reduced information volume.	mobile device;user interface	Yanqing Cui;Mikko Honkala	2013	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2013.03.004	user interface design;web service;user experience design;mobile search;mobile web;human–computer interaction;computer science;operating system;multimedia;world wide web	HCI	-51.5707576227435	-41.19662104713586	495
9c98aa7d9440dba99f345fe5bf25f6f42a61d494	on computing of arbitrary positive integer powers for tridiagonal matrices with elements -1, 0, 0, ..., 0, 1 in principal and 1, 1, 1, ..., 1 in neighbouring diagonals - i		In this paper we derive the general expression of the l th power ( l ∈ N ) for one type of tridiagonal matrices. Keywords Tridiagonal matrices Eigenvalues Eigenvectors Jordan’s form Chebyshev polynomials	sparse matrix	Jonas Rimas	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.10.022		ML	44.16490679733889	35.292363221130046	496
6db68bc0ba1f0ff582770cc6711aba10eb233556	h∞ control applied to the vibration minimization of the parallel robot par2	cycle time;frequency control;vibrations frequency control acceleration manipulators accelerometers trajectory;piezoelectric devices;vibration control control system synthesis end effectors h control industrial robots materials handling piezoelectric devices reduced order systems;robot arm;control system synthesis;materials handling;industrial robots;vibration control;subspace method;h control;parallel robot;high speed;reduced order systems;control strategy;end effectors;identification procedure reduced order h control law residual vibration minimization par2 parallel robot pick and place tasks industrial operation cycle time reduction end effector piezoelectric patches accelerometers	Conceived for high-speed and high-accuracy pick-and-place tasks, the parallel robot Par2 aims at reducing industrial operation cycle times. As a consequence of its high acceleration levels, the end-effector precision at the stop positions is subjected to undesirable vibrations of the manipulator's arms, leading consequently to an increase in the cycle time. Piezoelectric patches are wrapped around the robot arms in order to actively minimize these vibrations, which are by their turn measured by three accelerometers orthogonally oriented and placed on the end-effector. After submitting the robot to an identification procedure, the obtained model is used to synthesize a reduced order H∞ control law that succeeds in minimizing the residual vibrations.	coat of arms;optimal control;parallel manipulator;piezoelectricity;quantum superposition;rejection sampling;robot end effector;smt placement equipment;simulation;utility functions on indivisible goods	Luiz R. Douat;Isabelle Queinnec;Germain Garcia;Micaël Michelin;François Pierrot	2011	2011 IEEE International Conference on Control Applications (CCA)	10.1109/CCA.2011.6044455	control engineering;simulation;engineering;control theory	Robotics	73.97219554730832	-19.631200870915436	497
214052cdebf4661c68bc16631f41e378637618b9	on miquel's five-circle theorem	miquel s theorem;computational techniques;mathematics mechanization;breefs;conformal geometric algebra;geometric algebra;null bracket algebra	Miquel’s Five-Circle Theorem is difficult to prove algebraically. In this paper, the details of the first algebraic proof of this theorem is provided. The proof is based on conformal geometric algebra and its accompanying invariant algebra called null bracket algebra, and is the outcome of the powerful computational techniques of null bracket algebra embodying the novel idea breefs.	conformal geometric algebra;linear algebra	Hongbo Li;Ronghua Xu;Ning Zhang	2004		10.1007/11499251_18	composition algebra;geometric algebra;differential graded algebra;symmetric algebra;filtered algebra;division algebra;subalgebra;universal geometric algebra;universal enveloping algebra;virasoro algebra;pure mathematics;jordan algebra;difference algebra;quaternion algebra;mathematics;algebra representation;conformal geometric algebra;cellular algebra;lie conformal algebra;allen's interval algebra;current algebra;two-element boolean algebra;algebra	DB	50.16380664885913	34.594061664815925	498
66daebf3d8c8a3ccb90a60b782f91f1c9ea6797d	authentication and encryption in the snow disease surveillance network		The paper presents how authentication and encryption is implemented in the Snow disease surveillance network. Requirements for the authentication mechanism were collected from General Practitioners (GPs). The identity of each Snow user is preserved across health institutions allowing GPs to move freely between health institutions and use the system independent of location. This ability is combined with close to zero user account administration within the participating institutions. The system provides global user certificate revocation and end-to-end encryption.	arabic numeral 0;authentication;biologic preservation;certificate (record artifact);end-to-end encryption;user (computing)	Johan Gustav Bellika;Lars Ilebrekke;Per Atle Bakkevoll;Håvard D. Johansen;Jeremiah Scholl;Monika Alise Johansen	2009	Studies in health technology and informatics	10.3233/978-1-60750-044-5-725	disease surveillance;snow;revocation list;encryption;global positioning system;computer security;internet privacy;medicine;authentication	Security	-45.21100535083404	64.75139405755428	499
8c4a0f0094af80e7f7b3c689862d91722be73cad	an infinitary variant of metric temporal logic over dense time domains	metric temporal logic;infinitary logic;sequent calculus;ciencias basicas y experimentales;matematicas;time domain;grupo a	Abstract#R##N##R##N#We introduce a complete and cut-free proof system for a sufficiently expressive fragment of Metric Temporal Logic over dense time domains in which a schema of induction is provable. So doing we extend results previously obtained by Montagna et al. to unbounded temporal operators. (© 2004 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim)	temporal logic	Stefano Baratella;Andrea Masini	2004	Math. Log. Q.	10.1002/malq.200310096	mathematical analysis;discrete mathematics;linear temporal logic;topology;interval temporal logic;time domain;mathematics;programming language;sequent calculus;algorithm;algebra	Theory	-11.376137160704067	15.852684891438921	500
5130ce7f93fdf38121762843737bbdaa95c3593b	automatic verification of parameterized data structures	size specification;automatic verification;developpement logiciel;graph theory;iterative method;teoria grafo;theorie automate;logica temporal;automatic proving;structure programme;software verification;temporal logic;systems engineering;demostracion automatica;exactitude programme;satisfiability;theorie graphe;metodo iterativo;demonstration automatique;exactitud programa;estructura programa;methode iterative;desarrollo logicial;directed graph;estructura datos;graphe oriente;software development;ingenierie systeme;automata theory;grafo orientado;teoria automata;structure donnee;modele donnee;galibo;program structure;data structure;gabarit;logique temporelle;large classes;data models;program correctness	Verifying correctness of programs operating on data structures has become an integral part of software verification. A method is a program that acts on an input data structure (modeled as a graph) and produces an output data structure. The parameterized correctness problem for such methods can be defined as follows: Given a method and a property of the input graphs, we wish to verify that for all input graphs, parameterized by their size, the output graphs also satisfy the property. We present an automated approach to verify that a given method preserves a given property for a large class of methods. Examples include reversals of linked lists, insertion, deletion and iterative modification of nodes in directed graphs. Our approach draws on machinery from automata theory and temporal logic. For a useful class of data structures and properties, our solution is polynomial in the size of the method and size of the property specification.	algorithm;approximation;automata theory;brs/search;compiler;computation;correctness (computer science);data structure;decision problem;directed graph;encode;exptime;floor and ceiling functions;for loop;graph (discrete mathematics);hoare logic;iteration;lr parser;linked data structure;linked list;loop invariant;pointer (computer programming);polynomial;recursively enumerable set;refinement (computing);separation logic;shape analysis (digital geometry);shape analysis (program analysis);software verification;super robot wars;temporal logic;terminate (software);three-valued logic;time complexity;transducer;tree automaton;turing machine;undecidable problem;verification and validation;well-formed formula	Jyotirmoy V. Deshmukh;E. Allen Emerson;Prateek Gupta	2006		10.1007/11691372_2	data modeling;discrete mathematics;directed graph;data structure;temporal logic;software verification;computer science;graph theory;theoretical computer science;software development;automata theory;mathematics;iterative method;programming language;algorithm;satisfiability	Logic	-16.776065782659074	26.698676516175208	501
b4c1a46b928c4eb593c40e66156da5528f75df75	instantaneous frequency rate estimation based on the robust cubic phase function	gaussian noise;degradation;impulse noise environment;frequency estimation phase estimation noise robustness gaussian noise working environment noise polynomials maximum likelihood estimation parameter estimation additive noise degradation;robust cubic phase function;working environment noise;impulse noise;additive noise;frequency estimation;instantaneous frequency;maximum likelihood estimation;noise robustness;polynomials;higher order;polynomial phase signals;phase estimation;signal processing;instantaneous frequency rate estimation;signal processing frequency estimation gaussian noise polynomials;higher order phase function robust cubic phase function instantaneous frequency rate estimation polynomial phase signals gaussian noise impulse noise environment;parameter estimation;higher order phase function	The cubic phase function (CPF) is recently proposed to estimate the instantaneous frequency rate (IFR) for the polynomial phase signals (PPS) in a Gaussian noise environment. However, for an impulse noise environment, the performance of the standard CPF degrades significantly. In addition, the resulting noise in the CPF is a mixture of the Gaussian and impulse noise even for a Gaussian input noise. Hence, a modified robust CPF algorithm based on the alpha-trimmed form of L-estimation is proposed in this paper. Extension to the robust higher-order phase function (HPF) is also derived. Simulation results demonstrate that the robust CPF outperforms the standard CPF in impulse noise and is also valid to estimate the IFR in Gaussian noise	algorithm;coalition for patent fairness;cubic function;high performance fortran;impulse noise (audio);instantaneous phase;instrument flight rules;polynomial;simulation	Pu Wang;Igor Djurovic;Jianyu Yang	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660597	gaussian noise;instantaneous phase;econometrics;higher-order logic;degradation;impulse noise;computer science;signal processing;mathematics;maximum likelihood;estimation theory;statistics;polynomial	EDA	81.00271735289725	-36.24006374814939	502
d571ebcd3bccbb672768138855151ae79bc41038	on graphs with the smallest eigenvalue at least -1 - √2, part i		This is a continuation of the article with the same title. In this paper, the family H is the same as in the previous paper [11]. The main result is that a minimal graph which is not an H -line graph, is just isomorphic to one of the 38 graphs found by computer.		Tetsuji Taniguchi	2008	Ars Math. Contemp.	10.26493/1855-3974.35.fe9		Theory	34.59134225634407	35.50972465619445	503
ebe118946712b419fa3ab0a6aa119fca85711ff6	a knowledge-based approach for checking software information using a non-monotonic reasoning system	non monotonic reasoning;knowledge base	In this paper, the knowledge representation of software component relations using non-monotonic logic to assist the validity and integrity checking of software information is presented. Software components and their interconnection information are represented by axioms. These axioms exhibit the structure and behaviour of the software system. Another set of axioms represents the basic truisms about a software system in general. These axioms can easily be extended to cover a wide variety of software systems architectures. In the software development and maintenance phases, information about the software system can be derived from these axioms using an automated reasoning system, and the software system itself can easily be checked against a specification of the system and compared for validity. The developed knowledgebased system will be able to provide programmers useful software information and assist the software development and maintenance process.	automated reasoning;component-based software engineering;interconnection;knowledge representation and reasoning;knowledge-based systems;non-monotonic logic;programmer;reasoning system;software development;software system	Jeffrey J. P. Tsai;Thomas Weigert	1990	Knowl.-Based Syst.	10.1016/0950-7051(91)90026-X	knowledge base;software requirements specification;verification and validation;software sizing;computer science;artificial intelligence;package development process;backporting;software design;theoretical computer science;software framework;component-based software engineering;software development;software design description;software construction;software walkthrough;software analytics;resource-oriented architecture;algorithm;software metric;software system	SE	-45.65489421439816	27.95291215436492	504
b1283a15c3c63131347fe4ba4b74ae0bee4717f8	effective implementation of esterel programs	synchronous programming;synchronous languages;implementation;real time;data processing;synchronous language;reactive system;reactive systems;automata real time systems data processing program processors documentation;synchronous languages real time reactive systems implementation	Synchronous languages are dedicated to reactive system programming. A synchronous language like ESTEREL has to rely on some general purpose language for data processing facilities. This need for external code is explained in a first part of the paper. In order to execute “real-time” synchronous programs, special implementations techniques have to be used. Th.e second p art of the paper deals with temporally constrained executions of ESTEREL programs.	esterel	Charles André;Marie-Agnès Peraldi-Frati	1993	Fifth Euromicro Workshop on Real-Time Systems	10.1109/EMWRT.1993.639108	parallel computing;real-time computing;lustre;data processing;reactive system;computer science;programming language	Embedded	-23.980531450606126	34.18264391012959	505
1dcb39f9e7e88710a531546b1288ab805368c339	computability on random variables	sistema fila espera;systeme attente;theorie type;variable aleatoire;computability;variable aleatoria;calculabilite;type theory;queueing system;random variable;calculabilidad	Abstract   In this paper, we study aspects of computability concerning random variables under the background of Type 2 Theory of Effectivity (TTE). We show that the resulting definitions are “natural” as they suffice to successfully discuss questions of computability of basic queueing systems, e.g. of the so-called M/G/1-system.	computability	Norbert Th. Müller	1999	Theor. Comput. Sci.	10.1016/S0304-3975(98)00292-8	random variable;discrete mathematics;computer science;mathematics;computability;programming language;type theory;algorithm;statistics	ECom	41.494663321822955	12.975768293201055	506
f31ba3173443e7a6601d69ca2f33651b9ebee3b0	video monitoring of slope failure using spatiotemporal gabor filtering	moving object;low contrast;kernel;error factors video monitoring slope failure spatiotemporal gabor filtering tiny moving object hazardous slope;video signal processing;hazardous slope;error factors;gabor filters;video monitoring;video signal processing gabor filters;gabor filter;trajectory;monitoring;slope failure;trajectory spatiotemporal gabor filtering video monitoring tiny moving object low contrast low frame rate slope failure precursors;feature extraction;pixel;spatiotemporal phenomena;spatiotemporal gabor filtering;condition monitoring spatiotemporal phenomena gabor filters filtering 1f noise humans modeling video compression feature extraction soil;simulation analysis;tiny moving object;low frame rate;noise;videos;precursors	We propose a method for detecting precursors, such as small rock and/or soil fall, which occur prior to massive slope failure. The key feature of our method is directly recognizing the trajectory of a small collapse using spatiotemporal Gabor filtering. Simulation analysis, where the conditions of the simulation are quantitatively defined, reveals the effectiveness of the proposed method in detecting a tiny moving object with low contrast in the background under low frame-rate video monitoring. Experiments using actual monitoring videos of a hazardous slope confirmed the effectiveness of our method. The effects of error factors in an outdoor environment, which may inhibit recognition, are also evaluated.	experiment;gabor filter;image processing;sensor;simulation	Ken Okamoto;Toshio Watanabe;Akitoshi Hanazawa;Takashi Morie;Hiroshi Ban;Yuji Maeda	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346081	computer vision;kernel;simulation;feature extraction;computer science;noise;trajectory;pixel;computer graphics (images)	Robotics	41.9457815986764	-44.86483526211225	507
8a7adf993ac3a162eb621dab06a98293b4f947a7	applications of operator identities to the multiple q-binomial theorem and q-gauss summation theorem	u n 1 q binornial theorem;serie hypergeometrique;summation;identite operateur;sumacion;serie hipergeometrica;hypergeometric series;u n 1 q binomial theorem;u n 1 q gauss theorem;multiple basic hypergeometric series;transformation formula;operator identity;formule transformation;theoreme gauss;serie multiple;basic hypergeometric series;sommation;summation formula	In this paper, we first give an interesting operator identity. Furthermore, using the q-exponential operator technique to the multiple q-binomial theorem and q-Gauss summation theorem, we obtain some transformation formulae and summation theorems of multiple basic hypergeometric series.		Zhizheng Zhang;Maixue Liu	2006	Discrete Mathematics	10.1016/j.disc.2006.01.025	closed graph theorem;combinatorics;mathematical analysis;brouwer fixed-point theorem;hypergeometric function;borel summation;summation of grandi's series;contraction;factor theorem;poisson summation formula;atiyah–singer index theorem;summation;shift theorem;danskin's theorem;no-go theorem;von neumann's theorem;hellinger–toeplitz theorem;summation by parts;calculus;fundamental theorem;divergent series;mathematics;riesz–thorin theorem;basic hypergeometric series;picard–lindelöf theorem;fixed-point theorem;unbounded operator;algebra	Theory	74.90355023759848	13.09125043458846	508
3da3dc0e9b9b2abf3606dca9656b9adec61dc03d	design and analysis of on-chip cpu pipelined caches	on-chip cpu pipelined caches;chip	The access time of the first level on-chip cache usually imposes the cycle time of high-performance VLSI processors. The only way to reduce the effect of cache access time on processor cycle time is the use of pipelined caches. A timing model for on-chip caches has recently been presented in [1]. In this paper the timing model given in [1] is extended so as pipelined caches can be handled. Also the possible pipelined architectures of a cache memory are investigated. The speedup of the pipelined cache against the non-pipelined one is examined as a function of the pipeline depth, the organization and the physical implementation parameters.	central processing unit	C. Ninos;Haridimos T. Vergos;Dimitris Nikolos	1999			chip;tag ram;telecommunications;computer science;cpu shielding	Logic	-7.22083987115004	52.47975091534921	509
0092451e51b0b89e5f7c18673fd3aacd3add3d91	retargetable generation of code selectors from hdl processor models	libraries;hardware design languages;processor architecture;code quality;embedded coding;computer languages;high level languages;hierarchical scheduling;multimedia;004;application software;externally specified processor models;processor scheduling;very large scale integration;fixed function processing elements;hierarchical allocation;turnaround times;hardware description languages;code generation;task graph;automatic generation;embedded code generation;retargetable generation;retargetable compiler;process design;high level synthesis;precedence constraints;system level software synthesis;heterogeneous multiprocessors;hardware design languages application specific processors registers computer science very large scale integration process design libraries computer languages application software high level languages;registers;application specific integrated circuits;retargetable compiler retargetable generation code selectors hdl processor models code quality embedded code generation code generators externally specified processor models turnaround times processor architectures program execution speed asips;arbitrary interconnect topology;vlsi;application specific processors;code selectors;hdl processor models;multirate systems;programmable processing elements;computer science;code generators;hard real time constraints;program compilers;processor architectures;high level synthesis program compilers hardware description languages real time systems vlsi application specific integrated circuits instruction sets;asips;program execution speed;instruction sets;real time systems	Besides high code quality, a primary issue in embedded code generation is retargetability of code generators. This paper presents techniques for automatic generation of code selectors from externally specified processor models. In contrast to previous work, our retargetable compiler RECORD does not require tool-specific modelling formalisms, but starts from general HDL processor models. From an HDL model, all processor aspects needed for code generation are automatically derived. As demonstrated by experimental results, short turnaround times for retargeting are achieved, which permits to study the HW/SW trade-off between processor architectures and program execution speed.	central processing unit;code generation (compiler);compiler;embedded system;hardware description language;retargeting;shattered world;software quality;transform, clipping, and lighting	Rainer Leupers;Peter Marwedel	1997		10.1109/EDTC.1997.582348	dead code;computer architecture;parallel computing;real-time computing;computer science;code generation;unreachable code	EDA	2.6050297296939453	51.6535435849111	510
0d8beb4068a9ecb2b8970b9883f15a891282bb15	telemachus an effective electronic marker of students' programming assignments (poster session)	pedagogy;io;introductory computer science;java;toolkits	multimedia presentations using the authoring tool of Macromedia's Director TM, and begin to develop a CD-ROM portfolio. For many students in this course Director's Lingo scripting language is their first experience with programming. A clock project provided a sliding scale of implementation from keyframe and tweening, to actual coding in Lingo, to web deployment. Sample on WWW:	cd-rom;inbetweening;key frame;lingo (programming language);scripting language;software deployment;www	M. Satrazemi;V. Dajdiielis	2000		10.1145/343048.343231	computational science;human–computer interaction;computer science;programming language;java;pedagogy	DB	-83.12987647397775	-38.07845188635036	511
a18d67419223979fad8c59f5df17661e1d84cbc8	a new architecture of mobile payment system through social media network	buxter;data processor;facebook;data collector;request processor;crawling	The financial services industry is changing rapidly as a result of advances in information technology, telecommunications and the Internet. Technological innovations and increasing customer demand have led to the emergence of new services and new organizational forms for financial services firms. The gait of innovation in the Social media, the payment system is staggering. The biggest social media sites are Facebook, massive and used by almost everyone, Twitter, the micro-blogging, and LinkedIn, the online professional networking site. By using the social media connections as a method to exchange the funds, we can integrate our use of mobile wallets, PayPal, etc. to our social media sites. Believing on daily deal platforms, none of the current daily deal platforms can compete with the granularity and relevance with which banks could target a deal to a specific subscriber, having access to their extensive spending history and preferences. A feedback loop extending from the Bank to the merchants enabling the creation of accurate list of customers who consistently spend in specific retail categories can help merchants in delivering targeted offers to attract new customers.	mobile payment	Basudeo Singh;K. S. Jasmine	2014	JECO	10.4018/jeco.2014070104	economics;marketing;data processing system;crawling;advertising;management;world wide web;commerce	AI	-85.27814910154109	-12.580521682526111	512
693e68112198ddd275c47f5158d517a4f4c3c30c	evaluation of quality measure factors for the middleware based context-aware applications	context aware application;pervasive;reliability;context aware;context sensors context aware services accuracy reliability cognition equations;context information;sensors;pervasive qoc middleware context aware;pervasive computing;information technology;quality management middleware mobile computing;mobile computer;information space;accuracy;qoc;context aware service;informational efficiency;cognition;context aware services quality measure factors middleware based context aware applications information technology distributed mobile computing pervasive computing information space physical space context aware pervasive applications quality of context qoc management qos enriched context information;middleware;quality measures;quality of context;mobile computing;context;quality management;context aware services	With the rapid development of the information technology, it is inevitable that the distributed mobile computing will evolve to the pervasive computing gradually whose final goal is fusing the information space composed of computers with the physical space in which the people are working and living in. To achieve this goal, one of the problems is how to continuously monitor/capture and interpret the environment related information efficiently to assure high context awareness. One of the attentions has been paid to the research of the context-aware pervasive applications is the quality measure for the context. However, existing research just take the Quality of Context (QoC) into account in several aspects. Therefore, we have analyzed all the possible factors that affecting the quality of the context. Furthermore, based on our context-aware framework supporting QoC management, we can use these factors to refinery raw context, discard duplicate and inconsistent context so as to protect and provide QoS-enriched context information of users to context-aware applications and services.	computer;context awareness;experiment;middleware;mobile computing;pervasive informatics;quality of service;ubiquitous computing	Di Zheng;Kerong Ben	2012	2012 IEEE/ACIS 11th International Conference on Computer and Information Science	10.1109/ICIS.2012.46	computer science;knowledge management;database;world wide web	HCI	-42.65172499726049	43.647329943410135	513
1a5164f215adcbc74e448f21ec29372ec52ae019	a study of user continuance behavioral intentions toward privacy-protection practices				Ye Han;T. Selwyn Ellis	2018	IRMJ	10.4018/IRMJ.2018040102	knowledge management;engineering;continuance	HCI	-86.90587717302263	-10.626184052770665	514
ff61f61a7d90fdc120ab0031b1c631feab27a6ce	adjustment-based modeling for timing analysis under variability	adjustment based modeling;timing polynomials circuit simulation gaussian processes integrated circuit interconnections statistics sampling methods predictive models hypercubes sensitivity analysis;gaussian processes;polynomials;circuit simulation;statistical analysis;sensitivity analysis;gaussian process gp;integrated circuit interconnections;parameter variability;statistics;postsilicon timing prediction;statistical static timing analysis ssta adjustment based model gaussian process gp latin hypercube design lhd postsilicon timing prediction;hypercubes;timing analysis;predictive models;latin hypercube design;gaussian process;gaussian process adjustment based modeling timing analysis circuit timing model statistical static timing analysis parameter variability;latin hypercube design lhd;sampling methods;circuit analysis computing;high dimension;timing circuit analysis computing gaussian processes statistical analysis;statistical static timing analysis ssta;adjustment based model;statistical static timing analysis;circuit timing model;model simulation;timing	This paper presents an adjustment-based modeling framework for timing analysis under variability. Instead of building a complex model (such as polynomial one) directly between the circuit timing and parameter variability, we propose to build a model that adjusts an approximate variation-aware timing into an accurate one. The idea is that it is easier to build a model that adjusts an approximate estimate into an accurate one. In addition, it is more efficient to obtain an approximate circuit timing model. The combination of these two observations makes the use of an adjustment-based model a better choice for statistical static timing analysis with high dimension of parameter variability (e.g., at sign-off stage). It can also be used at the postsilicon stage to predict the circuit timing from a smaller subcircuit. To build the adjustment model, we use a simulation-driven approach based on Gaussian Process. Combined with the intelligent sampling, we show that an adjustment-based model can more effectively capture the nonlinearity of the circuit timing with respect to parameter variability compared to polynomial models. Simulation results show that with 42 independent device and interconnect parameter variations, our proposed adjustment-based model obtained using 200 circuit timing samples can achieve much higher accuracy than quadratic model obtained using 2000 samples.	approximation algorithm;benchmark (computing);gaussian process;heart rate variability;nonlinear system;polynomial;quadratic equation;sampling (signal processing);simulation;spatial variability;statistical static timing analysis	Lin Xie;Azadeh Davoodi;Jun Zhang;Tai-Hsuan Wu	2009	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2009.2018874	econometrics;real-time computing;computer science;gaussian process;mathematics;static timing analysis;statistics	EDA	24.383199829883278	58.81911992730737	515
0bd04640e93f7f03489abdbd29d1ddfaa9654fa2	floorplan design of vlsi circuits	search method;simulated annealing	In this paper we present two algorithms for the floorplan design problem. The algorithms are quite similar in spirit. They both use Polish expressions to represent floorplans and employ the search method of simulated annealing. The first algorithm is for the case where all modules are rectangular, and the second one is for the case where the modules are either rectangular or L-shaped. Our algorithms consider simultaneously the interconnection information as well as the area and shape information for the modules. Experimental results indicate that our algorithms perform well for many test problems.	algorithm;interconnection;simulated annealing;turing test;very-large-scale integration	Martin D. F. Wong;C. L. Liu	1989	Algorithmica	10.1007/BF01553890	mathematical optimization;simulated annealing;computer science;theoretical computer science;mathematics	EDA	14.85184292878579	50.54549947195488	516
0fb8136a3f910b98fc55ef1005fdbc37a4b2ff68	a comparison of optimized higher order spectral detection techniques for non-gaussian signals	second order;ji cuadrado;gaussian noise rejection property;gaussian noise;estimation theory gaussian noise higher order statistics optimisation spectral analysis interference suppression signal detection smoothing methods;detectors;hinich s test;conditional distributions;estimation theory;optimisation;test statistique;deteccion optimal;finite sample case optimized higher order spectral detection technique nongaussian signals gaussian noise rejection property higher order spectra bifrequency domain detectors bispectral estimates f test statistic hinich s test conditional distributions colored scenarios bispectral detectors optimal smoothing bandwidth detector performance;detection signal;gaussian processes;bispectral detectors;colored scenarios;finite sample case;test estadistico;signal detection;ruido gaussiano;statistical test;frequency estimation;testing;bifrequency domain detectors;detector performance;khi deux;optimal smoothing bandwidth;higher order;higher order statistics;interference suppression;bispectral estimates;chi square;statistical distributions;smoothing methods;deteccion senal;higher order spectra;signal processing;bruit gaussien;analyse spectrale;f test statistic;bandwidth;analisis espectral;detection optimale;spectral estimation;optimal detection;nongaussian signals;spectral analysis;testing detectors frequency estimation statistical distributions gaussian noise smoothing methods bandwidth gaussian processes higher order statistics signal processing;optimized higher order spectral detection technique;conditional distribution	Using the Gaussian noise rejection property of higher order spectra (HOS), HOS-based detectors have been proposed that outperform conventional second-order techniques in certain scenarios. Based on statistical tests proposed by Subba Rao and Gabr, as well as Hinich, recently, Kletter and Messer, and Hinich and Wilson, have developed similar bifrequencydomain detectors that are dependent on bispectral estimates of the observation process. Formalizing the estimate consistency requirements and the asymptotics for these detectors, we derive a new F-test statistic. We consider the detrimental effects of using spectral estimates in the denominator of Hinich’s test. We determine refined conditional distributions for thirdand fourth-order versions of his detector. We also modify his test for colored scenarios. Extending the bispectral detectors to their kth-order counterparts, we calculate the optimal smoothing bandwidth to use in constructing the HOS estimates, producing the best detection performances for both our F-test and Hinich’s test with our refined distributions. These new bandwidths yield significant improvements in detector performance over previous results. For the finite sample case, our calculations characterize the tradeoff between the two detectors and demonstrate that a larger smoothing bandwidth than the one suggested by previous researchers should he used. Our calculations are verified using simulations for both white and colored cases.	bandwidth (signal processing);noise reduction;performance;rejection sampling;requirement;sensor;simulation;smoothing;test case	Lee M. Garth;Yoram Bresler	1996	IEEE Trans. Signal Processing	10.1109/78.502332	conditional probability distribution;econometrics;speech recognition;signal processing;mathematics;statistics	Vision	54.008864127121996	11.877861963474167	517
ec947f738ee6447e03a2287e0e0a507ff228b381	design and development of data-intensive web sites: the araneus approach	databases;relation algebra;design process;development;design and development;internet;design and implementation;relational model;world wide web;database design;www;design methodology	Data-intensive Web sites are large sites based on a back-end database, with a fairly complex hypertext structure. The paper develops two main contributions: (a) a specific design methodology for data-intensive Web sites, composed of a set of steps and design transformations that lead from a conceptual specification of the domain of interest to the actual implementation of the site; (b) a tool called Homer, conceived to support the site design and implementation process, by allowing the designer to move through the various steps of the methodology, and to automate the generation of the code needed to implement the actual site.Our approach to site design is based on a clear separation between several design activities, namely database design, hypertext design, and presentation design. All these activities are carried on by using high-level models, all subsumed by an extension of the nested relational model; the mappings between the models can be nicely expressed using an extended relational algebra for nested structures. Based on the design artifacts produced during the design process, and on their representation in the algebraic framework, Homer is able to generate all the code needed for the actual generation of the site, in a completely automatic way.	data-intensive computing;database design;emoticon;high- and low-level;hypertext;linear algebra;relational algebra;relational model	Paolo Merialdo;Paolo Atzeni;Giansalvatore Mecca	2003	ACM Trans. Internet Techn.	10.1145/643477.643480	iterative design;the internet;relational model;design process;web design;design methods;idef4;computer science;operating system;data mining;relation algebra;database;world wide web;database design;generative design	DB	-46.70766531990204	20.4978112627039	518
979f87dd0d3383cbfd8778d8c26eb4fa1ed43d28	a new local search algorithm for continuous spaces based on army ant swarm raids	fitness landscape;evolutionary computation;combinatorial optimization problems local search algorithm continuous spaces army ant swarm raids evolutionary algorithms;local search algorithm;combinatorial optimization problem;search problems evolutionary computation;search problems;evolutionary algorithm;evolutionary computation australia chemicals military computing search methods design optimization design methodology potential energy proteins simulated annealing;local search	It is well known that evolutionary algorithms often perform much better when augmented with a local search mechanism. While many local search methods exist for combinatorial optimization problems, there are relatively few methods designed to work over continuous fitness landscapes. This paper describes a novel continuous space local search algorithm for evolutionary algorithms that emulates army ant swarm raids. Our preliminary results show the method is remarkably effective.	combinatorial optimization;emulator;evolutionary algorithm;local search (optimization);mathematical optimization;particle swarm optimization;search algorithm	Garrison W. Greenwood;Hussein A. Abbass	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424592	evolutionary programming;beam search;mathematical optimization;interactive evolutionary computation;cultural algorithm;tabu search;computer science;artificial intelligence;local search;hill climbing;machine learning;evolutionary algorithm;iterated local search;mathematics;incremental heuristic search;best-first search;combinatorial search;metaheuristic;memetic algorithm;guided local search;evolutionary computation;search algorithm	AI	25.66735291528389	-5.703481047081082	519
1941416a773f28b0ecb103ce7cb9a7fa230a86cd	the lynx distributed programming language: motivation, design and experience	lenguaje programacion;distributed system;sistema operativo;interprocess communication;systeme reparti;programming language;sistema informatico;lynx;distributed processing;distributed programs;transmission message;computer system;conceptual framework;message transmission;remote procedure call;distributed operating system;sistema repartido;type checking;operating system;distributed environment;message passing;langage programmation;exception handling;systeme exploitation;systeme informatique;traitement reparti;language design;geographic distribution;tratamiento repartido;transmision mensaje	A programming language can provide much better support for interprocess communication than a library package can. Most message-passing languages limit this support to communication between the pieces of a single program, but this need not be the case. Lynx facilitates convenient, typesafe message passing not only within applications, but also between applications, and among distributed collections of servers. Specifically, it addresses issues of compiler statelessness, late binding, and protection that allow run-time interaction between processes that were developed independently and that do not trust each other. Implementation experience with Lynx has yielded important insights into the relationship between distributed operating systems and language run-time support packages, and into the inherent costs of high-level message-passing semantics.	apl;compiler;distributed computing;distributed operating system;high- and low-level;inter-process communication;late binding;message passing;programming language	Michael L. Scott	1991	Comput. Lang.	10.1016/0096-0551(91)90008-W	exception handling;message passing;real-time computing;computer science;conceptual framework;database;distributed computing;programming language;remote procedure call;distributed computing environment;inter-process communication	PL	-26.49093820005748	41.175437050154066	520
3d9637554575b22882c213a29c267c79135342c6	"""a new """"switched kalman filter"""" for 3d-contourmeasuring problems with a laser diode range finder"""	kalman filter;laser diode		diode;kalman filter	Otmar Loffeld	1987		10.1007/978-3-642-73015-3_68	electronic engineering	Robotics	70.24271916521215	-8.189984436357756	521
0c756f7233bd6444de857108ffa0ca87a7d55348	first results in the coordination of heterogeneous robots for large-scale assembly	multi robot system;large scale;software architecture	While many multi-robot systems rely on fortuitous cooperation between agents, some tasks, such as the assembly of large structures, require tighter coordination. We present a general software architecture for coordinating heterogeneous robots that allows for both autonomy of the individual agents as well as explicit coordination. This paper presents recent results with three robots with very different configurations. Working as a team, these robots are able to perform a high-precision docking task that none could achieve individually.	autonomous robot;camera resectioning;docking (molecular);mobile manipulator;multitier architecture;robotic arm;software architecture;visual servoing;workspace	Reid G. Simmons;Sanjiv Singh;David Hershberger;Josué Guimarães Ramos;Trey Smith	2000		10.1007/3-540-45118-8_33	mobile robot;embedded system;software architecture;real-time computing;simulation;computer science	Robotics	58.00845311290036	-27.14266461588231	522
111127b7f3396336a08cf1ea15c474bb0b062157	on preconditioning the treecode-accelerated boundary integral (tabi) poisson-boltzmann solver			poisson–boltzmann equation;preconditioner;solver	Jiahui Chen;Weihua Geng	2018	J. Comput. Physics	10.1016/j.jcp.2018.07.011		Theory	85.95840637080052	11.05804622171869	523
36d8ded9087a93926635e652a784c860b8a121c2	proactive knowledge distribution for agile processes	formal specification;case based module monitored distribution proactive knowledge distribution knowledge artifact retrieval agile process md requirements;air traffic control monitoring military computing knowledge management airports educational institutions information science knowledge representation containers indexing;knowledge acquisition;knowledge based systems knowledge representation formal specification knowledge acquisition;just in time;knowledge representation;knowledge based systems	Monitored distribution (MD) is a case-based approach for proactive knowledge distribution. MD allows the dissemination of knowledge artifacts in a just-in-time fashion in the context of its applicable targeted processes. In MD, knowledge artifacts are retrieved when they are applicable to the task in which a user is currently engaged. We define MD’s requirements and argue that it can be applied to agile processes because the targeted processes are collected as an attribute of knowledge artifacts. 1. Monitored Distribution Monitored Distribution (MD) [1] is an approach for proactive distribution of knowledge artifacts [2]. It has been designed for the distribution of lessons-learned, which are knowledge artifacts that embed a validated strategy that positively impacts organizational results when reused [3]. Because of the strong association between lessons-learned and organizational results, MD is integrated with organizational processes. MD addresses problems associated with other distribution methods that are divorced from targeted organizational processes and require users to have the initiative and skills to access, manipulate and interpret knowledge artifacts. Most importantly, MD motivates the reuse of a knowledge artifact by bringing it to the attention of the user when and where it is applicable and by including a rationale for its reuse.[4] The MD approach shifts the burden of knowledge dissemination from the user to the software. MD can accomplish this by relying on an intelligent module that monitors when a lesson-learned should be disseminated to the user by matching the lesson to the user’s context. MD’s intelligent module relies on case-based reasoning (CBR). CBR is often recommended for knowledge management (KM) tasks [5] possibly due its flexible knowledge representation and because it uses different techniques to manage a set of knowledge containers [6]. In the case-based module, each case is represented by a lesson-learned that is applicable to a task within an organizational process. Table 1 presents an example of a lesson-learned (from the Navy Lessons Learned System [7]) and its representation structure, which combines indexing elements (i.e., applicable task, preconditions) and reuse elements (i.e., lesson suggestion, rationale). These elements can be subdivided into more parts, as Table 1 shows. Table 1. Lesson-learned example Applicable task Action: Assign air traffic controllers. Mission type: NEO Task: Provide for Movement Services in Theater of Operations Preconditions A civilian airport is used for military air traffic. Lesson suggestion Assign military air traffic controllers. Rationale Type: Failure What? Military traffic overloaded civilian controllers. Why? The rapid build-up of military flight operations at Mactan Intl Airport, Cebu quickly overloaded the civilian host nation controllers. 2. Requirements for Implementing MD First of all, to benefit from MD, targeted users have to deliver their actions or decisions using a computerized system (e.g., enterprise resource planning). This system has to be flexible enough to allow the integration of the MD approach, which monitors the user’s actions. MD is constantly trying to match the user’s context with previously recorded lessons-learned. Accordingly, the second requirement is that lessonslearned are stored in the MD’s case base using the representation elements exemplified in Table 1. Because MD has a case representation structure that assesses the similarity between user’s contexts and lessons-learned, MD does not require that all tasks be defined a priori. As lessons with new tasks are captured, they will match the contexts when these same tasks become current. Without meeting those conditions, MD’s applicabilityoriented distribution fails. In this case, MD may attempt to distribute knowledge that is not relevant; potentially preventing the positive impact on organizational processes that lessons-learned are meant to provide. 3. MD in Agile Processes The ability to adapt seems to be one of the key features to achieve business agility [8], and so should be learning from experience and incorporating new knowledge into organizational processes. Sometimes, incorporating knowledge means changing old or creating new tasks. Therefore the advantages of using MD in support of agile KM processes are two-fold. First it supports agile incorporation of experiential knowledge into organizational processes, allowing an organization to adapt while steered towards its goals. Second, MD represents a process-oriented KM approach that does not rely on static and previously determined processes to support knowledge sharing, making it amenable to implement in agile organizations with dynamic processes. Agile processes do not pose a difficult obstacle to the MD approach because new tasks and processes are potentially identified before or simultaneous to the identification and learning of knowledge that will eventually be reused in such tasks. Given the flexible framework in the MD approach, new tasks can be included at any point. MD will be able to identify a new task once there is a lesson-learned that is applicable to this task. 4. Direct and Indirect MD MD has been integrated with a plan authoring tool [4]. MD would display applicable lessons-learned on the screen so the user could decide whether to reuse it or not. This is a direct use of MD. An indirect use of MD does not deliver lessons directly to the end-user but to an intermediary system. This intermediary system may reuse lessons while producing an outcome for the end-user. It is reasonable to expect that agile businesses rely on intelligent systems that can adapt to their dynamic context. In this case, MD can deliver lessons to these systems, and incorporate knowledge to be used as an additional source of change and evolution to organizational processes. An example of this indirect use of MD is a computational intelligence (CI) tool for software testing being currently developed at the National Institute for System Test and Productivity (NISTP) at the University of South Florida. The CI-tool designs a software testing strategy. Because this task is very sensitive to experiential knowledge, it is desirable that lessons are distributed while strategies are being designed. In this case, the CItool is designed to execute a dynamic process. As with human software testers, the system has to adapt depending on the type of software to be tested, causing it to evolve as more experiences are gained. The MD approach can contribute to the agility of this process by disseminating lessons-learned directly to this system and incorporating experiential knowledge to improve and change the testing design process. 5. Discussion One major concern in choosing KM approaches originates from the recognition that successful KM should be accompanied by a correspondent change in culture [9]. On the other hand, today’s large organizations require technological KM solutions [10] because most of their processes are automated in enterprise-wise information systems. For example, some of the features that include Cisco in the selective set of agile organizations are its rapid, highly automated and virtually paperless processes [8]. Interestingly, the more automated the organization, the easier it is to implement an approach like MD, because these organizations tend to be highly automated and their systems tend to be flexible enough to allow the integration of new models. It is our intention to investigate the extent of the difficulties and challenges of the integration of MD when processes are agile.	agile software development;artifact (software development);case-based reasoning;computational intelligence;cryptographic hash function;design rationale;enterprise resource planning;information system;knowledge management;knowledge representation and reasoning;learning to rank;molecular dynamics;organizational behavior;precondition;proactive parallel suite;process (computing);requirement;software testing;system testing;user (computing)	Rosina O. Weber	2003		10.1109/ENABL.2003.1231433	knowledge representation and reasoning;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;formal specification;database;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;world wide web;computer security;domain knowledge	SE	-67.00301351804158	15.655170476853389	524
fe4f103f770220e50f08002a15a9f4bd25540eed	an algorithm for index multimedia data (video) using the movement oriented method for real-time online services	storage system;information retrieval;real time;indexation;multimedia data	Multimedia data is a form of data that can represent all types of data (images, sound and text). The use of multimedia data for the online application requires a more comprehensive database in the use of storage media, Sorting / indexing, search and system / data searching. This is necessary in order to help providers and users to access multimedia data online. Systems that use of the index image as a reference requires storage media so that the rules and require special expertise to obtain the desired file. Changes in multimedia data into a series of stories / storyboard in the form of a text will help reduce the consumption of media storage, system index / sorting and search applications. Oriented Movement is one method that is being developed to change the form of multimedia data into a storyboard. Keyword(s): Algorithm, Index, multimedia data, Movement Oriented. Subject description: H.2.Database Management | H.2.4.Systems \ Multimedia Databases	algorithm;image;real-time transcription;simulation;sorting;storyboard;web application	A. Muslim;A. B. Mutiara;C. M. Karyati;P. Musa	2009	CoRR		computer science;computer data storage;database;multimedia;world wide web;information retrieval	DB	-14.601302934623343	-54.62399018866011	525
34ce02c922d82419691137600db239af471545d0	stochastic projection-factoring method based on piecewise stationary renewal processes for mid- and long-term traffic flow modeling and forecasting	headway;traffic forecasting;reliability;piecewise stationary renewal process;stochastic projection and factoring;traffic counting;time periods;traffic flow;stochastic processes;time series analysis;structural time series analysis;long term traffic forecast;traffic volume	Forecasting traffic over a long period of time is of considerable interest and usefulness, but accurate forecasting is very difficult. Traditional projection and factoring methods for mid- and long-term cumulative traffic forecasting are deterministic and only provide a point prediction without specifying a statistical measure of prediction reliability. This paper constructs a stochastic projection and factoring method by casting long-term traffic volume counts into an integrated and rigorous framework of a more refined structural time series component model with piecewise stationary renewal processes capturing time-of-day, day-of-week, monthly, and yearly variations. By doing so, the new method roots itself in a solid theoretical foundation and generates two advantages. First, it results in a more accurate point prediction of cumulative traffic by taking into account the time-of-day traffic count variation in the modeling of unobservable future long-term traffic flow at temporary count stations or at a s...	integer factorization;stationary process	Lu Sun	2016	Transportation Science	10.1287/trsc.2015.0607	traffic generation model;stochastic process;econometrics;simulation;operations management;traffic flow;time series;reliability;mathematics;traffic volume;statistics;network traffic simulation	ML	8.711636260336329	-13.71328948720235	526
36bed54a903c6dc9fd4249eb414333ef4e1a5aa0	refactoring java generics by inferring wildcards, in practice	refactoring;generics;wildcards;polymorphism;definition site variance;use site variance;variance	Wildcard annotations can improve the generality of Java generic libraries, but require heavy manual effort. We present an algorithm for refactoring and inferring more general type instantiations of Java generics using wildcards. Compared to past approaches, our work is practical and immediately applicable: we assume no changes to the Java type system, while taking into account all its intricacies. Our system allows users to select declarations (variables, method parameters, return types, etc.) to generalize and considers declarations not declared in available source code. It then performs an inter-procedural flow analysis and a method body analysis, in order to generalize type signatures. We evaluate our technique on six Java generic libraries. We find that 34% of available declarations of variant type signatures can be generalized - i.e., relaxed with more general wildcard types. On average, 146 other declarations need to be updated when a declaration is generalized, showing that this refactoring would be too tedious and error-prone to perform manually.	algorithm;class implementation file;code refactoring;cognitive dimensions of notations;data-flow analysis;declaration (computer programming);generic programming;generics in java;java annotation;library (computing);type generalization;type signature;type system;variable (computer science);wildcard character	John Altidor;Yannis Smaragdakis	2014		10.1145/2660193.2660203	wildcard;polymorphism;wildcard character;computer science;data mining;variance;programming language;generic programming;code refactoring;algorithm;generics in java	PL	-25.497799015725086	28.551049785592692	527
62f0ca8398a881c1b9fd2e24adf1aa024fe0cd35	blind estimation of unknown time delay in periodic non-uniform sampling: application to desynchronized time interleaved-adcs	traitement du signal et de l image;estimation delays delay effects frequency division multiplexing telecommunications synchronization;intelligence artificielle;nonuniform sampling;analog to digital converters nonuniform sampling estimation stationary random process;vision par ordinateur et reconnaissance de formes;estimation;bandpass signals time delay blind estimation periodic nonuniform sampling scheme desynchronized time interleaved adc analog to digital converters ti adc sampling rate low rate adc multiplexing synchronization signal reconstruction formula elementary adc online calibration hardware correction sampling device pns scheme signal stationarity properties;traitement des images;analog to digital converters;synthese d image et realite virtuelle;stationary random process;synchronisation analogue digital conversion delay estimation signal reconstruction signal sampling	Increasing the sampling rate of Analog-to-Digital Converters (ADC) is a main challenge in many fields and especially in telecommunications. Time-Interleaved ADCs (TI-ADC) were introduced as a technical solution to reach high sampling rates by time interleaving and multiplexing several low-rate ADCs at the price of a perfect synchronization between them. Indeed, as the signal reconstruction formulas are derived under the assumption of uniform sampling, a desynchronization between the elementary ADCs must be compensated upstream with an online calibration and expensive hardware corrections of the sampling device. Based on the observation that desynchronized TI-ADCs can be effectively modeled using a Periodic Non-uniform Sampling (PNS) scheme, we develop a general method to blindly estimate the time delays involved in PNS. The proposed strategy exploits the signal stationarity properties and thus is simple and quite generalizable to other applications. Moreover, contrarily to state-of-the-art methods, it applies to bandpass signals which is the more judicious application framework of the PNS scheme.	analog-to-digital converter;application framework;broadcast delay;forward error correction;gibbs sampling;multiplexing;nonuniform sampling;sampling (signal processing);signal reconstruction;stationary process	Jean-Adrien Vernhes;Marie Chabert;Bernard Lacaze;Guy Lesthievent;Roland Baudin;Marie-Laure Boucheret	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472524	estimation;nonuniform sampling;telecommunications;control theory;mathematics;statistics	Visualization	54.26822235657634	68.21887449168254	528
c9b688b7be40b52001d13b1007905856d7b607a5	envelope and wavelet transform for sound localisation at low sampling rates in wireless sensor networks		High sampling frequencies in acoustic wireless sensor network (AWSN) are required to achieve precise sound localisation. But they are also mean analysis time and memory intensive (i.e., huge data to be processed and more memory space to be occupied which form a burden on the nodes limited resources). Decreasing sampling rates below Nyquist criterion in acoustic source localisation (ASL) applications requires development of the existing time delay estimation techniques in order to overcome the challenge of low time resolution. This work proposes using envelope and wavelet transform to enhance the resolution of the received signals through the combination of different time-frequency contents. Enhanced signals are processed using cross-correlation in conjunction with a parabolic fit interpolation to calculate the time delay accurately. Experimental results show that using this technique, estimation accuracy was improved by almost a factor of 5 in the case of using 4.8 kHz sampling rate. Such a conclusion is useful for developing precise ASL without the need of any excessive sensor resources, particularly for structural health monitoring applications.	acoustic cryptanalysis;algorithm;broadcast delay;cross-correlation;dspace;data acquisition;experiment;interpolation;mathematical optimization;multiresolution analysis;nyquist rate;nyquist stability criterion;parabolic antenna;sampling (signal processing);sensor;tinyos;wavelet transform	Omar Mabrok Bouzid;Guiyun Tian;Jeffrey A. Neasham;Bayan S. Sharif	2012	J. Sensors	10.1155/2012/680383	electronic engineering;speech recognition;telecommunications;engineering	Mobile	50.27871266009903	54.813182091102604	529
2a3afe736c11c0aa6684bf7d7106eedb6810bc32	writer profiling using handwriting copybook styles	handwriting analysis;handwriting copybook style;handwriting recognition;image database;data mining;handwriting recognition document image processing;writer profiling handwriting analysis handwriting copybook style;document image processing;writer profiling;copybook style identification writer profiling handwriting copybook styles document examination handwriting analysis system;forensics image analysis information analysis image databases data analysis data mining text analysis handwriting recognition clustering algorithms computer science	Handwriting originates from a particular copybook style such as Palmer or Zaner-Bloser that one learns in childhood. Since questioned document examination plays an important investigative and forensic role in many types of crime, it is important to develop a system that helps objectively identify a questioned document's handwriting style. We proposed a handwriting analysis system that can assist a document examiner in the identification of the writer's handwriting style and therefore of his/her origin or nationality. We collected 33 English alphabet copybook styles from 18 countries. Here, we extend the analysis using several data mining techniques to discover important information that can be gleaned from a handwriting copybook style image database, e.g., the most information bearing alphabet characters for the purpose of copybook style identification and the relationship between geographical regions and similarity based clusters of copybook styles.	data mining;graphology;include directive;symbolics document examiner	Sungsoo Yoon;Seung-Seok Choi;Sung-Hyuk Cha;Charles C. Tappert	2005	Eighth International Conference on Document Analysis and Recognition (ICDAR'05)	10.1109/ICDAR.2005.256	natural language processing;speech recognition;intelligent character recognition;computer science;multimedia;handwriting recognition	ML	-39.71534330269615	-66.87226707242878	530
4de9ca582574ffb81e8f7b1faca3cbe70594f0f7	energy-efficient traffic scheduling in ieee 802.15.4 for home automation networks	automatic control;energy efficiency;energy efficient traffic scheduling;guaranteed time slots;ieee standards;guaranteed time slot;ieee 802 15 4;energy efficient;job shop scheduling;bandwidth allocation;wireless sensor networks bandwidth allocation home automation personal area networks scheduling telecommunication traffic;actuators;indexing terms;energy efficiency telecommunication traffic home automation automatic control wireless sensor networks job shop scheduling ieee standards zigbee actuators communication system control;satisfiability;wireless sensor network;telecommunication traffic;home automation networks;scheduling;zigbee;duty cycle;energy efficient traffic scheduling guaranteed time slots wireless sensor networks home automation networks ieee 802 15 4;personal area networks;wireless personal area network;communication system control;wireless sensor networks;home automation	The IEEE 802.15.4 low rate wireless personal area network is a protocol suitable for wireless sensor networks. There has been growing interest in the IEEE 802.15.4 protocol in the home automation networks. This paper proposes a scheduling scheme obtaining optimal parameters regarding the IEEE 802.15.4 frame and subframes in the home automation networks. Guaranteed time slots (GTS) are exploited for the delivery of home automation traffic. Given a set of the requirements for bandwidths and periods assigned to nodes in the network, the beacon interval and the active/inactive subframe duration satisfying the requirements are selected considering the low duty cycle by the proposed scheme. Based on these parameters, analytic results provide the way to efficient use of network resources including energy.	bandwidth (signal processing);duty cycle;global telecommunications system;home automation;numerical analysis;quasiperiodicity;real-time clock;real-time operating system;requirement;scheduling (computing);throughput	Hyung Seok Kim;Joo-Han Song;Seok Lee	2007	IEEE Transactions on Consumer Electronics	10.1109/TCE.2007.381703	ieee 802.11s;embedded system;job shop scheduling;network allocation vector;real-time computing;wireless sensor network;ieee 802;inter-access point protocol;computer science;engineering;automatic control;ieee 802.11h-2003;efficient energy use;computer network;ieee 802.1q	Embedded	10.274220770730992	83.78155911148639	531
900621fb0fae7964072590ae68c20e34e6d4ff0b	on end-to-end performance of mimo multiuser in cognitive radio networks	performance evaluation;packet error rate;multiple input multiple output;array signal processing;spectrum;channel state information;strontium;feedback;vectors;cognitive radio;cognitive radio network;multiple secondary mimo user cognitive radio network multiple input multiple output multiuser transmission mimo multiuser transmission spectrum sharing constraint average packet error rate channel state information feedback delay multiuser scheduling performance evaluation;scheduling;antennas;scheduling cognitive radio delays feedback mimo communication;signal to noise ratio;mimo;strontium mimo cognitive radio signal to noise ratio vectors array signal processing antennas;mimo communication;delays	In this paper, a design for the multiple-input-multiple-output~(MIMO) multiuser transmission in the cognitive radio network is developed and its end-to-end performance is investigated under spectrum-sharing constraints. Firstly, the overall average packet error rate is analyzed by considering the channel state information feedback delay and the multiuser scheduling. Then, we provide corresponding numerical results to measure the performance evaluation for several separate scenarios, which presents a convenient tool for the cognitive radio network design with multiple secondary MIMO users.	beamforming;bit error rate;channel state information;cognitive radio;end-to-end principle;mimo;modulation;multi-user;multiplexing;network packet;network planning and design;numerical analysis;performance evaluation;scheduling (computing);transmitter	Yuli Yang;Sonia Aïssa	2011	2011 IEEE Global Telecommunications Conference - GLOBECOM 2011	10.1109/GLOCOM.2011.6134174	3g mimo;cognitive radio;real-time computing;telecommunications;computer science;statistics	Mobile	34.146381426977804	88.93187632736291	532
b74eda1cb369e25f48d759e563296e5d23930716	a secure scheme to share secret color images	discrete dynamical system;reversible model of computation;image processing;secret sharing;computational techniques;articulo;02 70 c;07 05 pj;statistical properties;02 70 c cellular automata;cryptography;linear dynamical system;45 30 s;model of computation;secret sharing scheme;cellular automata;color image	The main goal of this work is to study how discrete dynamical systems can be used to design secret sharing Specifically, the proposed scheme permits to share secret color images, and it is based on bidimensional cellular auto main idea is to analyze how a simple reversible model of computation allows one to compute the shares and then reverse computation in order to recover the secret image. Moreover, the proposed scheme exhibits good statistical pr  2005 Elsevier B.V. All rights reserved. PACS: 07.05.Pj; 45.30.+s; 02.70.-c	automata theory;dynamical system;model of computation;picture archiving and communication system;reverse computation;reversible cellular automaton;secret sharing	Gonzalo Álvarez;Ascensión Hernández Encinas;Luis Hernández Encinas;Ángel Martín del Rey	2005	Computer Physics Communications	10.1016/j.cpc.2005.07.002	model of computation;linear dynamical system;discrete mathematics;color image;image processing;cryptography;theoretical computer science;shamir's secret sharing;mathematics;distributed computing;homomorphic secret sharing;secure multi-party computation;secret sharing;verifiable secret sharing	Crypto	38.80647030540773	-8.421319792197057	533
db30409c55e6eaa2c1fe2a42bf50476393d15af2	mechanistic electronic model to simulate and predict the effect of heat stress on the functional genomics of ho-1 system: vasodilation	n terminal;biliverdin reductase;heat stress;jnk;body surface;po ah;mitogen activated protein kinase;erk;cgmp;extra cellular signal regulated kinases;body core;tre;mechanistic electronic model;input heat stress;tnf α;tumor necrosis factor α;skin temperature;heme oxygenase 1;digital logic;co;potassium channel;signalling pathway modelling;calcium;c jun n terminal protein kinase;skbf;hsf;signalling pathway;hsp;environmental temperature;skin blood flow;heat shock protein;forearm blood flow;guanylyl cyclase;biliverdin;fbf;tnz;soluble guanylyl cyclase;functional genomics;anterior hypothalamus preoptic area;ca;iron regulatory protein;calcium activated potassium channels;no;sgc;kca;bvr;guanosine 3 5 cyclic monophosphate;nitrogen oxide;ho 1;il;irp;blood flow;p38 mapk;tsk;protein kinase;p38 mitogen activated protein kinase;interleukins;thermoneutal zone;bv;carbon monoxide;heat shock factor	The present work is concerned to model the molecular signalling pathway for vasodilation and to predict the resting young human forearm blood flow under heat stress. The mechanistic electronic modelling technique has been designed and implemented using MULTISIM 8.0 and an assumption of 1V/ degrees C for prediction of forearm blood flow and the digital logic has been used to design the molecular signalling pathway for vasodilation. The minimum forearm blood flow has been observed at 35 degrees C (0 ml 100 ml(-1)min(-1)) and the maximum at 42 degrees C (18.7 ml 100 ml(-1)min(-1)) environmental temperature with respect to the base value of 2 ml 100 ml(-1)min(-1). This model may also enable to identify many therapeutic targets that can be used in the treatment of inflammations and disorders due to heat-related illnesses.		Yogender Aggarwal;Bhuwan Mohan Karan;Barda Nand Das;Rakesh Kumar Sinha	2010	Computers in biology and medicine	10.1016/j.compbiomed.2010.03.011	biliverdin reductase;endocrinology;carbon monoxide;calcium;immunology	HCI	10.316246891068587	-64.07995371935334	534
dbd5e6ba77be00bbf97d834d7d47bae301684a75	location discriminative vocabulary coding for mobile landmark search	vocabulary compression;journal article;mobile landmark search;compact visual descriptor;two way coding;drntu engineering electrical and electronic engineering electronic systems signal processing;descriptor adaption;system applications	With the popularization of mobile devices, recent years have witnessed an emerging potential for mobile landmark search. In this scenario, the user experience heavily depends on the efficiency of query transmission over a wireless link. As sending a query photo is time consuming, recent works have proposed to extract compact visual descriptors directly on the mobile end towards low bit rate transmission. Typically, these descriptors are extracted based solely on the visual content of a query, and the location cues from the mobile end are rarely exploited. In this paper, we present a Location Discriminative Vocabulary Coding (LDVC) scheme, which achieves extremely low bit rate query transmission, discriminative landmark description, as well as scalable descriptor delivery in a unified framework. Our first contribution is a compact and location discriminative visual landmark descriptor, which is offline learnt in two-step: First, we adopt spectral clustering to segment a city map into distinct geographical regions, where both visual and geographical similarities are fused to optimize the partition of city-scale geo-tagged photos. Second, we propose to learn LDVC in each region with two schemes: (1) a Ranking Sensitive PCA and (2) a Ranking Sensitive Vocabulary Boosting. Both schemes embed location cues to learn a compact descriptor, which minimizes the retrieval ranking loss by replacing the original high-dimensional signatures. Our second contribution is a location aware online vocabulary adaption: We store a single vocabulary in the mobile end, which is efficiently adapted for a region specific LDVC coding once a mobile device enters a given region. The learnt LDVC landmark descriptor is extremely compact (typically 10–50 bits with arithmetical coding) and performs superior over state-of-the-art descriptors. We implemented the framework in a real-world mobile landmark search prototype, which is validated in a million-scale landmark database covering typical areas e.g. Beijing, New York City, Lhasa, Singapore, and Florence.	antivirus software;cluster analysis;distortion;downstream (software development);entity–relationship model;global positioning system;ground truth;location awareness;mobile device;one-way function;online and offline;parallel computing;principal component analysis;prototype;scalability;spectral clustering;unified framework;upstream (software development);user experience;visual descriptor;vocabulary;world wide web	Rongrong Ji;Ling-yu Duan;Jie Chen;Hongxun Yao;Junsong Yuan;Yong Rui;Wen Gao	2011	International Journal of Computer Vision	10.1007/s11263-011-0472-9	computer vision;speech recognition	Mobile	20.941952555595932	-57.125650368063475	535
75aedfb35d227311fec7f006b2d7bce6e3476b3b	a coevolutionary chromosome encoding scheme for high dimensional search spaces	search problems encoding genetic algorithms genetics;evolutionary computation;high dimensionality;gene encoding;search space;genetics;coevolutionary chromosome encoding scheme;optimization problem;evolution biology;biological cells;optimization problem coevolutionary chromosome encoding scheme high dimensional search space gene encoding evolutionary technique;high dimensional search space;genetic algorithms;optimization;search problems;encoding;biological cells encoding evolutionary computation genetics optimization steady state evolution biology;evolutionary technique;steady state	This work introduces a co evolutionary chromosome encoding scheme for evolving solutions in a high dimensional search space. The chromosome is divided in m “genes” and m different populations are created (one population per gene). Each one of the m populations evolves an specific gene and good references to genes in the remaining populations. The candidate solution is built using such references and the encoded gene. Individuals in the same population compete among them to find the best gene while individuals from different populations work together in order to find the best candidate solution. Finally, the best candidate solution is selected from all the populations based on its performance. Some experiments are conducted on well-known binary and real defined functions using three different evolutionary techniques. The obtained results indicate that the proposed approach is able to improve the underline evolutionary technique when evolving solutions for optimization problems in high dimensional spaces.	bitwise operation;experiment;genetic algorithm;line code;mathematical optimization;optimization problem;population;well-known text	Jonatan Gómez;Elizabeth León Guzman	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586359	optimization problem;mathematical optimization;genetic algorithm;computer science;bioinformatics;steady state;encoding;evolutionary computation	DB	26.382579918842445	-7.792969733537144	536
7289b3082c0511f54a8fcbf63c2960288b57bfea	resource partitioning algorithms in a programmable service grid architecture	distributed system;systeme reparti;resource partitioning;systeme aide decision;heuristic method;service web;metodo heuristico;sistema ayuda decision;web service;algoritmo genetico;qualite service;computer network;grid;decision support system;divisible load theory;sistema repartido;rejilla;scheduling;algorithme genetique;grille;genetic algorithm;methode heuristique;service quality;ordonnancement;reglamento;servicio web;calidad servicio	We propose the use of programmable Grid resource partitioning heuristics in the context of a distributed service Grid management architecture. The architecture is capable of performing automated and exclusive resource-to-service assignations based on Grid resource status/properties and monitored service demand. We present two distinct approaches for the partitioning problem, the first based on Divisible Load Theory and the second built on Genetic Algorithms. Advantages and drawbacks of each approach are discussed and their performance is evaluated using NSGrid. Results show that automated resource-to-service partitioning simplifies scheduling decisions, improves service QoS support and allows efficient computational/network resource usage.	computational complexity theory;genetic algorithm;grid computing;heuristic (computer science);partition problem;scheduling (computing)	Pieter Thysebaert;Bruno Volckaert;Marc De Leenheer;Filip De Turck;Bart Dhoedt;Piet Demeester	2005		10.1007/11428862_35	web service;niche differentiation;real-time computing;simulation;genetic algorithm;decision support system;computer science;artificial intelligence;operating system;distributed computing;grid;scheduling;service quality;computer network	HPC	-14.457971139126352	63.591413137740844	537
aca9d9beb7828695f952d47a8baa6688439d4d08	generating interpretable hypotheses based on syllogistic patterns	hypotheses;syllogism	The ever-growing literature in biomedicine makes it virtually impossible for individuals to grasp all the information relevant to their interests.  Since even experts' knowledge is likely to be incomplete, important associations among key biomedical concepts may remain unnoticed in the flood of information.  Discovering those implicit, hidden knowledge is called hypothesis discovery. This paper reports our preliminary work on hypothesis discovery, which takes advantage of a syllogistic chain of relations extracted from existing knowledge (i.e., published literature).  We consider such chains of relations as implicit patterns or rules to generate potential hypotheses.  The generated hypotheses are then ranked according to their plausibility judged from the reliability of the rule which generated the hypothesis and the analogical resemblance between new and existing knowledge.  We discuss the validity of the proposed approach on the entire Medline database.		Takuya Hagimura;Kazuhiro Seki;Kuniaki Uehara	2012			hypothesis;computer science;artificial intelligence;syllogism;machine learning;data mining;algorithm;statistics	Vision	-37.31303482998576	-67.73849761458199	538
98ee6b683c9edfc4631ab5be8c2134e9031c2d6d	dna: from search to observation revisited	web observatory;observatory models;web science;social machines	"""In this paper, we describe extensions to the process model first described in the paper """"From Search to Observation"""" based on additional field interview work. This process model forms part of a triad of perspectives under the banner of a methodology known as DNA, which looks at structure (Definition), process (Nature) and motivations of actors (Archetypes) for Web Observatories (hereafter WO) and more generally the class of Social Machines. We discuss the rationale for the model enhancements, enumerate and summarise the changes and close with an introduction to future work around use of open source tools and languages for implementing and analyzing social machine processes using this model. The additional perspectives we are now considering are an extensive revision to the model (which now addresses more than three times the number of factors in the previous model) and hence a revised paper is called for in this space."""	design rationale;enumerated type;open-source software;process modeling	Ian C. Brown;Lisa Harris;Wendy Hall	2015		10.1145/2786451.2786511	simulation;engineering;artificial intelligence;operations research	Web+IR	-61.628548310005336	17.790493637430107	539
39bf0dfd83f71040288c7099f084f7972cb74732	the chi 2013 interactive schedule	multi surface interaction;video content;large displays	CHI 2013 offers over 500 separate events including paper presentations, panels, courses, case studies and special interest groups. Given the size of the conference, it is no longer practical to host live summaries of these events. Instead, a 30-second Video Preview summary of each event is available. The CHI'13 Interactive Schedule helps attendees navigate this wealth of video content in order to identify events they would like to attend. It consists of a number of large display screens throughout the conference venue which cycle through a video playlist of events. Attendees can interact with these displays using their mobile devices by either constructing custom video playlists or adding on-screen content to their personal schedule.	chi;computer monitor;digital video;mobile device;venue (sound system)	Arvind Satyanarayan;Daniel Strazzulla;Clemens Nylandsted Klokmose;Michel Beaudouin-Lafon;Wendy E. Mackay	2013		10.1145/2468356.2479591	multimedia;world wide web;computer graphics (images)	HCI	-45.906606746834015	-32.799673748092964	540
1cda8a26435988abe27963736fe5ff629de06bef	interaction with soft robotic tentacles		Soft robotics technology has been proposed for a number of applications that involve human-robot interaction. In this tabletop demonstration it is possible to interact with two soft robotic platforms that have been used in human-robot interaction experiments (also accepted to HRI'18 as a Late-Breaking Report and a video).	experiment;human–robot interaction;robot;soft robotics	Jonas Jørgensen	2018		10.1145/3173386.3177838	human–computer interaction;computer science;tentacle;soft robotic;soft robotics;human–robot interaction	Robotics	-37.70306497269471	-39.55266399571994	541
14bceab63d70024958c960b04638a8a919fbb345	proactive control method based on system margin in evolutional agent system	evolutional agent system multiagent system multimedia communication service qos;multiagent system;multimedia communication service;evolutional agent system;qos;multimedia communication service proactive control method system margin evolutional agent system multiagent system control control possibility control effect qos service provisioning system multimedia communication system;telecommunication control evolutionary computation multi agent systems multimedia communication quality of service self adjusting systems	We propose a new method for controlling multiagent systems proactively considering the gcontrol possibility h and gcontrol effect h for reducing the decrement of QoS to the absolute minimum while providing service in a service-provisioning system based on multiagent system. Moreover, we implement a multimedia communication system based on the proposed method to evaluate its effectiveness. Results from comparison with a conventional multimedia communication system show that the system can provide adequate multimedia communication services.	agent-based model;increment and decrement operators;multi-agent system;provisioning;quality of service	Akiko Takahashi;Mitsuru Abe;Wenpeng Wei;Tetsuo Kinoshita	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.201	real-time computing;mobile qos;simulation;quality of service;computer science;distributed computing	AI	-12.250322186365358	102.41856719389332	542
860953b31b3a5a8593f6bc0a17b58731197e17cd	some computational results of using the ahrens - finke method for handling degeneracy in fixed charge transportation problems	fixed cost;transportation problem;charge transport;branch and bound algorithm;supply and demand	In a recent paper, Ahrens and Finke present a procedure for resolving degeneracy in the fixed cost transportation problem by perturbing the supplies and demands. This paper presents computational results of using that procedure in both vertex ranking and branch-and-bound algorithms for the fixed charge transportation problems as compared to using previously available methods for resolving degeneracy in those algorithms.		Patrick G. McKeown	1978	Math. Program.	10.1007/BF01609041	transportation theory;mathematical optimization;mathematics;supply and demand;branch and bound;fixed cost	Theory	16.325649250705826	3.3587017152609024	543
a69d00f71890c3a21cf032cfef2281bd0013a756	a new robust blind watermarking scheme based on steerable pyramid and dct using pearson product moment correlation	geometric attacks;robust image watermarking;discrete cosine transform;steerable pyramid;pearson product moment correlation	In this paper, we present a new, robust digital watermarking scheme for ownership protection. The scheme is based on Steerable Pyramid (SP) and discrete cosine transform (DCT) using Pearson product moment correlation. During the process of watermark embedding, SP is performed on original host image, and corresponding oriented sub-band is selected to carry the watermark. DCT is applied to each block of size 8x8 of the selected sub-band. Two independent streams representing the watermark are embedded in low and mid-frequency of DCT components. Watermark detection is based on comparison result between Pearson product moment correlations of the two independent streams with each watermarked block. The experimental results demonstrate good visual imperceptibility and resiliency of the proposed scheme against an intentional and un-intentional variety of attacks.	digital watermarking;discrete cosine transform;embedded system;watermark (data file)	Azz El Arab El Hossaini;Mohamed El Aroussi;Khadija Jamali;Samir Mbarki;Mohamed Wahbi	2014	JCP	10.4304/jcp.9.10.2315-2327	pearson product-moment correlation coefficient;computer vision;steerable pyramid;speech recognition;computer science;theoretical computer science;discrete cosine transform;mathematics;algorithm	Security	40.914574561499265	-10.997681699468	544
85323f59a7bf5dae9bd329dc977c22e4cc8ab3ed	modern standard arabic speech corpus for implementing and evaluating automatic continuous speech recognition systems	t technology general	This paper presents our work towards developing a new speech corpus for Modern Standard Arabic (MSA), which can be used for implementing and evaluating Arabic speaker-independent, large vocabulary, automatic, and continuous speech recognition systems. The speech corpus was recorded by 40 (20 male and 20 female) Arabic native speakers from 11 countries representing three major regions (Levant, Gulf, and Africa). Three development phases were conducted based on the size of training data, Gaussian mixture distributions, and tied states (senones). Based on our third development phase using 11 hours of training speech data, the acoustic model is composed of 16 Gaussian mixture distributions and the state distributions tied to 300 senones. Using three different data sets, the third development phase obtained 94.32% and 8.10% average word recognition correctness rate and average Word Error Rate (WER), respectively, for same speakers with different sentences (testing sentences). For different speakers with same sentences (training sentences), this work obtained 98.10% and 2.67% average word recognition correctness rate and average WER, respectively, whereas for different speakers with different sentences (testing sentences) this work 2.00 & 2011 The Franklin Institute. Published by Elsevier Ltd. All rights reserved. .jfranklin.2011.04.011 ndence author. dresses: shariah@siswa.um.edu.my, shariah_um@yahoo.com (M.A.M. Abushariah). M.A.M. Abushariah et al. / Journal of the Franklin Institute 349 (2012) 2215–2242 2216 obtained 93.73% and 8.75% average word recognition correctness rate and average WER, respectively. & 2011 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;acoustic model;correctness (computer science);franklin electronic publishers;gulf of evaluation;mixture model;speaker recognition;speech corpus;speech recognition;speech synthesis;text-based (computing);vocabulary;word error rate	Mohammad Abd-Alrahman Mahmoud Abushariah;Raja Noor Ainon;Roziati Zainuddin;Assal A. M. Alqudah;Moustafa Elshafei;Othman Omran Khalifa	2012	J. Franklin Institute	10.1016/j.jfranklin.2011.04.011	natural language processing;speech recognition;computer science;mathematics	NLP	-23.2052917673721	-84.59115418023103	545
44d39b4155fcf280d5dc0f8e9ce8978d76401390	visual exploration of changing fpga architectures in the vtr project	hardware description languages;network routing;network routing field programmable gate arrays hardware description languages logic cad;field programmable gate arrays;logic cad;odin ii visual exploration fpga architecture vtr project field programmable gate array device computer aided design high level verilog hardware description programmed soft logic block routing structure verilog synthesis hardware mapping logical synthesis vtr cad flow fpga cad flow;clocks visualization field programmable gate arrays hardware design languages design automation video recording solid modeling	Developing applications for Field Programmable Gate Array (FPGA) devices utilizes Computer Aided Design (CAD) flows. The transition from a high level Verilog hardware description to the optimized structure of programmed soft logic blocks and routing structure includes stages such as Verilog synthesis, hardware mapping, logical synthesis, packing, placement and routing. The VTR CAD flow is a collaborative project consisting of Odin II (University of New Brunswick), ABC (University of California, Berkeley) and VPR (University of Toronto), which offers an FPGA CAD flow for research and experimentation purposes. This paper describes developments in the visualization and simulation modules of Odin II, the first stage of the CAD flow. The contributions include new netlist visualization possibilities as well as an extended netlist simulator capable of simulating circuits with multiple clocks and providing extended generic structure simulation abilities. This results in the possibility to explore and simulate a larger set of new FPGA architectures and evaluate them using the VTR flow.	bsd;computer-aided design;field-programmable gate array;high-level programming language;netlist;place and route;placement (eda);routing;set packing;simulation;verilog	Konstantin Nasartschuk;Rainer Herpers;Kenneth B. Kent	2013	2013 International Symposium on Rapid System Prototyping (RSP)	10.1109/RSP.2013.6683953	embedded system;routing;computer architecture;programmable logic array;computer science;operating system;hardware description language;programming language;register-transfer level;field-programmable gate array;computer engineering	EDA	8.599021002500146	51.488854557875655	546
4b546d1600ca92c6d94c76ac9059306580c68140	surveyor's forum: projecting problems		"""elsewhere appears to show considerably more than a good linear relationship between observed quantities, such as program length and programming time, and computed values using the """"length equation"""" and effort measure. Certainly high correlations between two quantities can exist although their actual respective values are nowhere near equal. This is not what is being observed in the software science experiments. Moranda's comment [MoRA78] suggests that regression statistics might provide a more appropriate barometer. Several of the reported studies contain such statistics [e.g., ELSH78a, ELSH78b, ZWEB77, ZWEB79], and some also contain goodness-of-fit analyses using proposed models from software science [ZWEB77, ZWEB79]. While these analyses are by no means the last word and are not presented in the classical form of a hypothesis testing problem, they too support Fitzsimmons and Love's conclusion that the """"theory cannot be dismissed easily."""" Those of us who report results in this area, or in any experimental science, must be careful of the conclusions we draw from our experiments and the manner in which we """"sell"""" them to others. I have personally believed that, in some cases, authors of software science papers have been guilty of overselling their research. In part this is due to the statistical analyses performed and the generally small samples used in the experiments. On the other hand, the fact that the numbers look so good, while tantalizing to be sure, is not necessarily suggestive of a """"systematic defect of the experiments ,"""" to use Fenichel's term. In fact, a 500,000 heads outcome of a million coin tosses is the most likely possibility, given a fair coin! If the experiment were repeated several times with identical outcomes, then I'd be willing to suspect a systematic defect. Software science needs this additional research , both experimental and analytical, to, as Fitzsimmons and Love put it, """"deter-mine the limitations of the theory and understand its foundations better."""""""	experiment;overselling;software bug	H. Christiaen	1979	ACM Comput. Surv.	10.1145/356778.356786	data science;data mining;computer science;surveyor	Logic	-61.18835872844015	-22.045154115059482	547
e3ad48c67696e3a24f7a54fdbfadbc18ba8004e2	a new tool for multi-level partitioning in teradata	star schema;fact table;multi level partitioning	This paper introduces a new tool that recommends an optimized partitioning solution called Multi-Level Partitioned Primary Index (MLPPI) for a fact table based on the queries in the workload. The tool implements a new technique using a greedy algorithm for search space enumeration. The space is driven by predicates in the queries. This technique fits very well the Teradata MLPPI scheme, as it is based on a general framework using general expressions, ranges and case expressions for partition definitions. The cost model implemented in the tool is based on the Teradata optimizer, and it is used to prune the search space for reaching a final solution. The tool resides completely on the client, and interfaces the database through APIs as opposed to previous work that requires optimizer code extension. The APIs are used to simplify the workload queries, and to capture fact table predicates and costs necessary to make the recommendation. The predicate-driven method implemented by the tool is general, and it can be applied to any clustering or partitioning scheme based on simple field expressions or complex SQL predicates. Experimental results given a particular workload will show that the recommendation from the tool outperforms a human expert. The experiments also show that the solution is scalable both with the workload complexity and the size of the fact table.	analysis of algorithms;application programming interface;cluster analysis;experiment;fits;greedy algorithm;mathematical optimization;predicate (mathematical logic);sql;scalability;teradata geospatial;wizard (software)	Young-Kyoon Suh;Ahmad Ghazal;Alain Crolotte;Pekka Kostamaa	2012		10.1145/2396761.2398604	computer science;theoretical computer science;machine learning;star schema;data mining;database;world wide web	DB	-30.74904942968237	3.5254577409482017	548
476dd94bfea6cd03c3fb700d435cdbd94747019f	groupers for deterministic guaranteed service in an fh-ofdm wireless system	metodo adaptativo;wireless networks;telecommunication sans fil;wireless networks real time systems modulation coding frequency shape communication system traffic control intelligent networks adaptive systems resource management quality of service;resource allocation;adaptive modulation;real time;wireless network;multiplexage frequence orthogonal;methode adaptative;indexing terms;approche deterministe;multiplaje frecuencia ortogonal;deterministic approach;modulation adaptative;communication saut frequence;orthogonal frequency division multiplexing;telecomunicacion sin hilo;modulacion adaptativa;mobile radio;adaptive method;ofdm modulation;enfoque determinista;deterministic guaranteed service;access protocols;telecommunication services;guaranteed service;temps retard;asignacion recurso;delay time;telecommunication channels;allocation ressource;orthogonal frequency division multiplexing guaranteed service delay bound real time session wireless network adaptive modulation technique fh ofdm wireless system frequency hopping;wireless systems;tiempo retardo;telecommunication channels telecommunication services delays real time systems adaptive modulation ofdm modulation frequency hop communication radio links mobile radio access protocols resource allocation;frequency hop communication;delays;delay bound;real time systems;radio links;wireless telecommunication	Groupers are proposed to guarantee strict delay bounds of a large number of real-time sessions in advanced wireless networks that adopt adaptive modulation techniques. We show that the number of admitted sessions is increased significantly, if groupers are used.	adaptive grammar;mimo-ofdm;modulation;real-time clock	Kihyun Pyun;Dong-Ho Cho;Hyun-il Lim	2005	IEEE Communications Letters	10.1109/LCOMM.2005.1413637	real-time computing;orthogonal frequency-division multiplexing;telecommunications;computer science;wireless network;computer network	Mobile	3.937529808378334	101.45917578538482	549
49bb46ef39694f48de20bb6034ea184114eba23b	modeling of stalkers' behavior and development of simulated experience game for education		This paper tackles stalking problem that has become socially serious. Various approaches have been proposed to prevent becoming a victim of stalking behavior, but there are few ones to prevent becoming a stalker. In this paper, we proposed a mathematical model of stalkers' behavior. Next on the basis of the model, we proposed a game for education which provides simulated experience of stalkers. It helps students to improve awareness in order to prevent becoming a stalker. Then applying the game in education, we showed that 75 percent of the students had changed their perception about stalking problem.	mathematical model	Shingo Yamaguchi;Yoshiko Tamura	2017	2017 IEEE 6th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2017.8229452	stalking;social psychology;law enforcement;petri net;political science	Robotics	-89.24503019977259	-19.612169017132317	550
44b1ab06847f015e2c9cf221670bf4160aa6aa7f	transmit power efficiency of multi-hop mrc diversity for a virtual cellular network	tecnologia electronica telecomunicaciones;transmit power control;maximal ratio combiner;power efficiency;virtual cellular network;mobile communication;tecnologias;grupo a;computer simulation;mobile terminal;core network	In virtual cellular network (VCN), proposed for highspeed packet mobile communications, the signal transmitted from a mobile terminal is received by wireless ports distributed in each virtual cell and relayed to the central port that acts as a gateway to the core network. In this letter, we apply the multi-hop maximal ratio combining (MHMRC) diversity and propose the route modification algorithm in order to improve transmit power efficiency degradation caused by the carrier frequency difference between the control and the data communication channels for VCN. The transmit power efficiency and the distribution of the number of hops are evaluated by computer simulation for a VCN. key words: virtual cellular network, multi-hop MRC diversity, transmit power control, wireless multi-hop	algorithm;carrier frequency;computer simulation;elegant degradation;hop;maximal set;mobile phone;network packet;performance per watt;virtual cell	Imane Daou;Eisuke Kudoh;Fumiyuki Adachi	2005	IEICE Transactions	10.1093/ietcom/e88-b.9.3643	computer simulation;core network;electrical efficiency;mobile telephony;telecommunications;computer science;computer network	Mobile	28.083528699517387	87.30490147242747	551
729d42b953603c911404d623178296a74617874d	operations in space: exploring a new industry		This article sets the stage for examining operations management research opportunities in the emerging industry that involves operations in outer space. Currently, space exploration is moving in new and exciting directions, thanks to private investment and a more collaborative, commercial industry structure. While we do not yet know if space will be the next big industry, we make the case that the potential is certainly there. In this article, we outline the challenges presented within the “space industry” and highlight opportunities for operations management researchers in three categories—manufacturing operations, supply chain management, and sustainable operations. We also outline important questions related to stakeholder decisions and needs. [Submitted: November 28, 2017. Revised: March 17, 2018. Accepted: March 19, 2018.] Subject Areas: Operations Management, Research, Space Industry, and Supply Chain.	behavior;categories;hl7publishingsubsection <operations>;outline (list);realms of the haunting;risk management;united states national aeronautics and space administration	J. Wooten;Christopher S. Tang	2018	Decision Sciences	10.1111/deci.12312	economics;management science;engineering management;supply chain;space industry	DB	-73.25881409251849	-4.346050200922384	552
1f1a4ea81013f4c580112ea96c6d8c969e4da580	learning discrete probability distributions with a multi-resolution binary tree	loi discrete;bayes estimation;discrete distribution;ley discreta;distribution donnee;control difusa;teoria sistema;loi conjointe;fuzzy control;intelligence artificielle;aprendizaje probabilidades;analyse multiresolution;data distribution;estimacion bayes;systems theory;arbol binario;theorie systeme;probability distribution;arbre binaire;ley conjunta;apprentissage probabilites;artificial intelligence;sistema difuso;inteligencia artificial;systeme flou;multi resolution;multiresolution analysis;distribucion dato;fuzzy system;probability learning;analisis multiresolucion;joint distribution;estimation bayes;commande floue;binary tree	In this paper a method for learning and representing joint probabilistic distributions, using binary trees, is shown. This method could be used with the Bayesian Programming formalism, being a very useful tool when working with real world data. It has the advantage of learning unknown probabilistic distributions directly from raw data, and to remain more balanced than other previous methods. Finally, an application to learn a fuzzy control system, using this approach, will be presented.	binary tree	F. A. Sanchís;Fidel Aznar Gregori;Mireia Sempere;Maria Del Mar Pujol López;Ramón Rizo Aldeguer	2006		10.1007/11875581_57	random binary tree;probability distribution;computer science;artificial intelligence;machine learning;mathematics;fuzzy control system;statistics	Logic	9.137365573575613	-30.273317960770747	553
bf94aba3c106f0568b881e00ea689105ef97cc61	the on-line coverage algorithm and localization technique of the intelligent cleaning robot	robot sensing systems;gyroscopes;sensor systems;complete coverage algorithm;motion control;critical point;intelligent robots;mobile robot;point to point;extended kalman filter online coverage algorithm intelligent cleaning robot mobile robot complete coverage algorithm choset algorithm motion patterns encoders gyroscope ultrasonic sensors accelerometers pose measure system sensor fusion;gyroscope;ultrasonic variables measurement;measurement system;service robots;mobile robots;pose measure system;online coverage algorithm;ultrasonic sensors;motion patterns;indoor environment;transportation;indoor environments;encoders;collision avoidance;sensor fusion;service robots collision avoidance mobile robots motion control;extended kalman filter;local minima;accelerometers;choset algorithm;intelligent cleaning robot;cleaning;intelligent robots cleaning robot sensing systems sensor systems gyroscopes ultrasonic variables measurement sensor fusion mobile robots transportation indoor environments;ultrasonic sensor	In the past mobile robot research was often focused to various kinds of point-to-point transportation tasks. Floor cleaning require the complete-coverage of an unstructured environment (real indoor environments). The complete-coverage algorithm herein proposed is Choset's algorithm. The system determines the distance between robot and obstacles by calculating the local minima of the range measurements with 16 sensors, and senses the critical point. Then Choset's algorithm can be executed. In order to deal with complex local environment, the system adopts local-coverage algorithm. The orientation of the robot must be considered whether using the complete-coverage algorithm or using the typical motion patterns. The robot uses a sensor package that includes encoders, a gyroscope, and 16 ultrasonic sensors. Thereinto, encoders, accelerometers, and gyroscope is used as a pose measure system, where sensor fusion is accomplished using an extended Kalman filter. 16 ultrasonic sensors are used to measure the distance to the obstacle.	3d floor plan;algorithm;critical point (network science);encoder;extended kalman filter;gyroscope;maxima and minima;mobile robot;online and offline;plasma cleaning;point-to-point (telecommunications);sensor;sputter cleaning	Jieru Chi;Guowei Yang;Jie Yang	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525352	control engineering;mobile robot;embedded system;monte carlo localization;computer vision;gyroscope;computer science;engineering;artificial intelligence;control theory;ultrasonic sensor	Robotics	55.374901419713865	-33.92900813755494	554
479dbf5d828c6cb4ef90bbc0a08c5e5a641aee27	holding a page: enhanced page level access control for database systems	access control;database system;shared memory	A new scheme for physically accessing tuples by multiple transactions is described for database systems using a shared memory buuer cache. Using this scheme, a transaction must rst obtain logical permission to access a tuple, for example, by taking a lock on the tuple. Once logical permission has been granted, a page level latch is taken for the initial physical access to the tuple. However, by the introduction of a new primitive, hold, into the latching system which ensures that the tuple is not moved, a latch on the page is not required to subsequently read the tuple. This brings the physical level of concurrency closer to the logical level of concurrency. Our scheme is compared with other schemes which can be used for access control.	access control;concurrency (computer science);database;physical access;shared memory	Evan P. Harris;Kotagiri Ramamohanarao	1996			world wide web;computer science;access control;static web page;database;page attribute table;page fault;shared memory	DB	-22.26559323528821	47.428546482330766	555
f7566cc559d704ac2f25d1f8981ae255738ac4c8	a calculus for concurrent system with higher-order streaming communication	streaming;labeled transition system;operational semantics;higher order π calculus;higher order φ calculus;higher order;equivalence relation;concurrency;concurrent systems;asynchronous communication;mobile code;wide area network	This paper presents a formal model of concurrent system that is equipped with capabilities of sending and receiving higher-order terms. That is a modification of the asynchronous higher order π-calculus. A new operation : input streaming is introduced. An input process consists of an input stream and a process P . It can receive a higher order term t during the execution of P . Input prefix and output process are also modified to represent non-atomic communication. The calculus models computations transferring mobile codes and links on a wide-area network in asynchronous manner. A labeled transition system (lts) is presented for the operational semantics. Equivalence relations based on the lts are intorduced. The equivalences are based on the idea of berbed bisimulation that is suitable for non atomic/asynchronous communicationg systems.	bisimulation;code;computation;concurrency (computer science);concurrent computing;mathematical model;operational semantics;p (complexity);perturbation theory;stream (computing);streaming media;transition system;turing completeness	Masaki Murakami	2004		10.1016/j.scico.2004.10.005	process calculus;higher-order logic;concurrency;computer science;theoretical computer science;asynchronous communication;distributed computing;equivalence relation;programming language;operational semantics;algorithm	PL	-28.536335256140593	32.619504968633905	556
2bd01841f19b33e0bc87797f4292b68a25b90d7e	predicting a user's next cell with supervised learning based on channel states	accuracy handover history support vector machines indexes wireless communication;mobility management mobile radio;support vector machines;cellular radio;telecommunication computing;manhattan grid scenario user next cell prediction supervised learning resource allocation location aware services machine learning based prediction system classification problem cellular networks channel state information csi handover history support vector machines svm pre processing structure;telecommunication computing cellular radio learning artificial intelligence mobility management mobile radio support vector machines;learning artificial intelligence	Knowing a user's next cell allows more efficient resource allocation and enables new location-aware services. To anticipate the cell a user will hand-over to, we introduce a new machine learning based prediction system. Therein, we formulate the prediction as a classification problem based on information that is readily available in cellular networks. Using only Channel State Information (CSI) and handover history, we perform classification by embedding Support Vector Machines (SVMs) into an efficient pre-processing structure. Simulation results from a Manhattan Grid scenario and from a realistic radio map of downtown Frankfurt show that our system provides timely prediction at high accuracy.	channel state information;location awareness;machine learning;preprocessor;simulation;supervised learning;support vector machine;synthetic intelligence	Xu Chen;François Mériaux;Stefan Valentin	2013	2013 IEEE 14th Workshop on Signal Processing Advances in Wireless Communications (SPAWC)	10.1109/SPAWC.2013.6612007	support vector machine;simulation;computer science;machine learning;data mining;active learning;computer network	ML	24.086200128230296	96.2885123858895	557
988c0a7ca06ddf06f076145efc0aaf42656691b6	defining and assessing safety functions performed by people	human reliability assessment;pedestrian safety;poison control;injury prevention;key safety actions;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;safety functions;ergonomics;suicide prevention	The design and safety assurance of systems require good requirements management. Within the nuclear industry, Safety Functional Requirements (SFRs) have become well-understood and form a central role as the design and safety case evolve. There are established engineering methods for the identification, classification, format and substantiation of SFRs. Despite striving for engineered protection systems, humans continue to provide, support and contribute to this protection. Identifying the human ‘component’ in systems has advantages in terms of integrating with non-Human Factors disciplines, improving awareness of human factors within safety and design, and ultimately making system design safer. This paper describes the identification and assessment of safety functions that are performed by people. It presents some of the difficulties in trying to model and predict human behaviour compared to engineered system components. With emphasis on the nuclear industry, the paper discusses how the relationship between detailed hazard analysis and formally vested Operating Rules can be improved through the identification of safety functions performed by people.	functional requirement;hazard analysis;human factors and ergonomics;human factors integration;humans;interaction;internet backbone;requirements management;systems design	Andrew S. Bardsley	2012	Cognition, Technology & Work	10.1007/s10111-012-0214-y	medicine;environmental health;engineering;suicide prevention;human factors and ergonomics;injury prevention;transport engineering;forensic engineering;system safety;computer security;mechanical engineering	SE	-11.165993159880767	-15.964911560791242	558
92dcfd7baeefa07ace3a8286d9e3fbbdebcf013d	a new delay-independent condition for global robust stability of neural networks with time delays	delayed systems;lyapunov functionals;neural networks;robust stability	This paper studies the problem of robust stability of dynamical neural networks with discrete time delays under the assumptions that the network parameters of the neural system are uncertain and norm-bounded, and the activation functions are slope-bounded. By employing the results of Lyapunov stability theory and matrix theory, new sufficient conditions for the existence, uniqueness and global asymptotic stability of the equilibrium point for delayed neural networks are presented. The results reported in this paper can be easily tested by checking some special properties of symmetric matrices associated with the parameter uncertainties of neural networks. We also present a numerical example to show the effectiveness of the proposed theoretical results.	activation function;artificial neural network;checking (action);dynamical system;lyapunov fractal;neural network simulation;numerical analysis;population parameter	Ruya Samli	2015	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2015.03.004	stochastic neural network;mathematical optimization;computer science;machine learning;control theory;mathematics;artificial neural network	ML	73.39621905337908	2.2577700585429996	559
776a27f1365bdc22b432a17c76182c15918e3dea	authenticated multiple key exchange protocols based on elliptic curves and bilinear pairings	one way hash function;elliptic curve;authentication;authenticated key exchange;key exchange;cryptography;bilinear pairing;security	Menezes et al. developed an MQV key exchange protocol that does not use a one-way hash function in 1995. Harn–Lin and Hwang–Shiau–Lai later respectively proposed efficient authenticated key exchange protocols. Lee and Wu recently proposed an enhanced authentication key exchange protocol to solve the drawbacks of the Hwang–Shiau–Lai protocol. Based on the Lee–Wu protocol, this work presents two new authenticated multiple key exchange protocols based on ECC and bilinear pairing. The proposed ECC-based protocol is more efficient than the Harn–Lin, Hwang–Shiau–Lai, and Lee–Wu protocols. Moreover, the proposed pairing-based protocol is better than other protocols in terms of the number of available shared session keys because all agreed session keys can be adopted by the communicating parties in the protocol. 2007 Elsevier Ltd. All rights reserved.	authenticated key exchange;authentication;bilinear filtering;communications protocol;cryptographic hash function;ecc memory;forward secrecy;internet protocol suite;kernighan–lin algorithm;key-agreement protocol;known-key distinguishing attack;local interconnect network;one-way function	Narn-Yih Lee;Chien-Nan Wu;Chien-Chih Wang	2008	Computers & Electrical Engineering	10.1016/j.compeleceng.2006.11.005	oakley protocol;yak;key exchange;computer science;cryptography;information security;key-agreement protocol;diffie–hellman key exchange;key management;authentication;mqv;distributed computing;internet privacy;elliptic curve;key distribution;computer security	Security	-43.38423691968066	75.2527262302155	560
5a97a3d51600326887b3938dbc1223650b27bb50	tele-sports and tele-dance: full-body network interaction	distributed system;data sharing;input device;virtual reality;physical activity;motion capture;collaborative environment;networking;pc cluster;collaborative virtual environment;open source software;virtual worlds;immersive virtual environment	Researchers have had great success using motion capture tools for controlling avatars in virtual worlds. Another current of virtual reality research has focused on building collaborative environments connected by networks. The present paper combines these tendencies to describe an open source software system that uses motion capture tools as input devices for realtime collaborative virtual environments. Important applications of our system lie in the realm of simulating interactive, multiparticipant physical activities like sport and dance. Several challenges and their respective solutions are outlined. First, we describe the infrastructure necessary to handle full-body articulated avatars as driven by motion capture equipment, including calibration and avatar creation. Next, we outline the PC cluster solution chosen to render our worlds, exploring methods of data sharing and synchronization, both within the PC cluster nodes and between different sites in the distributed system. Finally, virtual sports require physics, and we describe the simulation algorithms used.	algorithm;collaborative virtual environment;computer cluster;distributed computing;input device;motion capture;open-source software;simulation;software system;television;virtual reality;virtual world	Benjamin Schaeffer;Mark Flider;Hank Kaczmarski;Luc Vanier;Lance Chong;Yu Hasegawa-Johnson	2003		10.1145/1008653.1008673	motion capture;simulation;human–computer interaction;computer science;artificial intelligence;operating system;virtual reality;multimedia;physical fitness;input device	Visualization	-43.262879032589396	-36.23379299869238	561
2009a8aa01c7e7c38ba211973a92bbd0b5d240e5	experiences with the intel hypercube	comparative analysis;parallel algorithm;communication cost	This paper presents a comparative analysis of the performance of two different parallel algorithms for solving the prefix problem on the Intel Hypercube. Estimates of the key parameters that determine the cost of communication between nodes are also given. It turns out that the ratio of communication cost to that of unit computation is quite high (is in the range of 240 to 470) for this class of machines.	computation;parallel algorithm;qualitative comparative analysis	William J. Ouchark;Jay A. Davis;S. Lakshmivarahan;Sudarshan K. Dhall	1986		10.1145/800239.807163	parallel computing;computer science;theoretical computer science;distributed computing	HPC	0.00281864448556809	37.295654187349456	562
72315c558ff403170060491aabc4fa2ec5a03e05	robust recommender systems	recommender system;collaborative filtering;robustness;security;recommender systems	This tutorial will discuss vulnerabilities of collaborative recommendation algorithms: attacks that can be mounted against them and possible defenses that can be used. The tutorial will be of interest to researchers and practitioners in the area of collaborative recommendation.	algorithm;recommender system	Robin D. Burke	2008		10.1145/1454008.1454066	computer science;collaborative filtering;machine learning;data mining;internet privacy;world wide web;robustness;recommender system	ECom	-56.11735709792359	63.12234173921288	563
74773a6e737dd05e16b2b18079feff841a99a875	generalized antisymmetric filters for edge detection	edge detection filters generalized antisymmetric filters local gradients grayscale images tonal difference dissimilarity functions mathematical developments;convolution;image edge detection convolution proposals feature extraction computational modeling approximation methods pattern recognition;computational modeling;convolution filter image processing edge detection restricted dissimilarity function;image edge detection;feature extraction;pattern recognition;mathematical analysis edge detection image colour analysis image filtering;approximation methods;proposals	A large number of filters has been proposed to compute local gradients in grayscale images, usually having as goal the adequate characterization of edges. A significant portion of such filters are antisymmetric with respect to the origin. In this work we propose to generalize those filters by incorporating an explicit evaluation of the tonal difference. More specifically, we propose to apply restricted dissimilarity functions to appropriately measure the tonal differences. We present the mathematical developments, as well as quantitative experiments that indicate that our proposal offers a clear option to improve the performance of classical edge detection filters.	backup;computation;edge detection;experiment;feature extraction;gradient;grayscale;norm (social)	Nicolás Madrid;Carlos Lopez-Molina;Bernard De Baets	2013	2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2013.7054137	computer vision;edge detection;feature extraction;computer science;machine learning;pattern recognition;mathematics;convolution;computational model	Vision	52.003453594224624	-65.87188926528565	564
3926190a19f2cb9c0a2bd7553b207cb9e0bce244	verifying correct microarchitectural enforcement of memory consistency models	microarchitecture pipelines buffer storage analytical models program processors load modeling radio frequency;analytical models;system on chip memory architecture multi threading;microarchitecture;processor microarchitecture;pipecheck;buffer storage;computer architecture;formal verification;radio frequency;pipecheck correct microarchitectural enforcement memory consistency models memory references multithreaded cpu systems on chip;memory architecture;pipelines;formal verification memory consistency model pipecheck computer architecture memory architecture processor microarchitecture;load modeling;memory consistency model;program processors	Memory consistency models define the rules and guarantees about the ordering and visibility of memory references on multithreaded CPUs and systems on chip. PipeCheck offers a methodology and automated tool for verifying that a particular microarchitecture correctly implements the consistency model required by its architectural specification.	central processing unit;consistency model;microarchitecture;system on a chip;thread (computing);verification and validation	Daniel Lustig;Michael Pellauer;Margaret Martonosi	2015	IEEE Micro	10.1109/MM.2015.47	cache coherence;computer architecture;parallel computing;real-time computing;formal verification;microarchitecture;computer science;consistency model;pipeline transport;programming language;radio frequency;sequential consistency	Arch	7.538904585309209	52.625754797437196	565
a3308790c2cf276be92ba25f2edaad0ba9c1725d	a survey of random access control techniques for machine-to-machine communications in lte/lte-a networks		Machine-to-machine (M2M) communications refer to the autonomous interaction between connected devices without the human intervention. Recently, the Third-Generation Partnership Project (3GPP) introduced the Long-Term Evolution (LTE) and Long-Term Evolution-Advanced (LTE-A) as the improved version of the fourth-generation (4G) cellular networks. LTE/LTE-A networks have been considered an appropriate infrastructure for implementing M2M communications. However, the LTE/LTE-A networks were originally introduced for human-to-human (H2H) communications which have different characteristics from M2M communications. Thus, the LTE/LTE-A networks must be adapted to support the special characteristics of M2M communications. This work provides a comprehensive review of M2M communications over LTE networks, including M2M architectures, LTE structure, deployment challenges, and access control requirements. Moreover, this work introduces a novel classification for the current Random Access (RA) techniques that have been proposed for M2M communications in LTE networks. According to the main targeted objective, the current RA techniques are classified into three categories: massive access control techniques, energy efficiency techniques, and performance improvement techniques. Each category is further divided into two subcategories, and the relevant RA techniques are presented for each category. Furthermore, an analytical comparison has been provided among the different techniques according to the parameters for each approach. This work can be considered a good starting point for those who are interested in RA techniques for M2M communications over LTE/LTE-A networks.		Huda Althumali;Mohamed Othman	2018	IEEE Access	10.1109/ACCESS.2018.2883440	access control;software deployment;lte advanced;computer network;telecommunications link;distributed computing;computer science;logic gate;random access;machine to machine;cellular network	Mobile	22.665823591868577	89.42474156122972	566
cf86551b0e3203e10d375bdbea225cc6014e8d9a	comparing the predictive capability of social and interest affinity for recommendations	interest affinity;collaborative filtering;social affinity;evaluation;recommender systems	The advent of online social networks created new prediction opportunities for recommender systems: instead of relying on past rating history through the use of collaborative filtering (CF), they can leverage the social relations among users as a predictor of user tastes similarity. Alas, little effort has been put into understanding when and why (e.g., for which users and what items) the social affinity (i.e., how well connected users are in the social network) is a better predictor of user preferences than the interest affinity among them as algorithmically determined by CF, and how to better evaluate recommendations depending on, for instance, what type of users a recommendation application targets. This overlook is explained in part by the lack of a systematic collection of datasets including both the explicit social network among users and the collaborative annotated items. In this paper, we conduct an extensive empirical analysis on six real-world publicly available datasets, which dissects the impact of user and item attributes, such as the density of social ties or item rating patterns, on the performance of recommendation strategies relying on either the social ties or past rating similarity. Our findings represent practical guidelines that can assist in future deployments and mixing schemes.	affinity analysis;algorithm;collaborative filtering;kerrison predictor;mixing (mathematics);processor affinity;recommender system;social network;user (computing)	Alexandra Olteanu;Anne-Marie Kermarrec;Karl Aberer	2014		10.1007/978-3-319-11749-2_22	computer science;artificial intelligence;collaborative filtering;evaluation;data mining;database;world wide web;recommender system	Web+IR	-20.28686692795397	-45.772116829454966	567
3d5a1c54881351115b74f8a24680df2e20a4f7fd	electrocortical amplitude modulations of human level-ground, slope, and stair walking		This study investigates if the electrocortical amplitude modulations relative to the mean gait cycle are different across walking conditions (i.e., level-ground (LW), ramp ascent (RA), and stair ascent (SA)). Non-invasive electroencephalography (EEG) signals were recorded and a systematic EEG processing method was implemented to reduce artifacts. Source localization using independent component analysis and k-means clustering revealed the involvement of four clusters in the brain (Left and Right Occipital Lobe, Posterior Parietal Cortex (PPC), and Sensorimotor Area) during the walking tasks. We found that electrocortical amplitude modulations varied across different walking conditions. Specifically, our results showed that the modulations in the PPC shifted to higher frequency bands when the subjects walked in RA and SA conditions. Moreover, we found low γ modulations in the sensorimotor area in LW walking and the modulations in this cluster shifted to lower frequency bands in RA and SA walking. These results are a promising step toward the development of a non-invasive Neural-machine Interface (NMI) for locomotion mode recognition.	acsm3 gene;acoustic lobing;bands;cerebral cortex;cluster analysis;electroencephalography phase synchronization;epilepsy, temporal lobe;frequency band;gradient descent;hypothalamic area, lateral;independent component analysis;k-means clustering;limewire;morphologic artifacts;neural tube defects;non-maskable interrupt;ramp simulation software for modelling reliability, availability and maintainability;sensorimotor cortex;sulfanilamide;times ascent;traffic collision avoidance system;statistical cluster	Trieu Phat Luu;Justin A. Brantley;Fangshi Zhu;José Luis Contreras-Vidal	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037222	artificial intelligence;occipital lobe;computer vision;posterior parietal cortex;modulation (music);electroencephalography;communication;gait;amplitude;pattern recognition;psychology	Robotics	16.72937247388297	-80.39643012899089	568
77f8d7575819acb1d1fb43160d2b9c91e8bb4b11	automated polyp segmentation in colonoscopy frames using fully convolutional neural network and textons		In this paper, we presented a novel hybrid classification based method for fully automated polyp segmentation in colonoscopy video frames. It contains two main steps: initial region proposals generation and regions refinement. Both machine learned features and hand crafted features are taken into account for polyp segmentation. More specifically, the hierarchical features of polyps are learned by fully convolutional neural network (FCN), while the context information related to the polyp boundaries is modeled by texton patch representation. The FCN provides pixel-wise prediction and initial polyp region candidates. Those candidates are further refined by patch-wise classification using texton based spatial features and a random forest classifier. The segmentation results are evaluated on a publicly available CVC-ColonDB database. On average, our method achieves 97.54% of accuracy, 75.66% of sensitivity, 98.81% of specificity and DICE of 0.70%. The fast execution time (0.16 s/frame) demonstrates the promise of our method to be used in real-time clinical colonoscopic examination.	convolutional neural network	Lei Zhang;Sunil Dolwani;Xujiong Ye	2017		10.1007/978-3-319-60964-5_62	convolutional neural network;colonoscopy;dice;texton;random forest;computer vision;segmentation;artificial intelligence;computer science	Vision	31.942924530281992	-75.42434078988761	569
d5cfc468b0bd401aa5c10e6c48d80e75fbfe8a35	diagnosis for uncertain, dynamic and hybrid domains using bayesian networks and arithmetic circuits	cusum;prodiagnose;bepress selected works;hybrid;bayesian networks arithmetic circuits diagnosis electrical power systems adapt nasa aerospace;arithmetic circuits;diagnosis;bayesian networks	a r t i c l e i n f o a b s t r a c t System failures, for example in electrical power systems, can have catastrophic impact on human life and high-cost missions. Due to an electrical fire in Swissair flight 111 on September 2, 1998, all 229 passengers and crew on board sadly lost their lives. A battery failure most likely took place on the Mars Global Surveyor, which unfortunately last communicated with Earth and thus ended its mission on November 2, 2006. Fault diagnosis techniques that seek to hinder similar accidents in the future are being developed in this article. We present comprehensive fault diagnosis methods for dynamic and hybrid domains with uncertainty, and validate them using electrical power system data. Our approach relies on the use of Bayesian networks, which model the electrical power system, compiled to arithmetic circuits. We handle in an integrated way varying fault dynamics (both persistent and intermittent faults), fault progression (both abrupt and drift faults), and fault behavior cardinality (both discrete and continuous behaviors). Our work has resulted in a software system for fault diagnosis, ProDiagnose, that has been the top performer in three of the four international diagnostics competitions in which it participated. In this paper we comprehensively present our methods as well as novel and extensive experimental results on data from a NASA electrical power system.	arithmetic circuit complexity;bayesian network;color gradient;compiler;fire class;ibm power systems;integrated circuit;software system	Brian Ricks;Ole J. Mengshoel	2014	Int. J. Approx. Reasoning	10.1016/j.ijar.2014.02.005	real-time computing;simulation;hybrid;computer science;artificial intelligence;cusum;machine learning;bayesian network;statistics	AI	13.372233150618678	-14.580925630097616	570
e3d7242c895759b1235f5ff7651dd6db488acce5	representing temporal relationships between events and their effects	truth value;event relationships;action theory;temporal logic temporal relationship representation event relationships truth value proposition causal event delayed effects action theory change temporal causal relationships common sense assertions event calculus temporal reasoning;event calculus;temporal logic;delay effects;temporal reasoning temporal logic process algebra;pressing;change;causal event;qa75 electronic computers computer science;proposition;delay effects calculus ontologies pressing;calculus;temporal causal relationships;delayed effects;temporal relationship representation;ontologies;common sense;common sense assertions;process algebra;temporal reasoning;qa76 computer software	"""Temporal relationships between events and their effects are complex. As the ejjects of a given event, a proposition may change its truth value immediately after the occurrence of the event and remain true until some other events occur, while another proposition m y only become trueJalse from some time after the causal event has occurred. Expressing delayed @ects of events has been a problematic question in most existing theories of action and change. This paper presents a new formalism for representing general temporal causal relationships between events and their effects. It allows expressions of both immediate and delayed effects of events, and .supports common-sense assertions such as """"effects cannot precede their causes""""."""	causal filter;causality;semantics (computer science);side effect (computer science);synchronicity;theory	Jixin Ma;Brian Knight;Taoxin Peng	1997		10.1109/TIME.1997.600796	discrete mathematics;artificial intelligence;mathematics;algorithm	AI	-17.395009157593098	7.136823169508295	571
491a1fedfce1e7b9ad23ca7861549b37395a31c3	a bottom-up approach for learning visual object detection models from unreliable sources		The ability to learn models of computational vision from sample data has significantly advanced the field. Obtaining suitable train- ing image sets, however, remains a challenging problem. In this paper we propose a bottom-up approach for learning object detection models from weakly annotated samples, i.e., only category labels are given per image. By combining visual saliency and distinctiveness of local image features regions of interest are extracted in a completely automatic way without requiring detailed annotations. Using a bag-of-features representation of these regions, object recognition models can be trained for the given object categories. As weakly labeled sample images can easily be ob- tained from image search engines, our approach does not require any manual annotation effort. Experiments on data from the Visual Object Classes Challenge 2011 show that promising object detection results can be achieved by our proposed method.		Fabian Nasse;Gernot A. Fink	2012		10.1007/978-3-642-32717-9_49	computer vision;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition	Vision	31.335505975509694	-51.12136858608175	572
72641baddb140ecd4f97b363969fe4f0931a418a	noise aware decoupling capacitors for multi-voltage power distribution systems	minimisation;power distribution system;power supplies;microprocessors;impedance minimization;impedance;power consumption distribution networks electric potential power supply quality capacitors circuit noise random noise minimisation electric impedance inductance transfer functions;circuit noise;delivery system;transfer functions;distribution networks;power systems;power supply quality;power capacitors;electric impedance;voltage transfer function;power distribution;signal integrity;power supply;dual voltage power distribution system;random noise;signal integrity problems;overshoot free voltage response;voltage transfer function noise aware decoupling capacitors multi voltage power distribution systems multiple power supplies high performance ic microprocessors power consumption power integrity problems signal integrity problems power distribution networks impedance minimization effective series inductance dual voltage power distribution system overshoot free voltage response;energy consumption;transfer function;capacitors;high performance ic;voltage;multiple power supplies;decoupling capacitor;noise aware decoupling capacitors;electric potential;inductance;power consumption;frequency;effective series inductance;power distribution networks;high performance;multi voltage power distribution systems;power integrity problems;power distribution network;power capacitors power distribution power supplies voltage circuit noise frequency microprocessors energy consumption impedance power systems	Multiple power supply voltages are often used in modern high performance ICs, such as microprocessors, to decrease power consumption without affecting circuit speed. The system of decoupling capacitors used in power distribution systems with multiple power supplies is described. In order to minimize the total impedance of a multi-voltage power delivery system as seen from a particular power supply, a decoupling capacitor is placed between the power supplies. The noise at one power supply can couple into the other power supply, causing power and signal integrity problems in the overall system. With the introduction of a second power supply, therefore, the interaction between the two power distribution networks should be considered. The dependence of the magnitude of the voltage transfer function on the parameters of the power distribution system is investigated. It is shown that it is highly desirable to maintain the effective series inductance of the decoupling capacitors as low as possible to decrease the overshoot in the response of a dual voltage power distribution system over a wide range of operating frequencies. A criterion for an overshoot-free voltage response is presented. It is noted that the frequency range of the overshoot-free voltage response can be traded off with the magnitude of the response.	characteristic impedance;coupling (computer programming);frequency band;microprocessor;overshoot (signal);power supply;signal integrity;transfer function	Mikhail Popovich;Eby G. Friedman	2005	Sixth international symposium on quality electronic design (isqed'05)	10.1109/ISQED.2005.84	power supply rejection ratio;power gain;control engineering;power module;power budget;power-flow study;electronic engineering;switched-mode power supply applications;power factor;power control;engineering;sense;electrical engineering;constant power circuit;decoupling;switched-mode power supply;ac power;transfer function;capacitive power supply;volt-ampere;voltage optimisation;quantum mechanics;decoupling capacitor;voltage regulation	Arch	64.06794110305025	44.073397946083865	573
9ae8383b46b8f62e920177bd1ffaa601ccf5d698	combining linguistic knowledge and acoustic information in automatic pronunciation lexicon generation	error rate;word recognition;language model	This paper describes several experiments aimed at the long term goal of enabling a spoken conversational system to automatically improve its pronunciation lexicon over time through direct interactions with end users and from available Web sources. We selected a set of 200 rare words from the OGI corpus of spoken names, and performed several experiments combining spelling and pronunciation information to hypothesize phonemic baseforms for these words. We evaluated the quality of the resulting baseforms through a series of recognition experiments, using the 200 words in an isolated word recognition task. We also report here on a modification to our letter-to-sound system, utilizing a letter-phoneme -gram language model, either alone or in combination with our original “column-bigram” model, for additional linguistic constraint and robustness. Our experiments confirm our expectation that acoustic information drawn from spoken examples of the words can greatly improve the quality of the baseforms, as measured by the recognition error rate. Our ultimate goal is to allow a spoken dialogue system to automatically expand and improve its baseforms over time as users introduce new words or supply spoken pronunciations of existing words.	acoustic cryptanalysis;bigram;dialog system;experiment;interaction;language model;lexicon;protologism;spoken dialog systems	Grace Chung;Chao Wang;Stephanie Seneff;Edward Filisko;Min Tang	2004			robustness (computer science);end user;speech recognition;word error rate;language model;natural language processing;linguistics;computer science;artificial intelligence;spelling;word recognition;pronunciation;lexicon	NLP	-20.368360277005166	-84.13912840773823	574
e852c594491ddd5bbccd24ef4f5d47cf23e33815	using unlabeled data to improve classification in the naive bayes approach: application to web searches	unlabeled data;web pages;naive bayes;classification;info eu repo semantics article;discriminant analysis;text learning;materias investigacion economia y empresa;www searches;em algorithm	This paper introduces a method to build a classifier based on labeled and unlabeled data. We set up the EM algorithm steps for the particular case of the naive Bayes approach and show empirical work for the restricted web page database. Original contributions includes the application of the EM algorithm to simulated data in order to see the behavior of the algorithm for different numbers of labeled and unlabeled data, and to study the effect of the sampling mechanism for the unlabeled data on the results. Stella M. Salvatierra Universidad de Navarra Facultad de Ciencias Económicas y Empresariales Departamento de Métodos Cuantitativos 31080 Pamplona ssalvat@unav.es USING UNLABELED DATA TO IMPROVE CLASSIFICATION IN THE NAIVE BAYES APPROACH: APPLICATION TO WEB SEARCHES Stella M. Salvatierra Universidad de Navarra Spain ssalvat@unav.es	algorithmic inference;converge;expectation–maximization algorithm;iteration;naive bayes classifier;stella;sampling (signal processing);statistical classification;web page;world wide web	Stella M. Salvatierra	2004	J. Comput. Meth. in Science and Engineering		computer science;machine learning;data mining;world wide web	ML	-19.082469652063782	-62.112050936272276	575
2f58cf9af68fb8baaa9f63ba46980a25030d68dc	motivating users to build heritage collections using games on social networks		Efforts to motivate user participation and contribution towards digital libraries, such as heritage collections, are often unsuccessful, resulting in empty or underutilized collections. These collections have the potential to improve heritage preservation and education. However, without growth, they are of little use to society. Using a Facebook application, different techniques were compared for motivating user participation and contribution of content towards a heritage collection. It was found that direct competition outperformed a badge system and successfully motivated users to contribute. These results are particularly interesting since, in a developing country, such as where this research was carried out, community and collaboration are usually valued and favoured over competition.	digital library;library (computing);social network	Michelle Havenga;Kyle Williams;Hussein Suleman	2012		10.1007/978-3-642-34752-8_34	multimedia;world wide web	HCI	-82.81512205460807	-15.998916150225288	576
2c87dd0367bce868f3e11494474afdcfbd26cbf9	cycles through edges in cyclically k-connected cubic graphs	graph girth;graphe cubique;cycle graphe;edge graph;arete graphe;cycle graph;calibre graphe;grafo cubico;arista grafico;graphe cycliquement k connexe;cubic graph;ciclo diagrama	McCuaig, W., Cycles through edges in cyclically k-connected cubic graphs, Discrete Mathematics 103 (1992) 95-98. Jet F be a set of k independent edges in a cyclically (k + 1)-connected cubic graph G. We prove that if G has girth at least [k2/4] + 1, then the edges in F lie on a cycle. We also prove that if the distance between every pair of ends of edges in F is at least k 2, then the edges in F lie on a cycle. In this paper we use the notation and terminology of Bondy and Murty [3]. An edge cut S of a graph G is cycle-separating if two components of G S have cycles. A graph is cyclically k-connected if every cycle-separating edge cut has at least k edges. Let 8 be the cubic graph consisting of two vertices joined by three edges. For every cubic graph G not in (0, &, KS,,}, there is a largest k such that G is cyclically k-connected. Let {C,, . . . , C,} be a set of cycles in a cubic graph G. Let E’ be the set of edges in E(G) which are in an odd number of cycles in { C1, . . . , C,}. Define Czr Ci to be G[E’]. It is routine to show that CEi Ci is a disjoint union of cycles. Let A and B be subgraphs of a graph G. An (A, B)-path is a path P with origin in A and terminus in B such that no internal vertex of P is in V(A) U V(B). A set of edges is independent if the edges are pairwise non-adjacent. Thomassen [6] has shown that in any cyclically (2“+‘)-connected cubic graph any k independent edges lie on a cycle. Holton and Thomassen [4] conjecture that in any cyclically (k + 1)-connected cubic graph any k independent edges lie on a cycle. Aldred and Holton [2] define a subgraph A of a graph G to be a free * Support from NSERC is gratefully acknowledged. 0012-365X/92/$05.00	carsten thomassen;configuration interaction;cubic function;cycle (graph theory);discrete mathematics;girth (graph theory);graph (discrete mathematics);n-jet;travis ci;windows legacy audio components	William McCuaig	1992	Discrete Mathematics	10.1016/0012-365X(92)90043-F	combinatorics;discrete mathematics;topology;gray graph;hypercube graph;foster graph;cycle graph;cubic graph;mathematics;butterfly graph;semi-symmetric graph;strength of a graph;coxeter graph	Theory	28.60492638450475	30.880946419048602	577
10f7e18cdb9ce44ddfacd19c60db455e5993e43f	deep reinforcement learning based resource allocation for v2v communications		In this paper, we develop a decentralized resource allocation mechanism for vehicle-to-vehicle (V2V) communications based on deep reinforcement learning, which can be applied to both unicast and broadcast scenarios. According to the decentralized resource allocation mechanism, an autonomous “agent”, a V2V link or a vehicle, makes its decisions to find the optimal sub-band and power level for transmission without requiring or having to wait for global information. Since the proposed method is decentralized, it incurs only limited transmission overhead. From the simulation results, each agent can effectively learn to satisfy the stringent latency constraints on V2V links while minimizing the interference to vehicle-to-infrastructure (V2I) communications.	autonomous agent;autonomous robot;baseline (configuration management);interference (communication);local interconnect network;overhead (computing);reinforcement learning;simulation;unicast;vehicle-to-vehicle;vii	Hao Ye;Geoffrey Ye Li;Biing-Hwang Juang	2018	CoRR		latency (engineering);computer network;resource management;reinforcement learning;computer science;interference (wave propagation);unicast;resource allocation;broadcasting	Robotics	20.89940896700548	94.60193357965062	578
df4cf1540bfd6053bf64794d67b1c54891c9b512	formalization and verification of mobile systems calculus using the rewriting engine maude		BigTiMo calculus is for structure-aware mobile systems and it combines the TiMo calculus and the Bigraph model. Compared with TiMo, BigTiMo can model not only the locations of the components but also the connectivity of the components. Thus, our BigTiMo process can communicate not only locally with other process, but also remotely with other process. In this paper, we introduce the syntax and the operational semantics of the BigTiMo calculus. We also develop an executable formal specification of our Big-TiMo calculus in a declarative language called Maude. In addition, we verify safety properties of the mobile systems described by BigTiMo using state exploration and LTL model checking in Maude.	axiomatic semantics;bigraph;correctness (computer science);declarative programming;executable;formal specification;internet of things;liveness;maude system;model checking;operational semantics;process calculus;rewriting;trustworthy computing	Wanling Xie;Huibiao Zhu;Min Zhang;Gang Lu;Yucheng Fang	2018	2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2018.00034	formal specification;model checking;calculus;semantics;bigraph;declarative programming;operational semantics;executable;computer science;rewriting	Logic	-30.553334504708126	33.19756963550331	579
3c0c55f6950e717cd10aefb351900dd187613d5a	adaptive-compression based congestion control technique for wireless sensor networks	health research;uk clinical guidelines;low priority;data transmission;biological patents;discrete wavelet transform;europe pubmed central;citation search;wireless sensor network;uk phd theses thesis;congestion control;life sciences;congestion;sensor nodes;time domain;queue control;compression;frequency domain;uk research reports;medical journals;europe pmc;biomedical research;adaptive differential pulse code modulation;bioinformatics	Congestion in a wireless sensor network causes an increase in the amount of data loss and delays in data transmission. In this paper, we propose a new congestion control technique (ACT, Adaptive Compression-based congestion control Technique) based on an adaptive compression scheme for packet reduction in case of congestion. The compression techniques used in the ACT are Discrete Wavelet Transform (DWT), Adaptive Differential Pulse Code Modulation (ADPCM), and Run-Length Coding (RLC). The ACT first transforms the data from the time domain to the frequency domain, reduces the range of data by using ADPCM, and then reduces the number of packets with the help of RLC before transferring the data to the source node. It introduces the DWT for priority-based congestion control because the DWT classifies the data into four groups with different frequencies. The ACT assigns priorities to these data groups in an inverse proportion to the respective frequencies of the data groups and defines the quantization step size of ADPCM in an inverse proportion to the priorities. RLC generates a smaller number of packets for a data group with a low priority. In the relaying node, the ACT reduces the amount of packets by increasing the quantization step size of ADPCM in case of congestion. Moreover, in order to facilitate the back pressure, the queue is controlled adaptively according to the congestion state. We experimentally demonstrate that the ACT increases the network efficiency and guarantees fairness to sensor nodes, as compared with the existing methods. Moreover, it exhibits a very high ratio of the available data in the sink.	adaptive compression;adaptive differential pulse-code modulation;benign mixed epithelial and stromal tumor of kidney;deploy;discrete wavelet transform;exhibits as topic;experiment;fairness measure;itga9 gene;meltwater entrepreneurial school of technology;microelectromechanical systems;microprocessor;name;network congestion;network packet;node - plant part;protocols documentation;quantization (signal processing);rlc circuit;run-length encoding;sensor node;small;software deployment;transcutaneous electric nerve stimulation;tree (data structure)	Joahyoung Lee;Inbum Jung	2010		10.3390/s100402919	embedded system;electronic engineering;real-time computing;wireless sensor network;telecommunications;time domain;computer science;bioinformatics;electrical engineering;flow control;discrete wavelet transform;network congestion;compression;frequency domain;slow-start;data transmission	Mobile	8.862373815651402	83.77123300594592	580
a0a1ec11f89ee4453940cf2a0c55b790332e178f	a flexible bibliometric approach for the assessment of professorial appointments	citation analysis;professorial appointment;individual evaluation;bibliometrics;academic evaluation;academic recruitment	Recruitment and professorial appointment procedures are crucial for the administration and management of universities and higher education institutions in order to guarantee a certain level of performance quality and reputation. The complementary use of quantitative and objective bibliometric analyses is meant to be an enhancement for the assessment of candidates and a possible antidote for subjective, discriminatory and corrupt practices. In this paper, we present the Vienna University bibliometric approach, offering a method which relies on a variety of basicindicators and further control parameters in order to address the multidimensionality of the problem and to foster comprehensibility. Our “top counts approach” allows an appointment committee to pick and choose from a portfolio of indicators according to the actual strategic alignment. Furthermore, control and additional data help to understand disciplinary publication habits, to unveil concealed aspects and to identify individual publication strategies of the candidates. Our approach has already been applied to 14 professorial appointment procedures (PAP) in the life sciences, earth and environmental sciences and social sciences, comprising 221 candidates in all. The usefulness of the bibliometric approach was confirmed by all heads of appointment committees in the life sciences. For the earth and environmental sciences as well as the social sciences, the usefulness was less obvious and sometimes questioned due to the low coverage of the candidates’ publication output in the traditional citation data sources. A retrospective assessment of all hitherto performed PAP also showed an overlap between the committees’ designated top candidates and the bibliometric top candidates to a certain degree.	bibliometrics;interconnectedness;internationalization and localization;map;password authentication protocol;peer-to-peer;timeline	Juan Gorraiz;Christian Gumpenberger	2015	Scientometrics	10.1007/s11192-015-1703-6	public relations;social science;bibliometrics;computer science;data mining;management;operations research;law;citation analysis;world wide web	HCI	-78.4727770338532	-22.863148888636406	581
10b9d4ca2c4762cd36bd7dc3b18de78003f0396d	inclusion relationship between pseudo-euclidean logics				Yasusi Hashimoto;Akio Maruyama	2011	Reports on Mathematical Logic		discrete mathematics;mathematics;euclidean geometry	Logic	-10.589419830403221	11.193849070554487	582
9dbb9134a96619a427fd2fa32c2978b78ffc101e	on area and yield considerations for fault-tolerant vlsi processor arrays	performance measure;equipment;computers;electronic circuits;microelectronic circuits;general and miscellaneous mathematics computing and information science;fault tolerant;integrated circuit;electronic equipment;testing;computational availability;computer architecture;processor array;redundancy;fault tolerant systems;reconfiguration strategies;fault tolerance;area utilization;joining processes;parallel processing fault tolerance fault tolerant systems testing joining processes redundancy computer architecture;vlsi;programming 990200 mathematics computers;failures;integrated circuits;parallel processing;wafer yield;wafer yield area utilization computational availability fault tolerance processor array reconfiguration strategies redundancy vlsi	Fault-tolerance is undoubtedly a desirable property of any processor array. However, increased design and implementation costs should be expected when fault-tolerance is being introduced into the architecture of a processor array. When the processor array is implemented within a single VLSI chip, these cost increases are directly related to the chip silicon area. Thus, the increase in area should be weighed against the improved performance of the gracefully degrading fault-tolerant processor array. In addition, a larger chip area might reduce the wafer yield to an unaceptable level making the use of fault-tolerant VLSI processor arrays impractical. The objective of this paper is to devise performance measures for the evaluation of the effectiveness and area utilization of various fault-tolerant techniques. Another goal is to analyze the reduction in wafer yield and investigate the possibility of yield enhancement through redundancy.	bsd;computer science;electrical engineering;fault tolerance;processor array;redundancy (engineering);very-large-scale integration;wafer (electronics)	Israel Koren;Melvin A. Breuer	1984	IEEE Transactions on Computers	10.1109/TC.1984.5009312	embedded system;parallel processing;fault tolerance;parallel computing;real-time computing;computer science;integrated circuit	Arch	8.501798972116463	57.61149371228395	583
65a025096acaf6eb941a206ea505f2b9b8994227	digital marketing analytics: the web dynamics of inside blackberry blog	promotional content;blog;digital marketing;consumer engagement;organizational content;content organization;relational content	Technological advances and the speed with which new technologies are being embraced by organizations, along with the rising power of the consumers and their ability to get what they want, when they want it, from whomever they want, have opened up new challenges for customer relationship management and marketing. Thus the need for understanding the digital world and its application becomes one of the greatest competitive aspects for a business's survival. The exhortation of globalization holds no meaning without the concept of what is being termed as 'Digitization'. Blackberry has started a long and hard climb to regain its lost glory. Supporting its product improvement and repositioning strategies are a set of well-defined digital marketing strategies. This manuscript explores the dynamics of Inside Blackberry-an online endeavour of Blackberry to trace the E-Marketing objectives of the Blog and its ability to leverage the behavioral internet theory for online branding, building usability and reciprocity, strengthening credibility and consumer persuasion.	blackberry;blog;digital marketing;world wide web	Shirin Alavi;Vandana Ahuja	2014	IJIDE	10.4018/ijide.2014100104	digital marketing;economics;telecommunications;marketing;multimedia;advertising;management;world wide web	AI	-84.03018683275603	-13.070217607902391	584
3ac1fcf06c52a0429d39768af9969a8f0c15671e	normalization of chip-seq data with control	nf kappa b;data interpretation statistical;saccharomyces cerevisiae proteins;transcription factors;sequence analysis dna;binding sites;computational biology bioinformatics;chromatin immunoprecipitation;genome;algorithms;humans;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	ChIP-seq has become an important tool for identifying genome-wide protein-DNA interactions, including transcription factor binding and histone modifications. In ChIP-seq experiments, ChIP samples are usually coupled with their matching control samples. Proper normalization between the ChIP and control samples is an essential aspect of ChIP-seq data analysis. We have developed a novel method for estimating the normalization factor between the ChIP and the control samples. Our method, named as NCIS (Normalization of ChIP-seq) can accommodate both low and high sequencing depth datasets. We compare statistical properties of NCIS against existing methods in a set of diverse simulation settings, where NCIS enjoys the best estimation precision. In addition, we illustrate the impact of the normalization factor in FDR control and show that NCIS leads to more power among methods that control FDR at nominal levels. Our results indicate that the proper normalization between the ChIP and control samples is an important step in ChIP-seq analysis in terms of power and error rate control. Our proposed method shows excellent statistical properties and is useful in the full range of ChIP-seq applications, especially with deeply sequenced data.	biopolymer sequencing;dna microarray chip;dna binding site;estimated;experiment;false discovery rate;histone code;histones;interaction;matching;name;sequence number;simulation;transcription factor;transcription (software)	Kun Liang;Sündüz Keles	2011		10.1186/1471-2105-13-199	biology;molecular biology;chromatin immunoprecipitation;dna microarray;nfkb1;bioinformatics;binding site;chip-on-chip;genetics;genome;transcription factor	ML	3.6495392270366733	-54.1897470398629	585
1ab87ea77d64e8ed163a45f5c9ea15a38b5d787b	the connected cutset connectivity of a graph	graph theory;teoria grafo;coupure graphe;theorie graphe;conectividad diagrama;graph connectivity;graph cut;vertex graph;edge graph;arete graphe;cuspide grafico;connectivite graphe;cortadura grafico;arista grafico;sommet graphe	Abstract   The connected cutset connectivity (connected edge-cutset connectivity) of a nontrivial connected graph  G  is the minimum cardinality of a set  S  of vertices (edges) of  G  such that the subgraph induced by  S  is connected and  G - S  is either disconnected or trivial. The connected cutset connectivity and the minimum degree of a graph are compared, as are the connected cutset connectivity and connected edge-cutset connectivity. The connected edge-cutset connectivity is bounded below by its edge-connectivity and above by its minimum degree, and these bounds are shown to be sharp. A sufficient condition is established for the connected edge-cutset connectivity of a graph to equal its minimum degree that does not imply its edge-connectivity equals its minimum degree.	cut (graph theory)	Ortrud R. Oellermann	1988	Discrete Mathematics	10.1016/0012-365X(88)90058-1	algebraic connectivity;loop;k-edge-connected graph;combinatorics;discrete mathematics;connected component;directed graph;topology;cut;degree;graph toughness;regular graph;connectivity;graph theory;cycle graph;vertex;mathematics;distance-hereditary graph;biconnected graph;k-vertex-connected graph;windmill graph;complete graph;quartic graph;complement graph;strongly connected component;line graph;strength of a graph	Theory	26.592683047020994	30.832475654979497	586
907a0d536b9490949718fa50f1582176f82e9aa2	eliciting stakeholders' knowledge of goals and processes to derive it support				Stewart Green	2003			systems engineering;management science;computer science;knowledge management	HCI	-68.4218638144154	2.642921926591714	587
a71f279b78f71f53ab50834889c7b58c787084ee	histopathological image classification with bilinear convolutional neural networks		The computer-aided quantitative analysis for histopathological images has attracted considerable attention. The stain decomposition on histopathological images is usually recommended to address the issue of co-localization or aliasing of tissue substances. Although the convolutional neural networks (CNN) is a popular deep learning algorithm for various tasks on histopathological image analysis, it is only directly performed on histopathological images without considering stain decomposition. The bilinear CNN (BCNN) is a new CNN model for fine-grained classification. BCNN consists of two CNNs, whose convolutional-layer outputs are multiplied with outer product at each spatial location. In this work, we propose a novel BCNN-based method for classification of histopathological images, which first decomposes histopathological images into hematoxylin and eosin stain components, and then perform BCNN on the decomposed images to fuse and improve the feature representation performance. The experimental results on the colorectal cancer histopathological image dataset with eight classes indicate that the proposed BCNN-based algorithm is superior to the traditional CNN.	aliasing;artificial neural network;bilinear filtering;class;colorectal carcinoma;computational complexity theory;computer vision;convolutional neural network;deep learning;ensemble learning;eosine yellowish;fuse device component;hematoxylin and eosin stain method;image analysis;learning disorders;neural network simulation;outer product;silo (dataset);stains;algorithm;centrosomin protein, drosophila	Chaofeng Wang;Jun Shi;Qi Zhang;Shihui Ying	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037745	convolutional neural network;artificial intelligence;h&e stain;computer science;computer vision;deep learning;bilinear interpolation;contextual image classification;pattern recognition	Vision	32.207344103221146	-74.79207932637954	588
a74ed269abcb31fa97e70e8c7f17057f0fdc3ed5	designing worth is worth designing	research design;common ground;g900 others in mathematical and computing sciences;value centred design;worth centred design;motivation;w200 design studies;evaluation;value sensitive design;management development	Value is a unifying concept for design. The intended value of digital artefacts provides a focus for field research, design and evaluation, as well as common ground with project sponsors, future users, and other stakeholders. The challenge lies in operationalising value to create a well-defined, well-understood and manageable development process. This requires a clear definition and strong understanding of the nature of value as a human motivator. It further requires this understanding to be transformed into methods and techniques within a development framework that supports a focus on value from the initial identification of product opportunities to the installation and operation of digital products and services. This paper revisits the logic and evolution of value-centred design, and then renames it to the near synonym of worth-centred to avoid confusion (especially 'value' vs. 'values') and distracting associations of the word 'value'. It locates worth in arenas of individual and collective discourses which scope different human perspectives on value. Finally, it relates worth to an evolving development framework.	field research	Gilbert Cockton	2006		10.1145/1182475.1182493	motivation;computer science;knowledge management;evaluation;management science;management	HCI	-77.63840516663187	-1.5217677733133392	589
f4c7de2f9a4708450768ee85b9e35f2b41b80154	die support feature machine: eine odyssee in hochdimensionalen räumen			eine and zwei	Sascha Klement	2013				ML	-97.06816986829581	23.94681666019383	590
672b5d584ced2b901d949c5a13dc3d735b5519e1	commutativity, non-commutativity, and bilinearity	non commutative	One of the major open questions in algoritha analthe computational complexity of matrix multipkation, i.e. the number of multiplication steps or the tdaf number of arithmetic operations required to multiply two matrices by an algorithm in a chosen of algorithms. me usual class of algorithms studthe class of polynomid dgotithms, straight-line rithms using only the operations of addition, subion, and multiplication. In this paper we compare the computing power of ihree classes of a2gorithms, e particukn problem of computing a forms in an ordered pair 01 ,B) of sets, over a fold K of scalars, a generalization of atrix multiplication problem. More precisely, C is a s9t of K-bilinear ftinns in (A ,B) if esh element c of C may be wrken as	algorithm;bilinear filtering;computational complexity theory;ordered pair	Robert L. Probert	1976	Inf. Process. Lett.	10.1016/0020-0190(76)90078-8	computer science;mathematics	Theory	46.338832692507346	38.27139874120272	591
56a3922ea2c08014458e9041bbf6f3dbd5a17086	dual direction big data download and analysis	data replication;dual direction processing;big data;parallel processing	The term Big Data was recently coined as the amount of generated and stored digital data has grown so rapidly that it has become very hard to store, manage and analyze without coming up with new techniques that can cope with such challenges. Finding innovative approaches to support big data analysis has become a priority as both the research community and the industry are trying to make use of these huge amounts of available data. In this paper we introduce a new approach to enhance the overall big data analysis performance. The approach calls for utilizing data set replication, parallel download, and parallel processing over multiple compute nodes. The main concept calls for simultaneously parallelizing the download of the data (in partitions) from multiple replicated sites to multiple compute nodes that will also perform the analysis in parallel. Then the results are given to the client that requested the analysis.	big data;digital data;disk partitioning;download;parallel computing	Jameela Al-Jaroodi;Nader Mohamed;Abdulla Eid	2014	SIGMETRICS Performance Evaluation Review	10.1145/2627534.2627564	parallel processing;big data;computer science;operating system;data mining;database;world wide web;replication	HPC	-32.0824148592845	-1.0146323716021042	592
86b2d3114139f56bc2e35aa35b0b231e28627057	atomic actions and resource coordination problems having nonunique solutions	protocols;base donnee;concurrent computing;computer crashes;performance access synchronization algorithms atomic actions concurrency control crash recovery design;application software;performance;recouvrance;database;action atomique;recovery;calendars;systeme base donnee;atomic actions;algorithme;synchronisation;algorithm;algorritmo;transaction databases;synchronization;database systems;concurrency control;algorithms;design;controle concurrence;sincronizacion;access synchronization;transaction databases concurrent computing concurrency control calendars protocols computer crashes algorithm design and analysis programming database systems application software;programming;algorithm design and analysis;crash recovery	The concept of atomic actions is decomposed into database-dependent atomic actions and application-dependent atomic actions. There is a broad class of application-dependent atomic actions that can have nonunique solutions. These are resource coordination problems and are classified as problems of NU class. It is argued that a transaction modeling a problem of NU class provides lower concurrency. A concept of coordination is proposed which can model a broad range of NU class problems. An object model and a protocol are sugested which utilize the nonunique character of the solution to provide higher concurrency.	concurrency (computer science);cray-1;data structure;declaration (computer programming);distributed computing;linearizability;machine-dependent software;optimizing compiler;parallel computing;pascal;programmer;structured programming;two-phase commit protocol	Mukul K. Sinha	1985	IEEE Transactions on Software Engineering	10.1109/TSE.1985.232485	synchronization;real-time computing;concurrent computing;computer science;operating system;database;distributed computing;programming language	SE	-24.315945342820395	41.086231520884596	593
e59faa0092acb6c1c81dc33e0386b0d2ee776267	cwi at trec 2011: session, web, and medical		We report on the participation of the Interactive Information Access group of the CWI Amsterdam in the web, session, and medical track at TREC 2011. In the web track we focus on the diversity task. We find that cluster-based subtopic modeling approaches improve diversification performance compared to a non-cluster-based subtopic modeling approach. While gain was observed on previous years’ topic sets, diversification with the proposed approaches hurt the performance when compared to a non-diversified baseline run on this year’s topic set. In the session track, we examine the effects of differentiating between ‘good’ and ‘bad’ users. We find that differentiation is useful as the use of search history appears to be mainly effective when the search is not going well. However, our current strategy is not effective for ‘good’ users. In addition, we studied the use of random walks on query graphs for formulating session history as search queries, but results are inconclusive. In the medical track, we found that the use of medical background resources for query expansion leads to small improvements in retrieval performance. Such resources appear to be especially useful to promote early precision.	baseline (configuration management);cluster analysis;diversification (finance);experiment;information access;interactivity;query expansion;session (web analytics);spectral clustering;text retrieval conference;web search query	Jiyin He;Vera Hollink;Corrado Boscarino;Arjen P. de Vries;Roberto Cornacchia	2011			computer science;multimedia;world wide web;information retrieval	Web+IR	-33.0179978152376	-53.583486690330176	594
3babc87a611554e1580841c9b4776b20ee65a7f4	improvements to the identification process of vulnerable components: deciding about updates		Applications 1 may contain vulnerabilities for a variety of reasons, one of which is the use of vulnerable components. One of the solutions adopted to eliminate the vulnerabilities inserted by such components is to update the component to a more recent version that corrects the vulnerability. However, updating a component may require code refactoring, updating other components and inserting new vulnerabilities in the application. There are several tools that perform the analysis and management of dependencies of the projects, but few tools present information about vulnerabilities of the new versions, incompatibilities and updates of the dependencies of the components. This article, therefore, presents dep|ct (depict), a tool that aims to identify the known vulnerable components used by the applications and help in the decision on the updating of such components, in order to mitigate the vulnerabilities added to the projects through the vulnerable dependencies. Results of the empirical evaluation carried out on two projects show that the tool can be used to assist in deciding on the update of known vulnerable components.		Bruna Vuicik Mocelin;Kleinner Farias;Lucian Jos&#233; Gon&#231;ales;Vinícius Bischoff	2018		10.1145/3229345.3229391	software engineering;code refactoring;vulnerability;computer science	SE	-59.853740632499765	43.572569526745305	595
8a5ac93bf897b43d4043c13133378f473ded5c4a	fair exchange protocol of schnorr signatures with semi-trusted adjudicator	fair exchange;security model;discrete logarithm;random oracle model;schnorr signatures;fair exchange protocol	In this paper, we propose an optimistic fair exchange protocol of Schnorr signatures with a semi-trusted adjudicator. In this protocol, we enforce the adjudicator accountability in the protocol to relax excessive reliance on the trust of the adjudicator, so that the adjudicator only needs to be trusted by the signer. We present a security model and then show that the protocol is strong EUF–CMA secure under the standard Discrete Logarithm (DL) assumption in the random oracle model. Finally, we compare the performance of the fair exchange protocol of Schnorr signatures. 2010 Elsevier Ltd. All rights reserved.	aggregate data;antivirus software;cma-es;communications protocol;digital signature;discrete logarithm;encryption;fairness measure;internet;lu decomposition;pairing-based cryptography;random oracle;schnorr group;semiconductor industry	Zuhua Shao	2010	Computers & Electrical Engineering	10.1016/j.compeleceng.2010.03.005	random oracle;computer security model;discrete logarithm;computer science;distributed computing;internet privacy;schnorr signature;computer security;algorithm	Security	-42.924057240094776	75.07135871609297	596
c1ec28fa4817ac1f6d43122089bdb9afb2fa8934	umts w-cdma: evaluation of radio performance by means of link level simulations	evaluation performance;broadband networks;performance evaluation;erasure;etude theorique;telecommunication sans fil;bit error rate;cellular radio;evaluacion prestacion;circuit switched;packet radio networks;telecommunication computing;packet switched;packet switching;conmutacion por paquete;borradura;communication service mobile;multiuser channels;voice;data communication;voz;circuit switching;conmutacion circuito;error statistics cellular radio code division multiple access broadband networks radio links digital simulation software tools telecommunication computing radiowave propagation awgn channels multipath channels multiuser channels packet radio networks circuit switching data communication voice communication;taux erreur bit;awgn channels;code division multiple access;acces multiple code;voice communication;telecomunicacion sin hilo;mobile communication;effacement;estudio teorico;error statistics;software tools;mecanisme articule;480 kbit s radio performance evaluation link level simulations umts w cdma fdd component simulation results service classes voice service circuit switched data packet switched data bit error rate block erasure rate e sub b n sub 0 ratio mobile speed propagation channels etsi itu r third generation mobile telecommunications system international telecommunication union software simulation tool multipath fading generator additive white gaussian noise awgn 8 kbit s 144 kbit s;radiowave propagation;multipath channels;mecanismo articulado;theoretical study;tasa error bit;acceso multiple codificado;commutation paquet;3g mobile communication multiaccess communication bit error rate europe telecommunications frequency speech circuits large hadron collider proposals;digital simulation;linkage mechanism;voix;radio links;commutation circuit;wireless telecommunication	This article presents link level simulation results for the FDD component of UMTS, based on the W-CDMA access technique. Simulation results are given for different service classes: 8 kb/s voice, 144 kb/s circuit-switched data (LDD), and 480 kb/s packet-switched data (UDD). The performance is given in terms of the bit error rate and block erasure rate. Results are provided as a function of the E/sub b//N/sub 0/ ratio and the mobile speed in four different propagation channels defined by the ETSI and ITU-R.	simulation	Bruno Melis;Giovanni Romano	2000	IEEE Personal Commun.	10.1109/98.847922	telecommunications;computer science;circuit switching;computer network	Vision	26.84843889958756	79.44847751285958	597
cfa170800ab5268e885aa68aa29c9b3dcacedd06	a modeling study of budding yeast colony formation and its relationship to budding pattern and aging		Budding yeast, which undergoes polarized growth during budding and mating, has been a useful model system to study cell polarization. Bud sites are selected differently in haploid and diploid yeast cells: haploid cells bud in an axial manner, while diploid cells bud in a bipolar manner. While previous studies have been focused on the molecular details of the bud site selection and polarity establishment, not much is known about how different budding patterns give rise to different functions at the population level. In this paper, we develop a two-dimensional agent-based model to study budding yeast colonies with cell-type specific biological processes, such as budding, mating, mating type switch, consumption of nutrients, and cell death. The model demonstrates that the axial budding pattern enhances mating probability at an early stage and the bipolar budding pattern improves colony development under nutrient limitation. Our results suggest that the frequency of mating type switch might control the trade-off between diploidization and inbreeding. The effect of cellular aging is also studied through our model. Based on the simulations, colonies initiated by an aged haploid cell show declined mating probability at an early stage and recover as the rejuvenated offsprings become the majority. Colonies initiated with aged diploid cells do not show disadvantage in colony expansion possibly due to the fact that young cells contribute the most to colony expansion.	agent-based model;bipolar disorder;bud - plant part;cell aging;cell death;diploid cell;diploidy;haploidy;inbreeding;limited stage (cancer stage);saccharomycetales;simulation;diploidization;establishment of cell polarity;viral capsid secondary envelopment	Yanli Wang;Wing-Cheong Lo;Ching-Shan Chou	2017		10.1371/journal.pcbi.1005843	ploidy;cell polarity;genetics;mating of yeast;biology;budding;saccharomyces cerevisiae;yeast;mating type;population	ML	7.080976479796158	-65.88971931896576	598
8a748a1ff9de674047ac298e9ef3601bb071b568	artificial noise in read-write simulation of optical disk and drive	optical disk;optic disk;read write processes;mathematical models;optical recording;optical storage;artificial noise	When the mathematical model of the read write(R/W) simulation of write once and read many (WORM) optical disk and drive was formulated and numerical analysis and software implementation of the simulation were carried out for the noise-free R/W process, it was found that the numerical process of the simulation introduced artificial noise in the read back signal. The artificial noises were due to finite precision of integration, finite density of written spot, and finite number of pits simulated. The noises can be reduced, but at the expense of computational time.	read-write memory;simulation	Shiuh Chao;Tsong-Yo Yang	1991	Simulation	10.1177/003754979105600612	electronic engineering;optical disc;optical storage;computer hardware;computer science;theoretical computer science;mathematical model	Metrics	15.859942530150947	38.79374674521699	599
9b3ac95191aa5fce2588f769ab08d79cc9e80a82	functional pearl: the proof search monad		We present the proof search monad, a set of combinators that allows one to write a proof search engine in a style that resembles the formal rules closely. The user calls functions such as premise, prove or choice; the library then takes care of generating a derivation tree. Proof search engines written in this style enjoy: first, a one-to-one correspondence between the implementation and the derivation rules, which makes manual inspection easier; second, proof witnesses “for free”, which makes a verified, independent validation approach easier too. 1 Theory and practice 1.1 A minimal theory We are concerned with proving the validity of logical formulas; that is, with writing a search procedure that determines whether a given goal is satisfiable. To get started, we consider a system made up of conjunctions of equalities, along with existential quantifiers. Any free variables are assumed to be universally quantified. For instance, one may want to prove the following formula:	automated theorem proving;backtracking;care-of address;combinatory logic;free variables and bound variables;monad (functional programming);one-to-one (data model);parse tree;rewrite (programming);type system;universal quantification;web search engine	Jonathan Protzenko	2015			differentiation rules;combinatory logic;discrete mathematics;premise;search engine;free variables and bound variables;algorithm;monad (functional programming);derivation;mathematics	PL	-15.829318342372968	20.36252208370992	600
621886c009820f5c243667dd9f61d69cfd779af6	a precise estimation of the order of local testability of a deterministic finite automaton	deterministic finite automaton	A locally testable language L is a language with the property that for some nonnegative integer k, called the order or the level of local testability, whether or not a word u in the language L depends on (1) the prefix and suffix of the word u of length k - 1 and (2) the set of intermediate substrings of length k of the word u. For given k the language is called k-testable.	deterministic finite automaton	Avraham Trakhtman	1997		10.1007/BFb0031393	powerset construction;deterministic pushdown automaton;nondeterministic finite automaton with ε-moves;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;büchi automaton;nondeterministic finite automaton;state diagram;deterministic algorithm;theoretical computer science;two-way deterministic finite automaton;deterministic finite automaton;probabilistic automaton;continuous automaton;deterministic automaton;mathematics;dfa minimization;timed automaton;pushdown automaton	Logic	-1.405579360301247	21.579009726551455	601
56af221fb0aa39441cdb4b7333a209dfe0433188	influence maximization in dynamic social networks	directed graphs;social networking online directed graphs human factors;dynamic social networks influence maximization;human factors;heuristic algorithms probes algorithm design and analysis approximation algorithms twitter estimation;social networking online;static networks influence maximization dynamic social networks influence diffusion maximization social influence online social networks probing nodes	Social influence and influence diffusion has been widely studied in online social networks. However, most existing works on influence diffusion focus on static networks. In this paper, we study the problem of maximizing influence diffusion in a dynamic social network. Specifically, the network changes over time and the changes can be only observed by periodically probing some nodes for the update of their connections. Our goal then is to probe a subset of nodes in a social network so that the actual influence diffusion process in the network can be best uncovered with the probing nodes. We propose a novel algorithm to approximate the optimal solution. The algorithm, through probing a small portion of the network, minimizes the possible error between the observed network and the real network. We evaluate the proposed algorithm on both synthetic and real large networks. Experimental results show that our proposed algorithm achieves a better performance than several alternative algorithms.	approximation algorithm;dhrystone;expectation–maximization algorithm;social network	Honglei Zhuang;Yihan Sun;Jie Tang;Jialin Zhang;Xiaoming Sun	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.145	network science;directed graph;evolving networks;computer science;dynamic network analysis;artificial intelligence;human factors and ergonomics;machine learning;hierarchical network model;mathematics;distributed computing	DB	-17.113278072412292	-43.30604020730946	602
26d9a70ada7cebd81f828faadec0a866e75f2e92	improved t - ψ nodal finite element schemes for eddy current problems	magnetic field;benchmark problem;eddy current;three dimensional;finite element;t ψ decoupled scheme;nodal finite element;bounded domain;eddy current problem;error estimate;error estimates;saddle point	The aim of this paper is to propose improved  T  −  ψ  finite element schemes for eddy current problems in the three-dimensional bounded domain with a simply-connected conductor. In order to utilize nodal finite elements in space discretization, we decompose the magnetic field into summation of a vector potential and the gradient of a scalar potential in the conductor; while in the nonconducting domain, we only deal with the gradient of the scalar potential. As distinguished from the traditional coupled scheme with both vector and scalar potentials solved in a discretizing equation system, the proposed decoupled scheme is presented to solve them in two separate equation systems, which avoids solving a saddle-point equation system like the traditional coupled scheme and leads to an important saving in computational effort. The simulation results and the data comparison of TEAM Workshop Benchmark Problem 7 between the coupled and decoupled schemes show the validity and efficiency of the decoupled one.	finite element method	Tong Kang;Tao Chen;Huai Zhang;Kwang Ik Kim	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.05.062	three-dimensional space;mathematical optimization;magnetic field;eddy current;finite element method;calculus;mathematics;geometry;saddle point;quantum mechanics	Theory	92.431112304393	9.575280541709544	603
64ebbcaee4106734080e2ec409c1d7fe2017588d	look-ahead allocation in the presence of branches	register allocation;code generation;look ahead;spill code;compilers;compiler optimization;optimization	Most current register allocators utilize a register coloring algorithm. They use varied heuristics to decide which live ranges to break and where to break them when the number of variables exceeds the number of available registers. Such algorithms tend to average out a variable's use over its live range. This can lead to register decisions which are not sensitive to local code's needs. A prototype global look-ahead register allocator (GLORIA) avoids the register coloring paradigm and its averaging. Allocation decisions are made for each instruction based on the distances to the next uses of live variables. This allocator currently works only at the subprogram level and does not perform any interprocedural optimization. Comparisons between GLORIA and Chow and Hennessy's register coloring allocator have been made. Preliminary results indicate that look-ahead usually performs better. Memory use is on the order of the size of the program while allocation time is expected to be of the same order.	benchmark (computing);chaitin's algorithm;compiler;foremost;graph coloring;heuristic (computer science);interprocedural optimization;mathematical optimization;programming paradigm;prototype;register allocation;register file;rich internet application;subroutine;time complexity	James D. Shuler;Henry R. Bauer	1997		10.1145/331697.332339	dead code;computer architecture;compiler;parallel computing;profile-guided optimization;interprocedural optimization;computer science;dead code elimination;optimizing compiler;programming language;register allocation;code generation	PL	-17.909551757617564	37.003224549876705	604
b5bd3c456b6d769041a9e113e64b25913bd2d071	birkhoff's hsp-theorem for cumulative logic programs	logic programs;cumulant	Birkhoff's HSP theorem is that the models of a set of algebraic equations form a variety, i.e. a category of algebras which admits homomorphic images, subalgebras and products. We show here first, that every equational set of retract structures in combinatory logic is a variety, and second, that every set of combinators, closed under certain operations, is equational. It follows that the models of cumulative logic programs form an equational variety.	birkhoff interpolation;logic programming	Beatrice Amrhein	1993		10.1007/3-540-58025-5_48	discrete mathematics;mathematics;algorithm	AI	-10.914909143782992	16.20047215040492	605
5e9a46f3024e6889aa85f133e7a08fb39b021c61	analysis of resource increase and decrease algorithm in wireless sensor networks	constrained optimization;communication system traffic control;control systems;energy efficient;resource control;traffic control;wireless sensor network;condition monitoring;channel capacity;energy consumption;intelligent networks;computer science;source control;algorithm design and analysis intelligent networks wireless sensor networks communication system traffic control channel capacity computer science energy consumption traffic control condition monitoring control systems;algorithm design and analysis;wireless sensor networks	In this paper, we first attempt to formally define the resource control framework that adjusts the resource provisioning at the hotspot during congestion. In an effort to find the optimal resource control under the fidelity and energy constraints, we present a resource increase and decrease algorithm called Early Increase/Early Decrease (EIED) that tries to adjust the effective channel capacity quickly to the incoming traffic volume in an energy-efficient manner, thereby increasing the fidelity (or accuracy) level observed by the application during congestion. Under the framework of energy-constrained optimization, we prove this algorithm incurs the lowest overhead of energy consumption for the given fidelity level that is required by the application.	algorithm;channel capacity;constrained optimization;fidelity of quantum states;mathematical optimization;network congestion;overhead (computing);provisioning;throughput	JaeWon Kang;Yanyong Zhang;B. R. Badrinath	2006	11th IEEE Symposium on Computers and Communications (ISCC'06)	10.1109/ISCC.2006.37	constrained optimization;real-time computing;wireless sensor network;computer science;distributed computing;computer network	Embedded	11.19389149379458	81.2516407890482	606
8fd0aabe207cc6f5a81588fddfcce472da9c9ac8	a study on dynamic semantic web service composition	owl s;service composition;semantic web;description logic;ai planning	Description Logic possesses strong knowledge representation and reasoning capabilities and offers logical foundation for Semantic Web ontology languages such as OWL and OWL-S. However, the present implementations of OWL and OWL-S are deficient in semantic modeling for dynamic service composition. They also do not consider user preferences. AI planning possesses a better modeling capability of action state transformations in planning and provides an effective method for solving planning problem and task decomposition. However, it is limited in knowledge representation and reasoning capabilities. Based on merits of Description Logic, OWL-S and AI planning, this paper extends OWL-S model, proposes a service composition mechanism and testifies its feasibility in Description Logic. The results show that this composition mechanism is not only feasible but also helpful for semantic modeling of service composition in Semantic Web.	semantic web service;service composability principle	Yingjie Li;Xueli Yu;Rui Wang;Lili Geng;Li Wang	2013	Web Intelligence and Agent Systems	10.3233/WIA-130266	automated planning and scheduling;f-logic;description logic;semantic web rule language;computer science;knowledge management;artificial intelligence;semantic web;social semantic web;semantic web stack;database;world wide web;owl-s	AI	-43.27006269786426	14.011917996402644	607
7020daafba3d74ee92e1c7ea10f6856a653596cb	a non-intrusive biometric authentication mechanism utilising physiological characteristics of the human head	biometric authentication;auditory evoked response;authentication;biometrics;physiology;statistical analysis;non intrusive;error rate;head;acoustic waves	This paper proposes and evaluates a non-intrusive biometric authentication technique drawn from the discrete areas of biometrics and Auditory Evoked Responses. The technique forms a hybrid multi-modal biometric in which variations in the human voice due to the propagation effects of acoustic waves within the human head are used to verify the identity of a user. The resulting approach is known as the Head Authentication Technique (HAT). Evaluation of the HAT authentication process is realised in two stages. First, the generic authentication procedures of registration and verification are automated within a prototype implementation. Second, a HAT demonstrator is used to evaluate the authentication process through a series of experimental trials involving a representative user community. The results from the trials confirm that multiple HAT samples from the same user exhibit a high degree of correlation, yet samples between users exhibit a high degree of discrepancy. Statistical analysis of the prototype performance realised system error rates of 6% False Non-Match Rate (FNMR) and 0.025% False Match Rate (FMR). a 2007 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;authentication;biometrics;discrepancy function;modal logic;prototype;software propagation;virtual community	P. M. Rodwell;Steven Furnell;Paul L. Reynolds	2007	Computers & Security	10.1016/j.cose.2007.10.001	speech recognition;computer science;computer security;biometrics	Security	13.116389538411791	-96.04075098488057	608
9fdfec07c83a8dba8ab5eb8129142ecbc8fcdf1a	conception assistée d'une ontologie à partir d'une conceptualisation consensuelle exprimée de manière semi-formelle		Résumé : Cet article présente une méthodologie assistée de conception d'une ontologie à travers trois méthodes, soit une méthode d'élicitation des connaissances d'un domaine résultant en un modèle semi-formel de ces connaissances, une méthode de formalisation conduisant à la production d’une ontologie et une méthode de validation syntaxique et sémantique de l'ontologie. Les processus de formalisation et de validation sont assistés par un système expert à la formalisation dont la base de connaissances est une ontologie de transformation.	linear algebra;semiconductor industry	Michel Héon;Gilbert Paquette;Josianne Basque	2009			atomic physics;physics	Crypto	-106.13635658100907	14.09002916583443	609
2ee902987c7c9aa776f63aaf93b4c4e177ccd4d0	variable-dimension quantization of sinusoidal amplitudes using gaussian mixture models	quantization;gaussian mixture;wideband;quantization vectors speech wideband frequency discrete transforms matrix converters testing sampling methods cost function;cost function;weighted distortion criterion;wideband harmonic coder;informal listening tests;speech;speech coding;variable dimension quantizers;testing;transform coding;variable dimension quantization;speech coding quantisation signal gaussian distribution transform coding vocoders;sinusoidal amplitudes;quantisation signal;gaussian mixture model;vectors;general methods;discrete transforms;gaussian mixture models;variable to fixed dimension transform;vocoders;matrix converters;sinusoidal speech coder;sampling methods;wideband harmonic coder variable dimension quantization sinusoidal amplitudes gaussian mixture models variable dimension quantizers weighted distortion criterion variable to fixed dimension transform sinusoidal speech coder high rate bound approximation informal listening tests;frequency;gaussian distribution;high rate bound approximation	In this paper, Gaussian mixture (GM) models are used to design variable-dimension quantizers according to a weighted distortion criterion. A general method for combining a variable-to-fixed dimension transform, with GM modeling and quantization, is proposed. The method provides a convenient and efficient way to encode the amplitudes in a sinusoidal speech coder. Quantizers designed according to the proposed scheme are evaluated both according to weighted distortion criteria, and with respect to a high-rate bound approximation of the distortion. Informal listening tests suggest that the amplitudes can be encoded without subjective loss in a wideband harmonic coder, at a rate around 40 bits per frame (for the amplitudes only).	approximation;distortion;encode;mixture model;quantization (signal processing);speech coding	Jonas Lindblom;Per Hedelin	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1325945	discrete mathematics;speech recognition;computer science;mixture model;mathematics;statistics	Robotics	48.62501577151129	-10.069854963512853	610
b0aa53c6c5e88ea3382fd033518820ba5d685b30	sentiment analysis and topic classification based on binary maximum entropy classifiers	medios sociales;computacion informatica;filologias;maxima entropia;logistic regression;ciencia;info eu repo semantics article;topic detection;informacion documentacion;regresion logistica;linguistica;categorizacion de texto en temas de interes;ciencias basicas y experimentales;projetos;sentiment analysis;investigacao;publicacoes;analisis de sentimiento;grupo a;iscte iul;social media;ciencias sociales;article;grupo b;maximum entropy	This work was partially supported by national funds through FCT – Fundacao para a Ciencia e Tecnologia, under project PEst-OE/EEI/LA0021/2011, and by DCTI – ISCTEIUL – Lisbon University Institute.	sentiment analysis	Fernando Batista;Ricardo Ribeiro	2013	Procesamiento del Lenguaje Natural		social media;computer science;artificial intelligence;principle of maximum entropy;logistic regression;sentiment analysis	ML	-38.56019757570939	-77.09365864155146	611
e11f4a068eaba59aca10c2979e5ca34a97133099	count highly-cited papers instead of papers with h citations: use normalized citation counts and compare “like with like”!	bibliometric databases;highly-cited papers;h-index	Teixeira da Silva and Dobránszki (Scientometrics. https://doi.org/10.1007/s11192-018-2680-3, 2018) describe practical problems in using the h-index for the purpose of research evaluation. For example, they discuss the h-index differences among the bibliometric databases. In this Letter to the Editor, we argue for abstaining from using the h-index. One can use normalized indicators instead.	bibliometrics;database;paper;scientometrics;citation	Lutz Bornmann;Loet Leydesdorff	2018		10.1007/s11192-018-2682-1	information retrieval;letter to the editor;data mining;citation;normalization (statistics);scientometrics;computer science	ML	-77.67463829518451	-21.24864753136184	612
320a25f7725b598334a798f9ce18da0d21909f6d	defending against path-based dos attacks in wireless sensor networks	performance measure;hash chain;packet loss;sensor network;wireless sensor network;denial of service attack;long distance;sensor networks;denial of service;sensor nodes;security;denial of services attacks;dos attack	Denial of service (DoS) attacks can cause serious damage in resource-constrained, wireless sensor networks (WSNs). This paper addresses an especially damaging form of DoS attack, called PDoS (Path-based Denial of Service). In a PDoS attack, an adversary overwhelms sensor nodes a long distance away by flooding a multi-hop end-to-end communication path with either replayed packets or injected spurious packets. This paper proposes a solution using one-way hash chains to protect end-to-end communications in WSNs against PDoS attacks. The proposed solution is lightweight, tolerates bursty packet losses, and can easily be implemented in modern WSNs. The paper reports on performance measured from a prototype implementation.	adversary (cryptography);cryptographic hash function;end-to-end encryption;end-to-end principle;network packet;one-way function;pdos;prototype	Jing Deng;Richard Han;Shivakant Mishra	2005		10.1145/1102219.1102235	wireless sensor network;computer science;information security;key distribution in wireless sensor networks;internet privacy;computer security;denial-of-service attack;computer network	Mobile	-54.34005323920184	75.39594384676535	613
8d465713e26fdb169ed791cffd4d57ef20ecc5d8	sensemaking in a distributed environment	distributed environment	As globalization continues to increase, sensemaking in a distributed environment is likely to grow in importance as the expertise necessary to address increasing environmental complexity is more widely geographically dispersed. Paradoxically, the information and communication technologies that make sensemaking in a distributed environment possible simultaneously may create a context that inhibits the effectiveness of such efforts. The purpose of this research is to develop a better understanding of how sensemaking in a distributed environment can be effectively and efficiently carried out. Case studies of four teleconsultation projects involving virtual teams engaged in healthcare delivery are presented. This paper make a contribution in that it deals with critical problems that practitioners must address and it integrates research relating to wicked decision problems, virtual teams, and the role of IT in health care.	decision problem;sensemaking;wicked	David Paul	2006			globalization;sensemaking;computer science;knowledge management;management science;decision problem;health care;information and communications technology;distributed computing environment	HPC	-78.12870008147085	1.0910880880596079	614
3082b30d150f86500a100d199e11b5c0dfe052b5	ieee biomedical circuits and systems conference, biocas 2017, torino, italy, october 19-21, 2017					2017				EDA	-53.33029714018855	-5.620517836023248	615
f8663e1d88611d60c2cd8c5a4724af69becdb251	improving the peer-to-peer ring for building fault-tolerant grids		Peer-to-peer networks are gaining popularity in order to build Grid systems. Among different approaches, structured overlay networks using ring topology are the most preferred ones. However, one of the main problems of peer-to-peer rings is to guarantee lookup consistency in presence of multiple joins, leaves and failures nodes. Since lookup consistency and fault-tolerance are crucial properties for building Grids or any application, these issues cannot be avoided. We introduce a novel relaxed-ring architecture for fault-tolerant and cost-efficient ring maintenance. Limitations related to failure handling are formally identified, providing strong guarantees to develop applications on top of the relaxed-ring architecture. Besides permanent failures, the paper analyses temporary failures and broken links, which are often ignored.	algorithm;cost efficiency;fault tolerance;join (sql);level of detail;link rot;lookup table;network partition;overlay network;peer-to-peer;ring network;self-organization	Boris Mejías;Donatien Grolaux;Peter Van Roy	2008		10.1007/978-0-387-85966-8_16	architecture;grid;fault tolerance;peer-to-peer;overlay network;distributed computing;popularity;computer science;joins;ring network	Networks	-11.016863056459519	72.77780981268727	616
31cf37405c94b248d02ba4fadec4be9cc06e26db	refactorings to enable parallelization		We propose program analyses to identify parallelizable code fragments, and program transformations to change those fragments into applications of high-level parallel patterns. The methodology has been worked out, and is presented here, in the context of the Erlang programming language, but the approach is applicable in other languages	automatic parallelization;code refactoring;erlang (programming language);high- and low-level;parallel computing;program transformation;programming language	István Bozó;Viktoria Fordós;Dániel Horpácsi;Zoltán Horváth;Tamás Kozsik;Judit Köszegi;Melinda Tóth	2014		10.1007/978-3-319-14675-1_7	programming language;erlang (programming language);theoretical computer science;parallelizable manifold;list comprehension;computer science;control flow graph	PL	-15.701349496085923	36.755257157328245	617
152440d1ce77afed20a521c9133707e3b467af99	modeling and analysis of preemptive priority based handoffs in integrated wireless mobile networks	blocking probability;markov chain model;average transmission delay preemptive priority based handoffs integrated wireless mobile networks priority reservation preemptive priority procedures voice calls data calls voice handoff request calls data handoff request calls queues free channels three dimensional markov chain model blocking probabilities;queueing theory;wireless network;system performance;three dimensional;integrated voice data communication mobile radio markov processes queueing theory;mobile radio;markov processes;integrated voice data communication;intelligent networks quality of service real time systems mobile computing microcell networks traffic control computer networks distributed computing analytical models wireless networks;analytical model;mobile network;modeling and analysis	In this paper, we propose an analytical modeling for integrated wireless networks with priority reservation and preemptive priority procedures. In our scheme, calls are separated into four different classes: originating voice calls, originating data calls, voice and data handoff request calls. In the proposed scheme, there are queues for handoff request calls. Some number of channels in each cell are reserved exclusively for handoff request calls. Out of this number of channels, some are reserved exclusively for voice handoff requests. The remaining channels are shared by both originating and handoff request calls. Higher priority is given to voice handoff requests over data handoff requests and can preempt the service of ongoing data calls if on arrival it finds no free channels. A three-dimensional Markov chain model is used to determine system performance in terms of blocking probabilities of originating calls, forced termination probability of voice handoff requests calls, and average transmission delay of data calls.	preemption (computing)	Qing-An Zeng;Dharma P. Agrawal	2001		10.1109/VTC.2001.956603	three-dimensional space;cellular network;markov chain;real-time computing;telecommunications;computer science;wireless network;mathematics;computer performance;markov process;queueing theory;statistics;computer network	Mobile	3.732056016711163	98.18995398592172	618
2900e02143484212838f197a173ed670d93d6e4b	teaching digital libraries in spain: context and experiences	curriculum development;educational practices;undergraduate study;information technology;information science education;trend analysis;education work relationship;educational change;influence of technology;foreign countries;electronic libraries;graduate study;curriculum evaluation;instructional innovation	The situation of digital libraries teaching and learning in Spain up to 2008 is examined. A detailed analysis of the different curricula and subjects is provided both at undergraduate and postgraduate level. Digital libraries have been mostly a postgraduate topic in Spain, but they should become mainstream, with special subjects devoted to them, and ICT and traditional library topics should be brought together in a closer way.	digital library;experience;library (computing)	Francisco-Javier García Marco	2009	Education for Information	10.3233/EFI-2009-0878	library science;engineering management;political science;pedagogy	HCI	-75.33291802471973	-31.66330695707833	619
f129b070604313fcf965f27cc969208902e95da2	learning sites, references and notes					2009	Public Library Quarterly	10.1080/01616840802678881		Theory	-40.80329936882732	-16.132949808361648	620
41257871f2df7b6bced89690e1d761c7d8da1884	examining mobile-ip performance in rapidly mobile environments: the case of a commuter train	railways;adaptive mobile networking protocols;base station interleaving distances;performance evaluation;multimedia applications;information science;packet loss;in train wireless internet access;simulation;packet radio networks;mobile ip performance;multimedia application;tcp sessions;handoff;transport protocols;udp sessions;mobile environment;computer aided software engineering;internet;land mobile radio;adaptive systems;base station;streaming media;railways land mobile radio performance evaluation transport protocols packet radio networks multimedia communication adaptive systems internet;wireless internet;multimedia communication;commuter train;access protocols;ip networks;vehicles;mobile environments;adaptive mobile networking protocols mobile ip performance mobile environments commuter train in train wireless internet access multimedia applications throughput simulation handoff packet loss tcp sessions udp sessions base station interleaving distances;computer aided software engineering throughput internet ip networks home automation information science access protocols vehicles streaming media delay;mobile network;mobile ip;home automation;throughput	Trains travel at speeds ranging from 0 to 80m/s (0 to 288 Km/hr). Providing in-train wireless Internet access to multimedia applications will require the use of a mobile networking protocol, such as Mobile-IP, to achieve uninterrupted connectivity. Although Mobile-IP represents a promising solution, its performance under “extreme” mobility is questionable. We simulated a train scenario and identified the limitations of the current mobile-IP standard in terms of throughput, handoff, and packet loss of a train moving at different velocities. We investigated the performance of UDPand TCP-sessions, and examined the effect of different base station interleaving distances on throughput and packet loss. The results presented in this paper are part of an investigative research into adaptive mobile networking protocols in rapidly mobile networks.	communications protocol;forward error correction;internet access;internet protocol suite;mobile ip;network packet;throughput	Edwin Hernandez;Abdelsalam Helal	2001		10.1109/LCN.2001.990809	embedded system;cellular network;home automation;throughput;the internet;telecommunications;information science;computer science;base station;packet loss;computer-aided software engineering;transport layer;mobile ip;computer network	Mobile	-10.7620888033895	91.17541982851071	621
83b3104f08f7b071285f687d587e7a4bf99c73fc	an analysis of critique diversity in case-based recommendation	recommender system	Critiquing is a well-known form of user feedback in casebased recommender systems. A critique encodes the users preference in relation to a particular feature. For example, in a digital camera recommender a user may be allowed to indicate whether they are interested in cameras with a lower resolution than the one currently presented; so ‘lower resolution’ is an example of a critique over the resolution feature. Recent research demonstrates how the dynamic generation of compound critiques – critiques that operate over multiple features – can deliver significant performance improvements. However user-studies highlight diversity problems that arise during critique generation; for example, one compound critique might constrain resolution, memory and zoom while another might constrain resolution, memory and price. In this paper we describe how critique diversity can be improved and demonstrate that this can lead to significant usability and per-	digital camera;display resolution;recommender system;usability	Kevin McCarthy;James Reilly;Lorraine McGinty;Barry Smyth	2005			user modeling;digital camera;recommender system;usability;computer science;zoom;performance improvement;computer user satisfaction;multimedia	AI	-36.80392681374234	-51.36759073201668	622
81511e5bb877e2f8facb9472d8557dc666437e9a	the grid4all ontology for the retrieval of traded resources in a market-oriented grid	ontologie;resource offer;resources matchmaking;economie marche;systeme recherche;retrieval of traded resources;resource matchmaking;search system;grid;market economy;semantic information system;grid resources;resource offers;sistema investigacion;rejilla;market oriented resource retrieval;market oriented grid;market orientation;resource request;grille;traded resources;ontologia;ontology;economia mercado;resource requests;grid economy	One of the most challenging problems in grid environments concerns the matchmaking between resource requests and offers. As it happens in the physical economy, grid economy must be supported by services that locate resources based not only on their characteristics, but also on market-related properties, offerspsila and requestspsila properties and constraints, as well as on declarative specifications of peerspsila (providers and consumers) features. Resource retrieval in the context of a grid economy extends the notion of resource matchmaking to the process of discovering those markets that trade resources through market orders. This paper describes an ontology that represents resource orders (offers and requests) in a market-oriented resource retrieval process, showing preliminary results of its utilization for the retrieval of traded resources.		George A. Vouros;Andreas Papasalouros;Konstantinos Kotis;Alexandros G. Valarakos;Konstantinos Tzonas;Xavier Vilajosana;Ruby Krishnaswamy;Nejla Amara-Hachmi	2008	2008 International Conference on Complex, Intelligent and Software Intensive Systems	10.1504/IJWGS.2008.022545	knowledge management;ontology;database;grid;world wide web	HPC	-42.607293550114775	14.410093941043474	623
83371e267e96e2105343d63f0436c9aece0a61ef	macroblock-based algorithm for dual-bitstream mpeg video streaming with vcr functionalities	network bandwidth;video streaming;decoding;mpeg video;streaming media video recording decoding network servers video compression transform coding bandwidth signal processing algorithms costs switches;video compression;client server systems;dual bitstream mpeg video streaming;transform coding;predictive processing techniques;conference paper;reverse playback;network servers;decoding video streaming client server systems video on demand;streaming media;decoder complexity;vcr functionalities;video on demand;forward encoded bitstream;video recording;bandwidth;digital video;signal processing algorithms;decoder complexity dual bitstream mpeg video streaming vcr functionalities reverse playback predictive processing techniques backward encoded bitstream forward encoded bitstream macroblock network bandwidth;switches;video cassette recorder;macroblock;backward encoded bitstream	Reverse playback is the most common video cassette recording (VCR) function in many digital video players. However, the predictive processing techniques employed in MPEG severely complicate reverse-play operation. One approach to achieve reverse playback is to store an additional backward-encoded bitstream on the server. Once the client requests a backward-play operation, the server selects an appropriate frame for the client from either the forward or backward-encoded bitstream by considering the cost of network bandwidth and the decoder complexity. Unfortunately, the forward- and backward-encoded bitstreams are encoded separately. The frame that was previously decoded. by the client may not be exactly identical to the reference of the current selected frame and a drift problem occurs. We propose a macroblock-based approach to alleviate the drift problem with minimal requirements on the network bandwidth and the decoder complexity. Novel macroblock-based techniques are used to manipulate the necessary macroblocks in the compressed-domain and the server then sends the processed macroblocks to the client machine. Experimental results show that, as compared to the conventional dual-bitstream system, the new streaming system enhances the quality of the reconstructed frame significantly.	algorithm;bitstream;client (computing);compact cassette;digital video;generalized filtering;macroblock;moving picture experts group;requirement;server (computing);simulation;streaming media;tor messenger;videocassette recorder	Tak-Piu Ip;Yui-Lam Chan;Chang-Hong Fu;Wan-Chi Siu	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465176	data compression;embedded system;real-time computing;transform coding;computer hardware;network switch;computer science;mathematics;block-matching algorithm;videocassette recorder;macroblock;bandwidth;statistics	Arch	44.16364973333967	-20.787307630321674	624
a9ec0acc16f816dde98a4d4b7c4e0ecf68b5c32d	energy-efficient hardware/software co-synthesis for a class of applications on reconfigurable socs	application development;energy efficiency;dynamic programming;design process;reconfigurable system;software defined radio;energy efficient;dynamic programming algorithm;reconfigurable logic;energy dissipation;linear pipelines;indexing terms;hardware software cosynthesis;embedded system;synthesis;chip;reconfigurable soc;embedded systems;system on chip;hardware software;performance model;greedy algorithm;wireless systems;hardware implementation;reconfigurable	Reconfigurable System-on-Chip (RSoC) devices incorporate various components, such as processor core, reconfigurable logic, memory, etc., onto a single chip. They are being used to implement many wireless embedded systems, where energy efficiency is a major concern. When an application is synthesized on RSoCs, part of it can be executed using hardware implementations on the reconfigurable logic or software implementations on the processor core. Besides, the communication and reconfiguration costs between the tasks can significantly impact the overall system energy dissipation depending on how the application is synthesized on RSoC. In order to develop applications on RSoCs for energy efficiency, we propose a threestep design process in this paper. We develop (a) a performance model to abstract a general class of RSoC architectures for application development, (b) a mathematical formulation of the energy-efficient synthesis problem for a class of applications, and (c) a dynamic programming algorithm that minimizes the system energy dissipation. We illustrate our approach by implementing two beamforming applications on a state-of-theart RSoC device. Beamforming is one of the key techniques for improving the capacity of wireless systems such as software defined radio. Compared with a greedy algorithm, reduction in energy dissipation ranging from 41% to 54% is observed in our experiments.	beamforming;dynamic programming;embedded system;emoticon;experiment;greedy algorithm;multi-core processor;reconfigurable computing;system on a chip	Jingzhao Ou;Seonil B. Choi;Viktor K. Prasanna	2005	IJES	10.1504/IJES.2005.008811	embedded system;computer architecture;real-time computing;computer science;dynamic programming;efficient energy use	EDA	1.4322407142026286	54.13884229143252	625
48f4fa43c6691c57f8bce89e83d96da06c783c10	semantic context consolidation and rule learning for optimized transport assignments in hospitals	ibcn	The increase of ICT infrastructure in hospitals offer opportunities for cost reduction by optimizing workflows, while maintaining quality of care. This work-in-progress poster details the AORTA system, which is a semantic platform to optimize transportation task scheduling and execution in hospitals. It provides a dynamic scheduler with an upto-date view about the current context by performing semantic reasoning on the information provided by the available software tools and smart devices. Additionally, it learns semantic rules based on historical data in order to avoid future delays in transportation time.	itil;scheduling (computing);semiconductor consolidation;smart device	Femke Ongenae;Pieter Bonte;Jeroen Schaballie;Bert Vankeirsbilck;Filip De Turck	2016		10.1007/978-3-319-47602-5_19	real-time computing;simulation;computer science;knowledge management;artificial intelligence;database;world wide web	HCI	-53.083644345883094	9.383524090070626	626
8c0bcf589a7cb0a19c13e0fc434b203b4f4c28ab	an approach based on the i-iglowg and the iglwg operators to multiple attributes group decision making		In this paper, we investigate the multiple attribute group decision making problems with interval grey linguistic variables. We first introduce some operations on the interval grey linguistic variables. Then, we further develop the induced interval grey linguistic ordered weighted geometric (I-IGLOWG) operator. Then, we apply the induced interval grey linguistic ordered weighted geometric (I-IGLOWG) operator and interval grey linguistic weighted geometric (IGLWG) operator to deal with multiple attribute group decision making with the interval grey linguistic variables. Finally, an illustrative example for supplier selection in supply chain management with the interval grey linguistic variables is given to verify the developed approach.		Yan-Xia Zhang;Qian Zhang;Jin Zhao	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-131093	artificial intelligence;machine learning;mathematics	Robotics	-2.3472718642943775	-21.104399663760624	627
c7c5c46f48410eb9249de4dbc653cd742a2b179f	a non-stationary wideband channel model for massive mimo communication systems	massive mimo channel nonstationary wideband channel model massive mimo communication systems multiconfocal ellipse 2d channel model spherical wavefront plane wavefront assumption space time frequency correlation function power imbalance antenna array;telecommunication channels antenna arrays correlation methods direction of arrival estimation fading channels mimo communication statistical analysis;antenna arrays;arrays;multi confocal ellipse channel model;arrays mimo receiving antennas channel models antenna arrays transmitting antennas;birth death process massive mimo multi confocal ellipse channel model spherical wavefront non stationarity;receiving antennas;transmitting antennas;channel models;birth death process;mimo;non stationarity;massive mimo;spherical wavefront	This paper proposes a novel non-stationary wideband multi-confocal ellipse two dimensional (2-D) channel model for massive multiple-input multiple-output (MIMO) communication systems. Spherical wavefront is assumed in the proposed channel model, instead of the plane wavefront assumption used in conventional MIMO channel models. In addition, the birth-death process is incorporated into the proposed model to capture the dynamic properties of clusters on both the array and time axes. Statistical properties of the channel model such as the space-time-frequency correlation function and power imbalance on the antenna array are studied. The impact of the spherical wavefront assumption on the statistical properties of the channel model is investigated. Furthermore, numerical analysis shows that the proposed channel model is able to capture specific characteristics of massive MIMO channel as observed in measurements.	channel (communications);mimo;numerical analysis;stationary process	Shangbin Wu;Cheng-Xiang Wang;Harald Haas;Hadi M. Aggoune;Mohammed M. Alwakeel;Bo Ai	2015	IEEE Transactions on Wireless Communications	10.1109/TWC.2014.2366153	3g mimo;multi-user mimo;telecommunications;mathematics;birth–death process;precoding;statistics;mimo	Mobile	28.988588888050455	77.71252780158974	628
ca6026cb48363fd90613dbd8bfc3e39a03ebf73b	mixed reality for fun learning in primary school	new technology;primary school;teaching tool;fun learning;learning environment;solar system;mixed reality;tangible interaction	In this poster, classroom based Mixed Reality teaching tools are introduced. Our aims are introducing the new technology to primary schools, providing a more interesting and attractive learning environment to students. Cooperating with students and teachers, all contents are based on the textbook and design by teachers, classroom based teaching tools - Solar system and Plants system are produced and used in school.	mixed reality	Wei Liu;Adrian David Cheok;Sim Hwee;Ang Ivene	2006		10.1145/1178823.1178945	simulation;engineering;teaching and learning center;teaching method;multimedia;pedagogy	HCI	-70.69372841458757	-36.91198809048484	629
417a3985dbdd2369f099d194553f1e03b3fba736	design of fractal-based cmos bandpass filter for wirelesshd system	mm wave;insertion loss;dual mode;transmission loss;wirelesshd;fractal;bandpass filter;cmos	We proposed a fractal-based dual-mode bandpass filter (BPF) using a standard CMOS process for application of 60 GHz WirelessHD system. We first investigated the effect of coupling feedlines of I/O ports set at different layer of M3 and M4 layer on the transmission loss of the resonator, and verified the nature coupling of fractal-based dual-mode filter. Experimental result shows that the designed filter with a fractional-bandwidth (FBW) of 23%, an insertion loss about 7 dB and return loss larger than 10 dB. Additionally, two transmission zeros are appeared at the passband edges, thus much improve the selectivity of the proposed CMOS BPF. The result indicates that fractal-based structure is feasible and can meet the requirement in the mm-wave application. & 2011 Elsevier Ltd. All rights reserved.		Wei-Yu Chen;Shoou-Jinn Chang;Min-Hang Weng;Cheng-Yuan Hung	2011	Microelectronics Journal	10.1016/j.mejo.2011.08.004	insertion loss;electronic engineering;fractal;telecommunications;engineering;control theory;band-pass filter;extremely high frequency;cmos	EDA	65.56436140499564	56.188639069802726	630
0bce322337deb844ef0c2a710d295637d9fe2cc5	global properties of a delayed hiv infection model with ctl immune response	hiv infection;global attractivity;lyapunov functional;ctl immune response	In this paper, we study a delayed six-dimensional human immunodeficiency virus (HIV) model with Cytotoxic T Lymphocytes (CTLs) immune response. Our model describes the interaction of HIV with two target cells: CD4+ T cells and macrophages. We derive that the global asymptotic attractivity of the model is completely determined by the basic reproduction number R0R0 and the immune reproduction number R0∗ for the viral infection. By constructing Lyapunov functionals, we have shown that the infection-free equilibrium E0E0, the immune-free equilibrium E1E1 and the chronic-infection equilibrium E2E2 are globally asymptotically attractive when R0⩽1,R0∗⩽1 R0∗>1, respectively.		Xia Wang;Ahmed M. Elaiw;Xinyu Song	2012	Applied Mathematics and Computation	10.1016/j.amc.2012.03.024	mathematical optimization;virology;ctl*;virus;cytotoxic t cell;immune system;immunodeficiency;lyapunov function;basic reproduction number;mathematics	ECom	77.105603554499	4.694568453262271	631
5d76704bee53005d5df8d652ff6927d83a24608c	fine-grained cuda-based parallel intra prediction for h.264/avc	cuda architecture;intra prediction;optimized encoding order;various prediction formula;fine-grained parallelism;fine-grained cuda-based parallel intra;significant branch instruction;significant encoding time reduction;block-level parallelism;previous work;proposed algorithm	Recently, the power of the Graphics Processing Unit (GPU) has largely increased, whereas previous works of intra prediction on the GPU could not efficiently exploit the massive parallel opportunity. The related work only achieves frame-level, slice-level or block-level parallelism. It is a challenge to implement fine-grained parallelism on the Compute Unified Device Architecture (CUDA), such as pixel-level and mode-level, because the irregular formulas of intra prediction and the constraints posed by H.264/AVC cause significant branch instructions and the CUDA architecture is inherently not good at handling branches. In this paper, a CUDA-based approach that adopts fine-grained parallelism is presented. By transforming the various prediction formulas to the same form and introducing the predictor unit, an algorithm based on a lookup table is proposed to efficiently eliminate the branches. In addition, the combinatorial frame technique and the optimized encoding order are adopted to maximize the parallelism. Experimental results show that significant encoding time reduction can be achieved and the proposed algorithm outperforms previous works.	cuda;h.264/mpeg-4 avc	Wenbin Jiang;Min Long;Hai Jin;Pengcheng Wang	2014		10.1145/2597176.2578266	parallel computing;real-time computing;computer science;theoretical computer science	ML	10.893891801993194	38.932533473439385	632
e2bdd586635580c91b1753825ca3a263a4cbf9dd	automated question answering system for community-based questions		The emergence of community question answering sites, such as, Yahoo! Answer (Y!A), and Quora, indicate that for certain information needs, users prefer receiving focused answers to their questions, rather than a list of URLs from search results. This trend has sparked a rich area of investigation at the intersection of Information Retrieval (IR), Natural Language Processing (NLP), and Machine Learning (ML) of Automated Question Answering (QA). In this paper, we present our attempt at developing an efficient QA system for both factoid and non-factoid questions from any domain. Empirical evaluation of our system using multiple datasets demonstrates that our system outperforms the best system from the TREC LiveQA tracks, while keeping the response time to under less than half a minute.	emergence;information needs;information retrieval;machine learning;natural language processing;question answering;response time (technology);text retrieval conference;yahoo! answers	Chanin Pithyaachariyakul;Anagha Kulkarni	2018			natural language processing;artificial intelligence;machine learning;question answering;computer science	Web+IR	-31.231238708781685	-63.36038591129102	633
84e86623d253485f39810d2231954deccefee500	guided-processing outperforms duty-cycling for energy-efficient systems		Energy efficiency is highly desirable for sensing systems in the Internet of Things. A common approach to achieve low-power systems is duty cycling, where components in a system are turned OFF periodically to meet an energy budget. However, this paper shows that such an approach is not necessarily optimal in energy efficiency, and proposes guided processing as a fundamentally better alternative. The proposed approach offers: 1) explicit modeling of performance uncertainties in system internals; 2) a realistic resource consumption model; and 3) a key insight into the superiority of guided processing over duty cycling. Generalization from the cascade structure to the more general graph-based one is also presented. Once applied to optimize a large-scale audio sensing system with a practical detection application, empirical results show that the proposed approach significantly improves the detection performance (up to 1.7 times and 4 times reduction in false alarm and miss rate, respectively) for the same energy consumption, when compared with the duty-cycling approach.	call of duty: black ops;duty cycle;explicit modeling;ibm power systems;internet of things;low-power broadcasting	Long N. Le;Douglas L. Jones	2017	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2017.2690909	simulation;telecommunications;engineering;artificial intelligence	Embedded	13.611867743714157	68.94875792259035	634
12003a7d65c4f98fb57587fd0e764b44d0d10125	face recognition in the wild with the probabilistic gabor-fisher classifier	feature extraction face recognition databases face probabilistic logic batteries videos;image classification face recognition feature extraction gabor filters;pasc database face recognition wild gabor magnitude features probabilistic linear discriminant analysis probabilistic gabor fisher classifier gabor filters feature vector extraction plda pgfc technique face verification point and shoot face recognition challenge database	The paper addresses the problem of face recognition in the wild. It introduces a novel approach to unconstrained face recognition that exploits Gabor magnitude features and a simplified version of the probabilistic linear discriminant analysis (PLDA). The novel approach, named Probabilistic Gabor-Fisher Classifier (PGFC), first extracts a vector of Gabor magnitude features from the given input image using a battery of Gabor filters, then reduces the dimensionality of the extracted feature vector by projecting it into a low-dimensional subspace and finally produces a representation suitable for identity inference by applying PLDA to the projected feature vector. The proposed approach extends the popular Gabor-Fisher Classifier (GFC) to a probabilistic setting and thus improves on the generalization capabilities of the GFC method. The PGFC technique is assessed in face verification experiments on the Point and Shoot Face Recognition Challenge (PaSC) database, which features real-world videos of subjects performing everyday tasks. Experimental results on this challenging database show the feasibility of the proposed approach, which improves on the best results on this database reported in the literature by the time of writing.	database;decimation (signal processing);dimensionality reduction;experiment;facial recognition system;feature vector;gabor filter;linear discriminant analysis;naive bayes classifier	Simon Dobrisek;Vitomir Struc;Janez Krizaj;France Mihelic	2015	2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2015.7284835	computer vision;computer science;machine learning;pattern recognition	Vision	35.76718560907457	-51.837715354078036	635
632b722c64ae0327191b663abb36d3d80897c363	a novel swarm robot simulation platform for warehousing logistics		Swarm robot systems are widely applied in the warehouse logistics, which effectively improve logistics efficiency. Task allocation is the core problem for swarm robot systems. Therefore, evaluation of the task allocation strategy has very important practical significance. However, current simulation platforms, such as GAZEBO, require a lot of setup work before testifying the task allocation strategy for various applications. Regarding warehousing logistics, a few special factors need to be considered: man-robot coexistence in working environment, the energy consumption of robots and collision avoidance among robots. Therefore, we propose a novel swarm robot simulation platform, called MultiBots, based on multi-agent pathfinding (MAPF) method and collision avoidance strategy, which can correctly evaluate the effectiveness of task allocation strategy. Moreover, we design the charging process to supplement the energy consumption of robots. The experimental results show that the proposed MultiBots satisfies the requirements of task allocation strategy evaluation in warehouse logistics scenarios. The MultiBots can be applied in testifying the efficiency of task allocation strategy for logistics systems.	coexist (image);logistics;loss function;motion planning;multi-agent system;optimization problem;pathfinding;requirement;robot;simulation;strategic management;swarm	Yandong Liu;Lujia Wang;Huaiyang Huang;Cheng-Zhong Xu	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324822	resource management;swarm behaviour;control engineering;task analysis;simulation;robot;engineering;evaluation strategy;energy consumption;pathfinding;robot kinematics	Robotics	19.579212358053027	-2.5916833230190597	636
f8d1a9281115415a9ec7bcf813978f76e7c8d20f	coincidence theorems for set-valued mappings and ekeland's variational principle in fuzzy metric spaces	metric space;fixed point theorem;ekeland s variational principle;fuzzy coincidence theorem;set valued mapping;fuzzy metric space;probabilistic metric space;fuzzy variational principle;variational principle	Abstract   In this paper, we establish a coincidence theorem for set-valued mappings in fuzzy metric spaces with a view to generalizing Downing-Kirk's fixed point theorem in metric spaces. As consequences, we obtain Caristi's coincidence theorem for set-valued mappings and a more general type of Ekeland's variational principle in fuzzy metric spaces. Further, we also give a direct simple proof of the equivalence between these two theorems in fuzzy metric spaces. Some applications of these results to probabilistic metric spaces are presented.	calculus of variations;ekeland's variational principle;ivar ekeland	Jong Soo Jung;Yeol Je Cho;Shin Min Kang;Shih-Sen Chang	1996	Fuzzy Sets and Systems	10.1016/0165-0114(95)00084-4	convex metric space;coincidence point;mathematical optimization;mathematical analysis;injective metric space;topology;fubini–study metric;metric differential;product metric;variational principle;metric;fisher information metric;metric space;intrinsic metric;ultrametric space;stein's method;mathematics;t-norm;equivalence of metrics;fixed-point theorem;fréchet space;uniform continuity;metric map	ML	45.381115041396015	23.545446341016778	637
5f7c99da218dd2f5830e32cfafbbbaf5a0900418	character recognition through feature vector using weighted penalty by order.	feature vector;character recognition		feature vector	Byongmo Lee;Euiyoung Cha	2002			feature vector;feature;feature extraction;computer science;machine learning;pattern recognition;feature	Vision	33.77619845603147	-59.20519506680966	638
d0afe9f7e8669b842592a37e7c0096a661b9ba1b	calculating equilibrium probabilities for &lgr;(n)/ck/1/n queues	simulation;calibration;modeling;real time;benchmark;distributed processing;equilibrium state;birth death process	Equilibrium state distributions are determined for queues with load-dependent Poisson arrivals and service time distributions representable by Cox's generalized method of stages. The solution is obtained by identifying a birth-death process that has the same equilibrium state distribution as the original queue. Special cases of two-stage (C2) and Erlang-k (Ek) service processes permit particularly efficient algorithms for calculating the load - dependent service rates of the birth-death process corresponding to the original queue. Knowing the parameters of the birth-death process, the equilibrium state probabilities can be calculated straight-forwardly. This technique is particularly useful when subsystems are reduced to flow-equivalent servers representing the complementary network.	message queue	Raymond Marie	1980	SIGMETRICS Performance Evaluation Review	10.1145/1009375.806155		Vision	8.499421963849265	10.299329960133447	639
cd45427429fcf515e11512816f13f587eb749a19	a multidimensional typology of games		This paper builds on a general typology of textual communication (Aarseth 1997) and tries to establish a model for classifying the genre of “games in virtual environments”— that is, games that take place in some kind of simulated world, as opposed to purely abstract games like poker or blackjack. The aim of the model is to identify the main differences between games in a rigorous, analytical way, in order to come up with genres that are more specific and less ad hoc than those used by the industry and the popular gaming press. The model consists of a number of basic “dimensions”, such as Space, Perspective, Time, Teleology, etc, each of which has several variate values, (e.g. Teleology: finite (Half-Life) or infinite (EverQuest. Ideally, the multivariate model can be used to predict games that do not yet exist, but could be invented by combining the existing elements in new ways.	biological anthropology;blackjack;everquest;hoc (programming language);principle of good enough;qualitative comparative analysis;simulation;starcraft	Espen Aarseth;Solveig Marie Smedstad;Lise Sunnanå	2003			random variate;machine learning;social psychology;artificial intelligence;teleology;multivariate statistics;mathematics;typology	NLP	-52.35269910016415	-23.428040801107223	640
ffc1436bf08f218cd918ee721adef82aa3717854	model development in discrete-event simulation and system dynamics: an empirical study of expert modellers	modelizacion;protocol analysis;empirical study;systeme evenement discret;modele empirique;methode empirique;modelling practice;system dynamics;estructuracion;metodo empirico;hd28 management industrial management;simulation;empirical method;comparison;dynamical system;sistema acontecimiento discreto;resolucion problema;modelisation;systeme dynamique;codificacion;discrete event system;conceptual modelling;building simulation;coding;model development;empirical model;structuration;modelo empirico;modele donnee;sistema dinamico;simulation discrete event simulation system dynamics comparison modelling practice;modeling;article;problem solving;resolution probleme;codage;data models;discrete event simulation	An empirical study comparing the model development process followed by experts in DiscreteEvent Simulation (DES) and System Dynamics (SD) modelling is undertaken. Verbal Protocol Analysis (VPA) is used to study the modelling process followed by ten expert modellers (5 SD and 5 DES). Participants are asked to build simulation models based on a case study and to think aloud while modelling. The generated verbal protocols are divided into 7 modelling topics: problem structuring, conceptual modelling, data inputs, model coding, validation & verification, results & experimentation and implementation and then analyzed. Our results suggest that all modellers switch between modelling topics, however DES modellers follow a more linear progression. DES modellers focus significantly more on model coding and verification & validation, whereas SD modellers on conceptual modelling. Observations are made revealing some interesting differences in the way the two groups of modellers tackle the case. This paper contributes towards the comparison of DES and SD.		Antuela A. Tako;Stewart Robinson	2010	European Journal of Operational Research	10.1016/j.ejor.2010.05.011	simulation;computer science;artificial intelligence;empirical research;operations research;protocol analysis	SE	-22.43487206772349	-2.8736211535132217	641
575df3ff02ffbc54766c0452860ea6720f2c0bed	psato: a distributed propositional prover and its application to quasigroup problems	parallel algorithm;search space;distributed computing;efficient implementation;propositional logic;cyclic group;network of workstation;davis putnam;cumulant;concurrent process;propositional satisfiability	We present a distributed/parallel prover for propositional satisfiability (SAT), called PSATO, for networks of workstations. PSATO is based on the sequential SAT prover SATO, which is an efficient implementation of the Davis–Putnam algorithm. The master– slave model is used for communication. A simple and effective workload balancing method distributes the workload among workstations. A key property of our method is that the concurrent processes explore disjoint portions of the search space. In this way, we use parallelism without introducing redundant search. Our approach provides solutions to the problems of (i) cumulating intermediate results of separate runs of reasoning programs; (ii) designing highly scalable parallel algorithms and (iii) supporting “fault-tolerant” distributed computing. Several dozens of open problems in the study of quasigroups have been solved using PSATO. We also show how a useful technique called the cyclic group construction has been coded in propositional logic. c © 1996 Academic Press Limited	boolean satisfiability problem;davis–putnam algorithm;distributed computing;fault tolerance;parallel algorithm;parallel computing;propositional calculus;putnam model;scalability;slurm;workstation	Hantao Zhang;Maria Paola Bonacina;Jieh Hsiang	1996	J. Symb. Comput.	10.1006/jsco.1996.0030	discrete mathematics;parallel computing;cyclic group;computer science;theoretical computer science;mathematics;parallel algorithm;dpll algorithm;propositional calculus;algorithm;algebra;cumulant	AI	-11.451166438319687	37.05829090152356	642
c16d663bc7a2340831c333c916c9e1d03c8076aa	empirical evaluation of performance in hybrid 3d and 2d interfaces	input device;human computer interaction;evaluation method;experimental evaluation;3d input device;empirical evaluation;interaction technique;bimanual interaction	Experimental studies of spatial input devices have focused on demonstrating either the superiority of 3D input devices over 2D input devices, or the superiority of bimanual interaction over unimanual interaction. In this paper, we argue that hybrid interfaces that combine a 3D input device with a 2D input device have received little attention up to now and are potentially very useful. We demonstrate by means of an experimental evaluation that working with hybrid interfaces can indeed provide superior performance compared to strictly 3D and 2D interfaces.	input device	Sriram Subramanian;Dzmitry Aliakseyeu;Jean-Bernard Martens	2003			simulation;human–computer interaction;computer science;multimedia;interaction technique;input device	HCI	-46.75431432538025	-44.330174799161284	643
07e0f40812ca07b39679acb95a97e308d61448ec	a repeated signal difference for recognising patterns	brain;neural model;pattern sequence;sustained signal	This paper describes a new mechanism that might help with defining pattern sequences, by the fact that it can produce an upper bound on the ensemble value that can persistently oscillate with the actual values produced from each pattern. With every firing event, a node also receives an on/off feedback switch. If the node fires then it sends a feedback result depending on the input signal strength. If the input signal is positive or larger, it can store an ‘on’ switch feedback for the next iteration. If the signal is negative or smaller it can store an ‘off’ switch feedback for the next iteration. If the node does not fire, then it does not affect the current feedback situation and receives the switch command produced by the last active pattern event for the same neuron. The upper bound therefore also represents the largest or most enclosing pattern set and the lower value is for the actual set of firing patterns. If the pattern sequence repeats, it will oscillate between the two values, allowing them to be recognised and measured more easily, over time. Tests show that changing the sequence ordering produces different value sets, which can also be measured.	action potential;fits;feedback;iteration;neuron;simulation;switch	Kieran Greer	2016	CoRR		real-time computing	ML	16.28337140319195	-70.39790877031562	644
5f724eb656897b55b7c0d32a0fa824a392ab89f9	a plethora of challenges and opportunities	editorial board cloud standards open source ieee cloud computing;standards;cloud;editorial board;ieee cloud computing;open source	Welcome to the second issue of ieee cloud computing. This is such an exciting time for cloud computing, as not only is the field expanding very fast, but also many organizations, communities, and people are involved. Researchers have a plethora of cloud challenges they can work on. Companies are jockeying to grab as many cloud customers as they can, pitching their cloud solutions as superior to the competition. But what is encouraging is that companies, regardless of their cloud portfolio or usage paradigm, are seriously considering open source software for their internal use. At the same time and as expected, companies try to find means to differentiate themselves. Although I plan to devote an entire future issue to open source software for cloud computing, a few of the columns in this issue touch on the topic. “Cloud Tidbits” looks at how the Cloud Foundry is making the case to become the OpenStack of cloud platform as a service (PaaS), and “Blue Skies” discusses cloud interoperability challenges. On a related topic, “StandardsNow” looks at the role of communities in developing cloud standards. As you may recall, the May issue included a roundtable that touched on many cloud-related topics, getting a feel from the experts on current cloud challenges, how the industry is addressing these challenges, and what the endgame might look like. The participants in that roundtable were from industry. This issue features a similar roundtable, but the participants are from academic institutions. Our main objective here is to see how both camps look at cloud and its evolution. Although the flow of the discussion seems different, both camps seem to concur on the highlights of big picture issues and cloud evolution. That said, I’ll leave it to you to compare the two roundtables and fill in the details of how each camp views cloud computing, its challenges, and how cloud technologies are evolving. As I mentioned in the first issue of the magazine, I’ll continue to expand the editorial board to cover areas in which cloud computing plays a large role. The “Cloud and the Government” department will cover the use of cloud computing in governments, seek to build momentum among governments to adopt cloud computing, explore obstacles and opportunities for governments’ use of cloud, highlight barriers that discourage governments from using cloud computing, and inspire governments to update laws that will enable them to effectively deal with IT in general and cloud computing technologies in particular. Laura Taylor of Relevant Technologies will lead this department (see the sidebar for a brief biography). The next issue of IEEE Cloud Computing, scheduled for October 2014, will be a special issue looking at secure cloud computing techniques for big data. Look for articles on the intersection of cloud, big data, and security—an area that requires great deal of attention from us all.	big data;cloud foundry;cloud computing;column (database);interoperability;open-source software;platform as a service;programming paradigm	Mazin Yousif	2014	IEEE Cloud Computing	10.1109/MCC.2014.28	cloud computing security;simulation;cloud computing;computer science;operating system;distributed computing;world wide web	Visualization	-62.52090816971279	-20.513091810976896	645
6c07ca2c8bdb9952b014bfd2f747ddfa2af6b026	new results on exponential convergence for cellular neural networks with continuously distributed leakage delays	continuously distributed delay;cellular neural network;34c25;34k25;leakage term;exponential convergence;34k13	This paper discusses the issue of a class of cellular neural networks with continuously distributed delays in the leakage terms. By applying Lyapunov functional method and differential inequality techniques, without assuming the boundedness conditions on the activation functions, a new delay dependent sufficient condition is derived to ensure that all solutions of the networks converge exponentially to the zero point, which corrects some recent results of Xiong and Meng (Electron J Qual Theory Differ Equ, (10):1–12, 2013. http://www.math.u-szeged.hu/ejqtde/ ).	artificial neural network;converge;electron;lyapunov fractal;social inequality;spectral leakage	Aiping Zhang	2014	Neural Processing Letters	10.1007/s11063-014-9348-7	cellular neural network;real-time computing;computer science;machine learning;control theory;distributed computing	ML	73.60502021726631	2.564862776157023	646
4ede02edfecde288a280a1b59848ecc7a32e075a	on the amplitude distributions of bistatic scattered fields from rough surfaces		Non-Rayleigh distributed radar clutter is widely reported in studies of radar scattering from sea and land surfaces. Existing models of scattered field amplitude distributions have been developed primarily through empirical fits to the statistics of radar backscatter measurements. In contrast, this paper investigates a physics-based approach to determine the amplitude distributions of fields scattered from rough surfaces using Monte Carlo simulations and analytical methods, for both backscattering and bistatic configurations. The rough surface is represented using a “two-scale” model. An individual surface facet contains “small-scale” roughness, for which scattered fields are evaluated using the second-order small slope approximation. Individual surface facets are tilted by the slopes of the “large-scale” roughness in a given observation. The results show that non-Rayleigh amplitude distributions are obtained when tilting is performed, and that the departure from the Rayleigh distribution becomes more significant as the variance of the tilting slope increases. Further analysis shows that this departure results from variations in the mean scattering amplitude from a facet (the texture) as tilting occurs. The distribution of the texture is studied and compared with existing models. Finally, the distribution of the scattered field amplitude is modeled through the compound Gaussian model, first using the distribution of the texture, and then in terms of the probability density function of tilting slopes (which avoids the requirement of the knowledge of the texture distribution). The results from the above two methods are in good agreement and both agree well with the Monte Carlo simulation.	approximation;backscatter (email);clutter;fits;monte carlo method;radar;rayleigh–ritz method;simulation	Hongkun Li;Joel T. Johnson	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2017.2735862	mathematics;artificial intelligence;scattering amplitude;bistatic radar;backscatter;computer vision;surface roughness;monte carlo method;optics;rayleigh distribution;amplitude;scattering	Visualization	82.54143151790547	-67.52169097901145	647
14e831fee31b5598f1d8e0132e60ebabdbcb93f0	media monitoring system for latvian radio and tv broadcasts		Media monitoring allows to capture media exposure of people, organizations and other important topics. This paper presents a media monitoring system for Latvian radio and television broadcasts. This system uses an automatic speech recognition (ASR) module to convert audio and video files to text and to extract keywords of interest. The system has been developed in close cooperation with Latvian information agency LETA.	speech recognition;video file format	Arturs Znotins;Kaspars Polis;Roberts Dargis	2015			multimedia;media monitoring;latvian;computer science;telecommunications	HCI	-28.83327251747209	-88.52670708288547	648
e1b743388e397e6d172bd44d136da08951f4d60c	a new class of bayesian cyclic bounds for periodic parameter estimation	cost function;bayes methods;probability density function;frequency estimation;minimum mean cyclic error bayesian periodic parameter estimation circular statistics cyclic bayesian cramer rao bound cyclic bobrovsky zakai bound cyclic performance bounds;bayes methods cost function probability density function estimation error frequency estimation;signal processing bayes methods direction of arrival estimation frequency estimation maximum likelihood estimation mean square error methods phase estimation probability;von mises parameter estimation bayesian cyclic bounds periodic parameter estimation signal processing applications phase estimation frequency estimation direction of arrival estimation mean squared error mse risk modulo t error bayesian mse lower bounds bayesian cramer rao bound bcrb bobrovsky zakai bound bzb mean cyclic error mce mixed vector parameter estimation maximum a posteriori probability estimators;estimation error	Many practical signal processing applications involve estimation of parameters with periodic nature, such as phase, frequency and direction-of-arrival estimation. The commonly used mean-squared-error (MSE) risk does not take periodicity into account and thus, is inappropriate for periodic parameter estimation in which one is interested in the modulo- T error rather than the plain error value. As a result, MSE lower bounds are not valid for periodic estimation. In addition, conventional Bayesian MSE lower bounds, such as the Bayesian Cramér-Rao bound (BCRB) and the Bobrovsky-Zakai bound (BZB) require restrictive regularity conditions and usually do not exist in periodic settings. An alternative risk, which is commonly used in periodic parameter estimation problems, is the mean-cyclic-error (MCE). In this paper, we establish a new class of Bayesian lower bounds on the MCE of any estimator. The new class includes cyclic versions of the BCRB and the BZB that have less restrictive regularity conditions than those of the conventional BCRB and BZB, respectively. The tightest bound in the proposed class is derived and its tightness is discussed. In addition, the proposed class is extended to mixed vector parameter estimation with both periodic and nonperiodic parameters. The new cyclic lower bounds are compared with the MCE performance of the minimum MCE and maximum a-posteriori probability estimators for von-Mises parameter estimation and for frequency estimation.	bekenstein bound;direction of arrival;estimation theory;linuxmce;mean squared error;quasiperiodicity;signal processing;spectral density estimation;tinymce	Eyal Nitzan;Tirza Routtenberg;Joseph Tabrikian	2016	IEEE Transactions on Signal Processing	10.1109/TSP.2015.2478758	econometrics;mathematical optimization;probability density function;mathematics;statistics	ML	54.49223456824936	11.439908712370912	649
0c9ce7be5f60be3d6d67f1359129af1f252d8047	sparse sampling of non-stationary signal for radar signal processing	estimation theory;compressed sensing;correlation methods;correlation education;radar signal processing compressed sensing correlation methods estimation theory;radar signal processing;wide sense stationary radar signal processing sparse sampling nonstationary signal spectrogram estimation coprime sampling autocorrelation coefficients power spectrum density	Estimating the spectrogram of non-stationary signal relates to many important applications in radar signal processing. In recent years, coprime sampling and array attract attention for their potential of sparse sensing with derivative to estimate autocorrelation coefficients with all lags, which could in turn calculate the power spectrum density. But this theoretical merit is based on the premise that the input signals are wide-sense stationary. In this paper, we take the first step to design coprime sampling algorithm using with non-stationary signal and discuss how to attain the benefits of coprime sampling meanwhile limiting the disadvantages due to lack of observations for estimations.	algorithm;autocorrelation;coefficient;sampling (signal processing);signal processing;sparse matrix;spectral density;spectrogram;stationary process	Qiong Wu;Qilian Liang	2013	2013 IEEE International Conference on Communications Workshops (ICC)	10.1109/ICCW.2013.6649372	multidimensional signal processing;control theory;mathematics;estimation theory;compressed sensing;statistics	Robotics	55.383898763057054	10.919148993396282	650
4821f0fed51b5a8435865e69310f5bf2b02be7f2	on a piecewise-linear approximation for network revenue management	network revenue management;linear programming;approximate dynamic programming;lagrangian relaxation methods	The network revenue management (RM) problem arises in airline, hotel, media, and other industries where the sale products use multiple resources. It can be formulated as a stochastic dynamic program, but the dynamic program is computationally intractable because of an exponentially large state space, and a number of heuristics have been proposed to approximate its value function. In this paper we show that the piecewise-linear approximation to the network RM dynamic program is polynomial-time solvable; specifically we show that the separation problem can be solved a linear program. Moreover, the resulting compact formulation of the approximate dynamic program turns out to be exactly equivalent to the Lagrangian relaxation, an earlier heuristic method proposed for the same problem. We perform numerical comparison of solving the problem by generating separating cuts or as our compact linear program. We discuss extensions to versions of the network RM problem with overbooking as well as the difficulties of extending it to the choice model of network revenue RM.	approximation algorithm;basis (linear algebra);basis function;bellman equation;choice modelling;combinatorial optimization;computational complexity theory;concave function;decision problem;dynamic programming;heuristic (computer science);lagrange multiplier;lagrangian relaxation;linear approximation;linear programming relaxation;mathematical optimization;numerical analysis;overselling;polynomial;state space;subderivative;subgradient method;time complexity	Sumit Kunnumkal;Kalyan T. Talluri	2016	Math. Oper. Res.	10.1287/moor.2015.0716	mathematical optimization;linear programming;mathematics;mathematical economics	AI	23.26411847636353	11.095456333313686	651
d15a54213d00b2f9b1b2db21c7c61c78be3de25c	exploring aapi identity online: political ideology as a factor affecting identity work on reddit		Asian Americans and Pacific Islanders (AAPIs) are often perceived as a monolithic group, despite their distinct composition of ethnic cultures, political ideologies, and socioeconomic backgrounds. AAPIs increasingly engage in online forums to disclose their experiences and opinions, and in doing so, take part in lengthy discussions that shape the views of their community. We retrieved over 72,000 Reddit comments posted between January to July 2016 for a mixed-methods study of AAPI identity work, analyzing discursive patterns of user-deleted and banned comments. We found that while conservative AAPIs tend to comment anonymously more frequently, progressive AAPIs are less likely to ban comments that did not fit the behavior and norms of their community. AAPI redditors engage differently between conservative and progressive online communities through a process of what we conceptualize as identity work as deliberation.	conformity;experience;online community;progressive scan	Bryan Dosono;Bryan C. Semaan;Jeff Hemsley	2017		10.1145/3027063.3053185	public relations;political science;social psychology	HCI	-82.55673694224629	-16.56029690567678	652
1a503ad37677d9601a85a83fa184a9889c17feb4	towards pragmatic argumentative agents within a fuzzy description logic framework	formal property;fuzzy reasoning;flexible status;classical dung abstract argumentation;current argumentation;fuzzy description logic framework;fuzzy description logic;domain knowledge;towards pragmatic argumentative agent;different type;fuzzy relation;argumentation theory	To bring the level of current argumentation to the expressive and flexible status expected by human agents, we introduce fuzzy reasoning on top of the classical Dung abstract argumentation framework. The system is built around Fuzzy Description Logic and exploits the integration of ontologies with argumentation theory, attaining the advantage of facilitating communication of domain knowledge between agents. The formal properties of fuzzy relations are used to provide semantics to the different types of conflicts and supporting roles in the argumentation. The usefulness of the framework is illustrated in a supply chain scenario.	description logic;fuzzy logic	Ioan Alfred Letia;Adrian Groza	2010		10.1007/978-3-642-21940-5_13	computer science;knowledge management;artificial intelligence;probabilistic argumentation;algorithm	AI	-17.017458602323465	4.751623341438413	653
157635deac56b8149a9b6b8412802e71ac2a8492	statistical analysis and modeling of high definition video traces	high definition video streaming services;analytical models;multimedia communications;network scheduling;workload characterization;communication networks;video streaming;resource allocation;video clustering;video streaming high definition video internet multimedia communication resource allocation scheduling social networking online statistical analysis telecommunication standards;high definition video trace modeling;cluster analysis;youtube;internet;statistical analysis;autoregressive processes;streaming media;cluster analysis statistical analysis high definition video trace modeling internet online standard high definition video streaming services youtube hulu network scheduling resource allocation factor analysis;community networks;factor analysis;autoregressive processes high definition video streaming media mathematical model encoding correlation analytical models;network traffic;internet usage;scheduling;communication networks workload characterization factor analysis video clustering multimedia communications;telecommunication standards;multimedia communication;social networking online;online standard;high definition video;mathematical model;correlation;encoding;hulu;high definition	High definition video streams are gaining larger shares of the Internet usage for typical users on daily basis. This is an expected result of the current boom in the online standard and high definition (HD) video streaming services such as YouTube and Hulu. Because of these video streams' unique statistical characteristics and their high bandwidth requirements, they are considered to be a continuous challenge in both network scheduling and resource allocation fields. In this paper we provide a statistical analysis of over 50 high definition video traces that resembles wide varieties of high definition video traffic workloads. We performed both factor and cluster analysis on our collection of video traces to support a better understanding of video stream workload characteristics and their impact on network traffic. Additionally, we compare and evaluate different modeling approaches for high definition videos traces.	cluster analysis;network packet;requirement;scheduling (computing);streaming media;tracing (software)	Abdel Karim Al Tamimi;Raj Jain;Chakchai So-In	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583026	the internet;resource allocation;computer science;mathematical model;multimedia;cluster analysis;factor analysis;scheduling;world wide web;correlation;encoding;statistics;computer network	HPC	-11.043518162835808	98.84849820747449	654
ae5e3ef11f704200e89bc114752f8ebe78171994	mechanical and assembly units of viral capsids identified via quasi-rigid domain decomposition	capsid proteins;viruses;models molecular;mathematical biology;capsid;reproducibility of results;computer simulation	Key steps in a viral life-cycle, such as self-assembly of a protective protein container or in some cases also subsequent maturation events, are governed by the interplay of physico-chemical mechanisms involving various spatial and temporal scales. These salient aspects of a viral life cycle are hence well described and rationalised from a mesoscopic perspective. Accordingly, various experimental and computational efforts have been directed towards identifying the fundamental building blocks that are instrumental for the mechanical response, or constitute the assembly units, of a few specific viral shells. Motivated by these earlier studies we introduce and apply a general and efficient computational scheme for identifying the stable domains of a given viral capsid. The method is based on elastic network models and quasi-rigid domain decomposition. It is first applied to a heterogeneous set of well-characterized viruses (CCMV, MS2, STNV, STMV) for which the known mechanical or assembly domains are correctly identified. The validated method is next applied to other viral particles such as L-A, Pariacoto and polyoma viruses, whose fundamental functional domains are still unknown or debated and for which we formulate verifiable predictions. The numerical code implementing the domain decomposition strategy is made freely available.	biologic development;capsid;computation;domain decomposition methods;formal verification;mesoscopic physics;muscle rigidity;numerical analysis;polyomavirus;self-assembly;viral life cycle;virion;virus diseases	Guido Polles;Giuliana Indelicato;Raffaello Potestio;Paolo Cermelli;Reidun Twarock;Cristian Micheletti	2013		10.1371/journal.pcbi.1003331	computer simulation;biology;bioinformatics;virology;capsid	Comp.	8.534237588259204	-65.30616272071427	655
789f8c41f5ee9a242abc56796bfccf4e0f58cc47	current conduction mechanism of mis devices using multidimensional minimization system program	optimization method;temperature dependence;fowler nordheim;poole frenkel;mis;current conduction mechanisms	The present work presents an evaluation approach which enables the in-depth analysis of current–voltage (I–V) characteristics of MIS devices to determine their current transport mechanisms using a multidimensional minimization system program.#R##N##R##N#Exemplarily, the current transport mechanisms were determined for a TiN/SiO2/p-Si MOS and a TaN/HfSiO/SiO2/p-Si MIS structure by fitting the analytical expressions for different current transport mechanisms to experimental I–V data in a wide range of applied biases and temperatures. The considered mechanisms for the investigated samples include temperature dependent Fowler–Nordheim (FN) tunneling and Poole–Frenkel (PF) emission as well as ohmic conduction. The presented approach can easily be extended to account for additional mechanisms such as trap assisted tunneling (TAT) if relevant for different samples. In contrast to typical extraction procedures which determine current conduction mechanism parameters sequentially, in this work, the adjustable fit parameters are extracted in a single operation using the Levenberg–Marquardt algorithm (Nash, 1990) to obtain a least-square fit of the model to measured I–V characteristics. Thus, simultaneously occurring current mechanisms can properly be evaluated which allows to determine the fraction of each conduction mechanism quantitatively for each voltage.		N. Rouag;Z. Ouennoughi;Mathias Rommel;K. Murakami;Lothar Frey	2015	Microelectronics Reliability	10.1016/j.microrel.2015.05.001	electronic engineering;chemistry;engineering;electrical engineering;management information systems;forensic engineering	Arch	88.7104518141518	-14.625218910590746	656
3a87197ba45e18e7aba197b8ea578435c25268a5	discrete, nonlinear string vibrations				Donald Greenspan	1970	Comput. J.	10.1093/comjnl/13.2.195	classical mechanics;theoretical computer science;nonlinear system;computer science;vibration	Theory	85.38164806367595	3.174189589261027	657
5bd60f9a52df3651ce54dc4df8f8e9268ecbcc55	token ring network management: change management	change management;ring network	Abstract#R##N##R##N#This article, which is the fifth in the series on Token Ring Network Management, will discuss selected aspects of Change Management that are relevant in today's Token Ring networking environment.	ring network;token ring	Jesper Nilausen	1995	Int. Journal of Network Management	10.1002/nem.4560050104	token bus network;ring network;computer science;distributed computing;computer security;computer network	Networks	-20.952037062259883	87.73392151225677	658
1594ebc13ced92728330a5fdf16903db68d5ec2c	regal-tc: a distributed genetic algorithm for concept learning based on regal and the treatment of counterexamples	distributed genetic algorithms;data mining;genetics;classification rules;cooperative evolution;concept learning;distributed genetic algorithm;multi modalities;first order logic	This paper presents a proposal to improve REGAL, a concept learning system based on a distributed genetic algorithm that learns first-order logic multi-modal concept descriptions in the field of classification tasks. This algorithm has been a pioneer system and source of inspiration for others. Studying the philosophy and experimental behaviour of REGAL, we propose some improvements based principally on a new treatment of counterexamples that promote its underlying goodness in order to achieve better performances in accuracy, interpretability and scalability, so that the new system meets the main requirements for classification rules extraction in data mining. The experimental study carried out shows valuable improvements compared with both REGAL and G-Net distributed genetic algorithms and interesting results compared with some state-of-the-art representative algorithms in this field.	concept learning;data mining;experiment;first-order logic;first-order predicate;genetic algorithm;modal logic;performance;requirement;scalability	L. Ignacio Lopez;Juan M. Bardallo;Miguel A. De Vega;Antonio Peregrín	2011	Soft Comput.	10.1007/s00500-010-0678-8	mathematical optimization;concept learning;computer science;artificial intelligence;machine learning;first-order logic;mathematics;algorithm	AI	9.100088322427565	-44.52257951400073	659
e4bc8e9291fa08962216bfae18235b8587fc7407	designing nearest neighbour classifiers by the evolution of a population of prototypes			k-nearest neighbors algorithm	Fernando Fernández;Pedro Isasi Viñuela	2001			machine learning;artificial intelligence;nearest neighbour classifiers;pattern recognition;computer science;population	Vision	9.07322811556575	-36.76621557150473	660
ad8817eea71a35d5d8be891c604e1118772e0837	efficient effort estimation system viz. function points and quality assurance coverage	constructive cost model;software cost estimation;kemerer model;software system classification;software engineering;software maintenance project effort estimation model;software development effort estimation system;software development industry;function point metric;software quality iso standards pattern classification software cost estimation software maintenance;iso 9126 quality factors;cocomo;albrecht gaffney model;smpeem model;quality assurance coverage;fp matson barnett mellichamp model quality assurance coverage software development effort estimation system quality management software development industry software engineering software cost estimation constructive cost model software system classification cocomo iso 9126 quality factors weighing factors function point metric ms word 2007 albrecht gaffney model kemerer model smpeem model software maintenance project effort estimation model;weighing factors;quality management;ms word 2007;fp matson barnett mellichamp model	Software development effort estimation is important for quality management in the software development industry, yet its automation still remains a challenging issue. Accurate estimation of software effort is critical in software engineering. Existing methods for software cost estimation will use very few quality factors for the estimation. So, in order to overcome this drawback, the authors proposed an efficient effort estimation system based on quality assurance coverage. This study is a basis for the improvement of software effort estimation research through a series of quality attributes along with constructive cost model (COCOMO). The classification of software system for which the effort estimation is to be calculated based on COCOMO classes. For this quality assurance ISO 9126 quality factors are used and for the weighing factors the function point metric is used as an estimation approach. Effort is estimated for MS word 2007 using the following models: Albrecht and Gaffney model, Kemerer model, SMPEEM model (Software Maintenance Project Effort Estimation Model and FP Matson, Barnett and Mellichamp model.	analysis of algorithms;cocomo;cost estimation in software engineering;function point;iso/iec 9126;list of system quality attributes;microsoft word for mac;software development effort estimation;software maintenance;software system;viz: the computer game	H. Azath;R. S. D. Wahida Banu	2012	IET Software	10.1049/iet-sen.2011.0146	reliability engineering;quality management;verification and validation;software sizing;computer science;systems engineering;engineering;software engineering;cocomo;analysis effort method;software quality control;use case points;software quality;software metric;software quality analyst	SE	-64.80894177391298	29.6181989747841	661
f8979b85ad59b2ed7309d08fb38fe0d9813991ea	large-scale seismic waveform quality metric calculation using hadoop	mathematics computing and information science;geosciences	In this work we investigated the suitability of Hadoop MapReduce and Apache Spark for largescale computation of seismic waveform quality metrics by comparing their performance with that of a traditional distributed implementation. The Incorporated Research Institutions for Seismology (IRIS) Data Management Center (DMC) provided 43 terabytes of broadband waveform data of which 5.1 TB of data were processed with the traditional architecture, and the full 43 TB were processed using MapReduce and Spark. Maximum performance of ~0.56 terabytes per hour was achieved using all 5 nodes of the traditional implementation. We noted that I/O dominated processing, and that I/O performance was deteriorating with the addition of the 5 node. Data collected from this experiment provided the baseline against which the Hadoop results were compared. Next, we processed the full 43 TB dataset using both MapReduce and Apache Spark on our 18-node Hadoop cluster. These experiments were conducted multiple times with various subsets of the data so that we could build models to predict performance as a function of dataset size. We found that both MapReduce and Spark significantly outperformed the traditional reference implementation. At a dataset size of 5.1 terabytes, both Spark and MapReduce were about 15 times faster than the reference implementation. Furthermore, our performance models predict that for a dataset of 350 terabytes, Spark running on a 100-node cluster would be about 265 times faster than the reference implementation. We do not expect that the reference implementation deployed on a	apache hadoop;apache spark;baseline (configuration management);computation;experiment;input/output;mapreduce;reference implementation;terabyte;waveform	Steven Magaña-Zook;Jessie M. Gaylord;Douglas R. Knapp;Douglas A. Dodge;Stan D. Ruppert	2016	Computers & Geosciences	10.1016/j.cageo.2016.05.012	real-time computing;geology;computer science;machine learning;data mining;database	HPC	-26.27069767408896	15.465542000874704	662
966ebcc4fcd18830ed1df5b1dd6b63faddb63434	directional local extrema patterns: a new descriptor for content based image retrieval		In this paper, a new algorithm using directional local extrema patterns meant for content-based image retrieval application is proposed. The standard local binary pattern (LBP) encodes the relationship between reference pixel and its surrounding neighbors by comparing gray-level values. The proposed method differs from the existing LBP in a manner that it extracts the directional edge information based on local extrema in 0 $$^{\circ }$$ , 45 $$^{\circ }$$ , 90 $$^{\circ }$$ , and 135 $$^{\circ }$$ directions in an image. Performance is compared with LBP, block-based LBP (BLK_LBP), center-symmetric local binary pattern (CS-LBP), local edge patterns for segmentation (LEPSEG), local edge patterns for image retrieval (LEPINV), and other existing transform domain methods by conducting four experiments on benchmark databases viz. Corel (DB1) and Brodatz (DB2) databases. The results after being investigated show a significant improvement in terms of their evaluation measures as compared with other existing methods on respective databases.	algorithm;benchmark (computing);binary pattern (image generation);content-based image retrieval;database;experiment;local binary patterns;maxima and minima;pixel;viz: the computer game	Subrahmanyam Murala;R. P. Maheshwari;R. Balasubramanian	2012	International Journal of Multimedia Information Retrieval	10.1007/s13735-012-0008-2	computer vision;local binary patterns;pattern recognition;data mining	Vision	37.648065619006736	-59.8276720446919	663
33116a6f6f6aa725c2e4b09edfbe6a7c94bac375	equivalence, query-reachability, and satisfiability in datalog extensions	satisfiability	We consider the problems of equivalence, satisfiability and query-reachability for datalog programs with negation and dense-order constraints. These problems are important for optimizing datalog programs. We show that both query-reachability and satisfiability are decidable for programs with stratified negation provided that negation is applied only to EDB predicates or that all EDB predicates are unary. In the latter case, we show that equivalence is also decidable. The algorithms we present are also used to push constraints from a given query to the EDB predicates. Finally, we show that satisfiability is undecidable for datalog programs with unary IDB predicates, stratified negation and the interpreted predicate  ≠	algorithm;boolean satisfiability problem;bus (computing);cisco ios;datalog;negation as failure;predicate (mathematical logic);reachability;turing completeness;unary operation;undecidable problem	Alon Y. Halevy;Inderpal Singh Mumick;Yehoshua Sagiv;Oded Shmueli	1992		10.1145/153850.153860	discrete mathematics;computer science;algorithm;satisfiability	DB	-8.556524151024592	18.217398419566635	664
1c1180963438970056217250eb3028d8939eb642	autonomous data ingestion tuning in data warehouse accelerators		The IBM DB2 Analytics Accelerator (IDAA) is a state-of-the art hybrid database system that seamlessly extends the strong transactional capabilities of DB2 for z/OS with very fast processing of OLAP and analytical SQL workload in Netezza. IDAA copies the data from DB2 for z/OS into its Netezza backend, and customers can tailor data maintenance according to their needs. This copy process, the data load, can be done on a whole table or just a physical table partition. IDAA also offers an incremental update feature, which employs replication technologies for low-latency data synchronization. The accelerator targets big relational databases with several TBs of data. Therefore, the data load is performance-critical, not only for the data transfer itself, but the system has to be able to scale up to a large number of tables, i. e., tens of thousands to be loaded at the same time, as well. The administrative overhead for such a number of tables has to be minimized. In this paper, we present our work on a prototype, which is geared towards efficiently loading data for many tables, where each table may store only a comparably small amount of data. A new load scheduler has been introduced for handling all concurrent load requests for disjoint sets of tables. That is not only required for a multi-tenant setup, but also a significant improvement for attaching an accelerator to a single DB2 for z/OS system. In this paper, we present architecture and implementation aspects of the new and improved load mechanism and results of some initial performance evaluations.	algorithm;data synchronization;incremental backup;load balancing (computing);multitenancy;online analytical processing;operating system;overhead (computing);prototype;relational database;sql;scheduling (computing);table (database);throughput;user experience;z/os	Knut Stolze;Felix Beier;Jens Müller	2017			database;ingestion;data warehouse;computer science	DB	-15.867865148830388	54.36305260591246	665
b9fa2c7ee66bb08caf5087741bcfb60bdda5d05f	the control of contact forces as related to safe robot/human interaction	parallel manipulator compliance force control kinestatic control;payloads force fasteners manipulators actuators transmission line matrix methods;human robot interaction;manipulator kinematics;contact force control reduced degree of freedom end effector wrench determination compliant characteristics manipulator end effector compliant element theory of kinestatic control motion space torque control constraint contact wrench space planar manipulator control light curtains industrial robotic applications robot human interaction;industrial manipulators;end effectors;torque control;torque control end effectors force control human robot interaction industrial manipulators manipulator kinematics;force control	In industrial robotic applications, robots are segregated from humans. Light curtains are used to power-down the robot if a human comes into its workspace. For advanced interaction with humans, it will be necessary to control contact forces. During in-contact operations, a planar manipulator will have a reduced degree of freedom of motion (3-n) combined with an n degree of constraint contact wrench space. The problem is how to control the manipulator to simultaneously control the contact force/torque as it moves through its allowable motion space. This paper describes the Theory of Kinestatic Control whereby a compliant element is introduced between the manipulator end-effector and the environment. Knowledge of the compliant characteristics of this element allows for the determination of end-effector wrenches that will only cause movement without changing the contact force/torque and end-effector wrenches that will only cause changes in the contact force/torque while producing no motion. A sample implementation is presented.	heuristic (computer science);humans;industrial robot;reference implementation;robot end effector;workspace	Allen Nease;Michael Griffis;Phillip D. Adsit;Carl D. Crane	2013	IEEE ISR 2013	10.1109/ISR.2013.6739658	human–robot interaction;robot end effector;simulation;computer science;artificial intelligence;mobile manipulator	Robotics	68.42984358365788	-22.975993350353267	666
5305c27ea617817de89c85792a6a6e17badae4f5	scenario based stochastic mpc schemes for rivers with feasibility ssurance		Water is a valuable resource, and improved management of rivers by using control techniques is receiving increased attention. Along a river there will typically be inflows from tributaries over which we have no control, but for which forecasts exist. The use of Stochastic Model Predictive Control (S-MPC) or a randomised version of it is a promising control strategy since it can accommodate such forecasts. However, due to uncertainties in the forecasts, the feasibility of the optimisation problem cannot be guaranteed in the presence of constraints. In this paper we consider two schemes for S-MPC of rivers that provide satisfactory results and guarantee feasibility via relaxation of the constraints. Simulation results on a real river show that the schemes perform well.	constraint (mathematics);control theory;linear programming relaxation;mathematical optimization;simulation;stochastic gradient descent	Hasan Arshad Nasir;Simone Garatti;Erik Weyer	2016	2016 European Control Conference (ECC)	10.1109/ECC.2016.7810573	simulation;geography;operations management;management science	Robotics	11.973467857986014	-4.322211383975991	667
0ee6d0cb25e0c69061dcda16dcec3cdb634a7c90	comportamento mimético no abandono de sistemas erp: o caso de uma organização brasileira	institutional isomorphism mimetic pressure coercive pressure;project abandonment	This paper examines the external influences on ERP project abandonment. The recent literature supposes that the adoption of ICT can be motivated by external institutional pressures, in a process called isomorphism. To gain legitimacy, some firms accept mimetic, coercive, and normative pressures arising from its organizational field to adopt some structures and technologies. Same process is expected to occur in ICT abandonment situations. To investigate this proposal, a single case study about a ERP abandonment process was performed. The results show that abandonment field movement was followed by the organization in focus. This confirms that mimetic pressures have significant influence on project abandonment. Finally, contributions and implications for research and practice are discussed.	erp	André Moraes dos Santos;Antonio Carlos Gastaud Maçada	2010			geography;management;cartography	HCI	-83.74273693818336	3.8296074782578646	668
75027334dd5f3373577db3956b504588dab59d01	an analysis of errors in a reuse-oriented development environment	job engineering;statistical study;project management;ada programming language;qualite;program design;cost reduction;calcul erreur;conception programme;organizacion proyecto;ingenieria logiciel;development process;software engineering;computer programs;error analysis;development environment;quality;component reuse;structured programming;estudio estadistico;genie logiciel;organizacion trabajo;etude statistique;calculo error;system development;organisation travail;gestion projet;fortran;technical report;productivity;software reliability;software reuse;concepcion programa;calidad	Component reuse is widely considered vital for obtaining significant improvement in development productivity. However, as an organization adopts a reuseoriented development process, the nature of the problems in development is likely to change. In this article, we use a measurement-based approach to better understand and evaluate an evolving reuse process. More specifically, we study the effects of reuse across seven projects in narrow domain from a single development organization. An analysis of the errors that occur in new and reused components across all phases of system development provides insight into the factors influencing the reuse process. We found significant differences between errors associated with new and various types of reused components in terms of the types of errors committed. In addition, we identified differences when errors are introduced and the effect that the errors have on the development process.	agile software development;code reuse;component-based software engineering;error analysis (mathematics);error detection and correction;open road tolling;rework (electronics);third-party software component	William M. Thomas;Alex Delis;Victor R. Basili	1997	Journal of Systems and Software	10.1016/S0164-1212(96)00152-5	project management;productivity;simulation;computer science;systems engineering;engineering;technical report;software engineering;program design language;development environment;programming language;structured programming;software development process;software quality	SE	-66.54874021266038	32.24934103527503	669
7937158211aea6c87775864847981c6ae1f8c348	an analysis of degenerate sharing and false coherence	distributed data;distributed system;partition function;systeme reparti;general and miscellaneous mathematics computing and information science;distributed database;shared memory;partition functions;building block;executive codes;memoria compartida;semantics;data processing;transmission message;exactitude programme;mathematical logic;semantica;semantique;system performance;message transmission;sistema repartido;exactitud programa;degeneration;cost optimization;false sharing;processing 990230 mathematics mathematical models 1987 1989;analyse performance;supercomputers 1987 1989;performance analysis;computer codes;algorithms;optimization;distributed data processing;shared memory system;distributed shared memory;management;cost;data base management;functions;logical process;memoire partagee;transmision mensaje;analisis eficacia;program correctness	costs, these systems often replicate shared data in local caches. System designers often choose to implement these systems expecting their caches to be coherent—a site always reads the latest value written to a location. To maintain coherence, the system must implement some mechanism that transmits updates between caches to keep them from becoming stale. These coherence transmissions directly impact the performance of shared memory systems. Many shared memory systems rely on underlying memory management hardware to decide when a coherence transmission is necessary. Unfortunately, most memory management units (MMUs) only indicate that a transmission may be necessary. If the system uses the MMU to determine when a coherence transmission is necessary, there may be some unnecessary transmissions. Such excess communication potentially reduces system performance. There are two component costs of a data coherence operation: a fixed amount of overhead and a per-byte transmission cost. The overhead is substantial, so many systems process a block of cached data (i.e., cache line or page) to amortize the overhead over many words of the data. While this amortization suggests that larger blocks are better, the per-byte transmission cost effectively limits the maximum block size. As the block grows larger, it becomes more likely that the system will transmit extra data that the receiving site will never read. Since any modification of the block forces a transmission of the entire block, selecting the block size requires care. Given expected data access patterns (i.e., locality of reference), there exists an optimal balance between block size and per-byte transmission cost. Since transmission costs vary by system, this paper will assume that a coherence transmission is the basic unit of cost and we assume an optimal block size for the application. As the block size increases, so does the likelihood that different objects will be co-located in the block. Unfortunately, co-locating data structures produces undesirable side effects. Suppose two different data structures share a coherence block and one site exclusively accesses one of these objects while a second site exclusively accesses the other. If the first site modifies the first object, there will be an unnecessary coherence transmission when the second site accesses its object, even though the second site never uses the data written by the first site. This has been called the false sharing problem (see Fig. 1). We refer to this problem as the false coherence problem since the cost of JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING 34, 183–195 (1996) ARTICLE NO. 0054	amortized analysis;block size (cryptography);byte;cache (computing);cache coherence;coherence (physics);data access;data structure;distributed computing;false sharing;locality of reference;memory management unit;offset binary;overhead (computing);self-replicating machine;shared memory;side effect (computer science)	Randall L. Hyde;Brett D. Fleisch	1996	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.0054	distributed shared memory;shared memory;mathematical optimization;combinatorics;mathematical logic;parallel computing;partition function;false sharing;data processing;telecommunications;computer science;theoretical computer science;operating system;mathematics;distributed computing;semantics;partition function;programming language;distributed database;function;algorithm	Arch	-16.351291939507004	45.40220632118453	670
88b9c1a603b2c47696da417614195b8fdc4cdc8d	stacked ensemble coupled with feature selection for biomedical entity extraction	support vector machine svm;conditional random field crf;ga based feature selection;biomedical entity extraction;stack based ensemble	Entity extraction is one of the most fundamental and important tasks in biomedical information extraction. In this paper we propose a two-stage algorithm for the extraction of biomedical entities in the forms of genes and gene product mentions in text. Several different approaches have emerged but most of these state-of-the-art approaches suggest that individual system may not cover entity representations with arbitrary set of features and cannot achieve best performance. We identify and implement a diverse set of features which are relevant for the identification of biomedical entities and classification of them into some predefined categories. One most important criterion of these features is that these are identified and selected largely without using any domain knowledge. In the first stage we use a genetic algorithm (GA) based feature selection technique to determine the most relevant set of features for Support Vector Machine (SVM) and Conditional Random Field (CRF) classifiers. The GA based feature selection algorithm produces best population that can be used to generate different classification models based on CRF and SVM. In the second stage we develop a stacked based ensemble to combine the classifiers selected in the first stage. The proposed approach is evaluated on two benchmark datasets, namely JNLPBA 2004 shared task and GENETAG. The proposed approach yields the overall F-measure values of 75.17% and 94.70% for JNLPBA 2004 and GENETAG data sets, respectively. 2013 Published by Elsevier B.V.	benchmark (computing);conditional random field;dictionary;entity;entity bean;f1 score;feature selection;genetic algorithm;information extraction;mathematical optimization;multi-objective optimization;named-entity recognition;performance;precision and recall;selection algorithm;software release life cycle;statistical classification;support vector machine	Asif Ekbal;Sriparna Saha	2013	Knowl.-Based Syst.	10.1016/j.knosys.2013.02.008	computer science;machine learning;pattern recognition;data mining	NLP	-19.455023830943855	-65.18431122834001	671
4c4485954b99e14093c5b2200896fadaf42df8bc	towards model-based generation of self-priming and self-checking conformance tests for interactive system	software testing;software fault models;computacion informatica;grupo de excelencia;model based approach;system under test;automatic generation;conformance testing;interactive application;interactive system;ciencias basicas y experimentales;software test oracles;finite state automaton;model based testing;fault model;model based test generation	This paper describes a model-based approach to generate conformance tests for interactive applications. Our method addresses generation of: (1) small yet effective set of  test frames  for testing individual operations, (2) a  Set up  sequence that brings the system under test in an appropriate state for a test frame ( self - priming ), (3) a  Verification  sequence for expected output and state changes ( self - checking ), and, (4)  negative  test cases in the presence of exceptions. Our method exploits a novel mutation scheme applied to operations modeled as relationships among parameters and state variables; a set of novel abstraction techniques which result in a compact finite state automaton; and search techniques to automatically generate the set up and verification sequences. We illustrate our method with a simple ATM application.	conformance testing;interactivity	Amit M. Paradkar	2004	Information & Software Technology	10.1016/j.infsof.2003.09.005	model-based testing;simulation;computer science;engineering;theoretical computer science;software engineering;conformance testing;fault model;database;software testing;system under test;finite-state machine;programming language;algorithm	HCI	-49.92246114848936	34.82010817853793	672
900960caff408ffe1aeea6b2bc825c2cf9966253	adpcm with nonlinear prediction	speech coding linear codes neural nets;training speech speech coding quantization signal neural networks predictive models switches;neural networks;training;speech;speech coding;quantization signal;neural nets adpcm scheme speech coders linear prediction coding lpc speech signal nonlinear predictor;predictive models;switches	Many speech coders are based on linear prediction coding (LPC), nevertheless with LPC is not possible to model the nonlinearities present in the speech signal. Because of this there is a growing interest for nonlinear techniques. In this paper we discuss ADPCM schemes with a nonlinear predictor based on neural nets, which yields an increase of 1-2.5dB in the SEGSNR over classical methods. This paper will discuss the block-adaptive and sample-adaptive predictions.	adaptive differential pulse-code modulation;artificial neural network;kerrison predictor;linear predictive coding;nonlinear system	Marcos Faúndez-Zanuy;Oscar Oliva-Suarez	1998	9th European Signal Processing Conference (EUSIPCO 1998)	10.5281/zenodo.36510	voice activity detection;codec2;linear predictive coding;speech recognition;vector sum excited linear prediction;computer science;speech coding;pattern recognition;communication;code-excited linear prediction	EDA	47.76987637760255	-8.178788709379292	673
dd503c9c6277270ad9d586275a7a8ef9beb9dec4	avian attractor	depth cameras;gesture;collaboration;fine art;procedural aesthetics;urban wildlife;urban birds;post human embodiment;interactive installation	Avian Attractor is a gestural projection combining depth images of viewers and pre-captured shots of birds in natural and architectural environment. Surface impressions of viewers merge with those of urban birds and procedural agents that extend their flight paths and trajectories. These moving images are both seen and seen through -- motionscapes that combine figurative elements with cross-hatchings, tendrils and flocking agents. The Avian Attractor installation is supported by other art research activity that aims to provide tools for a diversity of users without programming skills or collaborators. This includes development of a point cloud camera-recorder and interface for non-programmers. Inspired by a bird feeder in a cold city, Avian Attractor uses off-the-shelf depth cameras and projection to generate a hybrid form of space where post-human embodiment can be explored and expanded.	flocking (behavior);point cloud;programmer;rössler attractor	Judith Doyle;Naoto Hieda	2016		10.1145/2851581.2891093	computer vision;simulation;gesture;management;world wide web;fine art;collaboration	HCI	-50.04903821574463	-29.754844732714076	674
6daad3c181b2dbffa29b990c464190ea702305c6	distributive high-rate space–frequency codes achieving full cooperative and multipath diversities for asynchronous cooperative communications	space time block codes;multiple input multiple output mimo;asynchronous cooperative communications;space frequency codes;relays block codes delay frequency division multiplexing ofdm fading receiving antennas transmitting antennas transmitters oscillators;fading;multiple input multiple output system distributive high rate space frequency codes full cooperative diversities multipath diversities asynchronous cooperative communications user cooperative communications space time coded orthogonal frequency division multiplexing space time block codes frequency selective fading;space time codes block codes fading mimo communication multipath channels ofdm modulation;orthogonal space time block code;multiple input multiple output;oscillators;asynchronous cooperative diversity;frequency selective fading;multiple input multiple output system;sensor network;multipath diversities;user cooperation;cooperative communication;sensor networks;space time code;frequency division multiplexing;space time codes;ofdm modulation;transmitters;ofdm;cooperative diversity;space time coded orthogonal frequency division multiplexing;space frequency codes asynchronous cooperative diversity multiple input multiple output mimo orthogonal frequency division multiplexing ofdm sensor networks;time domain;space frequency;multipath channels;receiving antennas;transmitting antennas;relays;frequency domain;full cooperative diversities;mimo;distributive high rate space frequency codes;block codes;mimo communication;orthogonal frequency division multiplexing ofdm;user cooperative communications;orthogonal frequency division multiplex	In user-cooperative communications, relay nodes are usually asynchronous. By realizing that the processing in the frequency domain is insensitive to the errors in the time domain, Mei and Shin recently applied the space-time-coded orthogonal frequency-division multiplexing (OFDM) technique to achieve full cooperative diversity for asynchronous cooperative communications, where orthogonal space-time block codes (particularly the Alamouti code) were used for relay nodes. In this paper, we consider asynchronous cooperative communications, and the channels from one node to another node are frequency-selective fading. We propose a high-rate space-frequency coding method and prove that it can achieve both cooperative and multipath diversities. Simulation results are shown to verify the performance of the constructed codes.	asynchronous i/o;code;multipath propagation;multiplexing;relay;simulation	Yabo Li;Wei Zhang;Xiang-Gen Xia	2009	IEEE Transactions on Vehicular Technology	10.1109/TVT.2008.923678	electronic engineering;real-time computing;wireless sensor network;orthogonal frequency-division multiplexing;telecommunications;computer science;mathematics;cooperative diversity;statistics	Mobile	43.608865295399006	74.54034546045699	675
a6c290b5846f33600171d0a58b0cce555b174d1b	increasing interoperability between heterogeneous smart city applications		Due to the increasing need for networked systems we can observe a rapid advance of IT-solutions in various sectors. However, most of the developed systems are custom-tailored solutions for specific problems and application areas, leaving us with a set of diverse frameworks. The resulting jungle of heterogeneous systems makes it difficult to find common interfaces for interconnecting the underlying businesses with each other, especially in regard to Smart City concepts. We envision a new paradigm shift towards “Smart City as a service” fueled by increased interoperability between different services with an additional emphasis on privacy-preserving data processing. This would contribute to a new level of connectivity between the environment, service providers, and people, facilitating our daily activities and enhancing the level of trust of the users. In order to achieve interoperability in the context of smart, connected cities, we propose the design of a generic, platform-independent novel architecture for interconnecting heterogeneous systems, their services, and user pools.	interoperability;smart city	Nino Chirico;Markus Pistauer;Christian Steger	2018		10.1007/978-3-030-02738-4_6	smart city;architecture;computer security;jungle;service provider;interoperability;paradigm shift;information privacy;data processing;computer science	ML	-43.47838403049997	48.97823404626103	676
ec87a4bdb04a667af57b2d9e5b462c99889191f9	square contractions of graphs				Richard Hammack	2006	Australasian J. Combinatorics		combinatorics;mathematics;graph	Theory	32.17339791728887	33.49571376877594	677
77db2276afd7b0afce83628beb7fd40bcd79cbc3	portfolio value-at-risk estimation in energy futures markets with time-varying copula-garch model	backtesting;copulas;risk management;value at risk;time varying models	This paper combines copula functions with GARCH-type models to construct the conditional joint distribution, which is used to estimate Value-at-Risk (VaR) of an equally weighted portfolio comprising crude oil futures and natural gas futures in energy market. Both constant and time-varying copulas are applied to fit the dependence structure of the two assets returns. The findings show that the constant Student t copula is a good compromise for effectively fitting the dependence structure between crude oil futures and natural gas futures. Moreover, the skewed Student t distribution has a better fit than Normal and Student t distribution to the marginal distribution of each asset. Asymmetries and excess kurtosis are found in marginal distributions as well as in dependence. We estimate VaR of the underlying portfolio to be 95% and 99%, by using the Monte Carlo simulation. Then using backtesting, we compare the out-of-sample forecasting performances of VaR estimated by different models. Copyright Springer Science+Business Media, LLC 2014	futures and promises;value at risk	Xunfa Lu;Kin Keung Lai;Liang Liang	2014	Annals OR	10.1007/s10479-011-0900-9	financial economics;econometrics;actuarial science;economics;risk management;copula;value at risk	Metrics	3.3300035607941765	-11.399811396354284	678
24e4a3dbe5757297eb99df6159979f5e2e4b0879	modeling the adoption of innovations in the presence of geographic and media influences	engineering;public library of science;models theoretical;mass media;biology;information network;self organizing system;physics;social network;open access;chemistry;inclusive;ante disciplinary;medicine;plos;technology adoption;adoption of innovation;innovation adoption;article;diffusion of innovation	While there is a large body of work examining the effects of social network structure on innovation adoption, models to date have lacked considerations of real geography or mass media. In this article, we show these features are crucial to making more accurate predictions of a social contagion and technology adoption at a city-to-city scale. Using data from the adoption of the popular micro-blogging platform, Twitter, we present a model of adoption on a network that places friendships in real geographic space and exposes individuals to mass media influence. We show that homophily both among individuals with similar propensities to adopt a technology and geographic location is critical to reproducing features of real spatiotemporal adoption. Furthermore, we estimate that mass media was responsible for increasing Twitter's user base two to four fold. To reflect this strength, we extend traditional contagion models to include an endogenous mass media agent that responds to those adopting an innovation as well as influencing agents to adopt themselves.	binary number;blog;blogging;geographic coordinate system;geography;mass media;social network	Jameson L. Toole;Meeyoung Cha;Marta C. González	2012		10.1371/journal.pone.0029528	social network;mass media	HCI	-20.018142824000257	-39.50370446183996	679
539ec2b760c656a153b252225cbe5920f6ca8a50	a perceptual study on the manipulation of facial features for trait portrayal in virtual agents		Human perceptual studies have shown that facial characteristics affect judgments about the personality of a person. For example, larger facial width has been associated with judgments of aggressiveness, dominance, and untrustworthiness. Previous studies of virtual faces have not been able to reflect the same perceptual rules, but have used characters with unrealistic feature sizes or highly abstract characters. For this study, we created virtual characters with realistic feature dimensions and investigated the effects of facial width and eye size on personality perception. Our results indicate that virtual characters may indeed follow different perceptual rules for facial width, and care must be taken when manipulating eye size. These findings are useful for effective character design for video games, movies, and embodied virtual agents.	intelligent agent	Ylva Ferstl;Rachel McDonnell	2018		10.1145/3267851.3267891	physiognomy;social psychology;trait;cognitive psychology;computer science;embodied cognition;personality;perception;face perception	HCI	-50.833285224999116	-51.30660735787602	680
12df69672929d113124494441fc52e9078a2e5b2	the vinum volume manager	virtual disk drive;block device interface;traditional slice view;maps data;raid-5 model;vinum volume manager;block device driver;disk hardware;disk storage	The Vinum Volume Manager is a block device driver which implements virtual disk drives. It isolates disk hardware from the block device interface and maps data in ways which result in an increase in flexibility, performance and reliability compared to the traditional slice view of disk storage. Vinum implements the RAID-0, RAID-1 and RAID-5 models, both individually and in combination.	device driver;disk image;disk storage;standard raid levels;vinum volume manager	Greg Lehey	1999			embedded system;real-time computing;computer hardware;computer science;operating system	OS	-18.278386518280644	51.10156642028624	681
e51dff58553c57e639c75c3afe7332606ee54db8	invisible genericity and 0#		"""0 # can be invisibly class generic. 1. Introduction Roughly, the main theorem of this paper is that some instances of any type of non-constructible object are class generic over L, in one sense. Since small large cardinal properties are inherited in L, let us begin by considering the sense in which 0 # can be generic. It is well known that 0 # is not set generic over L, or, indeed, over any inner model M such that 0 # / ∈ M. This is because, if 0 # / ∈ M , then unboundedly many M-cardinals are collapsed in L[0 # ], a trick no set forcing can perform. It is also known that there can be no L-definable class forcing P such that 0 # ∈ L[G] and L[G]; G ZFC, for some L-definably generic G ⊆ P. This is because it is known that in such a case the forcing relation on a cone of conditions rooted in G must be L-definable because L[G]; L, P, G ZFC; but then ∃p ∃ι p P """" ˇ ι is an indiscernible """" and L ι ϕ defines L-truth over L. Further, it is known that there is no L-amenable class forcing property P such that 0 # ∈ L[G] and L[G]; P, G ZFC, for some L; P-definably generic G ⊆ P. In such a case, again, the forcing relation on a cone of conditions rooted in G must be definable over L; P. Since P is L-amenable (and compatible with 0 #), the class P is definable over L[0 # ], and, in fact, the members of a tail of the indiscernibles are indiscernible I am grateful to Sy Friedman for pointing out a mistake in the first version, and to David Cook for pointing out an improvement regarding the definability of the forcing relation."""	cone (formal languages);generic programming;model m keyboard;zermelo–fraenkel set theory	M. C. Stanley	1998	J. Symb. Log.			Theory	-6.4405073666089425	13.036271203109589	682
7000158de9f7589c5e41054954bc8697312fd99d	grenzen, chancen und perspektiven von security information & event management systemen		Security Information and Event Management (SIEM) Systemewerden für den Betrieb großer, heterogener und verteilt administrierter Infrastrukturen immer wichtiger, um die Vielzahl der sicherheitsrelevanten Alarme, Events und Informationen überhaupt geeignet und zielgerichtet behandeln zu können. Das Leibniz-Rechenzentrum, das bereits seit mehreren Jahren erfolgreich ein SIEM-System betreibt, hat aufgrund fehlender IPv6-Unterstützung eine Evaluation potentieller Nachfolgelösungen durchgeführt. In diesem Papier werden die Grundlagen und Ergebnisse dieser Evaluation und die zum Teil massiven Defizite aktueller SIEM-Systeme vorgestellt, um daraus Anforderungen an ein next generation SIEM (ngSIEM) System abzuleiten.	eine and zwei;next-generation network;security information and event management;trusted computer system evaluation criteria;unified model	Wolfgang Hommel;Stefan Metzger;Helmut Reiser	2013	Praxis der Informationsverarbeitung und Kommunikation	10.1515/pik-2013-0038	computer science;distributed computing	AI	-102.16065011764663	36.159406397383414	683
dff15122eceaadc1478f1eadb46a6839d30f8353	adaptive feedforward/feedback architectures for multiuser detection in high data rate wireless cdma networks	recursive least square;quadrature phase shift keying;data transmission;multiuser detection;rayleigh fading;feedforward;radio receivers;least mean squares methods;multipath channel;signal sampling;intersymbol interference;bit error rate;direct matrix inversion;multipath fading channels;high data rate;packet radio networks;direct sequence;multiuser channels;radiofrequency interference;chip;spread spectrum communication;code division multiple access;error propagation;circuit feedback;rayleigh channels;digital filters;multiple access interference;error statistics;multipath channels;matched filter;minimum mean square error;2 mbit s adaptive feedforward feedback architectures multiuser detection high data rate wireless cdma networks design performance nonlinear minimum mean square error multiuser detectors direct sequence code division multiple access multipath fading channel multiple access interference intersymbol interference mai isi cyclostationarity feedforward filter parallel chip matched filters feedback filter detected symbols connectivity architectures fully connected filters nonconnected filters sampling multipath ray tracking multiuser recursive least squares algorithm direct matrix inversion approach uncoded bit error rate power control timing control multipath channels quasi static rayleigh fading packet based qpsk transmission ber analysis multiuser rls adapted detectors error propagation;feedback multiuser detection field flow fractionation filters detectors intersymbol interference multiaccess communication multiple access interference sampling methods bit error rate;radiofrequency interference code division multiple access rayleigh channels intersymbol interference multipath channels radio receivers multiuser channels least mean squares methods spread spectrum communication digital filters feedforward circuit feedback signal sampling error statistics quadrature phase shift keying packet radio networks;power control	We consider the design and performance of nonlinear minimum mean-square-error multiuser detectors for direct-sequence code-division multiple-access (CDMA) networks. With multiple users transmitting asynchronously at high data rates over multipath fading channels, the detectors contend with both multiple-access interference (MAI) and intersymbol interference (ISI). The cyclostationarity of the MAI and ISI is exploited through a feedforward filter (FFF), which processes the samples at the output of parallel chip-matched filters, and a feedback filter (FBF), which processes detected symbols. By altering the connectivity of the FFF and FBF, we define four architectures based on fully connected (FC) and nonconnected (NC) filters. Increased connectivity of the FFF gives each user access to more samples of the received signal, while increased connectivity of the FBF provides each user access to previous decisions of other users. We consider three methods for specifying the FFF sampling and propose a nonuniform FFF sampling scheme based on multipath ray tracking that can offer improved performance relative to uniform FFF sampling. For the FC architecture, we capitalize on the sharing of filter contents among users by deriving a multiuser recursive least squares (RLS) algorithm and direct matrix inversion approach, which determine the coefficients more efficiently than single-user algorithms. We estimate the uncoded bit-error rate (BER) of the feedforward/feedback detectors for CDMA systems with varying levels of power control and timing control for multipath channels with quasi-static Rayleigh fading. The FC-FFF/FC-FBF architecture is shown to offer significant improvement over the NC architectures by sustaining eight users in an asynchronous CDMA system with a processing gain of 8, 2-Mb/s quadrature phase-shift keying (QPSK) transmissions, a delay spread of 1.25 s, an average signal-to-noise ratio of 15 dB, with uncoded BER’s less than 10 4 and 10 3 with 97% and 99.3% probabilities, respectively. Simulations of packet-based QPSK transmission validate the theoretical BER analysis and demonstrate that the multiuser RLS adapted detectors train in several hundred symbols and avoid severe error propagation during data transmission mode. Paper approved by U. Mitra, the Editor for Spread Spectrum/Equalization of the IEEE Communications Society. Manuscript received September 16, 1998; revised June 15, 1999 and November 2, 1999. This work was supported in part by the 1996 IEEE Daniel E. Noble Graduate Fellowship, and in part by the NJ Center for Wireless Telecommunications funded by the NJ Commission on Science and Technology. This work was presented in part at the IEEE Sixth International Conference on Universal Personal Communications (ICUPC’97), San Diego, CA, October 12–16, 1997, and the Communication Theory Mini-Conference of the IEEE International Conference on Communications (ICC’99), Vancouver, BC, Canada, June 6–10, 1999. J. E. Smee was with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544 USA. He is now with Qualcomm Incorporated, San Diego, CA 92121 USA (e-mail: jsmee@qualcomm.com). S. C. Schwartz is with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544 USA (e-mail: stuart@princeton.edu). Publisher Item Identifier S 0090-6778(00)05405-2.	algorithm;bit error rate;catastrophic interference;coefficient;computer simulation;connectivity (graph theory);cyclostationary process;data rate units;electrical engineering;email;feed forward (control);feedforward neural network;fused filament fabrication;identifier;in-phase and quadrature components;information sciences institute;intelligence amplification;interference (communication);international conference on communications;key (cryptography);matched filter;modulation;multi-user;multipath propagation;network packet;nonlinear system;process gain;propagation of uncertainty;rayleigh fading;recursion;recursive least squares filter;sampling (signal processing);sensor;signal-to-noise ratio;software propagation;transmitter;x.690	John E. Smee;Stuart C. Schwartz	2000	IEEE Trans. Communications	10.1109/26.848562	chip;minimum mean square error;code division multiple access;electronic engineering;real-time computing;digital filter;bit error rate;telecommunications;power control;computer science;propagation of uncertainty;rayleigh fading;radio receiver;matched filter;spread spectrum;feed forward;intersymbol interference;data transmission	Visualization	45.848191169069636	71.7744406343999	684
f938355b4df02a3424c15e1b8b550858089418be	location based services	pervasive computing;location based service;wardriving;navigation;mobile computing;local search;robustness	"""Mobile devices (tablets, smart phones, laptops) are proving themselves to be the main means of accessing information of the future. The embodiment of Recommender Systems (RSs) into mobile environments, as a matter of fact, has come about to serve as a way to solve the nuisances of data overwhelming. RSs' main advantage is their ability to allow users to find useful information according to the users' preferences and location. Even though they are not free of shortcoming such as the limitation of mobile devices' function capability, the instability of wireless networks in remote regions or the lack of experience of end users, location-aware recommender systems are blessed with two great characteristics: """"location-awareness"""" and """"ubiquity"""" (the ability to be available anywhere, anytime needed). The combination of recommender systems, users' location data and social networks has paved the way for a new type of mobile service: Location Based Services (LBSs), a promising and sophisticated trend for the future of data services. However, LBSs do contain threats due to their ability to determine almost exactly the location of the service users. To make matters worse, since the concept of LBS has not been introduced for long enough time, even people with vast technical experience are not fully aware of such threats. So as to guarantee that LBS-related threats remain at their minimum level of sabotaging ability, several mechanisms dedicated to protecting the location privacy of LBS users has been researched and developed. Such protocols are named: Location Privacy Preserving Mechanisms (LPPMs). Finally, the thesis will dig into the business potentials of LBSs: how would business entities profit from implementing their services on a LBS-based platform and how would customer be pleased to use such services? Based on the knowledge collected from previous works on LBSs, personal observations and selected interviews of LBS users and experts, this thesis aims at creating a high quality source of references for those who may find it interesting in conducting further studies about LBSs and those who want to get some idea of how LBS-based businesses function."""	anytime algorithm;display resolution;entity;instability;laptop;location awareness;location-based service;mobile device;recommender system;smartphone;social network;tablet computer;threat (computer)	Bernhard Kölmel	2003		10.1007/978-3-319-17885-1_100704	computer network;location-based service;computer science	HCI	-41.34345090285703	53.32228459162775	685
66473b9734e6099d8f55bb6c6eed5b73bfdc0da9	battery embedded distributed sensing & control			control theory;embedded system	Adnan H. Anbuky;Darren Lim;Phillip E. Pascoe	2007			computer science;distributed computing;battery (electricity)	Robotics	9.886068400772281	71.40289239721703	686
c8a68e39ffb77080e01f9c1bf7908cfa3d3c2fd3	surface science spectra: a hybrid journal-database	surface structure;science and technology;databases mass spectroscopy chemical elements data analysis electrons laboratories electromagnetic radiation infrared spectra x ray diffraction energy measurement;database management systems;spectroscopy computing;publishing;secondary ion mass spectrometry;surface science spectra hybrid journal database;auger electron spectroscopy;ultraviolet photoelectron spectroscopy;electron energy loss spectroscopy;scientific communication;surface phenomena scientific information systems spectroscopy computing;scientific databases;surface phenomena;x ray photoelectron spectroscopy;scientific information systems	Spectroscopy is the study of the interaction of electromagnetic radiation with matter. It is one of the principal methods used by chemists, physicists, and material scientists to identify and quantify elements and molecules and to study the structure of chemical matter. There are dozens of kinds of spectroscopy. Some spectroscopies are well established and mature, while others are developing. As a newly developed spectroscopy demonstrates its utility and becomes widely practiced, practitioners begin asking for spectral databases. Major analytical techniques such as infrared spectroscopy, mass spectrometry, and x-ray diffraction are especially noted for their large and widely used database collections. There are several reasons to have these databases. For instance, analytical spectroscopists usually want large spectral data collections to aid in characterizing or identifying unknown materials. Another intriguing use of databases is for sharing and exchanging spectral data. A traditional scientific paper can only report a small fraction of the spectroscopic measurements, and even then, the spectral data are reduced to mere figures. With a community database, larger quantities of spectral data can be published and described in greater detail. It’s often the case that spectral data are used for purposes that were not the original motivation for making a measurement, even to the extent that “one investigator’s noise is another’s signal.”1 Database collections were quite minimal in surface spectroscopies in 1990, when the American Vacuum Society (now named AVS, The Science and Technology Society) began to develop a spectral database, called Surface Science Spectra (or SSS); http://sss.avs.org. (Surface spectroscopies include xray photoelectron spectroscopy [XPS], Auger electron spectroscopy [AES], ultraviolet photoelectron spectroscopy [UPS], secondary ion mass spectrometry [SIMS], electron energy loss spectroscopy [EELS], and other spectroscopic methods.) The AVS foresaw that producing a high-quality spectral database would be a benefit to the user community (see the “Surface Science Spectra Benefits” sidebar). Furthermore, because the mission of the AVS includes education, professional growth of its members, support of research, and dissemination of knowledge,2 the society especially wanted the scientific community involved in developing and contributing to the database. The contributor, especially a student or novice user, gains in skill and proficiency through the process of producing a detailed, high-quality spectral record of a material and having that data record peer reviewed by the surface science community. The contributor also gets the satisfaction and recognition of having his or her spectral data used in laboratories around the world. Finally, the world-community standards for data reporting are elevated as more researchers use and contribute to the database. Simultaneous with the launch of Surface Science Spectra, many changes occurred—both in the pubSURFACE SCIENCE SPECTRA: A HYBRID JOURNAL–DATABASE	database;electron;open xml paper specification;photoelectric effect;rca spectra 70;row (database);scientific literature;secondary ion mass spectrometry;spectral method;virtual community	Stephen W. Gaarenstroom	2003	Computing in Science and Engineering	10.1109/MCISE.2003.1196304	atomic physics;electron energy loss spectroscopy;auger electron spectroscopy;ultraviolet photoelectron spectroscopy;publishing;x-ray photoelectron spectroscopy;nuclear magnetic resonance;secondary ion mass spectrometry;transformational grammar;quantum mechanics;science, technology and society	DB	-8.679424233105314	-57.465680998284945	687
6a4b5b83443d0ffcc3d9b67d9ececde1162bc36d	zur strukturellen analyse von if-then-regelbasen mit methoden der mathematischen logik	zur strukturellen analyse von;if-then-regelbasen mit methoden der;mathematischen logik	1. R Lovassy, L T Kóczy, L Gál Multilayer Perceptron Implemented by Fuzzy Flip-Flops 2008 IEEE World Congress on Computational Intelligence (WCCI 2008) Hong Kong, June 1-6, 2008. pp. 1683-1688 2. L Gál, J Botzheim, L T Kóczy Improvements to the Bacterial Memetic Algorithm used for Fuzzy Rule Base Extraction IEEE International Conference on Computational Intelligence for Measurement Systems and Applications (CIMSA 2008) Istanbul, Turkey, 14-16 July 2008. pp. 38-43. 3. R Lovassy, L T Kóczy, L Gál Fuzzy Flip-Flop Based Neural Network as a Function Approximator IEEE International Conference on Computational Intelligence for Measurement Systems and Applications (CIMSA 2008) Istanbul, Turkey, 14-16 July 2008. pp. 44-49. 4. P Földesi, L T Kóczy, J Botzheim Fuzzy Solution for Non-linear Quality Models 12 International Conference on Intelligent Engineering Systems (INES 2008) Miami, Florida, February 25-29, 2008. pp. 269-275. 5. Á Ballagi, L T Kóczy Fuzzy Signature Based Mobil Robot Motion Control System 6 International Symposium on Applied Machine Intelligence and Informatics Herl’any, Slovakia, January 21-22, 2008. pp. 29-33. 6. L Gál, J Botzheim, L T Kóczy, A. E. Ruano Fuzzy Rule Base Extraction by the Improved Bacterial Memetic Algorithm 6 International Symposium on Applied Machine Intelligence and Informatics Herl’any, Slovakia, January 21-22, 2008. pp. 49-53. 7. K Tamás, L T Kóczy Selection from a Fuzzy Signature Database by Mamdani-algorithm 6 International Symposium on Applied Machine Intelligence and Informatics Herl’any, Slovakia, January 21-22, 2008. pp. 63-68.	artificial neural network;computation;computational intelligence;control system;flops;flip-flop (electronics);fuzzy rule;informatics;memetic algorithm;memetics;multilayer perceptron	Helmut Thiele	1994			fuzzy logic;algorithm;mathematics	Embedded	-43.811562679814834	-10.590932272131445	688
91dc4eaf46a527a13b5760b88d485f350025ff7a	resilient in-network aggregation for vehicular networks		Applications for vehicular ad hoc networks (VANETs) are an active field of research with the potential to significantly contribute to driver safety, traffic efficiency, and comfort. Messages are typically exchanged and forwarded between vehicles using wireless communication, thereby creating a wireless ad hoc network. Especially traffic efficiency applications require the dissemination of information over long distances. For instance, vehicles need to be informed about traffic jams early enough to consider alternative navigation decisions. Each vehicle acts as creator and as forwarder of information to implement the required multihop information dissemination. Two of the most prevalent challenges in designing suitable ad hoc communication protocols are dealing with the limited wireless channel capacity, as well as ensuring the resilience of communication protocols against potential attackers. The focus of this thesis is on the resilience of in-network information aggregation mechanisms for VANETs. In aggregation mechanisms, vehicles collaboratively exchange information and summarize this information as it is disseminated within the network. In contrast to traditional protocols, which often aggregate information at a centralized entity, the aggregation close to the information sources saves bandwidth and provides scalability. Yet, malicious users may be able to inject false information or even alter information summaries to disturb normal system operation. Both types of attacks are hard to detect, because original observations are usually discarded after aggregation and are not available to verify the correctness of claimed aggregated information. By addressing resilient in-network aggregation, this thesis provides solutions that contribute to both channel capacity conservation and protocol resilience. The main contributions of this thesis are (a) a model of the in-network aggregation dissemination process; (b) a detailed security analysis of in-network aggregation mechanisms including the introduction of a taxonomy for security paradigms; (c) the design of four novel security mechanisms for in-network aggregation and (d) their detailed analysis and evaluation using network simulations; and (e) a framework that combines and adapts secure aggregation mechanisms based on situational context, as well as on attack likelihood derived from information exchange. The model for in-network aggregation is comprised of an architecture model and an information flow model. It provides the foundation for understanding which components are essential in the design of aggregation mechanisms and for understanding how information spreads and evolves within the network. The taxonomy of security paradigms, which is based on the modeling results, identifies use of cryptographic tools, interaction between vehicles to facilitate collaborative agreement, and data-consistency checks as most suitable security paradigms to provide resilience for in-network aggregation mechanisms.	aggregate data;centralized computing;channel capacity;correctness (computer science);cryptography;emoticon;hoc (programming language);information exchange;information flow (information theory);scalability;simulation	Stefan Dietzel	2015			telecommunications;engineering;computer security;computer network	Security	-53.28934287286899	75.14290675392066	689
3f519c43165292923aae9614ad121da623cceaa2	a capacitive touch sensing technique with series-connected sensing electrodes		Touch sensing with multiple electrodes allows expressive touch interactions. The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.	computer simulation;interaction;mobile device;multi-touch;rapid application development;rapid prototyping;sensor;series and parallel circuits;simulation;software prototyping	Hiroyuki Manabe;Wataru Yamada	2017		10.1145/3126594.3126625	capacitor;capacitive sensing;adaptability;computer hardware;multi-touch;computer science;electrode;electrode array;capacitance	HCI	-42.41740287160253	-41.560907528949485	690
25129673481ed8335067c14cc4586df8e021f0bf	taxonomy for personalized recommendation service	filtering;electronic commerce;motion pictures;e commerce;taxonomy data mining motion pictures filtering navigation collaboration laboratories;collaboration;personalized recommendation;data mining;navigation;recommender system;e commerce sites;taxonomy;personalized recommendation service;research routine personalized recommendation service recommender systems e commerce sites personalized recommendation;research routine;recommender systems	The amount of information in the world is increasing far more quickly than our ability to process it. Recommender systems are used by e-commerce sites to suggest products to their customers and to provide consumers with information to help them determine which products to purchase. In this paper, in a higher level, we analyze current personalized recommendation from two respects, respectively recommendation task and criteria for recommendation. Based on these analyses, a complete taxonomy on recommendation is build. Through the taxonomy, research routine and challenge in the future are identified.	e-commerce;personalization;recommender system	Li Yu;Ming Dong;Rong Wang	2008	2008 International Symposium on Electronic Commerce and Security	10.1109/ISECS.2008.131	e-commerce;filter;navigation;computer science;multimedia;world wide web;information retrieval;recommender system;collaboration	Metrics	-24.561350152261788	-49.260852501023784	691
889dac044b9994cd28f0b5dfcec531dcc3bdfacb	automating derivations of abstract machines from reduction semantics: - a generic formalization of refocusing in coq		We present a generic formalization of the refocusing transformation for functional languages in the Coq proof assistant. The refocusing technique, due to Danvy and Nielsen, allows for mechanical transformation of an evaluator implementing a reduction semantics into an equivalent abstract machine via a succession of simple program transformations. So far, refocusing has been used only as an informal procedure: the conditions required of a reduction semantics have not been formally captured, and the transformation has not been formally proved correct. The aim of this work is to formalize and prove correct the refocusing technique. To this end, we first propose an axiomatization of reduction semantics that is sufficient to automatically apply the refocusing method. Next, we prove that any reduction semantics conforming to this axiomatization can be automatically transformed into an abstract machine equivalent to it. The article is accompanied by a Coq development that contains the formalization of the refocusing method and a number of case studies that serve both as an illustration of the method and as a sanity check on the axiomatization.	a new kind of science;abstract machine;axiomatic system;closure (computer programming);coinduction;computation;context-sensitive grammar;coq (software);correctness (computer science);executable;explicit substitution;functional programming;interpreter (computing);np-equivalent;olivier fourdan;operational semantics;program transformation;proof assistant;requirement;sanity check;succession;tracing (software);turing completeness	Filip Sieczkowski;Malgorzata Biernacka;Dariusz Biernacki	2010		10.1007/978-3-642-24276-2_5	computer science;theoretical computer science;programming language;algorithm	PL	-17.54473559969066	22.504967988705335	692
101161e796f589e77ddbbc38e856c7ced897f350	using a reinforced concept lattice to incrementally mine association rules from closed itemsets	incremental mining;data mining;frequent itemset;association rule;concept lattice;incremental algorithm	In the Data Mining area, discovering association rules is one of the most important task. It is well known that the number of these rules rapidly grows to be unwieldy as the frequency requirements become less strict, especially when collected data is highly correlated or dense. Since a big number of the frequent itemsets turns out to be redundant, it is sufficient to consider only the rules among closed frequent itemsets or concepts. In order to efficiently generate them, it is often essential to know the Concept Lattice, that also allows the user to better understand the relationships between the closed itemsets. We propose an incremental algorithm that mines all the closed itemsets, reading the data only once. The Concept Lattice is incrementally updated using a simple but essential structure directly connected to it. This structure allows to speed up the execution time and makes the algorithm applicable on both static and dynamic stream data and very dense datasets.	aggregate data;aggregate function;algorithm;apriori algorithm;association rule learning;constraint (mathematics);data mining;data structure;formal concept analysis;requirement;run time (program lifecycle phase)	Arianna Gallo;Rosa Meo	2006		10.1007/978-3-540-75549-4_7	pattern recognition;data mining;database;mathematics	ML	-6.240560470993318	-36.62100579793059	693
94c4acaa7bf75955fddcaf30021defddd7502590	interplay between chaperones and protein disorder promotes the evolution of protein networks	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Evolution is driven by mutations, which lead to new protein functions but come at a cost to protein stability. Non-conservative substitutions are of interest in this regard because they may most profoundly affect both function and stability. Accordingly, organisms must balance the benefit of accepting advantageous substitutions with the possible cost of deleterious effects on protein folding and stability. We here examine factors that systematically promote non-conservative mutations at the proteome level. Intrinsically disordered regions in proteins play pivotal roles in protein interactions, but many questions regarding their evolution remain unanswered. Similarly, whether and how molecular chaperones, which have been shown to buffer destabilizing mutations in individual proteins, generally provide robustness during proteome evolution remains unclear. To this end, we introduce an evolutionary parameter λ that directly estimates the rate of non-conservative substitutions. Our analysis of λ in Escherichia coli, Saccharomyces cerevisiae, and Homo sapiens sequences reveals how co- and post-translationally acting chaperones differentially promote non-conservative substitutions in their substrates, likely through buffering of their destabilizing effects. We further find that λ serves well to quantify the evolution of intrinsically disordered proteins even though the unstructured, thus generally variable regions in proteins are often flanked by very conserved sequences. Crucially, we show that both intrinsically disordered proteins and highly re-wired proteins in protein interaction networks, which have evolved new interactions and functions, exhibit a higher λ at the expense of enhanced chaperone assistance. Our findings thus highlight an intricate interplay of molecular chaperones and protein disorder in the evolvability of protein networks. Our results illuminate the role of chaperones in enabling protein evolution, and underline the importance of the cellular context and integrated approaches for understanding proteome evolution. We feel that the development of λ may be a valuable addition to the toolbox applied to understand the molecular basis of evolution.	biological evolution;buffers;conserved sequence;data buffer;estimated;interaction network;intrinsically disordered proteins;molecular chaperones;mutation;non-deterministic turing machine;population parameter;proteome;rafivirumab;protein folding;protein protein interaction	Sebastian Pechmann;Judith Frydman	2014		10.1371/journal.pcbi.1003674	biology;biochemistry;medical research;bioinformatics;genetics	Comp.	7.212319417010453	-63.97219498567597	694
a51e2f6c81b24cceeb0b118e93fdbbdd08f300d3	non-linear control of under-actuated mechanical systems	nonlinear control;under actuated system;robot control;acrobot;helicopter control;under actuated systems;non linear control;inverted pendulum;helicopter;mechanical systems	In this paper, non-linear controls of three kinds of under-actuated mechanical systems are summarised. The systems are: inverted pendulum systems, Acrobot system and helicopter experimental system. In particular, concerning the inverted pendulum system, a parallel cart-type double inverted pendulum and a serial cart-type double inverted pendulum are introduced. For the systems, different controller design schemes are considered. Experimental results on the systems are also shown.	control system	Akira Inoue;Mingcong Deng	2009	IJMIC	10.1504/IJMIC.2009.023528	control engineering;double inverted pendulum;inverted pendulum;simulation;nonlinear control;computer science;engineering;control theory;robot control;mechanical system	Robotics	67.69237363142167	-15.545132642521557	695
137933c0c0886716f06ec83f2183330c282996c6	context-aware adaptation of user interfaces	institutional repositories;fedora;context aware adaptation;vital;adaptive applications;vtls;multi dimensional adaptation;adaptable applications;ils	Efficient adaptation aims at ensuring that a user interface is adapted to a user’s task according to the context of use, since the end user is carrying out a task with one or several computing platforms in a physical environment. This tutorial presents key concepts of adaptation: principles that guide it, relevant context information and how to consider it, dimensions and abstraction levels subject to adaptation, as well as, languages, methods and techniques used in this domain. This tutorial aims at teaching major aspects to be considered for adaptation of user interfaces in general and concerning the context of use in particular, including the end user (or several of them, as in multi-user interfaces), the platform (or several of them, as in multi-device environments), and the physical environment (or several of them, as in multi-location systems).	multi-user;user interface	Vivian Genaro Motti;Jean Vanderdonckt	2011		10.1007/978-3-642-23768-3_122	user modeling;human–computer interaction;computer science;multimedia;user interface;world wide web	PL	-52.31551507991055	-37.642799747456785	696
cd5c3ff47c6ca41d8435d47b09028ec52d4bcb1f	a parallel algorithm for lossless image compression by block matching	parallel algorithms image coding computed tomography chromium data compression;sequential parsing algorithm parallel algorithm lossless image compression block matching rectangle greedy matching technique parallel architectures trees pyramid multigrid;image coding;parallel algorithm;computed tomography;data compression;rectangle greedy matching technique;image matching;lossless image compression;trees mathematics;trees;trees mathematics data compression image coding parallel algorithms parallel architectures image matching;parallel architectures;pyramid;chromium;multigrid;sequential parsing algorithm;block matching;parallel architecture;parallel algorithms	Storer [1] generalized the LZ1 method to lossless image compression and suggested that v ery fast encoders are possible b y showing a square greedy matching LZ1 compression heuristic, which can be implemented by a simple hashing scheme and achieves 60 to 70 percent of the compression of JBIG1 on the CCITT bi-level image test set. The image must be scanned in some linear order and in order to achieve a good compression performance, bidimensional matches hav e to be computed. A 64K table with one position for each possible 4x4 subarray is the only data structure used. All-zero and all-one squares are handled di erently. The encoding scheme is to precede each item with a ag eld indicating whether there is a monochromatic square, a match or ra w data. When there is a match, the 4x4 subarray in the current position is hashed to yield a pointer to a copy. This pointer is used for the current square greedy match and then replaced in the hash table b y a pointer to the current position. T o improv e the compression performance, Storer and Helfgott [2] in troduced a slo wer rectangle greedy matching technique requiring O(M logM) time where M is the size of the match. Both heuristics work with an unrestricted window. In a previous work we implemented the rectangle greedy matching heuristic using a nite window and a bound to the match size and ac hieved 75 to 90 percent of the compression of JBIG1 on the CCITT bi-level image test set. Now we show a parallel algorithm using a rectangle greedy matching technique which requires a linear n umber of processors and O(logM logn) time on the PRAM EREW model. The algorithm is suitable for practical parallel architectures as a mesh of trees, a p yramid ora multigrid. We implemented a sequential procedure which simulates the compression performed by the parallel algorithm and it achieves 95 to 97 percent of the compression of the sequential heuristic mentioned above. To achieve logarithmic time we partition an m x n image I in x x y rectangular areas where x and y are (logmn). In parallel for each area, one processor applies the sequential parsing algorithm so that in logarithmic time each area will be parsed in rectangles, some of which are monochromatic. Before encoding we compute larger monochromatic rectangles by merging the ones adjacent on the horizontal boundaries and then on the v ertical boundaries, doubling in this way the length and width of each area at each step.	binary image;black and burst;central processing unit;data structure;encoder;greedy algorithm;hash table;heuristic (computer science);image compression;jbig;lz77 and lz78;line code;lossless compression;lotus improv;map (parallel pattern);minimum bounding rectangle;monochrome;multigrid method;parallel algorithm;parallel random-access machine;parsing;period-doubling bifurcation;pointer (computer programming);test set;time complexity	Luigi Cinque;Sergio De Agostino;Franco Liberati	2002		10.1109/DCC.2002.999993	mathematical optimization;discrete mathematics;computer science;theoretical computer science;mathematics;parallel algorithm;computed tomography	Vision	12.08716739952515	36.92155006631942	697
ca20ffe245e4f4312f657eaac65e71ae037db007	a hardware accelerated robot middleware package for intelligent processing on robots		"""Service robots require implementation of intelligent processing, e.g., image processing. However, the computational resources of standard PCs typically used in service robots are not sufficient for such processes. Furthermore, robot middleware is widely used in many robots because such systems facilitate integration and are suitable for rapid prototyping. We propose a """"connective object for middleware to accelerator (COMTA),"""" which is a processing system that uses hardware accelerators, i.e., field programmable gate arrays (FPGAs), and robot middleware. Users can access the FPGAs in the proposed system via middleware interfaces; thus, complex internal circuits are not required. For human tracking using image processing, the proposed system can automatically generate from a single configuration file. The proposed system performs 3.3 times more efficiently relative to computation than standard PCs in robots."""	artificial neural network;computation;computational resource;deep learning;field-programmable gate array;hardware acceleration;image processing;logical connective;middleware;performance per watt;rapid prototyping;robot;robot operating system;shared memory;throughput	Yutaro Ishida;Takashi Morie;Hakaru Tamukoh	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351722	computer hardware;image processing;field-programmable gate array;electronic circuit;computation;robot;rapid prototyping;computer science;middleware	Robotics	3.554102433138419	46.64620376018558	698
101ea11c77eeae1f82b7626268ba3a2a1c41c3f1	a distributed experimental communications system	communication system;local area network lan;data management;packet switching;data communication;local area networks switches communication switching packet switching costs broadcasting distributed control system testing data communication intelligent networks;packet switching integrated services digital networks lans local area network lan multiple access communications;integrated services digital networks;lans;multiple access communications;distributed control;local area network	The packet experimental communications system (packet XCS) is a new experimental voice and data switch. It uses a local-area network (LAN) for digital voice transmission, with local intelligence for switching. The packet XCS also has highly distributed control. The individual sites cooperate to provide user services as well as internal data management. We have learned that several local networks, including CSMA/CD networks, can be made to work well for voice transmission and that highly distributed control is practical in such a system. A system has been constructed which is used as a testbed for distributed voice and data communications experiments. This system is purely for experimentation and does not indicate a direction for future Bell System product offerings.	distributed control system;experiment;network packet;testbed	John DeTreville;W. David Sincoskie	1983	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.1983.1146021	local area network;real-time computing;lan switching;frame relay;packet analyzer;fast packet switching;telecommunications;data management;computer science;packet radio;distributed-queue dual-bus;packet switch;packet switching;communications system;circuit switching;computer network	Networks	-20.95238983413527	91.04098035797269	699
e518bfa938ce12da65407d3ab2ca67c2a42e16c5	smarter compositing with the kinect	image processing;computer graphics;algorithms;composites	A image processing pipeline is presented that applies principles from the computer graphics technique of deferred shading to composite rendered objects into a live scene viewed by a Kinect. Issues involving the presentation of the Kinect's output are addressed, and algorithms for improving the believability and aesthetic matching of the rendered scene against the real scene are proposed. An implementation of this pipeline using GLSL shaders to perform this pipeline at interactive framerates is given. The results of experiments with this program are provided that show promise that the approaches evaluated here can be applied to improve other implementations.	algorithm;compositing;computer graphics;deferred shading;display resolution;experiment;image processing;kinect;opengl shading language;shader;synthetic intelligence	Alex Karantza;Roxanne L. Canosa	2013		10.1117/12.2004183	simulation;image processing;computer science;multimedia;computer graphics;computer graphics (images)	Graphics	63.09709889701666	-50.848141093183365	700
0c299caa4a9bea6a675209beb7bb163d24d5842f	on the specification of part-whole relations in conceptual modeling: an empirical study	conceptual modeling;ontology	Part-whole relation (PWR), as a fundamental element to model the real world, has long been concerned by researchers and practitioners. Theoretical work has been undertaken to develop the classifications of distinct types of part-whole relations (PWRs) and their properties in an attempt to clarify their semantics. There is no empirical evaluation, however, that supports whether it is necessary to specify distinct types of PWRs and their properties in conceptual modeling. In this light, this study first designs a specific representation of PWRs through a systematic review of the literature, and then by employing the theory of ontological clarity and the theory of cognitive fit, empirically compares its performance with that of conventional representation of PWRs. The findings are expected to enrich the growing body of knowledge that supports the usefulness of ontological and cognitive theories, and provide empirical evidence regarding how to model the PWRs for practitioners.	cognitive tutor;systematic review;theory	Yonggui Wang;Dongming Xu;Fiona H. Rohde	2011			management science;knowledge management;conceptual model;computer science;empirical research;body of knowledge;empirical evidence;ontology;conceptual system;semantics;cognition	AI	-46.26483142599709	6.515404846852318	701
f62cf89580af74af3028f14c924849fa49889762	exploiting dominance conditions for computing non trivial worst-case complexity for bounded combinatorial optimization problems	article accepte pour publication ou publie;max cut;maximum degree;combinatorial optimization problem;dominance conditions;search trees;branch and bound method;exact algorithm;set covering;worst case complexity;set cover	In the design of branch-and-bound methods for NP-hard combinatorial optimization problems, dominance conditions have always been applied. In this work we show how the use of dominance conditions within search tree algorithms can lead to non trivial worst-case upper time bounds for the considered algorithms on bounded combinatorial optimization problems. We consider here the MIN 3-SET COVERING and the maxcut problem with maximum degree three. Combining dominance conditions and intuitive combinatorial arguments, we derive two exact algorithms with worst-case complexity bounded above by O*(1.4492n) and O*(1.2920n) for the former and the latter problem, respectively, where notation O*(·) takes into account only exponential factors, and n is the number of subsets for MIN 3-SET COVERING and the number of vertices of the input-graph for maxcut.	best, worst and average case;combinatorial optimization;mathematical optimization;worst-case complexity	Federico Della Croce;Vangelis Th. Paschos	2008	Operational Research	10.1007/s12351-008-0020-8	optimization problem;mathematical optimization;maximum cut;combinatorics;discrete mathematics;combinatorial optimization;computer science;worst-case complexity;mathematics;set cover problem	Theory	23.08045217181561	15.365050650033329	702
9293aa81547a10f6835219133a14d5cdcc797353	fault-tolerance data aggregation for clustering wireless sensor network	clustering sensor network;estensibilidad;tratamiento datos;protection information;tolerancia falta;hierarchical clustering;network lifetime;red sin hilo;teledetection;filtering outlier data;filtering;evaluation performance;reseau capteur;filtrage;procesamiento informacion;performance evaluation;fault tolerant;detection signal;reseau sans fil;telecommunication sans fil;redundancia;evaluacion prestacion;signal detection;wireless network;filtrado;simulation;data processing;telecommunication network;outlier;simulacion;traitement donnee;vulnerability;data fusion;securite donnee;data mining;sensor network;wireless sensor network;data concealment;observacion aberrante;vida privada;vulnerabilite;deteccion senal;vulnerabilidad;red sensores;redundancy;resilience;private life;proteccion informacion;data privacy;fouille donnee;criptografia;red telecomunicacion;telecomunicacion sin hilo;cryptography;information protection;fusion donnee;data aggregation;remote sensing;fault tolerance;information processing;signal classification;teledeteccion;reseau telecommunication;sensor array;classification signal;observation aberrante;cryptographie;vie privee;extensibilite;scalability;resiliencia;classification automatique;traitement information;fusion datos;automatic classification;clasificacion automatica;busca dato;confidentialite donnee;security of data;tolerance faute;redondance;wireless telecommunication	The hierarchical cluster-based topology is commonly accepted as an optimal structure for sensor network to increase communication scalability, prolong network lifetime, and reduce data redundancy. However, the data privacy and security are challenging the proliferation of clustering wireless sensor network (CWSN) due to its highly constrained resources and violably deployed environments, which make it infeasible to directly apply traditional cryptography and therefore vulnerable to various attacks. This article proposes a scheme that provides efficient privacy-preserving data fusion as well as malicious data tolerance by mining concealed data within groups. And the dynamically organized groups in each cluster improves resilience against large number of node compromise comparing with the existing data aggregation schemes. The simulation results and mathematical comparison show the effectiveness and fitness of our scheme for CWSN in terms of fault tolerance and process efficiency, which costs a little of additional overheads in memory and communication.		Shu Qin Ren;Jong Sou Park	2009	Wireless Personal Communications	10.1007/s11277-008-9598-7	fault tolerance;wireless sensor network;data processing;information processing;telecommunications;computer science;data mining;key distribution in wireless sensor networks;computer security;statistics	Mobile	-50.545688127518424	79.20698599256895	703
ea0599e883836f3c7b77cba6113d3fc213cd8b8d	sram: a state-aware risk assessment model for intrusion response		Recent advances in Intrusion Risk Assessment (IRA) have brought promising solutions to enhance Intrusion Response Systems (IRS). However, current researches lack reasonable solutions to exploit system state information. Without the system state, the IRA results may suffer from the high false rate of Intrusion Detection Systems (IDS). To address this limitation, we propose a novel State-Aware Risk Assessment Model (SRAM) by taking both the outputs of IDS and system state information into account. Specific evaluation factors are formulated for different attack types to improve the pertinence of evaluation. To better meet the needs of Quality of Service (QoS), expected weights on Confidentiality, Integrity and Availability (CIA) are considered based on different response intensions. D-S evidence theory is introduced in fusing the evaluation factors to provide an objective assessment. Experimental results show our approach can increase the credibility of IRA results effectively.	confidentiality;quality of service;relevance;risk assessment;static random-access memory	Fenghua Li;Fangxin Xiong;Chao Li;Lihua Yin;Guozhen Shi;Boxiu Tian	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.9	risk management;static random-access memory;quality of service;data mining;credibility;intrusion detection system;computer science;risk assessment;attack model;exploit	SE	-63.08840612440462	61.86946685183475	704
3f3c94fa36d74f6aac866edb9b80dbf9dde68326	future tense: little brother is watching	televigilancia;remote supervision;telesurveillance	Future Tense, one of the revolving features on this page, presents stories and essays from the intersection of computational science and technological speculation, their boundaries limited only by our ability to imagine what will and could be. In a world of technology and fear, the public gets to know what it wants to know... and more than it can possibly digest.	computational science;cryptographic hash function	Greg Bear	2010	Commun. ACM	10.1145/1810891.1810918	artificial intelligence;algorithm	HCI	-58.44103975466499	-24.78897564783317	705
6eb1594d65c90e9f41292e14c7e2ad7fec18f7e7	on maximum independent sets in p5-free graphs	conjunto independiente;temps exponentiel;complexite;subexponential algorithms;optimisation;combinatorics;temps polynomial;independance;optimizacion;independent set;combinatoria;independence number;complejidad;combinatoire;complexity;68wxx;independence;ensemble independant;independencia;informatique theorique;68r10;polynomial time;graph algorithm;optimization;ensemble independant maximal;algorithme graphe;computer theory;tiempo polinomial;maximum independent set;informatica teorica	The complexity status of the Maximum Independent Set Problem (MIS) for the family of P5-free graphs is unknown. Although for many subclasses of P5-free graphs MIS can be solved in polynomial time, only exponential time MIS-algorithms for general graphs are known so far. In this note we present the first algorithm to solve MIS for P5-free graphs in subexponential time. © 2010 Elsevier B.V. All rights reserved.	algorithm;exptime;graph (discrete mathematics);independent set (graph theory);polynomial;time complexity	Bert Randerath;Ingo Schiermeyer	2010	Discrete Applied Mathematics	10.1016/j.dam.2010.01.007	strong perfect graph theorem;1-planar graph;pathwidth;combinatorics;discrete mathematics;independent set;longest path problem;clique problem;hopcroft–karp algorithm;metric dimension;lévy family of graphs;trapezoid graph;mathematics;maximal independent set;chordal graph;indifference graph;algorithm	Theory	21.373301516367643	26.8479800992598	706
ec05453890c6d65a314994f9044c0673b0279271	investigating the linkage between total quality management and environmentally responsible manufacturing	relationship total quality management environmentally responsible manufacturing tqm erm large scale survey confirmatory factor analysis structural equation modeling;environmental factors;pulp manufacturing;tqm;confirmatory factor analysis;selected works;manufacture;relationship;pollution measurement;structural equation modeling;structural equation model;indexing terms;environmentally responsible manufacturing;large scale;total quality management;bepress;couplings;environmental management;empirical evaluation;virtual manufacturing;supply chain management;large scale survey;quality management;large scale systems;erm;manufacture quality management environmental factors	This paper explicitly examines the relationship that exists between Total Quality Management (TQM) and Environmentally Responsible Manufacturing (ERM) systems. It has been presumed in numerous past studies that such a relationship does exist. It has been argued that those firms that have successfuhy implemented a TQM system are better positioned to successfully implement an ERM system. This relationship, however, has not yet been statistically and empirically evaluated. In this study, the authors evaluate this relationship using a large-scale survey of plant managers as the data source and Confirmatory Factor Analysis and Structural Equation Modeling as the statistical tools. The study develops a series of measures for various aspects of both TQM and ERM. The results show that there is indeed a strong relationship between TQM and ERM. In many ways, ERM is conditioned by the presence of TQM. Furthermore, ERM systems have a parallel structure when compared to TQM systems.	confirmatory factor analysis;linkage (software);structural equation modeling	Sime Curkovic;Steven A. Melnyk;Robert Handfield;Roger Calantone	2000	IEEE Trans. Engineering Management	10.1109/17.895340	structural equation modeling;quality management;supply chain management;economics;total quality management;engineering;operations management;management;manufacturing engineering	SE	-80.78202260368757	5.2073618270476185	707
e758728e6b1cc626a1fc43495becb40a50531a94	completely decomposable abelian groups -categorical over a subgroup		Let L be the usual language of abelian groups and L(P) be an expansion of it by a unary predicate P(x). We will consider L(P)-structures which are abelian groups such that the setofrealisations of P(x) are subgroups. If A is an abelian group and B is one of its subgroups then (A, B) will symbolise the L(P)-structure formed by A with B as the set of realisations of P. These kind of structures will be called L(P)-groups and the subgroup B will be called the P-part of(A, B). Furthermore the word group will always mean abelian group and these will be written additively. A ~) will denote the direct sum over the cardinal x. We will also consider direct sums of L(P)-groups; (A, B)•(C, D) being simply (A 9 C, B OD) and (A, B) (~) being (A ~), BI~)). For all unexplained terminology see [2].	predicate (mathematical logic);unary operation	Roger Villemaire	1992	Arch. Math. Log.	10.1007/BF01794983	rank of an abelian group;omega and agemo subgroup;elementary abelian group;coset;torsion subgroup;metabelian group;normal subgroup;commutator subgroup;characteristic subgroup	DB	-5.235482712125441	14.487380430671553	708
e3aaca829d5002d4d1a00c576c284520479338fe	high performance computing architecture with security	computer engineering;text;he;electronic dissertation;electrical engineering high performance computing architecture with security the university of arizona janet m roveda zhou;electrical computer engineering	................................................................................................................................ 13 CHAPTER		He Zhou	2015			electrical engineering technology;computing;computer science;engineering;software engineering;computer engineering	HPC	-52.967013427458156	-0.8656107890158223	709
fec5c63013ba6749f04c6635ef2934dd95f42996	note on the nonbinary bch code (corresp.)	bch codes;bch code	First Page of the Article	bch code	Avelino Ong	1969	IEEE Trans. Information Theory	10.1109/TIT.1969.1054368	mathematics;bch code	Theory	41.77047915493778	56.544938377874	710
e457c35d410196966c3943d6b423aa844e99d913	using prefetching to improve reference-counting garbage collectors	reference counting;garbage collection;garbage collector	Reference counting is a classical garbage collection method. Recently, a series of papers have extended the basic method to drastically reduce its notorious overhead and extend the basic method to run concurrently and efficiently on a modern computing platform. In this paper we investigate the use of prefetching to further improve the efficiency of the reference-counting collector. We propose potential prefetching opportunities for the advanced reference-counting collector and report an implementation of a collector that employs such prefetching. The proposed prefetch instructions were inserted into the Jikes referencecounting collector obtaining an average reduction of 8.7% of the memory management overheads.	cpu cache;garbage collection (computer science);jikes;link prefetching;memory management;overhead (computing);prefetch input queue;reference counting	Harel Paz;Erez Petrank	2007		10.1007/978-3-540-71229-9_4	garbage;parallel computing;real-time computing;computer science;operating system;garbage collection;programming language	PL	-12.313672702555962	50.97039294183858	711
18f422f62ad4d974d051ba10dddd00b878b25f54	evolved term-weighting schemes in information retrieval: an analysis of the solution space	genetic program;distance measure;information retrieval;genetic programming;term weighting schemes;term weighting;evolutionary computing	Evolutionary computation techniques are increasingly being applied to problems within Information Retrieval (IR). Genetic programming (GP) has previously been used with some success to evolve term-weighting schemes in IR. However, one fundamental problem with the solutions generated by this stochastic, non-deterministic process, is that they are often difficult to analyse. In this paper, we introduce two different distance measures between the phenotypes (ranked lists) of the solutions (term-weighting schemes) returned by a GP process. Using these distance measures, we develop trees which show how different solutions are clustered in the solution space. We show, using this framework, that our evolved solutions lie in a different part of the solution space than two of the best benchmark term-weighting schemes available.	axiomatic system;benchmark (computing);centrality;evolutionary computation;fits;feasible region;genetic programming;information retrieval;nondeterministic algorithm;programming paradigm;test data	Ronan Cummins;Colm O'Riordan	2006	Artificial Intelligence Review	10.1007/s10462-007-9034-5	genetic programming;mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary computation	AI	23.432845373386865	-8.371133698091526	712
b71469fe191ba54d91e0c94f2fc28d07a629e2c8	efficient tracking of the cross-correlation coefficient	tracking algorithm;stationary signals;audio systems;evaluation performance;time dependent;audio signal processing;convergence;performance evaluation;coeficiente correlacion;cross correlation;digital audio recording efficient tracking cross correlation coefficient audio processing algorithms discrete time signals time dependent correlation recursive formula warping operation deformed correlation sinusoidal signals convergence behavior stationary signals dynamic behavior stationary state nonstationary signals tracking algorithm stereo music fragments;correlation croisee;evaluacion prestacion;signal analysis;convergence of numerical methods;speech analysis;real time processing;discrete time;convergence of numerical methods correlation methods tracking audio signal processing music;audio recording;recursive formula;correlation methods;stationary state;algorithme;algorithm;tratamiento tiempo real;linear predictive coding;cross correlation coefficient;traitement temps reel;stereophonie;warping operation;traitement signal audio;efficient tracking;discrete time signals;acoustical engineering;signal processing;stereo music fragments;stereophony;estimacion parametro;sinusoidal signals;parameter estimation;estimation parametre;nonstationary signals;deformed correlation;correlation coefficient;acoustic measurements;digital audio recording;coefficient correlation;music;estereofonia;tracking;time dependent correlation;correlacion cruzada;signal processing convergence stationary state signal analysis audio recording audio systems acoustic measurements acoustical engineering linear predictive coding speech analysis;algoritmo;convergence behavior;dynamic behavior;audio processing algorithms	In many (audio) processing algorithms, involving manipulation of discrete-time signals, the performance can vary strongly over the repertoire that is used. This may be the case when the signals from the various channels are allowed to be strongly positively or negatively correlated. We propose and analyze a general formula for tracking the (time-dependent) correlation between two signals. Some special cases of this formula lead to classical results known from the literature, others are new. This formula is recursive in nature, and uses only the instantaneous values of the two signals, in a low-cost and low-complexity manner; in particular, there is no need to take square roots or to carry out divisions. Furthermore, this formula can be modified with respect to the occurrence of the two signals so as to further decrease the complexity, and increase ease of implementation. The latter modification comes at the expense that not the actual correlation is tracked, but, rather, a somewhat deformed version of it. To overcome this problem, we propose, for a number of instances of the tracking formula, a simple warping operation on the deformed correlation. Now we obtain, at least for sinusoidal signals, the correct value of the correlation coefficient. Special attention is paid to the convergence behavior of the algorithm for stationary signals and the dynamic behavior if there is a transition to another stationary state; the latter is considered to be important to study the tracking abilities to nonstationary signals. We illustrate tracking algorithm by using it for stereo music fragments, obtained from a number of digital audio recordings.		Ronald M. Aarts;Roy Irwan;Augustus J. E. M. Janssen	2002	IEEE Trans. Speech and Audio Processing	10.1109/TSA.2002.803447	stationary state;linear predictive coding;speech recognition;convergence;acoustics;audio signal processing;computer science;signal processing;music;mathematics;tracking;statistics	Web+IR	80.27674668614215	-33.372255440931674	713
a903861cbc77b3f8162fbb6ce9ddb1e69981fe4c	representation of parts within the foundational model of anatomy ontology		As biomedical ontologies grow in size and complexity it is crucial to develop methods for detecting inconsistencies within ontologies. The Foundational Model of Anatomy (FMA) ontology represents knowledge of human anatomy, with structural organization provided by class and part relationships. Using a manual audit, I identify types of inconsistencies arising from class and regional part relationships for regions of the body and the parts of organs. Inconsistencies arise from both explicitly declared relationships and relationships that are implied by the lexical constructs of class names. The purpose of this work is to propose methods of structural organization and lexical consistency that will make the FMA more compatible with computational auditing and increase its usability. Keywords—ontology; partonomy; anatomy	fma instruction set;foundational model of anatomy;meronomy;ontology (information science);sensor;usability	Melissa Clarkson	2016			natural language processing;foundational model of anatomy;ontology (information science);ontology;artificial intelligence;computer science	AI	-49.98836786439978	-66.56390436072022	714
eea28c981e90fcfe22395a8d37c983250b9757b4	agent-based computational economics: modelling economies as complex adaptive systems	agent based computational economics;complex adaptive systems;complex adaptive system;dynamic system;agent based computational economic	Agent-based computational economics (ACE) is the computational study of economies modelled as evolving systems of autonomous interacting agents. Thus, ACE is a specialization to economics of the basic complex adaptive systems paradigm. This paper outlines the main objectives and defining characteristics of the ACE methodology, and discusses several active research areas.	ace;agent-based computational economics;autonomous robot;complex adaptive system;computation;emergence;intelligent agent;interaction;partial template specialization;programming paradigm	Leigh Tesfatsion	2002	Inf. Sci.	10.1016/S0020-0255(02)00280-3	complex adaptive system;simulation;computer science;artificial intelligence;management science	AI	-19.39305031218518	-13.74692282604813	715
5d16414b9403ab6c343f5dece9b98f3126a66a2a	flightplan flight tests of an experimental da42 general aviation aircraft	nonlinear dynamical systems;kinematics;acceleration;aerospace control;trajectory;safety;aircraft	The recent emergence of unmanned aerial vehicles asked for both well-performing auto-flight systems and fly-by-wire architectures. Towards this end, the trajectory control module of an integrated auto-flight control system is introduced in this paper, which utilizes nonlinear second-order error dynamics of the position error and uses nonlinear dynamic inversion as a control methodology. Flight test results of a flightplan mission conducted on the institute's general aviation aircraft — a DA42 augmented with experimental fly-by-wire — are presented and the controller's performance is evaluated.	aerial photography;autopilot;control system;control unit;emergence;fly-by-wire;hardware-in-the-loop simulation;nonlinear system;unmanned aerial vehicle	Simon P. Schatz;Volker Schneider;Erik Karlsson;Florian Holzapfel;Thaddaus Baier;Christoph Dorhofer;Markus Hochstrasser;Agnes Gabrys;Christoph Krause;Patrick J. Lauffs;Nils C. Mumm;Kajetan Nurnberger;Lars Peter;Philip Spiegel;Lukas Steinert;Alexander W. Zollitsch	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838646	acceleration;kinematics;simulation;aerospace engineering;engineering;trajectory;aeronautics;flight simulator;control theory	Robotics	62.60006095579841	-16.57281796949709	716
13cd91c9fee79b5ab2778e9489bf546260ffbdb7	lan/wan interworking in the osi environment	transportation network;red transporte;interconnection;red local;layer;etude experimentale;couche;systeme ouvert;capa;local network;conexion;raccordement;interconnexion;open systems;reseau local;sistema abierto;connection;estudio experimental;interconeccion;reseau transport	Schepers, H.J.J.H., O.B.P. Rikkert de Koe, G.M.J. Havermans and D.K. Hammer, LAN/WAN interworking in the OSI environment, Computer Networks and ISDN Systems 23 (1992) 253-266. The rapid growing number of LAN and WAN services makes interworking between these two types of network increasingly important. This article describes the different LAN/WAN interworking scenarios, without referring to a particular type of LAN or WAN, i.e. it concentrates on OSI Layer 3 and 4. If there is a homogeneous network service, then interworking within the context of the Basic Reference Model is possible. If not, then such interworking is not possible. For the latter case, the international standardization community presents two interworking scenarios: the interworking functional unit and the distributed system gateway. The concepts of naming, addressing, routing and relaying in the OSI environment have been described in detail in part one [3] of this two part series.	distributed computing;execution unit;integrated services digital network;interoperability;lan messenger;osi model;reference model;routing	H. J. J. H. Schepers;O. B. P. Rikkert de Koe;G. M. J. Havermans;D. K. Hammer	1992	Computer Networks and ISBN Systems	10.1016/0169-7552(92)90077-4	local area network;telecommunications;connection;computer science;interconnection;layer;open system;computer network	Networks	-21.249847559028762	89.01364904519367	717
3e9c088ee0caf68de24866420452c4d3cf40bab8	introduction to the special issue on intelligent technologies in medicine and bioinformatics		The adoption of powerful and sophisticated Intelligent Technologies (IT), such as neural networks, support vector machines, evolutionary algorithms, clustering methods, and decision trees, has led to advances to several challenging real-world medical and bioinformatics problems during the last few years. Applications of these technologies include tumour classification, gene function analysis and prediction, protein modelling and prediction, pathway analysis, complex clinical data analysis, processing and visualization, medical and biomedical information extraction, knowledge discovery and management, intelligent retrieval and integration of biological and medical information and processing, analysis and interpretation of medical and microarrays images. After performing a pilot survey about the existence and evolution of IT-related publications in medicine, biology and bioinformatics using PubMed, the following were observed:	artificial neural network;bioinformatics;cluster analysis;decision tree;evolutionary algorithm;gene regulatory network;information extraction;microarray;pathway analysis;pubmed;support vector machine	George D. Magoulas;Georgios Dounias	2006	Comp. in Bio. and Med.	10.1016/j.compbiomed.2005.09.001	computational biology;computer vision;computer science;artificial intelligence	ML	4.632555204912073	-46.8073635492498	718
ea21548f98eece9006b56fde796445e12e395b89	all well if starts well? citation infancy of recently launched chemistry journals	self citation;analyse bibliometrique;citation analysis;life cycle;facteur impact;periodical;impact factor;analisis cita;autocitation;analyse citation;periodique;periodico;chimie;chemistry;quimica;bibliometric analysis;sci;analisis bibliometrico	The impact factor and the journal self-citation rate of 22 newly launched chemistry journals has been investigated. The dependence of these indicators on the journal's age was found to be rather characteristic to the initial period of a journal's “life cycle”.	enterprise life cycle;journal citation reports	Hajnalka Maczelka;Sándor Zsindely	1992	Scientometrics	10.1007/BF02028092	biological life cycle;computer science;operations research;citation analysis;world wide web	NLP	-75.1944228722813	-22.300997228565603	719
f528a82f933ea1ccc2677e70a5831d4e9e13b1dd	algorithm 717: subroutines for maximum likelihood and quasi-likelihood estimation of parameters in nonlinear regression models	second order;optimisation;analisis numerico;regression non lineaire;nonlinear least squares;algorithms measurement;optimizacion;maximum likelihood;quasi likelihood;gauss newton;performance;non linear regression;maximum vraisemblance;regresion no lineal;analyse numerique;algorithme;probabilistic model;algorithm;trust region;numerical analysis;choice models;theory;estimacion parametro;modele probabiliste;subroutine;probit model;general linear model;sous programme;optimization;fortran;parameter estimation;estimation parametre;maxima verosimilitud;subprograma;nonlinear regression;modelo probabilista;nonlinear model;algoritmo	We present FORTRAN 77 subroutines that solve statistical parameter estimation problems for general nonlinear models, e.g., nonlinear least-squares, maximum likelihood, maximum quasi-likelihood, generalized nonlinear least-squares, and some robust fitting problems. The accompanying test examples include members of the generalized linear model family, extensions using nonlinear predictors (“nonlinear GLIM”), and probabilistic choice models, such as linear-in-parameter multinomial probit models. The basic method, a generalization of the NL2SOL algorithm for nonlinear least-squares, employs a model/trust-region scheme for computing trial steps, exploits special structure by maintaining a secant approximation to the second-order part of the Hessian, and adaptively switches between a Gauss-Newton and an augmented Hessian approximation. Gauss-Newton steps are computed using a corrected seminormal equations approach. The subroutines include variants that handle simple bounds on the parameters, and that compute approximate regression diagnostics.	approximation algorithm;fortran;gauss–newton algorithm;generalized linear model;hessian;multinomial logistic regression;network switch;newton;non-linear least squares;nonlinear system;secant method;subroutine;trust region	David S. Bunch;David M. Gay;Roy E. Welsch	1993	ACM Trans. Math. Softw.	10.1145/151271.151279	econometrics;mathematical optimization;mathematics;nonlinear regression;statistics	ML	33.46250885998377	-24.50346464637883	720
d819c759138f5f2e3fb016f2b2a53e195d76e7d8	modification of the pascal-p compiler for a single-accumulator one-address minicomputer	portable compiler;pcode;pascal;one address code generation	Abstract#R##N##R##N#This paper describes changes made to the Pascal-P compiler in order to improve the efficiency of its implementation on a single-accumulator one-address computer, the PRIME 300. The aim of the project was to develop a true compiler rather than a threaded code or pure interpretation system. A comparison of timings for these three methods of implementing the Pascal-P compiler is also presented.	accumulator (computing);compiler;minicomputer;object pascal	B. J. Cornelius;D. J. Robson;M. I. Thomas	1980	Softw., Pract. Exper.	10.1002/spe.4380100309	compile time;computer architecture;compiler;parallel computing;loop-invariant code motion;pascal;profile-guided optimization;compiler correctness;interprocedural optimization;computer science;loop optimization;superoptimization;operating system;compiler construction;dead code elimination;optimizing compiler;array access analysis;bootstrapping;compilation error;programming language;inline expansion;intrinsic function;functional compiler;code generation;threaded code	PL	-21.704491149349565	33.63810886787837	721
7fdc484e5911b500dbbbc3c590247fcc31b70ab8	gamelifevis: visual analysis of behavior evolutions in multiplayer online games	game visualization;time-oriented data visualization;visual abstraction;multiplayer online games			Wei Chen;Junhua Lu;Dingke Kong;Zhiqi Liu;Yandi Shen;Yinyin Chen;Jingxuan He;Shu Liu;Ye Qi;Yingcai Wu	2017	J. Visualization	10.1007/s12650-016-0416-0	physics;simulation	HCI	-33.35353603422117	-32.39319561953094	722
6688709e68d46f520fe194d9f0c76df0461acea7	sparse recovery with coherent tight frame via analysis dantzig selector and analysis lasso	gaussian noise;lasso;restricted isometry property	This article considers recovery of signals that are sparse or approximately sparse in terms of a (possibly) highly overcomplete and coherent tight frame from undersampled data corrupted with additive noise. We show that the properly constrained l 1-analysis optimization problem, called analysis Dantzig selector, stably recovers a signal which is nearly sparse in terms of a tight frame provided that the measurement matrix satisfies a restricted isometry property adapted to the tight frame. As a special case, we consider the Gaussian noise. Further, under a sparsity scenario, with high probability, the recovery error from noisy data is within a log-like factor of the minimax risk over the class of vectors which are at most s sparse in terms of the tight frame. Similar results for the analysis LASSO are shown. The above two algorithms provide guarantees only for noise that is bounded or bounded with high probability (for example, Gaussian noise). However, when the underlying measurements are corrupted by sparse noise, these algorithms perform suboptimally. We demonstrate robust methods for reconstructing signals that are nearly sparse in terms of a tight frame in the presence of bounded noise combined with sparse noise. The analysis in this paper is based on the restricted isometry property adapted to a tight frame, which is a natural extension to the standard restricted isometry property.	additive white gaussian noise;algorithm;coherent;compressed sensing;frame (linear algebra);gaussian blur;lasso;mathematical optimization;minimax;optimization problem;restricted isometry property;signal-to-noise ratio;sparse matrix;utility functions on indivisible goods;with high probability	Junhong Lin;Song Li	2013	CoRR			ML	48.623983900587724	9.337132215197318	723
66bb12922c76f652df708fcfb91e2921d6d657b8	ontological analysis: an ongoing experiment	representacion conocimientos;metodologia;semantica denotacional;base connaissance;methodologie;resolucion problema;denotational semantics;base conocimiento;methodology;knowledge representation;representation connaissances;semantique denotationnelle;problem solving;resolution probleme;knowledge base	Knowledge engineering is a complex activity which permeated with problems inherent in the difficulties of choosing the correct abstractions. Knowledge-level analysis has been suggested as a technique to help manage this complexity. We have previously presented a methodology, called ontological analysis, which provides a technique for performing knowledge-level analysis of a problem space. This paper presents the experiences we have gained with knowledge-level analysis. Our experiences are reported and the criteria for a formal knowledge-level analysis language are discussed.		James H. Alexander;Michael J. Freiling;Sheryl Shulman;Steven Rehfuss;Steven Messick	1987	International Journal of Man-Machine Studies	10.1016/S0020-7373(87)80082-2	knowledge base;computer science;artificial intelligence;methodology;denotational semantics;algorithm	SE	-24.106288084906087	-3.109388515064912	724
38e2e04d7d951866a727c296951c60a295ee7b65	data-to-model: a mixed initiative approach for rapid ethnographic assessment	agent based simulation;text mining;network analysis;data analysis;social networks;meta network;newspaper data	Rapid ethnographic assessment is used when there is a need to quickly create a socio-cultural profile of a group or region. While there are many forms such an assessment can take, we view it as providing insight into who are the key actors, what are the key issues, sentiments, resources, activities and locations, how have these changed in recent times, and what roles do the various actors play. We propose a mixed initiative rapid ethnographic approach that supports socio-cultural assessment through a network analysis lens. We refer to this as the data-to-model (D2M) process. In D2M, semi-automated computer-based text-mining and machine learning techniques are used to extract networks linking people, groups, issues, sentiments, resources, activities and locations from vast quantities of texts. Human-in-the-loop procedures are then used to tune and correct the extracted data and refine the computational extraction. Computational post-processing is then used to refine the extracted data and augment it with other information, such as the latitude and longitude of particular cities. This methodology is described and key challenges illustrated using three distinct data sets. We find that the data-to-model approach provides a reusable, scalable, rapid approach for generating a rapid ethnographic assessment in which human effort and coding errors are reduced, and the resulting coding can be replicated.	computation;email;embedded system;experiment;geographic coordinate system;information extraction;machine learning;parallel computing;refinement (computing);scalability;semiconductor industry;simulation;stemming;text mining;universal instantiation;video post-processing	Kathleen M. Carley;Michael W. Bigrigg;Boubacar Diallo	2012	Computational & Mathematical Organization Theory	10.1007/s10588-012-9125-y	text mining;social science;simulation;network analysis;computer science;artificial intelligence;data science;data mining;data analysis;management;social network	HCI	-23.136865686590294	-54.7233231018553	725
1d504b29f5b157c3e5c6895dd5167918930137a2	modal decoupled dynamics-velocity feed-forward motion control of multi-dof robotic spine brace		Based on the parallel robotic manipulator, this paper proposes a motion control strategy for the novel robotic spine brace for spinal rehabilitation exercises. However, several shortcomings of this parallel robotic manipulator, such as dynamic coupling in joint space, low response frequency in roll and pitch directions, and bad influence of device’s gravity, result in bad effects on the performance of the robotic spine brace system. For solving these problems of parallel robotic manipulator, a new motion control structure, modal space dynamics-velocity feed-forward (MSDF) motion control strategy, is designed in this paper. A robotic spine brace system model and an actuator dynamic model are expressed using the Kane method. Stability of the robotic system with the MSDF control method is analyzed. For evaluating the performances of the proposed motion control structure, an experimental parallel robotic manipulator is built. Experimental results reveal that the presented MSDF motion control strategy can eliminate those disadvantages efficiently.		Xinjian Niu;Chifu Yang;Bowen Tian;Xiang Li;Junwei Han;Sunil Kumar Agrawal	2018	IEEE Access	10.1109/ACCESS.2018.2878278	brace;feed forward;actuator;modal;motion control;distributed computing;computer science;manipulator;coupling;system model;control theory	Robotics	70.26105048855375	-23.521300933340804	726
8e4f800f63291a77b5f6dc66821c2975986623d2	complexity reduction in the negotiation of new lexical conventions		In the process of collectively inventing new words for new concepts in a population, conflicts can quickly become numerous, in the form of synonymy and homonymy. Remembering all of them could cost too much memory, and remembering too few may slow down the overall process. Is there an efficient behavior that could help balance the two? The Naming Game is a multi-agent computational model for the emergence of language, focusing on the negotiation of new lexical conventions, where a common lexicon self-organizes but going through a phase of high complexity. Previous work has been done on the control of complexity growth in this particular model, by allowing agents to actively choose what they talk about. However, those strategies were relying on ad hoc heuristics highly dependent on fine-tuning of parameters. We define here a new principled measure and a new strategy, based on the beliefs of each agent on the global state of the population. The measure does not rely on heavy computation, and is cognitively plausible. The new strategy yields an efficient control of complexity growth, along with a faster agreement process. Also, we show that short-term memory is enough to build relevant beliefs about the global lexicon.	cognition;computation;computational model;emergence;heuristic (computer science);hoc (programming language);lexicon;multi-agent system;protologism;reduction (complexity)	William Schueller;Vittorio Loreto;Pierre-Yves Oudeyer	2018	CoRR		reduction (complexity);active learning;cognitive psychology;psychology;computation;lexicon;population;heuristics;homonym;negotiation	AI	-15.181394658134867	-12.456432163285228	727
c790c6279460e9fa80e3aeea158232f6c2b29c9f	the effect of computer self-efficacy on security training effectiveness	experimental method;training;computer self efficacy;security	The purpose of this paper is to determine the effectiveness different levels of instruction have on security tool usage for individuals at different levels of computer self-efficacy. This is accomplished by utilizing a quasi-experimental method to demonstrate the effect that computer self-efficacy has on people's usage of security tools as well as the affect that different levels of instruction have on computer self-efficacy and usage of security tools. Initial results support that a person's level of computer self-efficacy significantly impacts his or her use of security tools. Later data did not show that instruction was effective at increasing computer self-efficacy and use of security tools.	experiment	Robert E. Crossler;France Belanger	2006		10.1145/1231047.1231075	simulation;human–computer interaction;computer science;information security;multimedia;computer security	Security	-69.91882553093541	-48.28928657185372	728
a957941be1c7128207dd4ff1af27afa954e359bb	accelerometer-based event detector for low-power applications	mems;power efficiency;autocovariance;transducers;equipment failure analysis;acceleration;intelligent signal processing;equipment design;energy transfer;measurement instrumentation;security measures;accelerometer;accelerometry	In this paper, an adaptive, autocovariance-based event detection algorithm is proposed, which can be used with micro-electro-mechanical systems (MEMS) accelerometer sensors to build inexpensive and power efficient event detectors. The algorithm works well with low signal-to-noise ratio input signals, and its computational complexity is very low, allowing its utilization on inexpensive low-end embedded sensor devices. The proposed algorithm decreases its energy consumption by lowering its duty cycle, as much as the event to be detected allows it. The performance of the algorithm is tested and compared to the conventional filter-based approach. The comparison was performed in an application where illegal entering of vehicles into restricted areas was detected.	algorithm;computation;computational complexity theory;detector device component;detectors;drug vehicle;duty cycle;embedded system;embedding;filter bank;medication event monitoring system;micro-electrical-mechanical systems;microelectromechanical systems;online and offline;requirement;signal processing;signal-to-noise ratio;accelerometers;algorithm;sensor (device)	József Smidla;Gyula Simon	2013		10.3390/s131013978	embedded system;electronic engineering;real-time computing;engineering;nanotechnology;microelectromechanical systems;accelerometer;physics	EDA	56.33423596205404	45.883077728568054	729
0e2fb07ce488e2bed53240f0ba1e8d3e8a3bfc93	design of the speedos operating system kernel		(Eine inhaltsgleiche, deutsche Fassung dieser Übersicht ist ab Seite 243 zu finden.) The design of current operating systems and their kernels shows deficiencies in respect to the structuring approach and the flexibility of their protection systems. The operating systems and applications suffer under this lack of extensibility and flexibility. The protection model implemented in many operating systems is not powerful enough to represent arbitrary protection conditions on a more fine-grained granularity than giving read and/or write access to an entire object. Additionally current operating systems are not capable of controlling the flow of information between software units effectively. Confinement conditions cannot be expressed explicitly and thus confinement problems can only be solved indirectly. Further complications with the protection system and especially the software structure in modern operating systems based on the microkernel approach are caused by the use of the out-of-process model. It is extremely difficult to specify access rights appropriately, because the client/server paradigm does not easily allow a relationship to be established between the role of the client and the permissions of the server. Focusing on client/server structures favours a single, central server implementation. Specifying a software design and communication model for applications at the operating system level impairs their structure. In reaction to this observation, SPEEDOS follows the in-process model. Processes are the abstraction of activity and are orthogonal to the information-hiding objects. This model is part of the design of many object-oriented programming languages and a few operating systems. The method call does not switch processes, it transfers execution to another object in a controlled fashion. This model is almost equivalent to the out-of-process model, but the in-process model provides advantages, because the process identifier correlates to a subject. However this only helps with protection, but does not magically improve the protection system. The two major deficiencies identified and addressed in this thesis are the versatility of access right specification and the structuring of the operating system in conjunction with the applications. The SPEEDOS design places the emphasis on balancing the duties and powers of the kernel and the applications, in order to obtain a flexible and extensible overall system. SPEEDOS supports freely programmable protection checks for individual method invocations. These checks are implemented with bracket methods, which intercept other method invocations. The concept was invented in the context of componentoriented programming languages, which are meant to improve the software structure beyond the state of the art in object-oriented programming languages. In the	access control;client–server model;eine and zwei;extensibility;file system permissions;kernel (operating system);method (computer programming);microkernel;modern operating systems;norm (social);operating system;process identifier;process modeling;programming language;programming paradigm;server (computing);software design;visual intercept	Klaus Espenlaub	2005			embedded system;real-time computing;operating system	OS	-26.394988175355365	30.06487007981629	730
12fcf2ccee3a6720bcfcb35da87f7e996851b92a	interferometric processing algorithms of tandem-x data	sar signalverarbeitung;spectral matching;itp tandem x insar sar;azimuth;filtering;synthetic aperture radar image registration radar interferometry remote sensing;spectral matching interferometric processing algorithms tandem x data digital elevation model;radar interferometry;tandem x;tandem x data;digital elevation model;accuracy;sar;remote sensing;image registration;satellites;pixel;insar;coherence;correlation azimuth accuracy coherence satellites pixel filtering;correlation;interferometric processing algorithms;itp;synthetic aperture radar;phase unwrapping	The purpose of this paper is to provide an algorithmic overview of the interferometric processing embedded in the Integrated TanDEM-X Processor (ITP), settled to the generation of the raw digital elevation model (DEM). The main processing blocks are described, with a focus on the spectral matching of the azimuth spectra, the high-precision coregistration, the dual-baseline phase unwrapping and the geocoding of the products. The robustness of the algorithms is demonstrated through a dual-pass TerraSAR-X scenario.	algorithm;baseline (configuration management);decorrelation;digital elevation model;embedded system;error analysis for the global positioning system;geocoding;gradient;image registration;instantaneous phase;maximum flow problem;minimum-cost flow problem;tandem computers	Nestor Yague-Martinez;Cristian Rossi;Marie Lachaise;Fernando Rodríguez González;Thomas Fritz;Helko Breit	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5652406	filter;computer vision;synthetic aperture radar;coherence;digital elevation model;specific absorption rate;image registration;accuracy and precision;interferometric synthetic aperture radar;azimuth;optics;correlation;physics;satellite;pixel;remote sensing	Embedded	79.28242121082168	-64.57761340776753	731
cc0f291521fd98551e33ca752cf9abdc0409bbf5	classification of web documents using a naive bayes method	library of congress classification;web documents;document handling;classification algorithm;libraries smoothing methods computer science testing training data support vector machines support vector machine classification classification algorithms web sites information retrieval;automatic document classification system web document world wide web naive bayes method webdoc library of congress classification scheme knowledge base training data classification algorithm probability theory f measure;smoothing method;bayes methods;naive bayes;classification;web sites document handling bayes methods classification internet;internet;web sites;probability theory;feature selection;document classification;knowledge base	This paper presents an automatic document classification system, WebDoc, which classifies Web documents according to the Library of Congress classification scheme. WebDoc constructs a knowledge base from the training data and then classifies the documents based on information in the knowledge base. One of the classification algorithms used in WebDoc is based on Bayes’ theorem from probability theory. This paper focuses upon three aspects of this approach: different event models for the naive Bayes method, different probability smoothing methods, and different feature selection methods. In this paper, we report the performance of each method in terms of recall, precision, and F-measures. Experimental results show that the WebDoc system can classify Web documents effectively and efficiently.	algorithm;comparison and contrast of classification schemes in linguistics and metadata;document classification;feature selection;knowledge base;library of congress classification;naive bayes classifier;smoothing	Yong Wang;Julia E. Hodges;Bo Tang	2003		10.1109/TAI.2003.1250241	probability theory;knowledge base;the internet;naive bayes classifier;library of congress classification;biological classification;computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval	Web+IR	-22.404227396824204	-63.32101776771633	732
fa72dca02ad17c687f063e6cce13fb5891656547	coarse coding: applications to the visual system of salamanders	nervous system;three dimensional;depth perception;information processing;receptive field;neural network model;visual field;visual system;neural network	In a previous study, we calculated the resolution obtained by a population of overlapping receptive fields, assuming a coarse coding mechanism. The results, which favor large receptive fields, are applied to the visual system of tongue-projecting salamanders. An analytical calculation gives the number of neurons necessary to determine the direction of their prey. Direction localization and distance determination are studied in neural network simulations of the orienting movement an d the tongue projection, respectively. In all cases, large receptive fields are found to be essential to yield a high sensory resolution. The results are in good agreement with anatomical, electrophysiological and behavioral data.	artificial neural network;prey;simulation	Christian W. Eurich;Helmut Schwegler;Richard Woesler	1997	Biological Cybernetics	10.1007/s004220050365	psychology;three-dimensional space;computer vision;neuroscience;visual system;depth perception;computer science;surround suppression;machine learning;nervous system;communication;receptive field;artificial neural network	ML	20.2504145372903	-68.58692195040406	733
a5596ab0084bfc5af6be98f407d37de35b6440ef	clusterwise p* models for social network analysis	blockmodeling;cluster analysis;social network analysis;p models	Clusterwise p∗ models are developed to detect differentially functioning network models as a function of the subset of observations being considered. These models allow the identification of subgroups (i.e., clusters) of individuals who are ‘structurally’ different from each other. These clusters are different from those produced by standard blockmodeling of social interactions in that the goal is not necessarily to find dense subregions of the network; rather, the focus is finding subregions that are functionally different in terms of graph structure. Furthermore, the clusterwise p∗ approach allows for local estimation of network regions, avoiding some of the common degeneracy problems that are rampant in p∗ (e.g., exponential random graph) models.  2011 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 4: 487–496, 2011	data mining;degeneracy (graph theory);exponential random graph models;interaction;john d. wiley;social network analysis;time complexity	Douglas Steinley;Michael J. Brusco;Stanley Wasserman	2011	Statistical Analysis and Data Mining	10.1002/sam.10139	combinatorics;social network analysis;computer science;machine learning;data mining;mathematics;cluster analysis;statistics	ML	-14.392076256411258	-41.330097911638326	734
7c86f4bdb72635425462df8100167755cff97d00	next-state functions for finite-state vector quantization	nearest neighbor searches;histograms;nearest neighbor design;vector prediction;conditional histogram;image coding;finite state vector quantization;neural nets;reordering procedure;dynamic finite state vector quantization;bit rate;dfsvq;address prediction;supercodebook;vector quantization;marine vehicles;prediction theory;computational complexity;vector quantization histograms frequency nearest neighbor searches bit rate statistics computational complexity algorithm design and analysis marine vehicles image coding;subcodebook construction;nearest neighbor;computational complexity finite state vector quantization next state functions dynamic finite state vector quantization dfsvq subcodebook construction input vector supercodebook reordering procedure codevectors conditional histogram address prediction vector prediction nearest neighbor design frequency usage hit ratio;statistics;input vector;vector quantizer;neural nets computational complexity vector quantisation prediction theory image coding;vector quantisation;frequency;hit ratio;algorithm design and analysis;next state functions;frequency usage;codevectors	The finite-state vector quantization scheme called dynamic finite-state vector quantization (DFSVQ) is investigated with regard to its subcodebook construction. In the DFSVQ, each input block is encoded by a small codebook called the subcodebook which is created from a much larger codebook called supercodebook. Each subcodebook is constructed by selecting, using a reordering procedure, a set of appropriate code-vectors from the supercodebook. The performance of the DFSVQ depends on this reordering procedure; therefore, several reordering procedures are introduced and their performance are evaluated. The reordering procedures investigated, are based on the conditional histogram of the code-vectors, index prediction, vector prediction, nearest neighbor design, and the frequency usage of the code-vectors. The performance of the reordering procedures are evaluated by comparing their hit ratios (the number of blocks encoded by the subcodebook) and their computational complexity. Experimental results are presented and it is found that the reordering procedure based on the vector prediction performs the best when compared with the other reordering procedures.	codebook;computational complexity theory;large;single linkage cluster analysis;thrombocytopenia;vector quantization	Nasser M. Nasrabadi;Syed A. Rizvi	1994	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/ICASSP.1994.389439	algorithm design;computer science;theoretical computer science;machine learning;frequency;pattern recognition;histogram;mathematics;linde–buzo–gray algorithm;computational complexity theory;k-nearest neighbors algorithm;vector quantization;artificial neural network;statistics	Visualization	44.367968798552674	-13.382440734092386	735
fde22b959a96046ee267a7866e1e81ca3654ead9	why usability gets lost or usability in in-house software development	user participation;software development process;computer supported work;risk factors;visual display unit work;visual display unit;health problems;software development;occupational health;user centred design;usability	This study tries to shed some light on what happens to usability and occupational health issues in a bespoke software development project. Usability is an essential quality in software, in particular in a work context where poor usability and other risk factors related to the software and computers may cause health problems. We have interviewed a number of software developers, usability people and users about their attitudes to and practices for integrating usability and users' health concerns in software development. The interviews were conducted in two Swedish organisations with in-house development of bespoke software. Our main conclusion is that several factors combine to push usability and occupational health matters aside, some of which are attitudes to usability and users' health issues, unclear responsibilities, poor support for user-centeredness and usability in software development models, ineffective user participation and usability and users' health being ignored or forgotten in decisions about the software, its use and its design.	in-house software;software development;usability	Inger Boivie;Carl Åborg;Jenny Persson;Mats Löfberg	2003	Interacting with Computers	10.1016/S0953-5438(03)00055-9	usability goals;pluralistic walkthrough;think aloud protocol;web usability;component-based usability testing;cognitive walkthrough;interactive systems engineering;usability;human–computer interaction;agile usability engineering;computer science;knowledge management;system usability scale;software development;software engineering;usability engineering;universal usability;occupational safety and health;software walkthrough;software documentation;heuristic evaluation;risk factor;software development process;usability lab;usability inspection;software peer review	SE	-70.51776750521387	24.64789293405508	736
200c6fa1808b9cc9ad2bc1f14e673ab1844ff8bf	implementing a commercial-strength parallel hybrid movie recommendation engine	databases;history;measurement;motion pictures;information filtering;time factors;intelligent systems;pattern recognition;history recommender systems motion pictures educational institutions databases time factors measurement;information search and retrieval;recommender systems;intelligent systems recommender systems pattern recognition information search and retrieval information filtering	AMORE is a hybrid recommendation system that provides movie recommendations for a major triple-play services provider in Greece. Combined with our own implementations of several user-, item-, and content-based recommendation algorithms, AMORE significantly outperforms other state-of-the-art implementations both in solution quality and response time. AMORE currently serves daily recommendation requests for all active subscribers of the provider's video-on-demand services and has contributed to an increase of rental profits and customer retention.	algorithm;recommender system;response time (technology)	Emmanouil Amolochitis;Ioannis T. Christou;Zheng-Hua Tan	2014	IEEE Intelligent Systems	10.1109/MIS.2014.23	intelligent decision support system;computer science;artificial intelligence;multimedia;world wide web;information retrieval;measurement;recommender system	AI	-29.020859961429316	-52.06796604461597	737
3190a8e52d98895ebc2b7e4186cef2cc069cfbde	an efficient huffman table sharing method for memory-constrained entropy coding of multiple sources	iterative method;image processing;faisceau superpose;haz superpuesto;complexite calcul;approximation method;senal compuesta;coaccion;contrainte;procesamiento imagen;entropy coding;search method;iterative algorithm;traitement image;capacidad memoria;merging beams;metodo iterativo;huffman code;codificacion;complejidad computacion;performance improvement;capacite memoire;constraint;memory capacity;codigo huffman;computational complexity;methode iterative;composite signal;code huffman;coding;codage;signal composite	For optimal entropy coding of multiple sources, the encoder and the decoder have to retain a separate Huffman table for each source. In many practical cases, however, available memory is usually restricted and therefore it is necessary for some sources to share a H&man table. Recently, we developed an iterative algorithm, which leads to locally optimal sharing of Huffman tables (Lee et al., 1995). In this paper, we examine the iterative algorithm in detail and present some modification methods which improve the sharing performance and reduce computational complexity. First, considering that the iterative algorithm provides only locally optimum, we introduce an unused table processing method and two initialization methods, splitting and merging, for the iterative algorithm. And fixed-length coding is introduced for the encoding of some sources, which provides considerable performance improvement in case only a small number of H&man tables are used. In addition, we present a simple approximation method of codelengths, which reduces the computational complexity of the sharing algorithm with little performance degradation. Simulations show that performance can be further improved with these modifications. For practical applications, we also present a search method for fast determining the number of H&man tables and the H&man table size, which, under a given memory constraint, minimize average bit-rate. The proposed sharing method can be widely applied to many high-order entropy coding systems with memory constraints. @ 1998 Elsevier Science B.V. All rights reserved.	algorithm;approximation;computational complexity theory;computer simulation;elegant degradation;encoder;entropy encoding;huffman coding;iterative method;local optimum	Seung Jun Lee;Choong Woong Lee	1998	Sig. Proc.: Image Comm.	10.1016/S0923-5965(97)00050-7	shannon–fano coding;canonical huffman code;image processing;computer science;artificial intelligence;theoretical computer science;mathematics;tunstall coding;iterative method;algorithm;statistics;huffman coding;range encoding	AI	45.2309054083158	-12.678612181429285	738
eec634c1293cd031dee2f47bf27734efd266a1f7	an error-tolerant keyword search scheme based on public-key encryption in secure cloud computing	public key encryption;keyword search;error tolerant;cloud computing	An error-tolerant keyword search scheme permits to make searches on encrypted data with only an approximation of some keyword. The scheme is suitable to the case where users' searching input might not exactly match those pre-set keywords. An error-tolerant keyword search scheme can be generated based on a public-key encryption scheme, in which anyone with access to a user's public key can generate the trapdoors and indexes to keywords, and only the user holding the decryption key can obtain the records it retrieves. In this paper, we first present a general framework for searching on error-tolerant keywords based on a public-key encryption scheme. Then, we propose a concrete scheme based on the Cramer-Shoup cryptosystem. The scheme is adaptive chosen-ciphertext attack secure and suitable for all similarity metrics including Hamming distance metric, edit distance metric, and set difference metric. It does not require the user to construct and store anything in advance, other than the cryptosystem used to calculate the trapdoor of keywords and to encrypt data documents. Thus, our scheme tremendously eases the users' burden. What is more, our scheme is able to transform the servers' searching for error-tolerant keywords on ciphertexts to searching for exact keywords on plaintexts. The server can use any existing approaches of exact keywords search to search plaintexts on an index table. Copyright © 2015John Wiley & Sons, Ltd.	cloud computing;encryption;error-tolerant design;public-key cryptography;search algorithm	Bo Yang;Mingwu Zhang;Jun-Qiang Du	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3521	cloud computing;computer science;theoretical computer science;operating system;database;internet privacy;public-key cryptography;world wide web	Crypto	-40.260973958817345	67.65488212669074	739
424e9b0e3035633ea3c363b2599f5ae3b1d6cdb6	caged-opossum: motif enrichment analysis from cage-derived tsss		UNLABELLED With the emergence of large-scale Cap Analysis of Gene Expression (CAGE) datasets from individual labs and the FANTOM consortium, one can now analyze the cis-regulatory regions associated with gene transcription at an unprecedented level of refinement. By coupling transcription factor binding site (TFBS) enrichment analysis with CAGE-derived genomic regions, CAGEd-oPOSSUM can identify TFs that act as key regulators of genes involved in specific mammalian cell and tissue types. The webtool allows for the analysis of CAGE-derived transcription start sites (TSSs) either provided by the user or selected from ∼1300 mammalian samples from the FANTOM5 project with pre-computed TFBS predicted with JASPAR TF binding profiles. The tool helps power insights into the regulation of genes through the study of the specific usage of TSSs within specific cell types and/or under specific conditions.   AVAILABILITY AND IMPLEMENTATION The CAGEd-oPOSUM web tool is implemented in Perl, MySQL and Apache and is available at http://cagedop.cmmt.ubc.ca/CAGEd_oPOSSUM CONTACTS: anthony.mathelier@ncmm.uio.no or wyeth@cmmt.ubc.ca   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;dna binding site;didelphidae;emergence;gene ontology term enrichment;histocompatibility testing;mammals;motif;mysql;perl;precomputation;refinement (computing);regulatory sequences, nucleic acid;transcription (software);transcription initiation site;transcription, genetic;cell type;transcription factor binding	David J. Arenillas;Alistair R. R. Forrest;Hideya Kawaji;Timo Lassmann;Wyeth W. Wasserman;Anthony Mathelier	2016		10.1093/bioinformatics/btw337	biology;molecular biology;bioinformatics;genetics	Comp.	-0.0624645857740671	-58.91429707521486	740
81a0304e275905c38d530e0699f2c2d2a039bbb4	methods, tools and standards for the analysis, evaluation and design of modern automotive architectures	synchronization policies;automotive engineering;analytical models;rtos;integrated systems;automotive engineering computer architecture delay time to market cost function safety timing jitter middleware analytical models performance analysis;timing automotive engineering formal verification software performance evaluation synchronisation;performance evaluation;cost function;integrable system;bridging fault simulation;software performance evaluation;function performance;function correctness;worst case static analysis;synchronisation;computer architecture;formal verification;resistive bridging faults;queuing policies;hw platform selection;safety;timing correctness;performance analysis;middleware;time to market;advance validation;system level timing behaviour;static analysis;timing jitter;worst case static analysis modern automotive architectures advance validation integrated systems function correctness function performance system level timing behaviour hw platform selection synchronization policies queuing policies middleware rtos performance evaluation timing correctness;modern automotive architectures;timing	Automotive systems are increasingly distributed and complex. Reduced time-to-market, cost and safety concerns require advance validation of the integrated systems and its components, from the functional, timing, and reliability standpoints. In particular, function correctness and performance may depend on communication and computation delays imposed by the selected architecture platform. Hence, the need for methods and tools capable of predicting the system-level timing behaviour (latencies and jitter), resulting from the HW platform selection, the synchronization between tasks and messages, and also from the synchronization and queuing policies of the middleware and RTOS levels. In this paper, we review methods and tools for the evaluation of the function performance and its timing correctness by simulation or by worst case static analysis.	best, worst and average case;computation;computer performance;correctness (computer science);middleware;simulation;static program analysis	E. Frank;Reinhard Wilhelm;Rolf Ernst;Alberto L. Sangiovanni-Vincentelli;Marco Di Natale	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403536	reliability engineering;embedded system;integrable system;synchronization;electronic engineering;real-time computing;real-time operating system;formal verification;computer science;operating system;middleware;static analysis	EDA	-7.7012402420064925	59.73422870634883	741
0256bfa7109a2b57f6fbb0449b2a16e11d15da28	bypass aodv: improving performance of ad hoc on-demand distance vector (aodv) routing protocol in wireless ad hoc networks	ad hoc networks;routing protocol;resource utilization;ad hoc network;wireless ad hoc network	Bypass-AODV, a local recovery protocol, is proposed to enhance the performance of AODV routing protocol by overcoming several inherited problems such as unnecessary error recovery invocations, newly non-optimal reconstructed routes, high packet drop ratios, and high routing overheads. Bypass-AODV uses cross-layer MAC-notification to identify mobility-related link break, and then setup a bypass between the broken-link end nodes via an alternative node while keeps on the rest of the route. Therefore, Bypass-AODV enhances resource utilization by avoiding unnecessary error recovery cycles and consequently increases the network throughput. On the other hand, Bypass-AODV enhances route reliability; it avoids dropping packets by transmitting them over the constructed bypass. The simulation results show that when running 1-TCP connection, Bypass-AODV performs better than AODV. In particular, this behavior is rapidly changed with increasing the physical distance between the TCP connection end nodes beyond 2 hops. For example, when number of hops is equal to 6, goodput is enhanced by more than 100% compared to AODV for a 1-TCP connection and about 24% for multiple TCP connections. Further, the ratio of packet drop is reduced from 16% to 2%. Moreover, considering the hop count, the Bypass-AODV shows less sensitivity to the ongoing number of TCP connections.	distance-vector routing protocol;goodput;hoc (programming language);network packet;simulation;throughput;transmitter	Ahed Alshanyour;Uthman A. Baroudi	2008			vehicular ad hoc network;wireless routing protocol;wireless ad hoc network;optimized link state routing protocol;adaptive quality of service multi-hop routing;mobile ad hoc network;dynamic source routing;ad hoc wireless distribution service;ad hoc on-demand distance vector routing;computer network	Mobile	2.4290802753757914	84.12386803070885	742
c4395598d990c1e05026da5558fcde7ca4a0d36f	emotishare: supporting emotion communication through ubiquitous technologies	mobile;human computer interaction;social networking;for 0806 information systems;emotion;affective computing	This research hypothesises that existing social networking systems do not adequately support emotion communication between members of a social group. To investigate this, a system (called Emotishare) was built to facilitate this type of communication and track the emotion sharing behaviours of participants when using the system.  Emotishare is a web and mobile platform that allows its users to track, share and respond to the emotional states of their friends. The system has been trialled with both large and small groups to explore the possibility for support of emotion communication using this type of service. It was found that small groups were most effective for supporting the sharing of emotional state and that mobile devices were best suited to support the sharing of personal information such as emotional state.	mobile device;personally identifiable information;type of service;world wide web	Matthew John Willis;Christian Martyn Jones	2012		10.1145/2414536.2414635	emotion;human–computer interaction;computer science;affective computing;multimedia	HCI	-57.643082825313286	-40.8449907827648	743
202ce9d38146d888538ec5c338175d9743452dd3	error bounds for dynamic responses in forced vibration problems	65l70;70j35;forced vibrations;73k12;lanczos method;matrix functions;mode superposition;dynamic response;error bounds;error bound;65f50;65f30;65f15	When using mode superposition in large applications, generally only relatively few approximate eigenmodes are linearly combined. Block Lanczos iteration is an efficient method of determining such modes. In this paper new a posteriori bounds are developed that estimate the error when approximating the exact result of mode superposition with a linear combination of the output vectors of block Lanczos iteration. Mode superposition can be regarded as a way of computing $g(S)f$, a function g of a selfadjoint matrix S applied to a vector. One formula is developed that estimates the norm of the unknown error vector. A second inequality gives a bound for the error when computing linear functionals (v, $g(S)f$) of the response. The error bounds require that f and possibly v are contained in the Lanczos starting block and that all Ritz vectors are used to compute the result. No gaps in the spectrum of S need to be known. The bounds can be evaluated at a small cost compared to the eigenpair extraction in large syste...		Christian Cabos	1994	SIAM J. Scientific Computing	10.1137/0915001	matrix function;mathematical optimization;combinatorics;mathematical analysis;mathematics;algebra	HPC	81.6513783523375	22.83087036044819	744
944e0c0440604a1cef880dcec37765352b601dc9	a new approach to improving the grooming performance with dynamic traffic in sonet rings	simulation ordinateur;teletrafic;dynamic change;reponse dynamique;evaluation performance;multiplexage longueur onde;respuesta dinamica;splitting;evolutionary computation;algorithm performance;splitting method;performance evaluation;traffic grooming;ring network;evaluacion prestacion;telecommunication network;calcul evolutionniste;algoritmo genetico;norme sonet;red fibra optica;minimizacion costo;teletrafico;telecomunicacion optica;telecommunication optique;minimisation cout;red anillo;cost minimization;resultado algoritmo;red telecomunicacion;reseau anneau;reseau fibre optique;dynamic response;reseau telecommunication;performance algorithme;teletraffic;algorithme genetique;optical telecommunication;genetic algorithm;optical fiber network;simulacion computadora;sonet wdm ring network;computer simulation;optical fiber communication;sonet;multiplaje longitud onda;communication fibre optique;evolutionary computing;wavelength division multiplexing	Traffic grooming is widely employed to minimize the number of ADM’s in today’s WDM rings to reduce the total cost of the network. Since traffic often changes over time, the problem of grooming dynamic traffic becomes more and more important. In this paper, we use the technique of splitting traffic to the grooming of arbitrary dynamically changed traffic in a strictly non-blocking way in SONET/WDM rings. Two splitting methods: traffic-cutting and trafficdividing are introduced, and the third method, dividing a traffic first and then cutting it, is also proposed. The grooming performance for static and dynamic traffic with these three methods is analyzed in detail. A genetic algorithm (GA) based on these methods is proposed. Computer simulation results under different conditions show that our algorithm is efficient in reducing both the numbers of ADM’s and wavelengths required. 2004 Elsevier B.V. All rights reserved.	blocking (computing);computer simulation;genetic algorithm;non-blocking algorithm;software release life cycle;synchronous optical networking;wavelength-division multiplexing	Kun-hong Liu;Yong Xu	2004	Computer Networks	10.1016/j.comnet.2004.03.031	ring network;simulation;genetic algorithm;traffic grooming;telecommunications;computer science;synchronous optical networking;splitting;telecommunications network;wavelength-division multiplexing;evolutionary computation	Metrics	-4.837377250040597	78.62609456156855	745
85d86721f586dc0e3040a5285709be659b35fd59	morris's garbage compaction algorithm restores reference counts	reference counting;storage management;garbage collection;garbage collector;shared space	The two-pass compaction algorithm of F.L. Morris, which follows upon the mark phase in a garbage collector, may be modified to recover reference counts for a hybrid storage management system. By counting the executions of two loops in that algorithm where upward and downward references, respectively, are forwarded to the relocation address of one node, we can initialize a count of active references and then update it but once. The reference count may share space with the mark bit in each node, but it may not share the additional space required in each pointer by Morris's algorithm, space which remains unused outside the garbage collector.	algorithm;data compaction;garbage collection (computer science);hierarchical storage management;pointer (computer programming);reference counting;relocation (computing)	David S. Wise	1979	ACM Trans. Program. Lang. Syst.	10.1145/357062.357070	garbage;parallel computing;real-time computing;ephemeron;computer hardware;computer science;garbage collection;programming language	PL	-11.952034209021319	53.560186899734035	746
90cbd6de1aba8b340f0cf722f975ac619b85c779	event mining and indexing in basketball video	content management;media weaving;motion statistics event mining event indexing baseketball video video program tv program video content management ontology description shot ontology shot manipulation shot detection shot type classification score board detection;image motion analysis;video signal processing;video signal processing content management data mining image classification image motion analysis indexing ontologies artificial intelligence sport;color;skin;media content management;shot ontology;games ontologies indexing films color image color analysis skin;image classification;user preferences;data mining;ontologies artificial intelligence;media weaving shot ontology media content management event mining indexing;indexing;image color analysis;games;indexation;event mining;content management system;ontologies;sport;films;color image	The video program has been produced by TV programs that were mass growing. How to recognize and to make sure the important events for indexing in video content management issues are become more and more important. In this paper, we developed a shot ontology description based for the basketball video. Shot ontology is inferred by shot manipulations those included: shot detection, shot type classification, score board detection and motion statistics. This video content management system provided event feature manipulations at multiple levels: signal, structural, or semantic in order to meet user preferences while striking the overall utility of the video. The experiment results showed that our proposed methodologies could correctly detect interested events, long shots, and close-up shots and also achieved the purpose of video indexing and weaving for what user preferences.	content management system;digital video;shot transition detection;user (computing)	Yung-Hui Chen;Lawrence Y. Deng	2011	2011 Fifth International Conference on Genetic and Evolutionary Computing	10.1109/ICGEC.2011.98	computer vision;color image;content management;computer science;video quality;sport;video tracking;multimedia;skin;world wide web	Vision	-13.679504958506701	-55.564602634825135	747
68b419ee2d7beba384eccf51338e5241d87835ea	the multiagent stress analysis and design of rectangular tube forming	stress;spline;shapes structures;forming processes;roller;rolling;agent;rectangular tube;stress analysis;structural engineering computing;cad cam;agent rectangular tube roller caliber;structural engineering computing cad cam forming processes pipes rolling shapes structures;electron tubes strain stress strips spline computer simulation;strips;pipes;electron tubes;strain;computer simulation;caliber;agent stressing model multiagent stress analysis rectangular tube forming fitness equipment skew rolling forming process schematic diagram caliber design multiagent technology stress situation	Analyses the characteristics of fitness equipment with rectangular tubes, which use the first round and the rear square and skew rolLing forming process according to its different ratio of long and wide, given the sChematic diagram of forming process and the basic calculation method of caLiber design, using the multiAgent technology analyzing the stress situation of skew rolLing process of the rectangular tubes, then estabLishing corresponding Agent stressing model. In determining the basis of forming passes, determine the deformation amount and corner quantity per passes, and reaLized the forming process of virtual in computer.	diagram;stress–strain analysis	Guochang Li	2011	2011 Second International Conference on Digital Manufacturing & Automation	10.1109/ICDMA.2011.320	structural engineering;engineering;engineering drawing;mechanical engineering	EDA	83.80063198311586	-15.851920829167543	748
49068be3a2f289b1a77d1a49a6f7898d1eb510ee	navi: neighbor-aware virtual infrastructure for information collection and dissemination in vehicular networks	telecommunication network reliability information dissemination mobile radio;navi short range network vehicle selection message penetration vehicular mobility trace multiple communication technology communication technology variable penetration rate data dissemination reliability information exchange vehicular network information dissemination information collection neighbor aware virtual infrastructure;vehicles peer to peer computing indexes measurement communication networks vehicle dynamics data collection	Vehicular Networks enable a vast number of innovative applications, which rely on the efficient exchange of information between vehicles. However, efficient and reliable data dissemination is a particularly challenging task in the context of vehicular networks due to the underlying properties of these networks, limited availability of network infrastructure and variable penetration rates for distinct communication technologies. This paper presents a novel system and mechanism for information collection and dissemination based on virtual infrastructure selection in combination with multiple communication technologies. The system has been evaluated using a simulation framework, involving network simulation in conjugation with realistic vehicular mobility traces. Simulation results show the feasibility of the proposed mechanism to achieve maximum message penetration in a geographical area with reduced overhead. The judicious vehicle selection also enables scalable data collection and leads to improved network utilization through the offload of traffic to the short-range communication network.	algorithm;geographic coordinate system;limited availability;overhead (computing);quaternions and spatial rotation;rate of convergence;replay attack;requirement;scalability;semantic network;semiconductor industry;simulation;telecommunications network;tracing (software)	Pedro M. d'Orey;Nitin Maslekar;Idoia De-la-Iglesia;Nikola K. Zahariev	2015	2015 IEEE 81st Vehicular Technology Conference (VTC Spring)	10.1109/VTCSpring.2015.7145945	computer science;distributed computing;vehicular communication systems;computer security;computer network	HPC	2.8606111229578954	88.0397167626129	749
4e665d86ed3647c2af318b7c8b60fa2ac3eab71a	context-enhanced video understanding	extraction information;analisis contenido;audiovisual;multimedia;information extraction;sensors;deteccion;detector;detecteur;detection;content analysis;indexing;audiovisuel;comprension;contexto;indexation;indizacion;contexte;speech recognition;analyse contenu;video;extraccion informacion;context;comprehension	Many recent efforts have been made to automatically index multimedia content with the aim of bridging the semantic gap between syntax and semantics. In this paper, we propose a novel framework to automatically index video using context for video understanding. First we discuss the notion of context and how it relates to video understanding. Then we present the framework we are constructing, which is modeled as an expert system that uses a rule-based engine, domain knowledge, visual detectors (for objects and scenes), and different data sources available with the video (metadata, text from automatic speech recognition, etc.). We also describe our approach to align text from speech recognition and video segments, and present experiments using a simple implementation of our framework. Our experiments show that context can be used to improve the performance of visual detectors.	align (company);bridging (networking);experiment;expert system;feature extraction;knowledge base;logic programming;sensor;speech recognition;text retrieval conference	Alejandro Jaimes;Milind R. Naphade;Harriet J. Nock;John R. Smith;Belle L. Tseng	2003		10.1117/12.479745	speech recognition;computer science;video tracking;multimedia;world wide web	Vision	-14.855023539901707	-57.61619888641007	750
e22c453b00df5a0255df48b3cc0d6ea89675e11e	building semantic causal models to predict treatment adherence for tuberculosis patients in sub-saharan africa		Poor adherence to prescribed treatment is a major factor contributing to tuberculosis patients developing drug resistance and failing treatment. Treatment adherence behaviour is influenced by diverse personal, cultural and socio-economic factors that vary between regions and communities. Decision network models can potentially be used to predict treatment adherence behaviour. However, determining the network structure (identifying the factors and their causal relations) and the conditional probabilities is a challenging task. To resolve the former we developed an ontology supported by current scientific literature to categorise and clarify the similarity and granularity of factors.	categorization;causal filter;causality;failure;influence diagram;scientific literature	Olukunle A. Ogundele;Deshendran Moodley;Christopher John Seebregts;Anban W. Pillay	2014		10.1007/978-3-319-63194-3_6	cognitive psychology;causal model;tuberculosis;scientific literature;communication;ontology;adherence behaviour;medicine	ML	-62.402907928427865	-57.74700115943513	751
f28de988c81fadd610d83392dc6476dc581799d7	applying situation awareness approach to cooperative play in interactive installation storytelling system	immersive situation;user interface;peer learning;virtual reality;cooperative play;interactive installation storytelling system;immersive environment;computer vision;digital content;situation awareness;role play;physical user interface	Owing to its character of virtual reality situation, Interactive Installation Storytelling System with Cooperative Play (IISSCP) provides children an immersive environment and situation awareness (SA) to interact with digital content more directly, intuitively, psychically and mentally. SA improves the interactivity between children and digital content. Through the interaction of play, children participate in learning willingly, give feedback quickly and enjoy peer learning. By integrating virtual computer vision applications with stage props as physical user interface, this system aims to create a natural immersive situation to attain four setting goals which are memory training, imagination of shape, music creation and training of physical strength and reaction. The target users are children aged 8 to 12. However, the cognitive differences of peers cause some problems during cooperative play. Based on findings from this research, we discuss the problems causing cooperation failure during play which are: (1) the cognitive differences when using icons of user interface corresponding to physical user interface, (2) the situation design affects role play during integrating the virtual and physical user interface into a mental gestalt and (3) the interaction complexity and difficulty affect children ability of situation awareness.		Wen-Hwa Cheng;Chich-Jen Shieh;Yung-Hoh Sheu;Hsiao-Chuan Chang	2010		10.1007/978-3-642-16066-0_5	situation awareness;simulation;human–computer interaction;computer science;engineering;virtual reality;multimedia;user interface	HCI	-55.34403737761004	-48.159936320060716	752
7c252946b444f5df0d7de7d6104e437f1df5f682	emits: an experience management system for it management support	decision support;experience management;clustering;optimization;it management	This research focuses on the identification of relevant experience required for solving IT (Information Technology) problems in small- to medium-sized enterprises. To achieve this, we integrated information retrieval techniques with clustering and optimization techniques to design and develop a custom-built Experience Management system for IT management support. We have built and evaluated our system on three different publicly available data sets: Princeton, Parallels and GoDaddy. Results support that it is possible to provide the right mix of automation and manual activity for IT experience management while achieving a high accuracy.	management system	Can Bozdogan;A. Nur Zincir-Heywood;Ibrahim Zincir	2015	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194015400197	information technology management;decision support system;data management;computer science;engineering;knowledge management;machine learning;data mining;management science;structure of management information;cluster analysis;management	SE	-67.85579033056723	9.791565313107624	753
9b750721ceb10c4480db63dde1a3c4b2b4b6ce54	integrating functional and architectural views of reactive systems	industrial case study;integrable model;design and implementation;requirement engineering;case tool;reactive system;integral functional	An integrated model-based development approach has to capture the relationship between requirements, design, and implementation models. In the requirements engineering phase, the most important view is the functional one, which specifies functionalities offered by the system and relationships between them. In the design phase, the component-based view describes the system as a network of interacting components. Via their interaction, they have to realize the black-box behavior specified in the functional view. To ensure the consistency between both views, a formal integration of them is necessary. The presented formal framework captures both functionand component-based models. In particular, we provide a correct-by-construction procedure, which transforms a functional specification into a component-based architecture. Applicability of the method is evaluated on an industrial case study in a CASE tool.	automata theory;black box;closing (morphology);component-based software engineering;computer architecture;computer-aided software engineering;functional programming;functional specification;interaction;microsoft outlook for mac;model transformation;model-driven engineering;requirement;requirements engineering;service-oriented architecture;simulation	Jewgenij Botaschanjan;Alexander Harhurin	2009		10.1007/978-3-642-02414-6_10	control engineering;reliability engineering;reactive system;computer science;systems engineering;functional specification;requirements engineering;functional requirement;non-functional requirement	SE	-50.20235121234185	26.77152575677694	754
32349276c09d1e434db9b8f1976afb98a43c49c4	solving engineering optimization problems with the simple constrained particle swarm optimizer	constrained optimization;engineering design;objective function;optimization problem;particle swarm optimizer;particle swarm optimization algorithm	This paper introduces a particle swarm optimization algorithm to solve constrained engineering optimization problems. The proposed approach uses a relatively simple method to handle constraints and a different mechanism to update the velocity and position of each particle. The algorithm is validated using four standard engineering design problems reported in the specialized literature and it is compared with respect to algorithms representative of the state-of-the-art in the area. Our results indicate that the proposed scheme is a promising alternative to solve this sort of problems because it obtains good results with a low number of objective function evaluations.	algorithm;engineering design process;mathematical optimization;optimization problem;particle swarm optimization;velocity (software development)	Leticia C. Cagnina;Susana C. Esquivel;Carlos A. Coello Coello	2008	Informatica (Slovenia)		optimization problem;mathematical optimization;multi-swarm optimization;constrained optimization;meta-optimization;derivative-free optimization;particle swarm optimization;metaheuristic	AI	26.211510550829765	-3.374462956827051	755
38328ac13b9326fb891f79d06e7f8d49be76d1fb	floor-cleaning robot using omni-directional wheels	gestion energia;air bag;limpieza;esquiva colision;educational robotics;motion control;control technology;sintesis control;vehiculo omnidirecional;cinematica;locomotion;roue;robotics;rueda;kinematics;remote operation;vehicule omnidirectionnel;commande mouvement;control movimiento;research purpose;gestion energie;design method;nettoyage;teleaccion;synthese commande;cinematique;power management;coussin securite;robotica;collision avoidance;robotique;esquive collision;omnidirectional vehicle;wheel;locomocion;control synthesis;cleaning;energy management;teleoperation;design methodology	Purpose – The purpose of this paper is to present an omni‐directional floor‐cleaning robot equipped with four omni‐directional wheels. The research purposes are to design a robot for cleaning jobs in domestic, narrow and crowded places and to provide a robotics‐study platform in a laboratory.Design/methodology/approach – The robot system using Swedish wheels, one dust collector (brush) switching device and a sort of air‐bag sensing device is designed. The kinematics and the motion control conditions of the robot are analyzed. Specifically, a design method of wheels is described.Findings – The configuration of the robot, parameters of the wheel and controlling methods are studied and demonstrated. The smooth locomotion capability and high‐working efficiency are verified by experiments.Practical implications – The robot can perform its work in semi‐autonomous and tele‐operated mode. Moreover, the robot can pivot around, avoid obstacles and is provided with automatic power management system.Originality/value...	robot;sputter cleaning;wheels	Xueshan Gao;Yan Wang;Dawei Zhou;Koki Kikuchi	2009	Industrial Robot	10.1108/01439910910932612	control engineering;mobile robot;embedded system;bang-bang robot;robot end effector;cartesian coordinate robot;simulation;articulated robot;design methods;computer science;engineering;artificial intelligence;robot control;robotics;robot calibration	Robotics	76.63265668345696	-21.082715906435727	756
ef0b4b1125db731694f6d661953637711999dac7	presynaptic learning and memory with a persistent firing neuron and a habituating synapse: a model of short term persistent habituation	short term memory;synaptic models;habituation;persistent neural activity	Our paper explores the interaction of persistent firing axonal and presynaptic processes in the generation of short term memory for habituation. We first propose a model of a sensory neuron whose axon is able to switch between passive conduction and persistent firing states, thereby triggering short term retention to the stimulus. Then we propose a model of a habituating synapse and explore all nine of the behavioral characteristics of short term habituation in a two neuron circuit. We couple the persistent firing neuron to the habituation synapse and investigate the behavior of short term retention of habituating response. Simulations show that, depending on the amount of synaptic resources, persistent firing either results in continued habituation or maintains the response, both leading to longer recovery times. The effectiveness of the model as an element in a bio-inspired memory system is discussed.	aclarubicin;afferent neuron;artificial neural network;axon;british informatics olympiad;cns disorder;cascade device component;causality;computer simulation;consciousness;drug habituation;information processing;inspiration function;mathematical model;mathematics;neural network simulation;persistence (computer science);spiking neural network;synapse;synaptic package manager;weight	Kiruthika Ramanathan;Ning Ning;Dhiviya Dhanasekar;Guoqi Li;Luping Shi;Prahlad Vadakkepat	2012	International journal of neural systems	10.1142/S0129065712500153	short-term memory;habituation	ML	18.1972131618374	-71.29884389930982	757
166e5662c083925f544d52c9b1e9c3fd6e310120	asynchronous adaptive task allocation	randomized algorithms;distributed computing;asynchronous shared memory distributed computing write all randomized algorithms task allocation;asynchronous shared memory;resource management complexity theory indexes program processors computer crashes additives computational modeling;shared memory space asynchronous adaptive task allocation randomized algorithm atomic test and set operations internal memory space write all problem distributed computing;shared memory systems computational complexity;write all;task allocation	We present a randomized algorithm for asynchronous task allocation, also known as the write-all or do-all problem. Our algorithm has work complexity O(n+k2 log3 k) with high probability, where n the number of tasks and k the number of processes that participate in the computation. Our solution uses O(n) shared memory space that supports atomic test-and-set operations and with high probability each participating process uses O(k) internal memory space. This is the first adaptive solution for the write-all problem that has work n plus some additive term which depends only on the number of participating processes k and not the size of the problem n.	computation;computer data storage;dspace;randomized algorithm;shared memory;test-and-set;utility functions on indivisible goods;with high probability	Sotiris Kentros;Chadi Kari;Aggelos Kiayias;Alexander Russell	2015	2015 IEEE 35th International Conference on Distributed Computing Systems	10.1109/ICDCS.2015.17	distributed shared memory;shared memory;interleaved memory;parallel computing;distributed memory;computer science;theoretical computer science;static memory allocation;distributed computing;randomized algorithm	Theory	-15.089743749652342	57.925615713881996	758
5c6e45bfef47f87fdddeca5cded4ca04f37d07b6	bautin ideal of a cubic map	analytic functions;fonction analytique;cycle limite;order 3;third order;polynomials;limit cycle;limit cycles;bautin;polynome;ideal	We compute the radical of the ideal generated by the first three focus quantities of maps defined by irreducible branches of a cubic curve on the real plane. It is shown that the ideal is not radical in this case.	cubic function	I. Benediktovitch;Valery G. Romanovski	2001	Appl. Math. Lett.	10.1016/S0893-9659(00)00129-4	ideal;combinatorics;mathematical analysis;analytic function;mathematics;geometry;limit cycle;polynomial;algebra	NLP	42.68027996797035	32.96165280828453	759
562e11dbb462aa86e5c0e8ecbf9aa8e0af35a463	hidden markov models for wavelet image separation and denoising	hidden markov tree;gaussian mixture;image segmentation;hidden markov model;blind source separation;bayes methods;hidden markov fields;prior distribution;hidden markov models noise reduction source separation wavelet domain bayesian methods wavelet coefficients covariance matrix independent component analysis context modeling vectors;wavelet transforms;interference suppression;hidden markov models;hidden markov models wavelet transforms interference suppression image denoising image segmentation bayes methods blind source separation gaussian distribution;image denoising;source separation;gaussian distribution;chmf model hidden markov models wavelet image separation image denoising blind source separation 2d images bayesian formulation bayes bss wavelet domain prior distributions wavelet coefficients unobservable sources independent gaussians mixture model hidden markov tree model contextual hidden markov field model igm model hmt model	In this paper, we consider the problem of blind source separation of 2D images under a Bayesian formulation (Bayes-BSS). We transport the problem to the wavelet domain to be able to define appropriate prior distributions for the wavelet coefficients of the unobservable sources: an independent Gaussians mixture (IGM) model, a hidden Markov tree (HMT) model and contextual hidden Markov field (CHMF) model. Indeed, we consider a limiting case of the aforementioned prior models to propose a simple procedure for joint source separation and denoising. This procedure shows to be efficient, especially for highly noisy observations. Simulation examples and comparisons with standard classical methods are presented to show the performances of the proposed approach.	blind signal separation;coefficient;hidden markov model;markov chain;markov random field;noise reduction;performance;simulation;source separation;wavelet	Mahieddine Ichir;Ali Mohammad-Djafari	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416281	normal distribution;prior probability;computer science;machine learning;hidden semi-markov model;pattern recognition;mathematics;blind signal separation;image segmentation;markov model;hidden markov model;statistics;wavelet transform	ML	61.00590202042225	-70.84348983304935	760
69674e8df5e12639a989662c805018feed00d171	randomness testing and comparison of classical and quantum bit generators		Randomness is crucial to enabling secure and robust communications. Ideally one should harness high entropy physical processes, but this is difficult so pseudorandomness is usually substituted for randomness. We introduce improved complexity randomness tests and use them to judge three pseudorandom bit generators; the AES block cipher (standard, strongly believed to be secure), the Dragon stream cipher (eStream finalist), and the GNU C library function rand(). We also test the output from a quantum random bit generator (QRBG). While the two ciphers can easily be distinguished from the much inferior rand(), the output statistics of the two classical generators are similar to that of the QRBG, and both provide high-quality pseudorandom bits.	block cipher;estream;gnu c library;pseudorandom number generator;pseudorandomness;qubit;randomness tests;stream cipher	Serdar Boztas;Benjamin A. Burton	2017	2017 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2017.8024660	randomness;computer science;block size;block cipher;pseudorandom number generator;randomness tests;algorithm;pseudorandomness;estream;stream cipher	Crypto	-36.20008734049209	80.16930593828076	761
76c161fdfacc71aa9c5c48d5829658e7dd39d2ee	cp2: cryptographic privacy protection framework for online social networks	efficient privacy protection framework;osn provider;online social network;user privacy;personal information;cryptographic privacy protection framework;online social networks;users interact;information sharing;full control;complexity evaluation;recommended framework shift	efficient privacy protection framework;osn provider;online social network;user privacy;personal information;cryptographic privacy protection framework;online social networks;users interact;information sharing;full control;complexity evaluation;recommended framework shift	cryptography;privacy;social network	Fatemeh Raji;Ali Miri;Mohammad Davarpanah Jazi	2013	Computers & Electrical Engineering	10.1016/j.compeleceng.2012.09.002	privacy software;information privacy;privacy by design;internet privacy;world wide web;computer security;computer network	Security	-44.915680420151936	61.577821128766054	762
a2f5bc9bab0f3cf9a6be7ed2385c437b5c526193	sparse least-squares methods in the parallel machine learning (pml) framework	regularized least squares;least squares approximations;sparse least squares methods;high dimensionality;parallel machine learning framework;training;parallel methods;ibm blue gene p parallel computer;least square method;data mining;empirical squared error term;symmetric matrices;training data;accuracy;large scale;machine learning;least squares problem;loss function;fast regression technique;lenclos based methods;parallel computer;parallel machines;regression analysis;parallel implementation;document classification;learning artificial intelligence;parallel machines machine learning cloud computing data mining decision trees costs machine learning algorithms clustering algorithms computer networks data processing;ibm blue gene p parallel computer parallel machine learning framework sparse least squares methods parallel methods document classification fast regression technique empirical squared error term lenclos based methods;sparse matrices;regression analysis learning artificial intelligence least squares approximations microprocessor chips parallel machines;microprocessor chips	We describe parallel methods for solving large-scale, high-dimensional, sparse least-squares problems that arise in machine learning applications such as document classification. The basic idea is to solve a two-class response problem using a fast regression technique based on minimizing a loss function, which consists of an empirical squared-error term, and one or more regularization terms. We consider the use of Lenclos-based methods for solving these regularized least-squares problems, with the parallel implementation in the Parallel MachineLearning (PML) framework, and performance results on the IBM Blue Gene/P parallel computer.	blue gene;computation;computational model;computer;distributed memory;document classification;least squares;locality of reference;loss function;machine learning;multi-core processor;parallel algorithm;parallel computing;scalability;shared memory;sparse matrix;speedup;test set;thread (computing);time complexity	Ramesh Natarajan;Vikas Sindhwani;Shirish Tatikonda	2009	2009 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2009.106	training set;sparse matrix;computer science;theoretical computer science;machine learning;pattern recognition;accuracy and precision;least squares;regression analysis;symmetric matrix;loss function	HPC	19.397262992949404	-37.181668691081285	763
0beeb77c16e7866a23991f3718b25312af55d3e9	deeprank: a new deep architecture for relevance ranking in information retrieval		This paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance. According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected; 2) local relevances are determined; 3) local relevances are aggregated to output the relevance label. In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU). Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score. DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can significantly outperform learning to ranking methods, and existing deep learning methods.	artificial neural network;benchmark (computing);convolutional neural network;deep learning;experiment;heuristic (computer science);information retrieval;learning to rank;random neural network;relevance;semantic matching;simulation	Liang Pang;Yanyan Lan;Jiafeng Guo;Jun Xu;Jingfang Xu;Xueqi Cheng	2017		10.1145/3132847.3132914	information retrieval;ranking (information retrieval);human–computer information retrieval;concept search;relevance (information retrieval);learning to rank;data mining;adversarial information retrieval;deep learning;computer science;okapi bm25;pattern recognition;artificial intelligence	Web+IR	-17.86164305859673	-67.91796628756727	764
c42cb0aac46f03999f31d1777e0ee17844dafe37	gsm refarming analysis based on orthogonal sub channel and interference optimization	multimedia communications;gsm multiplexing mobile communication interference dynamic scheduling radio spectrum management channel allocation;umts;3g networks;lte;interference optimization;multimedia communication 3g mobile communication cellular radio long term evolution;cellular radio;itu r defined fourth generation;frequency 900 mhz;long term evolution;interference;multiplexing;saic;orthogonal sub channel;gsm refarming analysis;3g mobile communication;dynamic frequency and channel allocation;data services;multimedia communication;frequency 900 mhz gsm refarming analysis orthogonal sub channel interference optimization mobile communication 3g networks long term evolution lte itu r defined fourth generation multimedia communications voice services data services dynamic frequency and channel allocation umts;voice services;mobile communication;radio spectrum management;voice evolution;gsm;channel allocation;dhr;voice evolution gsm umts dhr orthogonal sub channel dynamic frequency and channel allocation saic;dynamic scheduling	The current mobile communication environment is advancing rapidly with 3G networks' data rates growing and LTE representing already the preliminary version of the ITU-R-defined fourth generation. While the 3G and LTE development provides a new era for the multimedia communications with considerably higher bit rates and lower latencies, the legacy systems as 2G are still important as their market share remains high. Regardless of the relatively low spectral efficiency of GSM compared to the new access technologies, both emerging and mature markets benefit from it. 2G still provides with a sufficiently functional platform for the basic voice and data services; moreover, 2G terminal penetration figure remains high, and the radio coverage is typically wider than of more modern networks being deployed in higher frequencies. Furthermore, with the new functionalities of 2G, such as Orthogonal Sub Channel (OSC) and Dynamic Frequency and Channel Allocation (DFCA), the gradual transition towards the new access technologies is granted. This paper investigates the effect of these functionalities on the refarming of GSM frequencies for UMTS/LTE deployment. The study concentrates on urban scenarios and 900 MHz frequency band.	compaq lte;frequency band;interference (communication);legacy system;mathematical optimization;software deployment;spectral efficiency	Sebastian Lasek;Dariusz Tomeczko;Jyrki T. J. Penttinen;David Valerdi;Inigo Guemes	2012	2012 8th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP)	10.1109/CSNDSP.2012.6292715	gsm;mobile telephony;telecommunications;dynamic priority scheduling;computer science;lte advanced;operating system;interference;data as a service;umts frequency bands;multiplexing;computer network	Arch	24.34730109219358	86.80934098317512	765
f2f4084973fa4d44ceaab03b7b7d1925219f4061	ambienttalk: modern actors for modern networks	mobile networks;event loops;futures;actors;peer to peer	We introduce AmbientTalk, a dynamic, object-oriented, distributed programming language. AmbientTalk focuses primarily on the domain of mobile peer-to-peer applications, which are becoming more and more widespread in the wake of smartphone platforms such as iOS and Android. AmbientTalk runs on Android, and can be thought of as a scripting language for that platform.  We will discuss AmbientTalk's roots and devote special attention to its concurrent and distributed language features, which are founded on the actor model. Next, we will showcase the language by explaining a peer-to-peer application. We show how resilient mobile applications can be constructed with familiar building blocks like objects and messages, but also in what way these building blocks have to be adapted to fit the characteristics of mobile networks.  For the past six years, we have been using AmbientTalk as a research and teaching platform at the Software Languages Lab of the University of Brussels. We will close the talk by relating our experiences in designing and using the language.	actor model;ambienttalk;android;distributed computing;mobile app;peer-to-peer;programming language;scripting language;smartphone;ios	Tom Van Cutsem	2012		10.1145/2318202.2318204	simulation;computer science;distributed computing;communication	PL	-36.14532102911732	40.64697447192785	766
b6f6c48753cde30b00d80f05f7a2909c80443442	cromsci: development of a climbing robot with negative pressure adhesion for inspections	universitaet;embedded transducer;biologisch;robot movil;rrlab;karsten berns;remote control;multiagent system;chambre a vide;robot design;ensayo no destructivo;dam;soupape;non destructive inspection;seminar;essai non destructif;autonomous system;valve;reproductibilite;roboter;bridges;captador presion;robotik;roue;robotics;modele physique;rueda;working conditions;studium;comportement grimpeur;praktikum;inspection;remote operation;sistema autonomo;adhesion;autonomic system;pressure sensor;vacuum chamber;forschung;barrage;maniobrabilidad;robot mobile;non destructive test;transducteur enfoui;informatik;teleaccion;bien condicionado;agrosy;puente;adherence;valvula;systeme autonome;reproductividad;well conditioned;pont;maneuverability;cost efficiency;modelo fisico;robotica;lehre;construction materials;adherencia;camara de vacio;robotique;bien conditionne;transductor embebido;computer science;physical model;autonom;presa;kaiserslautern;climbing behavior;vorlesung;sistema multiagente;capteur pression;wheel;comportamiento trepador;robot;manoeuvrabilite;moving robot;systeme multiagent;teleoperation;reproducibility;design methodology	The non-destructive inspection of large concrete walls with autonomous systems is still an unsolved problem. One of the main difficulties is to develop a very flexible platform, which is able to move and inspect horizontal and vertical surfaces safely, fast and cost-efficient. This report will present the climbing robot Cromsci which is designed for the described task. The propulsion system consists of three omnidirectional driven wheels which are airproof and completely rotatable for a maximum of maneuverability. To detect critical situations each wheel is equipped with a load cell which can measure occuring forces and torques and allow force-balancing. The adhesion is done by a vacuum system of seven controllable vacuum chambers and one large reservoir chamber. Pressure sensors and valves are integrated for controlling which allows fast reaction on changing conditions. The rough and sharp-edged surface of concrete walls causes strong requirements concerning leak tightness and attrition to the sealing between vacuum chambers and walls. Therefore, each sealing must be flexible to allow a good adaption to the ground but also let the robot slip when it is moving.	attrition (website);autonomous system (internet);cost efficiency;critical point (network science);experiment;memory leak;prototype;record sealing;requirement;robot;sensor;simulation;wheels	Carsten Hillenbrand;Daniel Schmidt;Karsten Berns	2008	Industrial Robot	10.1108/01439910810868552	simulation;nondestructive testing;computer science;engineering;artificial intelligence;robotics	Robotics	77.34678620666168	-20.82994094423065	767
913c280e40a8e305a3a41acf485107afd35f0be1	lexicon management tools for large textual databases: the lexinet system	traitement automatise;terminologie;software;mise a jour;terminologia;procesamiento informacion;logiciel;combinatorial algorithm;indexation automatique;lexinet;lexicons;artificial intelligent;texto completo;texte integral;tratamiento automatizado;information processing;automatic indexing;logicial;terminology;management tool;puesta al dia;full text;lexico;traitement information;updating;indizacion automatica;automated processing;lexique	This paper deals with the problem of lexicon creation and update for large textual databases. The problem is a particularly difficult one for research fields where the terminology evolves rapidly. The Lexinet system offers a solution to this problem. It provides a set of statistical and combinatorial algorithms to detect significant items in a corpus of documents from any domain. The results reported concern a corpus of 2,380 titles and abstracts from papers on artificial intelligence, extracted from the French national power company’s database. A methodology is descnbed to compare existing terminology with the terminology detected by the Lexinet system.		Ghislaine Chartron	1989	J. Information Science	10.1177/016555158901500605	natural language processing;information processing;computer science;artificial intelligence;linguistics;terminology;algorithm	AI	-35.967772768447595	-64.4079211733702	768
29eaf7a5a2454728b0ca718a45226d81115d5ea0	improved fingercode matching function	fingercode matching;image matching;fingerprint recognition gabor filters image matching fingers pixel data mining filter bank frequency iris recognition euclidean distance;gabor filters;matching function;texture features;image texture;directional texture feature vector;fingerprint correlation matching;feature vector;gabor filter;spatial distribution;feature extraction;directional texture feature vector fingercode matching fingerprint correlation matching texture information gabor filter;texture information;image texture feature extraction fingerprint identification gabor filters image matching;fingerprint identification	FingerCode is a fingerprint correlation matching scheme that relies on texture information. In this scheme, the oriented components are extracted from a fingerprint image using a bank of Gabor filters, and a directional texture feature vector is computed for each oriented component. The feature vectors from the input and template images are compared and a matching score is obtained. Here, we explore ways to improve the matching score for the FingerCode method by using more complex matching functions. The best results were obtained by applying a nonlinear function to the texture values and weighting the texture vectors based on the spatial distribution	feature vector;fingerprint;gabor filter;nonlinear system	Gustavo de Sa;Roberto de Alencar Lotufo	2006	2006 19th Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2006.25	computer vision;machine learning;pattern recognition	Vision	35.405016480333586	-61.06268118457213	769
442fc39e9a7cd3b390628f6eaabf410d85cba086	patent citation indicators: one size fits all?	patent citations;multivariate analysis;62h20;patent family;62h30;62h25;pct;uspto;epo	The number of citations that a patent receives is considered an important indicator of the quality and impact of the patent. However, a variety of methods and data sources can be used to calculate this measure. This paper evaluates similarities between citation indicators that differ in terms of (a) the patent office where the focal patent application is filed; (b) whether citations from offices other than that of the application office are considered; and (c) whether the presence of patent families is taken into account. We analyze the correlations between these different indicators and the overlap between patents identified as highly cited by the various measures. Our findings reveal that the citation indicators obtained differ substantially. Favoring one way of calculating a citation indicator over another has non-trivial consequences and, hence, should be given explicit consideration. Correcting for patent families, especially when using a broader definition (INPADOC), provides the most uniform results.	emoticon;fits;focal (programming language);peer-to-patent	Jurriën Bakker;Dennis Verhoeven;Lin Zhang;Bart Van Looy	2015	Scientometrics	10.1007/s11192-015-1786-0	computer science;data science;data mining;multivariate analysis;statistics	AI	-78.07299758431346	-21.773274025352855	770
48aa55b8fc6ad56bc438f35e87594044b3739ac4	vulnerability modeling and simulation for dns intrusion tolerance system construction	dns;modeling and simulation;vulnerability analysis;computer network;fault tolerant system;devs formalism;intrusion tolerance;atomic vulnerability	To construct the ITS(Intrusion Tolerance System), we should concern not only the FTS(Fault Tolerant System) requirements but also intrusion and vulnerability factors. But, in the ITS, we can not take into account the intrusion and vulnerability as they are, because the characteristics and pattern of them is unknown. So, we suggest vulnerability analysis method that enable ITS to know the pattern of vulnerability exploitation more specifically. We make use of the atomic vulnerability concept to analyze the vulnerability in DNS system, and show how to make use of the analysis result as monitoring factors in our DNS ITS system. Also, this analysis result is used in modeling and simulation to see the dynamics of computer network for vulnerability and external malicious attack. This paper shows simulation execution examples making use of the vulnerability analysis result.	intrusion tolerance;simulation	Hyung-Jong Kim	2004		10.1007/978-3-540-30583-5_9	vulnerability management;intrusion tolerance;fault tolerance;computer science;vulnerability assessment;modeling and simulation;distributed computing;computer security;domain name system	Logic	-62.348381161290845	60.48384947037934	771
1fc094959a015b897402f7f5fe5a4748cc2ebf5c	correspondence: response to botting's comments	writing;object oriented programming;error correction	The author indicates some things that need clarifying in the paper cited in the title. The paper does not make it clear that a class of objects is not equivalent to a set of tuples. In a set of tuples two different tuples cannot contain the same data. Two different objects in a given class can contain the same data. In most of the paper this does not matter. However, the simulation function between a subclass and a superclass must be injective, but the (overloaded) simulation function between the sets of tuples usually cannot be injective		Robert H. Bourdeau;Betty H. C. Cheng	1996	IEEE Trans. Software Eng.	10.1109/TSE.1996.553640		SE	-5.854074968268565	13.373011332543781	772
1fc3b0cd2135184cc74e1e3bd814ed3071d2280c	developing a web-based pattern recognition system for the pattern search of components database by a parallel computing	database system;engineering graphics;pattern search;real time;automation electrical engineering company web based pattern recognition system pattern search components database parallel computing associative memory real time pattern recognition engineering component recognition parallel computing system component patterns internet client server network recurrent neural network rnn database matching;parallel programming;parallel programming engineering information systems engineering graphics pattern recognition visual databases recurrent neural nets content addressable storage internet;client server;internet;engineering information systems;parallel computer;pattern recognition;associative memory;pattern recognition parallel processing data engineering associative memory database systems recurrent neural networks real time systems shape internet pattern matching;recurrent neural nets;recurrent neural network;network structure;electrical engineering;content addressable storage;visual databases	The research investigates the use of pattern recognition (PR) technologies with associative memory for real-time pattern recognition of engineering components by a parallel computing system. Component patterns are stored in the database system. Their properties and specifications are also attached to the data field of each component pattern except the pattern of engineering component. A distant engineer is able to draw directly the shape of engineering components via the browser, and the recognition system will search for the component database of the company via the Internet. Component patterns with the matching approach of database system will be able to improve the capacity of the recognition system effectively. The recognition system makes use of parallel computing, and it raises the recognition rate of the system. The recognition system is a client-server network structure using the Internet. The system uses a recurrent neural network (RNN) with associative memory to perform training and recognition. The system is being implemented in the Automation Electrical Engineering Company.	artificial neural network;automation;client–server model;content-addressable memory;database;electrical engineering;internet;parallel computing;pattern recognition;pattern search (optimization);random neural network;real-time clock;recurrent neural network;server (computing)	Kuo-Chin Fan;Sung-Jung Hsiao;Wen-Tsai Sung	2003	Eleventh Euromicro Conference on Parallel, Distributed and Network-Based Processing, 2003. Proceedings.	10.1109/EMPDP.2003.1183625	pattern search;parallel computing;the internet;feature;intelligent character recognition;computer science;recurrent neural network;theoretical computer science;machine learning;data mining;client–server model	Robotics	12.341364153972346	-25.76154498067983	773
5a1fc4283d9317d7573b7c308b8155d67d82d330	how may i help you?	llamada telefonica;lenguaje natural;linguistic model;language understanding;evaluation performance;base donnee;appel telephonique;performance evaluation;reconocimiento palabra;spoken dialog system;information retrieval;man machine dialogue;evaluacion prestacion;speech processing;langage naturel;database;tratamiento palabra;traitement parole;base dato;topic classification;salient phrase aquisition;experimental result;stochastic language modeling;call classification;classification appel;automatic recognition;modele linguistique;natural language;modelo linguistico;resultado experimental;speech recognition;dialogo hombre maquina;telephone call;comprehension langage;reconnaissance parole;resultat experimental;language model;reconocimiento automatico;reconnaissance automatique;spoken language understanding;dialogue homme machine	We are interested in providing automated services via natural spoken dialog systems. There are many issues that arise when such systems are targeted for large populations of non-expert users. In this paper, we describe an experimental vehicle to explore these issues, that of automatically routing calls based on a user’s fluently spoken response to open-ended prompts such as ‘ How may I help you?’ A spoken dialog system for call-routing has been constructed, with subsequent processing for information retrieval and form-filling. To enable experimental evaluations, a database has been generated of 10,000 fluently spoken transactions between customers and human agents. We report on preliminary experimental results for that database.	database;dialog system;electronic billing;experimental system;information retrieval;input/output;nonlinear gameplay;population;routing;spoken dialog systems	Allen L. Gorin;Giuseppe Riccardi;Jeremy H. Wright	1997	Speech Communication	10.1016/S0167-6393(97)00040-X	speech recognition;computer science;artificial intelligence;speech processing;linguistics;natural language;language model	NLP	-23.296656447547548	-86.55565728925144	774
ca5ece4c479a0b0667e5c1c45804f9c4ff6e9dae	on a class of reflected ar(1) processes	queueing;universiteitsbibliotheek;scaling limit;reflected process	In this paper, we study a reflected AR(1) process, i.e., a process (Zn)n obeying the recursion Zn+1 = max{aZn + Xn, 0}, with (Xn)n a sequence of i.i.d. random variables. We find explicit results for the distribution of Zn (in terms of transforms) in case Xn can be written as Yn − Bn, with (Bn)n being a sequence of independent random variables which are all exp(λ) distributed, and (Yn)n i.i.d.; when |a| < 1 we can also perform the corresponding stationary analysis. Extensions are possible to the case that (Bn)n are of phasetype. Under a heavy-traffic scaling, it is shown that the process converges to a reflected Ornstein-Uhlenbeck process; the corresponding steady-state distribution converges to the distribution of a Normal random variable conditioned on being positive.	autoregressive model;image scaling;obedience (human behavior);recursion;severo ornstein;stationary process;steady state	Onno J. Boxma;Michel Mandjes;Josh Reed	2016	J. Applied Probability	10.1017/jpr.2016.42	scaling limit;combinatorics;mathematical analysis;calculus;mathematics;queueing theory;statistics	ML	43.58175846394235	12.356700811291416	775
4cf7a15f3f604a5911c4954e22cf8822636021fc	the design and evaluation of link: a computer-based learning system for correlation	conceptualization;enseignement superieur;computer software development;higher education;psychology;student;computer assisted instruction;summative evaluation;enseignement assiste par ordinateur;printed materials;statistics;psychologie;correlation;computer based learning;etudiant;courseware;conceptualisation;instructional materials	This paper describes the design and evaluation of a computer-assisted learning program called Link, which was designed to be used by psychology students to review their understanding of correlation. Unlike existing computer-assisted learning programs that were reviewed, Link makes use of data from authentic research studies in psychology and provides learner activities that are specifically designed to address students' misconceptions about correlation. A summative evaluation study of Link involving fifty psychology students was carried out to assess the effect on students' understanding of correlation. It was found that the use of Link significantly contributed to students' general understanding of correlation. However, it was found that students use of paper-based instructional materials could also achieve this. The implication of this research outcome is considered in relation to the design and use of computer-assisted learning applications for statistics in higher education.		Erica Morris	2001	BJET	10.1111/1467-8535.00175	psychology;mathematics education;conceptualization;social science;simulation;computer science;sociology;higher education;management;correlation;summative assessment;statistics;pedagogy	AI	-75.47999674780439	-41.06173446237239	776
7de6e22c701d2eec885c80307294526afee9337b	polyominoes on twisted cylinders	polycubes;polyominoes	In this video we show how to enumerate polyominoes on twisted cylinders, and explain how to use them for setting lower bounds on the asymptotic growth rate of polyominoes in the plane.	enumerated type;twisted pair	Gill Barequet;Mira Shalah	2013		10.1145/2462356.2462357	combinatorics;discrete mathematics;mathematics;geometry;polyomino	Theory	33.28516809589735	24.67012965308006	777
81252e091db8bd446fb91ae5dca34a215d256ff9	a digital simulation of the vibration of a two-mass two-spring system	mass spring system;natural frequency;eigenvector;eigenvalue;physics;dynamics;vibration;digital simulation	Abstract#R##N##R##N#In this study, we developed a computer program to simulate the vibration of a two-mass two-spring system by using Visual BASIC. Users can enter data for the two-mass two-spring system. The software will derive the eigenvalue problem from the input data. Then the software solves the eigenvalue problem and illustrates the results numerically and graphically on the screen. In addition, the program uses animation to demonstrate the motions of the two masses. The displacements, velocities, and accelerations of the two bodies can be shown if the corresponding checkboxes are selected. This program can be used in teaching courses, such as Linear Algebra, Advanced Engineering Mathematics, Vibrations, and Dynamics. Use of the software may help students to understand the applications of eigenvalue problems and related topics such as modes of vibration, natural frequencies, and systems of differential equations. © 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 18: 563–573, 2010; View this article online at wileyonlinelibrary.com; DOI 10.1002/cae.20241	simulation	Wei-Pin Lee;Ming-Shiun Lu	2010	Comp. Applic. in Engineering Education	10.1002/cae.20241	simulation;eigenvalues and eigenvectors;computer science;engineering;electrical engineering;artificial intelligence;theoretical computer science;control theory;mathematics;mechanical engineering	DB	-36.35896256666813	-21.90153273562374	778
b6da4a9f199ca83b1abbee4c32bba6209a40e2bb	emlog: tamper-resistant system logging for constrained devices with tees		Remote mobile and embedded devices are used to deliver increasingly impactful services, such as medical rehabilitation and assistive technologies. Secure system logging is beneficial in these scenarios to aid audit and forensic investigations particularly if devices bring harm to end-users. Logs should be tamper-resistant in storage, during execution, and when retrieved by a trusted remote verifier. In recent years, Trusted Execution Environments (TEEs) have emerged as the go-to root of trust on constrained devices for isolated execution of sensitive applications. Existing TEE-based logging systems, however, focus largely on protecting server-side logs and offer little protection to constrained source devices. In this paper, we introduce EmLog – a tamper-resistant logging system for constrained devices using the GlobalPlatform TEE. EmLog provides protection against complex software adversaries and offers several additional security properties over past schemes. The system is evaluated across three log datasets using an off-the-shelf ARM development board running an open-source, GlobalPlatform-compliant TEE. On average, EmLog runs with low run-time memory overhead (1MB heap and stack), 430–625 logs/second throughput, and five-times persistent storage overhead versus unprotected logs.	arm architecture;algorithm;assistive technology;authentication;cryptography;embedded system;expectation propagation;formal verification;hardware restriction;microprocessor development board;mobile device;open-source software;overhead (computing);persistence (computer science);requirement;server-side;tamper resistance;tee (command);threat model;throughput;trust anchor;wearable technology	Carlton Shepherd;Raja Naeem Akram;Konstantinos Markantonakis	2017		10.1007/978-3-319-93524-9_5	computer security;throughput;logging;trusted computing;tamper resistance;software;persistence (computer science);heap (data structure);computer science;audit	Security	-54.52248718117402	57.87246289890124	779
0491473a0c0015abc3eef45ba8a45db000c865a9	analyzing the lead time and shipping lot-size in a chaotic supply network	oscillations;chaotic behavior;chaotic system;system dynamics;lead time;complex system;lot sizing;nonlinear dynamic system;chaos theory	"""Supply network issues recently have attracted a lot of attention from industrial practitioners and academics worldwide. Supply networks are highly complex systems. The oscillations in demand and inventory as orders pass through the system have been widely studied in literature. Studies have shown that supply networks can display some of the key characteristics of chaotic systems. Chaos theory is the study of complex, nonlinear, dynamic systems; therefore it can be useful for studying the dynamics of supply networks. In this paper the authors implemented a system dynamic approach and simulated a chaotic multi-level supply network. The authors analyzed the effects of decision parameters, delivery lead time and shipping lot-size on chaotic behavior of the whole supply network. The simulation revealed that an increment in lead times or shipping lot-size has a similar impact on chaotic behavior of the system and reduces the chance of chaotic behavior occurrence. observe a time delay between when a decision is made and when we feel its effect, which further complicates the interaction between partners. Feedback, interaction, and time delay exist in many supply chain processes. This makes the supply chain a complex dynamic system (Hwarng & Xie, 2008). By increasing the partners of supply chain in each level, ‘supply network’ is introduced in the literature. In a supply network, for a given product and speed required by the customer, handling of logistics operations become more and more complex when vertical, horizontal, spatial and relational complexities grow (Romano, 2009). DOI: 10.4018/jal.2011100102 16 International Journal of Applied Logistics, 2(4), 15-28, October-December 2011 Copyright © 2011, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. The introduction of chaos theory goes back to the study by Lorenz (1963) on weather forecasting systems. Feigenbaum (1978) and Mandelbrot (1982) contributed to this field. One of the significant achievements of chaos theory is its ability to demonstrate how simple deterministic relationships can produce patterned yet unpredictable behavior. The key features of chaos are listed in Williams (1997) and Wilding (1998): (1) Sensitivity to initial conditions (the butterfly effect): small changes multiply over time due to nonlinearity and the dynamic, repetitive behavior of the system; (2) Irregular and random-like behavior: the system behavior seems to be random; (3) Non-randomness: chaos only happens in deterministic systems; (4) Strange attractor: a specific pattern is observed in the phase space; (5) Bounds: variables are bounded. Because of butterfly effect long-term planning and forecasting are very difficult in chaotic systems. Chaos theory has attracted a lot of interest in many fields. This might be because chaos theory suggests we can find order and structure beneath complex behaviors (Levy, 1994). There has been a lot of interest in utilizing chaos theory implications in financial, economical and management studies but there are limited studies in the fields of inventory management and supply chain management (Hwarng & Xie, 2008). Hwarng and Xie (2008) considered chaos in a multi-level supply chain, classical beer distribution model with some modifications. They offered viewpoints about how to manage interrelated supply chain factors in order to eliminate or reduce system chaos. They considered various supply chain factors, i.e., demand pattern, ordering policy, demand-information sharing, and lead time, and studied the dynamics of the system under their influence. Romano (2009) investigated the appropriate configuration of supply chains and business processes in the time-sensitive casual wear industry in order to achieve time performance. He explained the relations among the supply network configuration, business process structure and time performance using fluid dynamics. He described the relation between time performance and configuration decisions. Wilding (1998) considered the management and design of supply networks from the viewpoint of chaos theory. He suggested that chaos theory can partly explain the failure of traditional approaches to significantly reduce uncertainty. He suggested that supply networks can exhibit some of the key characteristics of chaotic systems, e.g., sensitivity to initial conditions that undermines computer accuracy; exhibiting temporary stability; generating patterns while being highly complex and invalidating the reductionist view. Levy (1994) suggested that chaos theory can provide an appropriate theoretical framework through which we can better understand the dynamics of industries and the complex relations among industry entities. He demonstrated that industries are complex, nonlinear, dynamic systems that exhibit both unpredictability and underlying order. He suggested the use of chaos theory for strategy and management. Xu et al. (2009) designed a Prediction model of supply chain demand which has been built by fuzzy neural network based on a chaotic time series to enhance the prediction accuracy of supply chain demand. Stapleton et al. (2006) discussed forecasting, product design, and inventory management challenges in supply chains using chaos theory principles. They demonstrated that principles of chaos theory can be used as a tool to improve effectiveness of supply chain. Prater (2005) studied different types of uncertainties facing supply chains and their information systems. He considered various types of macro level uncertainties and broke them down into more specific types seen in supply chains, the impact of them on the supply chain and its supporting IS, and the current methods and tools for dealing with them. Wu and Zhang (2007) simulated the interaction between suppliers and customers in a 12 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/analyzing-lead-time-shipping-"""	artificial neural network;broadcast delay;business process;butterfly effect;chaos theory;complex systems;dynamical system;entity;feigenbaum constants;information system;initial condition;inventory;logistics;mandelbrot set;neuro-fuzzy;nonlinear system;randomness;reductionism;simulation;system dynamics;time series;web page;x image extension	Mohammad Jafar Tarokh;Sina Golara	2011	IJAL	10.4018/jal.2011100102	complex systems;simulation;operations management;synchronization of chaos;chaos theory;system dynamics;oscillation	AI	-11.586694586775632	-18.24770647211832	780
ed3769f663000b728f3fec2ce31cd12649624900	utaclir -: general query translation framework for several language pairs	cross lingual information retrieval;dictionary based retrieval;query translation;clir			Heikki Keskustalo;Turid Hedlund;Eija Airio	2002		10.1145/564376.564489	natural language processing;query expansion;speech recognition;rdf query language;web search query;information retrieval;query language	NLP	-32.934981698102234	-65.06409890151623	781
5317b63553f217d9eb5d1befb970bbdbbff64798	extended pin authentication scheme allowing multi-touch key input	personal identification number;mobile device;multi touch;input scheme;password;pin;smartphone;user authentication	In this paper, we report our trial to make a better form of personal identification number(PIN) authentication for a mobile device. We think that mobile users should be given a more secure alternative authentication because PIN authentication has well-known flaws. However, proposed alternative schemes change the authentication method drastically and that may discomfort mobile users. Our approach is to just change the input operation of PIN authentication by allowing more than one number at a time using a multi-touch enabled screen. We implemented a web-based prototype system and conducted an informal user study using it. The results of the study indicate that PIN input time, input errors and secret memorability of the proposed scheme were no worse than those of conventional PIN authentication. We also discuss the mathematical security level and other advantages of the scheme.	authentication;mobile device;multi-touch;personal identification number;prototype;usability testing;web application	Tetsuji Takada;Yuki Kokubun	2013		10.1145/2536853.2536944	chip authentication program;challenge–response authentication;computer science;authentication protocol;operating system;lightweight extensible authentication protocol;multi-factor authentication;mobile device;internet privacy;world wide web;password;computer security	HCI	-50.88414246351735	65.58870324103512	782
66e7b439dd0e5975b7eb5ef7e00be1438a1c228c	requirements and system architecture for a healthcare wireless body area network		Wireless body area networks enable new opportunities for personal healthcare monitoring and personal healthcare applications. This paper presents a comprehensive set of requirements and challenges for building a wireless body area network to support diverse user groups and a corresponding set of healthcare applications. Based on the identified requirements, the paper presents an architecture for a wireless body area network and describes how this architecture is connected to an existing it-infrastructure supporting healthcare at home. Finally the paper presents our on-going research with development of an ASE-BAN test bed. The major goal for this test bed is to be a platform for research and experiments with development of an ultra-low power body area network including sensor, communication nodes, communication protocols and a body gateway component.	adaptive server enterprise;experiment;requirement;sensor;testbed	Finn Overgaard Hansen;Thomas Skjødeberg Toftegaard	2011			embedded system;wi-fi;real-time computing;wireless wan;heterogeneous network;wireless site survey;wireless network;body area network;municipal wireless network;wi-fi array	Mobile	-20.458975815273725	81.27060240216173	783
6eda34b5e66b444b017a18b7aed059cf546e97a1	background subtraction for static & moving camera	image motion analysis cameras computer vision image colour analysis;image color analysis lighting training cameras classification algorithms videos change detection algorithms;object motion moving camera static camera machine vision systems environmental conditions multiple background model based background subtraction algorithm optimal color space selection background model bank camera jitter dynamic background;background model bank background subtraction background modelling binary classifiers	Background subtraction is one of the most commonly used components in machine vision systems. Despite the numerous algorithms proposed in the literature and used in practical applications, key challenges remain in designing a single system that can handle diverse environmental conditions. In this paper we present Multiple Background Model based Background Subtraction Algorithm as such a candidate. The algorithm was originally designed for handling sudden illumination changes. The new version has been refined with changes at different steps of the process, specifically in terms of selecting optimal color space, clustering of training images for Background Model Bank and parameter for each channel of color space. This has allowed the algorithm's applicability to wide variety of challenges associated with change detection including camera jitter, dynamic background, Intermittent Object Motion, shadows, bad weather, thermal, night videos etc. Comprehensive evaluation demonstrates the superiority of algorithm against state of the art.	algorithm;background subtraction;cluster analysis;color space;machine vision	Hasan Sajid;Sen-Ching Samson Cheung	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351664	computer vision;camera auto-calibration;camera resectioning;background subtraction;multimedia;computer graphics (images)	Vision	42.609543594891655	-47.150909567686334	784
a36031ac8e8fb5ac95cca38a7bd934576014724f	secure and efficient querying over personal health records in cloud computing		Abstract Information seeking is becoming an indispensable activity in daily life, especially in the medical cloud. Body Area Network (BAN) is becoming more and more popular with respect to the development and popularity of mobile devices. People are starting to back up the medical data to cloud, make data accessible by the doctors from almost anywhere using mobile terminals. In this paper, we present an efficient and secure fine-grained access control scheme which not only achieves authorized users to access the records in cloud storage, but also supports a small set of physicians to write on the records. In order to improve the efficiency, we put forward a novel technique called match-then-decrypt, which is used to perform the decryption test without decryption. Also, the scheme outsources bilinear pairing operations to a gateway without revealing the data content, and thus largely eliminates this overhead for users to a great extent. The performance assessments demonstrate the efficiency of our proposed solution in terms of computation, communication, and storage.	access control;algorithm;authorization;backup;bilinear filtering;cloud computing;cloud storage;computation;data access;encryption;information seeking;key generation;mobile device;outsourcing;overhead (computing);public-key cryptography	Xuejiao Liu;Yingjie Xia;Wei Yang;Fengli Yang	2018	Neurocomputing	10.1016/j.neucom.2016.06.100	cloud storage;computer network;machine learning;access control;artificial intelligence;mobile device;computation;cloud computing;default gateway;computer security;body area network;computer science;information seeking	Security	-42.9295290988224	65.7445981022619	785
1a9e209808b667a5398ae53ea96fe14e88a0d446	a novel and accurate local 3d representation for face recognition		In this paper, we intend to introduce a novel curved 3D face representation. It is constructed on some static parts of the face which correspond to the nose and the eyes. Each part is described by the level curves of the superposition of several geodesic potentials generated from many reference points. We propose to describe the eye region by a bipolar representation based on the superposition of two geodesic potentials generated from two reference points and the nose by a three-polar one (three reference points). We use the BU-3DFE database of 3D faces to test the accuracy of the proposed approach. The obtained results in the sense of the Hausdorff shape distance prove the performance of the novel representation for 3D faces identification. The obtained scores are comparable to the state of the art methods in the most of cases.	facial recognition system	Soumaya Mathlouthi;Majdi Jribi;Faouzi Ghorbel	2017		10.1007/978-3-319-70353-4_14	computer vision;geodesic;artificial intelligence;pattern recognition;three-dimensional face recognition;computer science;facial recognition system;superposition principle;hausdorff space	Vision	41.40004233066214	-56.72098839378075	786
459404b0c7ae23d279d159220805ebc4078fb3d0	optimization of sensor locations for measurement of flue gas flow in industrial ducts and stacks using neural networks	optimisation;gas detectors fluid flow measurement flue gases gas industry ducts pollution measurement neural networks sampling methods iso standards power generation;neural networks;flowmeters;neural nets;iso standards;air pollution measurement;optimised sampling neural network modelling sensor location optimisation flue gas flow measurement industrial ducts industrial stacks case study power plant measurement accuracy reduced manual operation pitot tube inverse model velocity profiles three layered feedforward network levenberg marquardt training optimisation criterion iso10780 standard;fluid flow;gas detectors fluid flow measurement flue gases gas industry ducts sampling methods neural networks testing predictive models elbow;indexing terms;power plants flow measurement flowmeters optimisation modelling feedforward neural nets learning artificial intelligence iso standards;fluid flow measurement;gas flow;power plant;research paper;power plants;confined flow flow measurement data acquisition computerised instrumentation neural nets optimisation air pollution measurement;optimization;feedforward neural nets;confined flow;computerised instrumentation;neural network model;learning artificial intelligence;flow measurement;experience base;data acquisition;sensor location;pitot tube optimization sensor location flue gas flow industrial ducts stacks data acquisition neural network modeling air pollution measurement accuracy;neural network	This paper presents a novel application of neural network modeling in the optimization of sensor locations for the measurement of flue gas flow in industrial ducts and stacks. The proposed neural network model has been validated with an experiment based upon a case-study power plant. The results have shown that the optimized sensor location can be easily determined with this model. The industry can directly benefit from the improvement of measurement accuracy of the flue gas flow in the optimized sensor location and the reduction of manual measurement operation with Pitot tube.	artificial neural network;mathematical optimization;network model;sensor;williams tube	Haizhuang Kang;Qingping Yang;Clive Butler;Tuqiang Xie;Fabrizio Benati	2000	IEEE Trans. Instrumentation and Measurement	10.1109/19.843054	control engineering;power station;electronic engineering;computer science;engineering;electrical engineering;machine learning;artificial neural network	Embedded	58.98390628038513	-12.822882328086573	787
ef0177ac9927f02898527d07ce9518361b123cb0	special session 12a: hot topic counterfeit ic identification: how can test help?	educational institutions counterfeiting integrated circuits silicon hardware manufacturing	Integrated circuit counterfeiting is a severe challenge for semiconductor companies, system integrators and product end-users. Substantial revenue losses by individual enterprises as well as detrimental economy-wide effects have triggered significant interest in counterfeit detection and prevention by commercial actors and governments. This resulted in a number of large-scale research initiatives and networks that focus on this topic, in North America, Europe and elsewhere. The hot-topic special session will introduce the test community to counterfeit detection techniques and identify open problems which can be solved using tools and methods from the testing area.	anomaly detection;integrated circuit;semiconductor	Ilia Polian;Mark Mohammad Tehranipoor	2013	2013 IEEE 31st VLSI Test Symposium (VTS)	10.1109/VTS.2013.6548944	simulation;computer science;engineering;electrical engineering;computer security	Logic	-80.43989961808572	23.715022755410764	788
8275697e8d58e677fd7b9945245a7ce3c4737fd7	evaluating drug-drug interaction information in ndf-rt and drugbank	biological patents;biomedical journals;drug drug interactions;drugbank;text mining;europe pubmed central;citation search;data mining and knowledge discovery;citation networks;computational biology bioinformatics;ndf rt;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;combinatorial libraries;computer appl in life sciences;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	BACKGROUND There is limited consensus among drug information sources on what constitutes drug-drug interactions (DDIs). We investigate DDI information in two publicly available sources, NDF-RT and DrugBank.   METHODS We acquire drug-drug interactions from NDF-RT and DrugBank, and normalize the drugs to RxNorm. We compare interactions between NDF-RT and DrugBank and evaluate both sources against a reference list of 360 critical interactions. We compare the interactions detected with NDF-RT and DrugBank on a large prescription dataset. Finally, we contrast NDF-RT and DrugBank against a commercial source.   RESULTS DrugBank drug-drug interaction information has limited overlap with NDF-RT (24-30%). The coverage of the reference set by both sources is about 60%. Applied to a prescription dataset of 35.5M pairs of co-prescribed systemic clinical drugs, NDF-RT would have identified 808,285 interactions, while DrugBank would have identified 1,170,693. Of these, 382,833 are common. The commercial source Multum provides a more systematic coverage (91%) of the reference list.   CONCLUSIONS This investigation confirms the limited overlap of DDI information between NDF-RT and DrugBank. Additional research is required to determine which source is better, if any. Usage of any of these sources in clinical decision systems should disclose these limitations.	bibliographic index;didanosine;drug interactions;drugbank;interaction information;multum drug;silo (dataset)	Lee B. Peters;Nathan J. Bahr;Olivier Bodenreider	2015		10.1186/s13326-015-0018-0	text mining;computer science;bioinformatics;data science;drugbank;data mining	Comp.	-2.6034356804163976	-65.16049665372411	789
28d9f4046413ef814534c32a9293a84eb61a0562	gtcf based larc contouring motion control of an industrial x-y linear-motor-driven stage	system dynamics robustness adaptation models robust control uncertainty symmetric matrices adaptive systems;global task coordinate frame contouring motion linear motor adaptive robust control iterative learning;uncertainty;system dynamics;robust control;symmetric matrices;robust control adaptive control iterative learning control linear motors machine control motion control;adaptive systems;robustness;adaptation models;iterative learning control global task coordinate frame gtcf learning adaptive robust controller larc contouring motion control industrial x y linear motor driven stage contouring error calculation model	A global task coordinate frame (GTCF) based learning adaptive robust controller (LARC) is practically synthesized for an industrial X-Y linear-motor-driven stage to achieve not only good parametric adaptation ability and uncertain disturbance robustness, but also excellent transient/steady-state contouring performance. With the employment of GTCF which is globally based on the shape of the desired contour, the real-time contouring error calculation model is rather accurate and the coordination of multi-axes motion is guaranteed. Then a LARC control framework is proposed for the coupled system dynamics in GTCF, where the adaptive robust control term is to deal with the strongly coupled system model, and the iterative learning control term is to address the effect of unmodelled dynamics. Comparative experiments are carried out on an industrial linear-motor-driven stage, and the results consistently verify that the proposed GTCF based LARC controller has excellent contouring performance, and robustness to parametric variations and external disturbances.	experiment;iterative method;nonlinear system;real-time clock;robust control;robustness (computer science);steady state;system dynamics;univac larc	Chuxiong Hu;Yu Zhu;Zhipeng Hu;Ze Wang	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7525238	robust control;control engineering;simulation;uncertainty;computer science;adaptive system;control theory;mathematics;system dynamics;statistics;robustness;symmetric matrix	Robotics	65.46574539674246	-10.7361248920698	790
e380f2ce61fe1879f1a02f20698492694c6f79ba	discrete time quasi-sliding mode control of nonlinear uncertain systems		The control of nonlinear uncertain systems is still an open area of research, and sliding mode control (SMC) is one of the robust and effective methods to cope with uncertain conditions. In this paper, a new sliding mode control algorithm for a class of discrete-time nonlinear uncertain systems is proposed. By using an estimator of uncertainties and external disturbances, the proposed algorithm ensures the stability of the closed loop system as well as the reference tracking. The controller designed using the above technique is completely insensitive to the parametric uncertainty and the external disturbances. Simulations are carried out on a numerical example and a bioreactor benchmark, and the results confirm the effectiveness of our proposition.	nonlinear system	Ibtissem Bsili;Jalel Ghabi;Hassani Messaoud	2018	IJMIC	10.1504/IJMIC.2018.10011581	discrete time and continuous time;estimator;control theory;control engineering;mathematics;sliding mode control;control theory;nonlinear system;parametric statistics	Robotics	66.66081078707948	-5.554522799846036	791
e242c5dbbf977c3272faea7bdea3c5f21a4b4717	petri net simulation algorithm of the shortest path in transportation of wartime	libraries;analytical models;computer figure emulation;transportation networks;shortest path;simulation;theory and method;military vehicles;transportation military vehicles petri nets simulation;petri net simulation algorithm;computational modeling;military transport network wartime transportation petri net simulation algorithm computer figure emulation;transportation communication system traffic control computational modeling computer simulation military communication computer networks computer graphics military computing space technology control systems;transportation;wartime transportation;petri nets;petri net;graphics;military transport network;military computing;data models	"""Regard transport network shortest path in wartime for the research object, adopt Petri net theory and method which the computer figure emulation combine, solve the shortest path in transport network of wartime. This method is added and describes the element of the network on the basis of general Petri network, meanwhile, introduce """"the sport mark"""" concept and a new one """" Happen"""" rule, deal with artificial operation and number value which pursue the artificial target's system of the network, define operation rule and step of the artificial device of Petri net, change the undirected transport network into EPN model automatically with the artificial device of Petri net, then ask the corresponding shortest path automatically. This method is more vivid than the existing method, the ocular, pace is faster, it is more practical method and means."""	algorithm;computation;euref permanent network;emoticon;emulator;fermat's principle;graph (discrete mathematics);jam;p3m;p6 (microarchitecture);pathfinding;petri net;shortest path problem;simulation	Liu Xuan;Huang Sheng Guo	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.106	simulation;computer science;theoretical computer science;distributed computing;petri net	Robotics	72.32032535640464	-47.092605207808454	792
f74a1569ecab57e68f620276fe70f48d838614b0	sensitivity to evidence in gaussian bayesian networks using mutual information	gaussian bayesian network;estadistica matematica;sensitivity analysis;evidence propagation;mutual information;entropy	We introduce a methodology for sensitivity analysis of evidence variables in Gaussian Bayesian networks. Knowledge of the posterior probability distribution of the target variable in a Bayesian network, given a set of evidence, is desirable. However, this evidence is not always determined; in fact, additional information might be requested to improve the solution in terms of reducing uncertainty. In this study we develop a procedure, based on Shannon entropy and information theory measures, that allows us to prioritize information according to its utility in yielding a better result. Some examples illustrate the concepts and methods introduced. 2014 Elsevier Inc. All rights reserved.	bayesian network;information theory;mutual information;shannon (unit)	Miguel Ángel Gómez-Villegas;Paloma Main;Paola Viviani	2014	Inf. Sci.	10.1016/j.ins.2014.02.025	bayesian average;econometrics;entropy;bayesian experimental design;information diagram;multivariate mutual information;machine learning;mathematics;mutual information;bayesian statistics;sensitivity analysis;interaction information;statistics	AI	26.389058546967515	-19.016040080617042	793
a7445ee74d0d3e722b5534429c7941b1b52dc594	scheduling optimisation of a manufacturing design area: a case study		This paper presents the work implemented to improve the production scheduling of a real life manufacturing plant in Ireland. Since the scheduling algorithms are integrated in a wider cognitive system, where human and machine intelligence collaborate, an important constraint will be the maximum allowed computational time. This paper will also demonstrate the impact of using heuristic rules for the generation of initial solutions and provide a comparison between Hill Climbing, Harmony Search and Simulated Annealing.	mathematical optimization;scheduling (computing)	C. A. Garcia-Santiago;Anna Rotondo;Fergus Quilligan	2017		10.1007/978-981-10-3728-3_13	operations management;scheduling	Robotics	19.339160311338127	-1.2142962570621731	794
76b4651c996e159eaf33e78a2f98909d670997fd	defining taxonomic hierarchies: their implications for multiple inheritance.	multiple inheritance		multiple inheritance	Esperanza Marcos	2001			multiple inheritance;computer science	NLP	-22.925858533223185	20.22272636082199	795
bb0bcd5b275ea2a3542a4af56279149d7c76554f	a cryptographic moving-knife cake-cutting protocol		This paper proposes a cake-cutting protocol using cryptogr aphy when the cake is a heterogeneous good that is represented by an interval on a real line. Althou g the Dubins-Spanier moving-knife protocol with one knife achieves simple fairness, all players m ust execute the protocol synchronously. Thus, the protocol cannot be executed on asynchronous netwo rks such as the Internet. We show that the moving-knife protocol can be executed asynchronou sly by a discrete protocol using a secure auction protocol. The number of cuts is n−1 wheren is the number of players, which is the minimum.	communications protocol;cryptography;cutting stock problem;discrete mathematics;efficient cake-cutting;fairness measure;internet;sly 3: honor among thieves	Yoshifumi Manabe;Tatsuaki Okamoto	2012		10.4204/EPTCS.78.2	reverse address resolution protocol;otway–rees protocol;general inter-orb protocol;universal composability;real-time computing;neighbor discovery protocol;two-phase commit protocol;stateless protocol;resource reservation protocol;internet protocol control protocol;msi protocol;computer science;authentication protocol;tunneling protocol;cryptographic protocol;distributed computing;computer security;internetwork protocol	Theory	-42.22901694853434	73.8482232933967	796
a77ad987b03889fa64773c257cec0d0877451529	computational methods for the animation of heat transfer evolution and steel solidification			computation	Adán Ramírez-López;David Fernando Muñoz;Sergio Romero-Hernández;Simón López-Ramírez	2016			heat transfer;animation;mechanical engineering;materials science	Graphics	90.65564074742584	-3.2535594249484108	797
bcf556b3fa9d42cb05f88da82f5a5d343b6ec538	reuse in object-oriented information systems design	object oriented information systems;design reuse;concepcion sistema;information retrieval;software development process;artificial intelligent;development tool;object oriented;system design;object oriented approach;oriente objet;aspect oriented;tool integration;information system;orientado objeto;conception systeme;systeme information;sistema informacion	Improving reuse is still an important issue of Information Systems Engineering. Several current object-oriented approaches,  such as patterns, frameworks or business components, address this topic at different phases of the software development process.  This workshop focused on tools, techniques and methods developed to improve the reuse of design elements.    Reuse is today mainly gained with empirical tools and methods, and it is necessary to make reuse more systematic in object-oriented  information systems design. Reuse at the design phase can be considered from two different yet complementary perspectives:  (1) design for reuse or (2) design by reuse. 1. Design for reuse (1) deals with identifying reusable elements, specifying  and organizing components, and integrating sets of and models for specifying reusable artifacts. 2. Design by reuse (2) needs  to define new information systems engineering processes, and develop tools supporting systematic reuse of components in information  systems.        Special attention has been given to contributions aiming to improve the above techniques by adapting novel or “non traditional”  approaches at the edge of object-orientation (e.g. aspect-orientation or multi-viewpoints approaches, tools integrating artificial  intelligence techniques, information retrieval, . . . ).      	information system;systems design	Daniel Bardou;Agnès Front;Liz Kendall	2002		10.1007/3-540-46105-1_11	aspect-oriented programming;computer science;artificial intelligence;programming language;object-oriented programming;software development process;information system;systems design	PL	-52.77927263779131	23.70647131070944	798
a8b6dbaa8358117d16caf2f42e3c5404a19f574e	two-stage pooling of deep convolutional features for image retrieval	neural networks;feature extraction principal component analysis image retrieval image representation benchmark testing aggregates neural networks;aggregates;image representation;feature extraction;principal component analysis;benchmark testing;compact descriptor image representation image retrieval convolutional neural network feature pooling;image retrieval	Convolutional Neural Network (CNN) based image representations have achieved high performance in image retrieval tasks. However, traditional CNN based global representations either provide high-dimensional features, which incurs large memory consumption and computing cost, or inadequately capture discriminative information in images, which degenerates the functionality of CNN features. To address those issues, we propose a two-stage partial mean pooling (PMP) approach to construct compact and discriminative global feature representations. The proposed PMP is meant to tackle the limits of traditional max pooling and mean (or average) pooling. By injecting the PMP pooling strategy into the CNN based patch-level mid-level feature extraction and representation, we have significantly improved the state-of-the-art retrieval performance over several common benchmark datasets.	benchmark (computing);convolutional neural network;feature extraction;image retrieval	Tiancheng Zhi;Ling-yu Duan;Yitong Wang;Tiejun Huang	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532802	benchmark;computer vision;feature detection;visual word;feature extraction;image retrieval;computer science;machine learning;pattern recognition;principal component analysis	Vision	26.118852416725048	-52.590107227905136	799
3f7db882e4e960edccefa668dcde1d01597b0542	temporal precision in population—but not individual neuron—dynamics reveals rapid experience-dependent plasticity in the rat barrel cortex	health research;uk clinical guidelines;biological patents;effective connectivity;barrel cortex;europe pubmed central;citation search;uk phd theses thesis;dynamic bayesian network;life sciences;uk research reports;medical journals;europe pmc;biomedical research;experience dependent plasticity;whisker pairing;bioinformatics	Cortical reorganization following sensory deprivation is characterized by alterations in the connectivity between neurons encoding spared and deprived cortical inputs. The extent to which this alteration depends on Spike Timing Dependent Plasticity (STDP), however, is largely unknown. We quantified changes in the functional connectivity between layer V neurons in the vibrissal primary somatosensory cortex (vSI) (barrel cortex) of rats following sensory deprivation. One week after chronic implantation of a microelectrode array in vSI, sensory-evoked activity resulting from mechanical deflections of individual whiskers was recorded (control data) after which two whiskers on the contralateral side were paired by sparing them while trimming all other whiskers on the rat's mystacial pad. The rats' environment was then enriched by placing novel objects in the cages to encourage exploratory behavior with the spared whiskers. Sensory-evoked activity in response to individual stimulation of spared whiskers and adjacent re-grown whiskers was then recorded under anesthesia 1-2 days and 6-7 days post-trimming (plasticity data). We analyzed spike trains within 100 ms of stimulus onset and confirmed previously published reports documenting changes in receptive field sizes in the spared whisker barrels. We analyzed the same data using Dynamic Bayesian Networks (DBNs) to infer the functional connectivity between the recorded neurons. We found that DBNs inferred from population responses to stimulation of each of the spared whiskers exhibited graded increase in similarity that was proportional to the pairing duration. A significant early increase in network similarity in the spared-whisker barrels was detected 1-2 days post pairing, but not when single neuron responses were examined during the same period. These results suggest that rapid reorganization of cortical neurons following sensory deprivation may be mediated by an STDP mechanism.	arm cortex-m;cerebral cortex;dependent ml;documented;dynamic bayesian network;inference;ion implantation;microelectrodes;neuron;onset (audio);phase i/ii trial;physical object;rem sleep behavior disorder;resting state fmri;scientific publication;software documentation;somatosensory cortex;vibrissae	Seif Eldawlatly;Karim G. Oweiss	2014		10.3389/fncom.2014.00155	psychology;developmental plasticity;neuroscience;computer science;bioinformatics;artificial intelligence;communication;dynamic bayesian network	ML	18.43521619105016	-80.01058500455557	800
38ba9d5c8c659d39803290a485037588da0d8c21	iterated tabu search for the car sequencing problem	chaine fabrication;automovil;linea montaje;iterated local search;heuristic method;metodo heuristico;satisfiability;paint;production line;optimisation combinatoire;busca local;automobile;scheduling;motor car;peinture;assembly line;tabu search;linea fabricacion;car sequencing problem;methode heuristique;pintura;combinatorial optimization;local search;ordonnancement;recherche locale;busqueda tabu;chaine montage;reglamento;recherche tabou;optimizacion combinatoria	This paper introduces an iterated tabu search heuristic for the daily car sequencing problem in which a set of cars must be sequenced so as to satisfy requirements from the paint shop and the assembly line. The iterated tabu search heuristic combines a classical tabu search with perturbation operators that help escape from local optima. The resulting heuristic is flexible, easy to implement, and fast. It has produced very good results on a set of test instances provided by the French car manufacturer Renault.	display resolution;heuristic;iterated function;iteration;local optimum;paintshop pro;requirement;robustness (computer science);search algorithm;tabu search	Jean-François Cordeau;Gilbert Laporte;Federico Pasin	2008	European Journal of Operational Research	10.1016/j.ejor.2007.04.048	mathematical optimization;simulation;combinatorial optimization;tabu search;production line;computer science;local search;operations management;iterated local search;mathematics;scheduling;guided local search;satisfiability	Robotics	19.03814998987264	6.3438664645762035	801
07146855385f9d558807203d72749707236713fa	profile-driven program synthesis for evaluation of system power dissipation	temporal correlation;mixed integer linear program;switching activity;microprocessors;integer linear programming;power estimation;application software;branch prediction;mobile computing system;energy dissipation;program synthesis;functional programming;chip;computational modeling;permission;energy consumption;system design;power dissipation;power dissipation microprocessors circuit synthesis permission computational modeling energy consumption application software costs switches energy dissipation;energy cost;covering problems;power consumption;switches;register transfer level;high performance;circuit synthesis;generic programming	This paper presents a new approach for estimatingpower dissipation in a high performance microprocessor chip.First, characteristic profile (including parameters such as thecache miss rate, branch prediction miss rate, pipeline stalls,instruction mix, memory references, etc.) is extracted fromapplication programs. Then, mixed integer linear programmingand heuristic rules are used to gradually transform a genericprogram template to into a fully functional program. Thesynthesized program exhibits the same performance and powerdissipation behavior (as characterized by the extracted profile),yet it has an instruction trace orders of magnitude smaller thanthe initial trace. The synthesized program is subsequentlysimulated on a register-transfer level description of the targetmicroprocessor to provide the power dissipation value. Resultsobtained for the Intel's Pentium processor executing standardbenchmark programs show a simulation time reduction by 3-5orders of magnitude.	branch predictor;cpu power dissipation;functional programming;heuristic;instruction pipelining;microprocessor;program synthesis;register-transfer level;simulation;spec#	Cheng-Ta Hsieh;Massoud Pedram;Gaurav Mehta;Fred Rastgar	1997		10.1145/266021.266288	embedded system;electronic engineering;parallel computing;real-time computing;computer science;dissipation;operating system;programming language;functional programming;algorithm	Arch	-1.8956099404446243	53.78773710020275	802
562a7502e9c127acd96e56c308b27ba29b647cb3	exploring workaround situations in business processes		Business process management (BPM) systems are implemented by organizations in order to gain a full control of processes and ensure their efficient and effective performance according to specified procedures. However, a common phenomenon found in organizations is that processes are bypassed and worked around by their participants. The premise underlying this paper is that workarounds are performed for reasons. Understanding these reasons may reveal flaws in process design or in the implementation of BPM systems. The paper reports an exploratory multiple-case study, performed in three organizations, intended to gain an understanding of business process workarounds and the situations in which they are performed. The study identified six workaround types and 24 situational factors related to them.	business process;workaround	Nesi Outmazgin	2012		10.1007/978-3-642-36285-9_45	process management	Vision	-81.14653513228374	-1.1245342591076186	803
791bd0cf968204ff5df06f3963d379c009f458aa	practical near-collisions on the compression function of bmw	practical near-collisions;sha-3 candidate;message pair;active bit;similar attack;compression function;blue midnight wish;iterated hash function;input pair;colliding bit;near-collision attack	Blue Midnight Wish (BMW) is one of the fastest SHA-3 candidates in the second round of the competition. In this paper we study the compression function of BMW and we obtain practical partial collisions in the case of BMW-256: we show a pair of inputs so that 300 pre-specified bits of the outputs collide (out of 512 bits). Our attack requires about 2 evaluations of the compression function. The attack can also be considered as a near-collision attack: we give an input pair with only 122 active bits in the output, while generic algorithm would require 2 operations for the same result. A similar attack can be developed for BMW-512, which will gives message pairs with around 600 colliding bits for a cost of 2. This analysis does not affect the security of the iterated hash function, but it shows that the compression function is far from ideal. We also describe some tools for the analysis of systems of additions and rotations, which are used in our attack, and which can be useful for the analysis of other systems.	algorithm;collision attack;fastest;generic programming;hash function;iteration;one-way compression function;sha-3	Gaëtan Leurent;Søren S. Thomsen	2011		10.1007/978-3-642-21702-9_14	simulation;engineering;operations management;computer security	Crypto	-36.7708884520906	80.42593707266609	804
321edbdd749a109b1bb5f2620aff33f8bc45fb99	electro-thermal characterization of a differential temperature sensor in a 65 nm cmos ic: applications to gain monitoring in rf amplifiers	electro thermal characterization;cmos integrated circuits;thermal coupling characterization;cmos differential temperature sensors;laser interferometer measurements;ir camera measurements;article	This paper reports on the design solutions and the different measurements we have done in order to characterize the thermal coupling and the performance of differential temperature sensors embedded in an integrated circuit implemented in a 65 nm CMOS technology. The on-chip temperature increases have been generated using diode-connected MOS transistors behaving as heat sources. Temperature measurements performed with the embedded sensor are corroborated with an infra-red camera and a laser interferometer used as thermometer. A 2 GHz linear power amplifier (PA) is as well embedded in the same silicon die. In this paper we show that temperature measurements performed with the embedded temperature sensor can be used to monitor the PA DC behavior and RF activity.	audio power amplifier;cmos;die (integrated circuit);diode;embedded system;integrated circuit;performance;radio frequency;sensor;transistor	Josep Altet;José Luis González;Didac Gómez;X. Perpiñà;Wilfrid Claeys;Stéphane Grauby;Cédric Dufis;Miquel Vellvehí;Diego Mateo;Ferran Reverter;Stefan Dilhaire;Xavier Jordà	2014	Microelectronics Journal	10.1016/j.mejo.2014.02.009	electronic engineering;electrical engineering;cmos	EDA	57.32021133346651	47.30642855756082	805
4aaabc03c24fa9151065bc5f14b90ff507bd5108	multitaskprotdb-ii: an update of a database of multitasking/moonlighting proteins		Multitasking, or moonlighting, is the capability of some proteins to execute two or more biological functions. MultitaskProtDB-II is a database of multifunctional proteins that has been updated. In the previous version, the information contained was: NCBI and UniProt accession numbers, canonical and additional biological functions, organism, monomeric/oligomeric states, PDB codes and bibliographic references. In the present update, the number of entries has been increased from 288 to 694 moonlighting proteins. MultitaskProtDB-II is continually being curated and updated. The new database also contains the following information: GO descriptors for the canonical and moonlighting functions, three-dimensional structure (for those proteins lacking PDB structure, a model was made using Itasser and Phyre), the involvement of the proteins in human diseases (78% of human moonlighting proteins) and whether the protein is a target of a current drug (48% of human moonlighting proteins). These numbers highlight the importance of these proteins for the analysis and explanation of human diseases and target-directed drug design. Moreover, 25% of the proteins of the database are involved in virulence of pathogenic microorganisms, largely in the mechanism of adhesion to the host. This highlights their importance for the mechanism of microorganism infection and vaccine design. MultitaskProtDB-II is available at http://wallace.uab.es/multitaskII.	accession number (identifier);accession number (bioinformatics);cdisc sdtm microorganism terminology;code;computer multitasking;contain (action);drug design;multi-function printer;ncbi taxonomy;protein data bank;uniprot;virulence	Luís Franco-Serrano;Sergio Hernández;Alejandra Calvo;María A. Severi;Gabriela Ferragut;JosepAntoni Perez-Pons;Jaume Piñol;Òscar Pich;Angel Mozo-Villarias;Isaac Amela;Enrique Querol;Juan Cedano	2018		10.1093/nar/gkx1066	organism;uniprot;database;virulence;protein moonlighting;human multitasking;phyre;protein data bank (rcsb pdb);biology	Comp.	-0.6038092928464851	-61.077790470176794	806
298e528bbdad573b6f70213136107b2eeef4d853	icon design for older users of project management software	human computer interaction;project management;icon design;age differences;eye tracking;usability	Working in projects is an important part of many jobs in service industry. Due to their knowledge and experience project planning is often accomplished by older employees. Therefore, and with regard to the demographic change an integration of the needs of older employees into the work environment is required. Common to most IT-based systems, including project management software, is the use of icons. To investigate different icon sets in project management software, regarding age related differences, two studies were conducted. The first study aimed at investigating two different icon sets regarding execution times and eye movements in an applied setting. The second study consisted of a questionnaire where subjects had to map different icons to their corresponding functions and had to compare these icons regarding their intuitiveness. Results revealed that older users profit from an icon design which is low in complexity but no impact by different icon designs was found for younger users.		Christina Bröhl;Jennifer E. Bützler;Nicole Jochems;Christopher M. Schlick	2013		10.1007/978-3-642-39265-8_14	project management;simulation;usability;human–computer interaction;eye tracking;computer science;multimedia	HCI	-64.63960872715185	-56.68722790068879	807
2e602b4dab8b53a80fef0aaee735bbe17b3b9a71	composing pattern-based components and verifying correctness	architectural design;design component;integrable model;temporal logic;object z;component composition;software systems;development process;integration;design pattern;software development;composition pattern;reusable component;modeling;correctness proof;formal specification and verification	Designing large software systems out of reusable components has become increasingly popular. Although liberal composition of reusable components saves time and expense, many experiments indicate that people will pay for this (liberal composition) sooner or later, sometimes paying even a higher price than the savings obtained from reusing components. Thus, we advocate that more rigorous analysis methods to check the correctness of component composition would allow combination problems to be detected early in the development process so that people can save the considerable effort of fixing errors downstream. In this paper we describe a rigorous method for component composition that can be used to solve combination and integration problems at the (architectural) design phase of the software development lifecycle. In addition, we introduce the notion of composition pattern in order to promote the reuse of composition solutions to solve routine component composition problems. Once a composition pattern is proven correct, its instances can be used in a particular application without further proof. In this way, our proposed method involves reusing compositions as well as reusing components. We illustrate our approach through an example related to the composition of design patterns as design components. Structural and behavioral correctness proofs about the composition of design patterns are provided. Case studies are also presented to show the applications of the composition patterns. 2007 Elsevier Inc. All rights reserved.	class diagram;component-based software engineering;correctness (computer science);design pattern;downstream (software development);experiment;fairness measure;object composition;oldversion.com;plug and play;software development process;software system;systems design;temporal logic;theory;verification and validation	Jing Dong;Paulo S. C. Alencar;Donald D. Cowan;Sheng Yang	2007	Journal of Systems and Software	10.1016/j.jss.2007.03.005	real-time computing;systems modeling;temporal logic;computer science;systems engineering;software development;operating system;software engineering;design pattern;programming language;software development process;algorithm;composite pattern;software system	SE	-43.52533154344292	29.533452496539457	808
129b9bd29c418abbd0b2da561c27c61a61a9f4eb	optimization tuning of pi controller of quadruple tank process	online resources;linear systems;information resources;scholarly research;swarm intelligence;information sources;academic research;online databases;education resources;publishing;performance index;research databases;swarm intelligence control system synthesis decentralised control feedback linear systems multivariable control systems optimal control particle swarm optimisation performance index pi control;optimal control;feedback;decentralised control;australasian research information;control system synthesis;multivariable system optimization tuning quadruple tank process swarm intelligence method particle swarm optimization optimal proportional integral controller parameters qtp multivariable zero right half plane rhp nonminimum phase system linear feedback design system performance minimum phase decentralized pi controller tuning pso pi controller performance estimation performance index itae settling time maximum overshoot step response improvement;south east asian information;information databases;full content;education databases;australian databases;multivariable control systems;commissioning;electronic publisher;particle swarm optimisation;pi control;online;e titles;tk electrical engineering electronics nuclear engineering;tuning particle swarm optimization process control optimization performance analysis manuals control systems;library resources	In this paper, a swarm intelligence method, particle swarm optimization (PSO) is presented for determining the optimal proportional-integral (PI) controller parameters of a quadruple tank process (QTP). In QTP, when the multivariable zero is in the right half plane (RHP), the system becomes a nonminimum phase system which makes the tuning of controller difficult since limitation occur for linear feedback design. To overcome the limitation, the system's performance during minimum phase and non-minimum phase is investigated for a decentralized PI-tuning. To estimate the performance of the proposed PSO-PI controller, the performance index i.e. ITAE is utilized. From the results obtained, the proposed technique yields better performance in terms of settling time and maximum overshoot compared to the manual tuning method, hence an efficient technique to improving the step response of a quadruple tank process.	hp unified functional testing;mathematical optimization;minimum phase;overshoot (signal);particle swarm optimization;phase-shift oscillator;program optimization;quadruple-precision floating-point format;settling time;step response;swarm intelligence	Nurhuda Zainal Abidin;Shafishuhaza Sahlan;Norhaliza Abdul Wahab	2013	2013 Australian Control Conference	10.1109/AUCC.2013.6697294	control engineering;simulation;engineering;control theory	EDA	65.40656528756682	-8.216958865396785	809
f0172dcfa35b5135ba1b4b42c60f474edbd34810	new products					2003	IEEE Computer Graphics and Applications	10.1109/MCG.2003.10006		Visualization	-60.82826126276356	6.464063865489734	810
06768bc48f75256e70db2019b87494b787634012	model-view-controller separation in max using jamoma		The Model-View-Controller (MVC) software architecture pattern separates these three program components, and is well-suited for interactive applications where flexible human-computer interfaces are required. Separating data presentation from the underlying process enables multiple views of the same model, customised views, synchronisation between views, as well as views that can be dynamically loaded, bound to a model, and then disposed. Jamoma 0.6 enables MVC separation in Cycling’74 Max through custom externals and patching guidelines for developers. Models and views can then be nested for a hierarchal structuring of services. A local preset system is available in all models, along with namespace and services that can be inspected and queried application-wide. This system can be used to manage cues with modular, stringent and transparent handling of priorities. It can also be expanded for inter-application exchange, enabling the distribution of models and views over a network using OSC and Minuit. While this paper demonstrates key principles via simple patchers, a more elaborate demonstration of MVC separation in Max is provided in [1].		Trond Lossius;Théo de la Hogue;Pascal Baltazar;Timothy A. Place;Nathan Wolek;Julien Rabin	2014			real-time computing;simulation;system model;computer science;engineering drawing	Visualization	-38.38372243997444	37.533824810858015	811
30ccf0fd0fafd5bf08367724a0462a3bd1f7cedc	choosing between terminal and independently based gain and offset error in the adc histogram test	histograms;gain and offset error estimation;adc testing;additive noise estimation histograms uncertainty monte carlo methods bars;monte carlo method mcm;uncertainty;precision of the estimators additive noise analog to digital converter adc gain and offset error estimation histogram method monte carlo method mcm;additive noise;measurement uncertainty;precision of the estimators;error analysis;estimation;analog to digital converter adc;measurement uncertainty analogue digital conversion error analysis;analogue digital conversion;monte carlo method;histogram method;analog to digital converter;experimental evaluation;error estimate;bars;histogram method gain error adc histogram test analog voltage analog to digital converter digital output adc gain high accuracy measurement uncertainty offset error estimator;monte carlo methods	To recover the analog voltage at the input of an analog-to-digital converter (ADC) from its digital output, one needs to know at least the ADC gain and offset. For high-accuracy measurements, it is necessary to estimate the actual values of these parameters since they are usually different from the ideal values (one and zero, respectively). This estimation inevitably has an uncertainty, which contributes to the uncertainty of any measurement made with the ADC. Here, the precision of gain and offset error estimators, based on the histogram method for ADC testing is analyzed. The “terminal based” and “independently based” definitions are compared, both through simulation and experimental evaluation. Our conclusion is that, in typical conditions, the “independently based” definition is more precise.	analog-to-digital converter;simulation	Francisco André Corrêa Alegria;Hugo Silva	2012	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2161014	econometrics;electronic engineering;successive approximation adc;computer science;mathematics;integrating adc;statistics;monte carlo method	Visualization	68.55420365816175	50.969059923471114	812
ad20e74081a4255e00023fbb9098c8c32d126be0	modeling of mos transistors based on genetic algorithm and simulated annealing	analytical models;pmos;mos devices;ga;mosfets;bsim3 model;circuit simulator mos transistor modeling genetic algorithm simulated annealing metal oxide semiconductor transistors ga heuristic elements mos i v characteristic pmos nmos bsim3 model;simulated annealing;satisfiability;metal oxide semiconductor transistors;heuristic elements;i v characteristic;circuit simulation;nmos;mosfets genetic algorithms simulated annealing circuit simulation semiconductor device modeling predictive models laboratories mos devices analytical models spice;semiconductor device modeling;mos i v characteristic;circuit simulation simulated annealing genetic algorithms mosfet;circuit simulator;genetic algorithm;predictive models;genetic algorithms;mosfet;spice;mos transistor modeling;power modeling;metal oxide semiconductor;compact model	A novel method to extract an efficient model for metal-oxide-semiconductor (MOS) transistors in order to satisfy a specific accuracy is presented. The approach presented here utilizes a genetic algorithm (GA) to choose the necessary physical and heuristic elements in order to define a compact yet accurate model for MOS I-V characteristic. Then the values of the free parameters related to each element are determined using simulated annealing (SA). For a desired accuracy considered here, the accuracy of the results predicted by our model were within 3.1%, for PMOS, and 1.3%, for NMOS, of the results of BSIM3 model while having much less complexity compared to the BSIM3 model. When this model with a variable accuracy is implemented in a circuit simulator, it provides the freedom of making a selection between the time and the accuracy of the simulation.	electronic circuit simulation;genetic algorithm;heuristic;nmos logic;pmos logic;spice;semiconductor;simulated annealing;transistor;v-model	Mohammad Taherzadeh-Sani;Ali Abbasian;Behnam Amelifard;Ali Afzali-Kusha	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1466061	electronic engineering;simulation;genetic algorithm;computer science;engineering;electrical engineering;machine learning	Embedded	27.27927636151376	49.64392127721684	813
b812ecaf9018c7cc6852a48e582b8b0255ff7478	csmetapred: a consensus method for prediction of catalytic residues	active site residues;catalytic residue prediction;meta-approach	Knowledge of catalytic residues can play an essential role in elucidating mechanistic details of an enzyme. However, experimental identification of catalytic residues is a tedious and time-consuming task, which can be expedited by computational predictions. Despite significant development in active-site prediction methods, one of the remaining issues is ranked positions of putative catalytic residues among all ranked residues. In order to improve ranking of catalytic residues and their prediction accuracy, we have developed a meta-approach based method CSmetaPred. In this approach, residues are ranked based on the mean of normalized residue scores derived from four well-known catalytic residue predictors. The mean residue score of CSmetaPred is combined with predicted pocket information to improve prediction performance in meta-predictor, CSmetaPred_poc. Both meta-predictors are evaluated on two comprehensive benchmark datasets and three legacy datasets using Receiver Operating Characteristic (ROC) and Precision Recall (PR) curves. The visual and quantitative analysis of ROC and PR curves shows that meta-predictors outperform their constituent methods and CSmetaPred_poc is the best of evaluated methods. For instance, on CSAMAC dataset CSmetaPred_poc (CSmetaPred) achieves highest Mean Average Specificity (MAS), a scalar measure for ROC curve, of 0.97 (0.96). Importantly, median predicted rank of catalytic residues is the lowest (best) for CSmetaPred_poc. Considering residues ranked ≤20 classified as true positive in binary classification, CSmetaPred_poc achieves prediction accuracy of 0.94 on CSAMAC dataset. Moreover, on the same dataset CSmetaPred_poc predicts all catalytic residues within top 20 ranks for ~73% of enzymes. Furthermore, benchmarking of prediction on comparative modelled structures showed that models result in better prediction than only sequence based predictions. These analyses suggest that CSmetaPred_poc is able to rank putative catalytic residues at lower (better) ranked positions, which can facilitate and expedite their experimental characterization. The benchmarking studies showed that employing meta-approach in combining residue-level scores derived from well-known catalytic residue predictors can improve prediction accuracy as well as provide improved ranked positions of known catalytic residues. Hence, such predictions can assist experimentalist to prioritize residues for mutational studies in their efforts to characterize catalytic residues. Both meta-predictors are available as webserver at: http://14.139.227.206/csmetapred/ .	3-iodobenzylguanidine;benchmark (computing);binary classification;computation;expedited report;experiment;kerrison predictor;meta;receiver operator characteristics;receiver operating characteristic;sensitivity and specificity;silo (dataset);web server	Preeti Choudhary;Shailesh Kumar;Anand Kumar Bachhawat;Shashi Bhushan Pandit	2017		10.1186/s12859-017-1987-z	biology;residue (complex analysis);bioinformatics;catalysis	Comp.	10.433821703937992	-56.50092259488086	814
887f0db1f07cedf0b2f5e93afe723c074d7595b4	deep layer aggregation		"""Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been """"shallow"""" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes."""		Fisher Yu;Dequan Wang;Trevor Darrell	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00255	branching (version control);artificial intelligence;machine learning;merge (version control);computer science;hierarchy	Vision	25.173643253675692	-52.35885399341548	815
19ff1b024df539ea7aba640d4ad45cb309768610	task-driven adaptive statistical compressive sensing of gaussian mixture models	protocols;compressed sensing;gaussian processes;task driven sensing adaptive compressive sensing classification gaussian mixture models mutual information reconstruction sequential hypothesis testing;journal article;signal reconstruction compressed sensing computational complexity gaussian processes information theory protocols signal classification;computational complexity;signal classification;signal reconstruction;sensors adaptation models dictionaries image reconstruction compressed sensing computational modeling;computational complexity task driven adaptive statistical compressive sensing gaussian mixture models standard sparsity model task specific sensing protocols signal classification signal reconstruction information theory landsat satellite attributes;information theory	A framework for adaptive and non-adaptive statistical compressive sensing is developed, where a statistical model replaces the standard sparsity model of classical compressive sensing. We propose within this framework optimal task-specific sensing protocols specifically and jointly designed for classification and reconstruction. A two-step adaptive sensing paradigm is developed, where online sensing is applied to detect the signal class in the first step, followed by a reconstruction step adapted to the detected class and the observed samples. The approach is based on information theory, here tailored for Gaussian mixture models (GMMs), where an information-theoretic objective relationship between the sensed signals and a representation of the specific task of interest is maximized. Experimental results using synthetic signals, Landsat satellite attributes, and natural images of different sizes and with different noise levels show the improvements achieved using the proposed framework when compared to more standard sensing protocols. The underlying formulation can be applied beyond GMMs, at the price of higher mathematical and computational complexity.	compressed sensing;computational complexity theory;information theory;mixture model;programming paradigm;sparse matrix;statistical model;synthetic intelligence	Julio Martin Duarte-Carvajalino;Guoshen Yu;Lawrence Carin;Guillermo Sapiro	2013	IEEE Transactions on Signal Processing	10.1109/TSP.2012.2225054	signal reconstruction;communications protocol;information theory;computer science;machine learning;pattern recognition;data mining;gaussian process;mathematics;computational complexity theory;compressed sensing;statistics	Vision	61.874158666159275	-72.30567784292904	816
21359e851656222447bb92818fd0283388d366fc	a real time attachment free, psycho physiological stress and heart rate measurement system	human computer interaction;biofeedback systems;galvanic skin response;psychosomatic condition;skin conductance;intelligent tutoring systems;biomedical instruments;emotion detection;affective computing	The challenges in the development of a system performing real time detection of physiological parameters are fundamentally aversive because of the incommodities caused by the wires and sensing attachments onto the user, making the measurement sessions uncomfortable. Another factor is that the sensing accessories influence the plausibility of the measurements. In this paper, the authors introduce a system based on a device that can acquire physiological signals from a computer user with no prerequisites, postural, kinetic, or other constraints in the environment of normal usage of the home computer for the detection of their psychosomatic state and optimally their affect and emotional responses. The authors also discuss issues that could otherwise compromise the credibility of the results. Redundancy and special adaptive and corrective algorithms have been developed to improve reliability and achieve acceptable standards of quality. Measurements include skin conductance (SC) and heart rate (HR) detected by sensors positioned on the vertical sides of a computer mouse. The system is intended for interactive educational environments, during assessment, e-learning, psychosomatic user profiling, mobile and web based interfaces, and for Human Computer Interaction (HCI) platforms.	algorithm;artificial skin;attachments;computer mouse;conductance (graph);human computer;human–computer interaction;plausibility structure;profiling (computer programming);sensor;system of measurement;user (computing)	Anthony Psaltis;Costas Mourlas	2011	IJMTIE	10.4018/ijmtie.2011040101	simulation;engineering;multimedia;communication	HCI	-50.04811837063179	-46.09395477529844	817
1529809495fa0991f0755938b76cfbb706048194	shortest paths in the plane with polygonal obstacles	camino mas corto;shortest path;movimiento;mover s problem;algorithm performance;diagramme voronoi;algorithm analysis;espace euclidien;cepillado;rabotage;espacio euclidiano;robotics;motion;planing;polygonal obstacles;euclidean plane;triangulacion;resultado algoritmo;mouvement;performance algorithme;chemin plus court;motion planning;robotica;euclidean space;analyse algorithme;triangulation;robotique;connected component;diagrama voronoi;data structure;analisis algoritmo;convex polygon;minimal movement problem;voronoi diagram	We present a practical algorithm for finding minimum-length paths between points in the Euclidean plane with (not necessarily convex) polygonal obstacles. Prior to this work, the best known algorithm for finding the shortest path between two points in the plane required <italic>&OHgr;(n<supscrpt>2</supscrpt></italic> log <italic>n)</italic> time and <italic>O</italic>(n<supscrpt>2</supscrpt>) space, where <italic>n</italic> denotes the number of obstacle edges. Assuming that a triangulation or a Voronoi diagram for the obstacle space is provided with the input (if is not, either one can be precomputed in <italic>O</italic>(<italic>n</italic> log <italic>n)</italic> time), we present an <italic>O(kn)</italic> time algorithm, where <italic>k</italic> denotes the number of  “islands”  (connected components) in the obstacle space. The algorithm uses only <italic>O(n)</italic> space and, given a source point <italic>s</italic>, produces an <italic>O(n)</italic> size data structure such that the distance between <italic>s</italic> and any other point <italic>x</italic> in the plane (<italic>x</italic>) is not necessarily an obstacle vertex or a point on an obstacle edge) can be computed in <italic>O</italic>(1) time. The algorithm can also be used to compute shortest paths for the movement of a disk (so that optimal movement for arbitrary objects can be computed to the accuracy of enclosing them with the smallest possible disk).	algorithm;connected component (graph theory);data structure;precomputation;shortest path problem;voronoi diagram	James A. Storer;John H. Reif	1994	J. ACM	10.1145/185675.185795	planning;mathematical optimization;combinatorics;connected component;topology;voronoi diagram;data structure;triangulation;computer science;euclidean space;motion;mathematics;geometry;motion planning;shortest path problem;robotics	Theory	30.620846962544263	18.26541865743754	818
75a5a0dd92802e319befaf23e2adb9225fe0d2d6	marking text features of document images to deter illicit dissemination	libraries;line shift coding;image storage;impedance;image coding;decoding;document images;copyright;text analysis;word shift coding;photocopying text feature marking document images illicit dissemination deterrence copyright feature coding techniques document feature analysis techniques line shift coding word shift coding character coding;computer displays;photocopying;text feature marking;feature coding techniques;electronic publishing;decoding electronic publishing data security image coding costs computer displays libraries image storage impedance text analysis;feature analysis;document feature analysis techniques;character coding;security of data;illicit dissemination deterrence;data security	A major impediment to the widespread adoption of services for electronic distribution of copyrighted material is the ease with which illicit copies can be made and disseminated. In this paper, we describe feature coding techniques to mark document images with codes that are indiscernible by readers but can be decoded by document feature analysis techniques. These codes can be used to trace the source of errant documents. We propose three coding methods: line-shift coding, word-shift coding, and character coding. We describe how the coding features are found, inserted, and decoded. Finally, we show preliminary experimental results indicating robustness of the line-shift coding method where decoding can be performed even from images of up to ten generations of photocopying.		Jack Brassil;Steven H. Low;Nicholas F. Maxemchuk;Lawrence O'Gorman	1994		10.1109/ICPR.1994.576927	pattern recognition;speech recognition;computer science;electrical impedance;data security;electronic publishing;world wide web;information retrieval	NLP	39.558244608340836	-14.952925949794421	819
0c4eeda6c29ea16714462d65b31530048e293912	accurate prior modeling in the locally adaptive window-based wavelet denoising		The locally adaptive window-based (LAW) denoising method has been extensively studied in literature for its simplicity and effectiveness. However, our statistical analysis performed on its prior estimation reveals that the prior is not estimated properly. In this paper, a novel maximum likelihood prior modeling method is proposed for better characterization of the local variance distribution. Goodness of fit results shows that our proposed prior estimation method can improve the model accuracy. A modified LAW denoising algorithm is then proposed based on the new prior. Image denoising experimental results demonstrate that the proposed method can significantly improve the performance in terms of both peak signal-to noise ratio (PSNR) and visual quality, while maintain a low computation.	noise reduction;wavelet	Yun-Xia Liu;Yang Yang;Ngai-Fong Law	2016		10.1007/978-3-319-42294-7_47	wavelet packet decomposition	Vision	57.86710477692089	-68.04474042861581	820
78f222cec2d946106918bb2004a81e8006a5e755	crime analysis through spatial areal aggregated density patterns	computadora;tratamiento datos;computers;distribucion espacial;densite;exposition;ordinateur;density tracing;data processing;traitement donnee;densidad;crime analysis;modelo;spatial distribution;modele;monde;density;mundo;distribution spatiale;areal aggregated data;exhibits;models;global	Intelligent crime analysis allows for a greater understanding of the dynamics of unlawful activities, providing possible answers to where, when and why certain crimes are likely to happen. We propose to model density change among spatial regions using a density tracing based approach that enables reasoning about large areal aggregated crime datasets. We discover patterns among datasets by finding those crime and spatial features that exhibit similar spatial distributions by measuring the dissimilarity of their density traces. The proposed system incorporates both localized clusters (through the use of context sensitive weighting and clustering) and the global distribution trend. Experimental results validate and demonstrate the robustness of our approach.		Peter Phillips;Ickjai Lee	2011	GeoInformatica	10.1007/s10707-010-0116-1	econometrics;data processing;geography;density;computer science;database;cartography	Vision	-4.9572768295487375	-31.68927303531674	821
e563397e4fa46db6fd8c3825f3258fb5f813a658	permanence and extinction of regime-switching predator-prey models	extinction;predator prey model;60h10;regime switching diffusion;permanence;92d25;60j60	In this work we study the permanence and extinction of a regime-switching predatorprey model with the Beddington–DeAngelis functional response. The switching process is used to describe the random changes of corresponding parameters such as birth and death rates of a species in different environments. When a prey will die out in some fixed environments and will not in others, our criteria can justify whether it dies out in a random switching environment. Our criteria are rather sharp, and they cover the known on-off type results on permanence of predator-prey models without switching. Our method relies on the recent study of ergodicity of regime-switching diffusion processes.	ergodicity;functional response;lotka–volterra equations;prey;switch	Jianhai Bao;Jinghai Shao	2016	SIAM J. Math. Analysis	10.1137/15M1024512	extinction;object permanence	Theory	78.12725490565497	6.308439616425121	822
8cbddda1ddec2640108f4ea4403ee3799479d20c	sparse signal recovery with unknown signal sparsity	signal image and speech processing;quantum information technology spintronics	In this paper, we proposed a detection-based orthogonal match pursuit (DOMP) algorithm for compressive sensing. Unlike the conventional greedy algorithm, our proposed algorithm does not rely on the priori knowledge of the signal sparsity, which may not be known for some application, e.g., sparse multipath channel estimation. The DOMP runs binary hypothesis on the residual vector of OMP at each iteration, and it stops iteration when there is no signal component in the residual vector. Numerical experiments show the effectiveness of the estimation of signal sparsity as well as the signal recovery of our proposed algorithm.	channel state information;compressed sensing;detection theory;experiment;greedy algorithm;iteration;multipath propagation;openmp;sparse matrix	Wenhui Xiong;Jin Cao;Shaoqian Li	2014	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2014-178	computer science;theoretical computer science;machine learning;pattern recognition;compressed sensing	ML	54.115221200412805	6.815420695279206	823
5843f84a30cec64a847d0ed41318e7797d404ddd	ein lexikographischer suchalgorithmus zur lösung allgemeiner ganzzahliger programmierungsaufgaben	branch and bound	In dieser Arbeit wird ein slexikographischer Suchalgorithmus zur Losung von allgemeinen diskreten Optimierungsaufgaben, bei denen Zielfunktion und Restriktionen beliebige Funktionen sein konnen, angegeben. Es handelt sich hierbei um ein spezielles Branch-and-Bound Verfahren, bei welchem die ubliche Schrankenbedingung (Bound) nicht als Auswahlkriterium, sondern nur als Verwerfkriterium benutzt wird. Die Auswahlstrategie ist lexikographisch. Die Anwendbarkeit des Verfahrens auf ganzzahlige und gemischt-ganzzahlige Programmierungsprobleme und auf besondere Spezialfalle sowie auf diophantische Gleichungs- und Ungleichungssysteme wird diskutiert. Auserdem werden alle anderen bekannten Verfahren zur ganzzahligen Programmierung kurz erwahnt und deren Rechenzeiten und Effizienz mit den entsprechenden Daten des Suchalgorithmus an Hand von zahlreichen Testbeispielen verglichen.		Wilhelm Krelle;Bernhard Korte;Walter Oberhofer	1970	Unternehmensforschung	10.1007/BF01918269	mathematical optimization;mathematics;branch and bound	NLP	-98.46935160335266	35.50831956769578	824
9784ec739068617d1e91a0e46f449eafd758ce19	innovativeness, empowerment and it capability: evidence from smes	it capability;information technology;poland;innovativeness;small to medium sized enterprises;employee empowerment;firm performance	Purpose – The purpose of this paper is to explore two basic research questions: what are the effects of information technology (IT) capability and employee empowerment on the innovativeness of small to medium‐sized enterprises (SMEs), and what are the effects of innovativeness and IT capability on firm performance in SMEs?Design/methodology/approach – Data from 109 Polish SMEs were collected. In order to identify empirical dimensions of innovativeness, empowerment and IT capability, a factor analysis was carried out. Multiple regression analysis was employed to examine the effects of IT capability and employee empowerment on innovativeness, and the effects of IT capability and innovativeness on firm performance.Findings – The following results are offered: innovation activity of SMEs is positively related to technological turbulence, climate for innovation, investments in innovation and use of IT in internal communications; innovation activity and IT knowledge have a positive effect on subjective measures...		Roman Kmieciak;Anna Michna;Anna Meczynska	2012	Industrial Management and Data Systems	10.1108/02635571211232280	marketing;law;information technology;commerce	Robotics	-82.67901567347725	3.2078768141596354	825
a5e9100e1bedfdf088a9ef6f167a8540151a17b1	validating denial of service vulnerabilities in web services	service oriented architecture service denial vulnerability web service service attack memory exhaustion cpu time exhaustion xml parser;cpu time exhaustion;cpu utilization;storage management;computer crime;xml computer crime simple object access protocol servers monitoring;web service;web services vulnerabilities;denial of service attack;memory utilization;software architecture;servers;service attack;monitoring;denial of service;web services;xml message authentication software architecture storage management web services;xml;memory exhaustion;xml vulnerabilities;xml parser;message authentication;service oriented architecture;simple object access protocol;service denial vulnerability;cpu utilization web services service oriented architecture denial of service xml parser xml vulnerabilities web services vulnerabilities memory utilization	The loosely-coupled and dynamic nature of web services architectures has many benefits, but also leads to an increased vulnerability to denial of service attacks. While many papers have surveyed and described these vulnerabilities, they are often theoretical and lack experimental data to validate them, and assume an obsolete state of web services technologies. This paper describes experiments involving several denial of service vulnerabilities in well-known web services platforms, including Java Metro, Apache Axis, and Microsoft.NET. The results both confirm and deny the presence of some of the most well-known vulnerabilities in web services technologies. Specifically, major web services platforms appear to cope well with attacks that target memory exhaustion. However, attacks targeting CPU-time exhaustion are still effective, regardless of the victim’s platform.	apache axis;central processing unit;denial-of-service attack;experiment;high memory;java;vulnerability (computing);web service;xml	Suriadi Suriadi;Andrew J. Clark;Desmond Allan Schmidt	2010	2010 Fourth International Conference on Network and System Security	10.1109/NSS.2010.41	web service;xml;computer science;ws-policy;database;world wide web;computer security;denial-of-service attack	SE	-53.97893014580915	58.98513928081188	826
7e01e21dcfb41020b4ab10cbbb7132d161195177	knowledge-based relevance filtering for efficient system-level test-based model generation	second order;experimental analysis;model generation;power optimization;expert knowledge;knowledge base	Test-based model generation by classical automata learning is very expensive. It requires an impractically large number of queries to the system, each of which must be implemented as a system-level test case. Key in the tractability of observation-based model generation are powerful optimizations exploiting different kinds of expert knowledge in order to drastically reduce the number of required queries, and thus the testing effort. In this paper, we present a thorough experimental analysis of the second-order effects between such optimizations in order to maximize their combined impact.	automata theory;automaton;experiment;extrapolation;mathematical optimization;maximal set;mealy machine;procedural generation;relevance;speedup;test case;test suite;workbench	Tiziana Margaria;Harald Raffelt;Bernhard Steffen	2005	Innovations in Systems and Software Engineering	10.1007/s11334-005-0016-y	knowledge base;computer science;artificial intelligence;machine learning;data mining;power optimization;second-order logic;experimental analysis of behavior	SE	-10.392818982828224	29.114178065210623	827
72fe7370a77c29aa23e412216f518faee4171d7b	routine- und ausnahmebetrieb im mobilen kontext des rettungsdienstes	routinebetrieb;gebrauchstauglichkeit;rettungsdienst;muc langbeitrag vortrage;mobile computing;ausnahmebetrieb	Mobile computerbasierte Dokumentationsund Informationssysteme können die Arbeit von Notärzten und Rettungsfachpersonal unterstützen und vereinfachen. Jedoch stellt der Nutzungskontext Rettungsdienst aufgrund seines mobilen, sicherheitskritischen und komplexen Charakters besondere Anforderungen an die Gebrauchstauglichkeit der Anwendungssysteme. Eine spezielle Herausforderung ist die Realisierung einer durchgängigen und konsistenten Systemunterstützung der Rettungskräfte vom täglichen Routinebetrieb bei Krankentransporten und Notfalleinsätzen bis zum seltenen Ausnahmebetrieb bei Massenanfällen von Verletzten (MANV). In diesem Beitrag werden sowohl der Entwicklungsprozess als auch die Ergebnisse eines auf die aufgabenangemessene und benutzergerechte Gestaltung der Benutzungsschnittstelle fokussierten Projektes beschrieben.	eine and zwei	Tilo Mentler;Michael Herczeg	2013			internet privacy;mobile computing;computer science	AI	-103.65680712113678	37.18054463344958	828
5ec8f761ee1dcec04d56479686db879a46c97deb	a clustering-oriented closeness measure based on neighborhood chain and its application in the clustering ensemble framework based on the fusion of different closeness measures	closeness measure;clustering;clustering ensemble;geometric distance;neighborhood chain	"""Closeness measures are crucial to clustering methods. In most traditional clustering methods, the closeness between data points or clusters is measured by the geometric distance alone. These metrics quantify the closeness only based on the concerned data points' positions in the feature space, and they might cause problems when dealing with clustering tasks having arbitrary clusters shapes and different clusters densities. In this paper, we first propose a novel Closeness Measure between data points based on the Neighborhood Chain (CMNC). Instead of using geometric distances alone, CMNC measures the closeness between data points by quantifying the difficulty for one data point to reach another through a chain of neighbors. Furthermore, based on CMNC, we also propose a clustering ensemble framework that combines CMNC and geometric-distance-based closeness measures together in order to utilize both of their advantages. In this framework, the """"bad data points"""" that are hard to cluster correctly are identified; then different closeness measures are applied to different types of data points to get the unified clustering results. With the fusion of different closeness measures, the framework can get not only better clustering results in complicated clustering tasks, but also higher efficiency."""	algorithm;algorithmic efficiency;centrality;cluster analysis;computation;computational complexity theory;data point;euclidean distance;feature vector;layer (electronics);numerous;run time (program lifecycle phase);density;statistical cluster	Shaoyi Liang;Deqiang Han	2017		10.3390/s17102226	engineering;correlation clustering;cluster analysis;complete-linkage clustering;random walk closeness centrality;fuzzy clustering;machine learning;feature vector;single-linkage clustering;closeness;artificial intelligence;pattern recognition	ML	1.204391027737405	-41.65020879688954	829
53e25d4eed674b38f4dcf6be5ca50bf2578f7d3e	a dynamic adaptive acknowledgment strategy for tcp over multihop wireless networks	end to end bandwidth utilization;wireless networks;media access protocol;degradation;wireless channels;data and acknowledgment packet;mac protocol;wireless application protocol;tcp;ad hoc network;spread spectrum communication wireless networks media access protocol bandwidth transport protocols wireless application protocol ad hoc networks degradation access protocols performance loss;spurious retransmission;wireless channel;transport protocols;spread spectrum communication;congestion control;hidden node problem;congestion control mechanism;access protocols;bandwidth;ad hoc networks;dynamic adaptive acknowledgment strategy;telecommunication channels transport protocols wireless lan;wireless lan;power consumption;telecommunication channels;dynamic adaptation;power consumption dynamic adaptive acknowledgment strategy tcp multihop wireless network mac protocol ad hoc network hidden node problem end to end bandwidth utilization congestion control mechanism data and acknowledgment packet spurious retransmission wireless channel;performance loss;multihop wireless network	Multihop wireless networks based on the IEEE 802.11 MAC protocol are promising for ad hoc networks in small scale today. The 802.11 protocol minimizes the well-known hidden node problem but does not eliminate it completely. Consequently, the end-to-end bandwidth utilization may be quite poor if the involved protocols do not interact smoothly. In particular, the TCP protocol does not manage to obtain efficient bandwidth utilization because its congestion control mechanism is not tailored to such a complex environment. The main problems with TCP in such networks are the excessive amount of both spurious retransmissions and contention between data and acknowledgment (ACK) packets for the transmission medium. In this paper, we propose a dynamic adaptive strategy for minimizing the number of ACK packets in transit and mitigating spurious retransmissions. Using this strategy, the receiver adjusts itself to the wireless channel condition by delaying more ACK packets when the channel is in good condition and less otherwise. Our technique not only improves bandwidth utilization but also reduces power consumption by retransmitting much less than a regular TCP does. Extensive simulation evaluations show that our scheme provides very good enhancements in a variety of scenarios.	acknowledgement (data networks);acknowledgment index;end-to-end principle;hoc (programming language);network congestion;simulation;smoothing;whole earth 'lectronic link	Ruy de Oliveira;Torsten Braun	2005	Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.	10.1109/INFCOM.2005.1498465	wireless ad hoc network;tcp delayed acknowledgment;tcp westwood plus;telecommunications;computer science;distributed computing;zeta-tcp;tcp acceleration;computer network	Mobile	-2.450982060105508	91.97260196552044	830
7edb696497c02e3e1086b5f11127e7a20ef280af	automated identification of best-quality coronary artery segments from multiple-phase coronary ct angiography (ccta) for vessel analysis	coronary arteries;image segmentation;performance evaluation;arteries;angiography;computer aided detection;curved planar reformation;vessel segmentation;vessel tracking	We are developing an automated method to identify the best quality segment among the corresponding segments in multiple-phase cCTA. The coronary artery trees are automatically extracted from different cCTA phases using our multi-scale vessel segmentation and tracking method. An automated registration method is then used to align the multiple-phase artery trees. The corresponding coronary artery segments are identified in the registered vessel trees and are straightened by curved planar reformation (CPR). Four features are extracted from each segment in each phase as quality indicators in the original CT volume and the straightened CPR volume. Each quality indicator is used as a voting classifier to vote the corresponding segments. A newly designed weighted voting ensemble (WVE) classifier is finally used to determine the best-quality coronary segment. An observer preference study is conducted with three readers to visually rate the quality of the vessels in 1 to 6 rankings. Six and 10 cCTA cases are used as training and test set in this preliminary study. For the 10 test cases, the agreement between automatically identified best-quality (AI-BQ) segments and radiologist’s top 2 rankings is 79.7%, and between AI-BQ and the other two readers are 74.8% and 83.7%, respectively. The results demonstrated that the performance of our automated method was comparable to those of experienced readers for identification of the best-quality coronary segments. © (2016) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	ct scan;computed tomography angiography	Chuan Zhou;Heang-Ping Chan;Lubomir M. Hadjiiski;Aamer Chughtai;Jun Wei;Ella A. Kazerooni	2016		10.1117/12.2217261	computer vision;image segmentation	Logic	38.84619298932603	-81.05886322924044	831
69f310e36cc7a430c12d3a295f8739805d97fc74	cost trade-offs in system on chip designs	manufacturing cost;technology migration;cost trade offs;integrated circuit economics;test cost;integrated circuit design;costs system on a chip manufacturing radio frequency random access memory packaging graphics logic design for manufacture design for testability;system on chip;total system costs cost trade offs system on chip designs test costs design complexity soc designs system level cost trade offs die level cost trade offs;integrated circuit testing;vlsi;vlsi integrated circuit design integrated circuit economics integrated circuit testing	Advances in technology have led to a drive towards System on Chip (SoC) designs. However, manufacturing and test costs have increased as rapidly as design complexity. Hence, in order to produce SoC designs at reasonable cost, both system-level and die-level cost trade-offs must be made. This paper illustrates the methodologies used in analyzing such trade-offs. Examples in the paper indicate that using advanced technologies to manufacture SoC designs may sometimes be detrimental in terms of total system costs.	semiconductor fabrication plant;system on a chip;systems architecture	Jitendra Khare;Hans T. Heineken;M. d'Abreu	2000		10.1109/ICVD.2000.812606	system on a chip;embedded system;electronic engineering;computer science;engineering;electrical engineering;very-large-scale integration;integrated circuit design	EDA	10.450299733938284	55.06686839775958	832
77db77dd761d8a4483befbac635cf5d26d2d7f25	the implementation of objectmath - a high-level programming environment for scientific computing	symbolic computation;object oriented model;elektroteknik och elektronik;programming environment;electrical engineering electronic engineering information engineering;computer algebra system;design and implementation;scientific computing;modeling and analysis	We present the design and implementation of ObjectMath, a language and environment for high-level equation-based modeling and analysis in scientific computing. The ObjectMath language integrates object-oriented modeling with mathematical language features that make it possible to express mathematics in a natural and consistent way. The implemented programming environment includes a graphical browser for visualizing and editing inheritance hierarchies, an application oriented editor for editing ObjectMath equations and formulae, a computer algebra system for doing symbolic computations, support for generation of numerical code from equations, and routines for graphical presentation. This programming environment has been successfully used in modeling and analyzing two different problems from the application domain of machine element analysis in an industrial environment.	application domain;computation;computational science;computer algebra system;graphical user interface;high- and low-level;high-level programming language;integrated development environment;interpretation (logic);numerical analysis;procedural programming;prototype	Lars Viklund;Johan Herber;Peter Fritzson	1992		10.1007/3-540-55984-1_28	computational science;computing;symbolic computation;computer science;theoretical computer science	HPC	-34.45023753563682	26.34076932751116	833
931a0646b8c114b7d87ef8da491c0ec5331d568f	multi-modal measurement of the myelin-to-axon diameter g-ratio in preterm-born neonates and adult controls	sensitivity and specificity;female;extremely premature;brain;middle aged;axons;male;myelin sheath;infant;newborn;image interpretation;adult;multimodal imaging;reproducibility of results;humans;computer assisted;young adult;diffusion tensor imaging	Infants born prematurely are at increased risk of adverse functional outcome. The measurement of white matter tissue composition and structure can help predict functional performance and this motivates the search for new multi-modal imaging biomarkers. In this work we develop a novel combined biomarker from diffusion MRI and multi-component T2 relaxation measurements in a group of infants born very preterm and scanned between 30 and 40 weeks equivalent gestational age. We also investigate this biomarker on a group of seven adult controls, using a multi-modal joint model-fitting strategy. The proposed emergent biomarker is tentatively related to axonal energetic efficiency (in terms of axonal membrane charge storage) and conduction velocity and is thus linked to the tissue electrical properties, giving it a good theoretical justification as a predictive measurement of functional outcome.	axon;biological markers;emergence;infant, newborn;linear programming relaxation;modal logic;numerous;scanning;velocity (software development);white matter;viral capsid secondary envelopment	Andrew Melbourne;Zach Eaton-Rosen;Enrico De Vita;Alan Bainbridge;Manuel Jorge Cardoso;David Price;Ernest Cady;Giles S. Kendall;Nicola J. Robertson;Neil Marlow;Sébastien Ourselin	2014	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-10470-6_34	diffusion mri;pathology;young adult;surgery	Robotics	20.72831003247959	-81.20916542867786	834
332296bc8efb3a896a34689d26e1d98a20a38d7c	intelligent decision support: a fuzzy stock ranking system	intelligent decision support system;decision support;trade volume;forecasting model;fuzzy rule base;information market;information processing;portfolio management;prediction model;business intelligence	This paper presents an intelligent decision support system for financial portfolio management. An adaptive business intelligence approach combines optimization, forecasting and adaptation with application specific financial information processing and quantitative investment paradigms.#R##N##R##N#The methodology involves constructing a ranking of stocks by strength of a buy or sell recommendation which is inferred using an adapting forecasting model that considers a range of factors. These include company balance sheet information, market price and trading volume as well as the wider economy. The system adjusts its prediction model dynamically as market conditions change. An evolving fuzzy rule base mechanism encodes a model of relationships between model factors and a recommendation to buy, sell or hold securities.		Adam Ghandar;Zbigniew Michalewicz;Ralf Zurbruegg	2009		10.1007/978-3-642-04735-0_16	actuarial science;decision support system;intelligent decision support system;marketing;business;commerce	HCI	4.572973507002003	-17.69182204841132	835
3539b1a029d1f2a4f20dcac5e672e24c09634e7e	pattern recognition problems in bubble chamber physics	pattern recognition	Abstract The data extraction process for bubble chamber film is described from a pattern recognition viewpoint. Major problem areas and semi-automatic measuring devices are discussed.	pattern recognition	Richard M. Brown	1969	Pattern Recognition	10.1016/0031-3203(69)90011-9	computer vision;simulation;computer hardware;computer science	Vision	85.76691198698326	-23.858207311024522	836
7cb53605bab585df3efc306d555560e4a80d6411	metaproperty aspects	metaproperties;aspect oriented programming	Currently, it is possible to use Aspect-Oriented languages to attach behavior to code based on semantic or syntactic properties of that code. There is no language, however, that allows developers to attach behavior based on static metaproperties of code. Here, we demonstrate a technique for applying AOP methods to metaproperties of source code. We use advice to coherently define runtime behavior for subsets of code that need not share semantic or syntactic properties. To illustrate the approach, we use Java as a base language, and define a family of pointcuts based on the edit time of the source lines, then build a simple debugging application that applies runtime tracing to only the most recently changed code. Using this technique, the tracing code is neatly modularized and need not depend on any semantic properties of the base code. We believe that this approach has powerful applications for debugging as well as for software engineering researchers looking to explore the runtime effects of extra-linguistic features.	aspect-oriented programming;code reuse;debugging;eclipse;java;pointcut;program lifecycle phase;software engineering	Clayton G. Myers;Elisa L. A. Baniassad	2009		10.1145/1509239.1509273	compile time;real-time computing;aspect-oriented programming;object code;computer science;theoretical computer science;dead code elimination;programming language;code generation;program animation;source code	SE	-26.404873390676617	29.61405069839917	837
c41b426aec6d66b5937c0d499aaef32fddd29770	fusion4d: 4d unencumbered direct manipulation and visualization	human computer interaction;human machine interfaces fusion4d 4d unencumbered direct manipulation 4d unencumbered direct visualization holographic interactive displays fourth data dependent dimension user testing visualization technology;user interfaces data visualisation interactive devices solid modelling;data visualisation;three dimensional displays visualization solid modeling navigation mice analytical models augmented reality;augmented reality;user interfaces;solid modelling;interactive devices;augmented reality human computer interaction	It is possible to predict that sometime in the future, holographic interactive displays will be available as household commodities, requiring new interaction techniques. Fusion4D is our proposal for unencumbered direct manipulation interfaces involving three dimensions in physical space, as well as a fourth data-dependent dimension (such as time-varying information of an object.) A proof-of-concept prototype has been developed and subjected to user testing. The results of the tests indicate that there are some points for improvement, such as the visualization technology, but the system is well accepted by users.	3d interaction;avatar (computing);data dependency;direct manipulation interface;discoverability;holographic display;holography;interaction technique;prototype;timeline;usability testing;virtual reality	Roberto Sonnino;Keila Keiko Matsumura;João Luiz Bernardes;Ricardo Nakamura;Romero Tori	2013	2013 XV Symposium on Virtual and Augmented Reality	10.1109/SVR.2013.40	information visualization;interactive visualization;human–computer interaction;computer science;multimedia;computer graphics (images)	Visualization	-44.860635236558785	-40.45198851938536	838
85dcca362d646d95c10f9b55fbd80b350c884519	uppaal/dmc- abstraction-based heuristics for directed model checking	search method;estimating function;model checking;state space	UPPAAL/DMC is an extension of UPPAAL that provides generic heuristics for directed model checking. In this approach, the traversal of the state space is guided by a heuristic function which estimates the distance of a search state to the nearest error state. Our tool combines two recent approaches to design such estimation functions. Both are based on computing an abstraction of the system and using the error distance in this abstraction as the heuristic value. The abstractions, and thus the heuristic functions, are generated fully automatically and do not need any additional user input. UPPAAL/DMC needs less time and memory to find shorter error paths than UPPAAL’s standard search methods.	dynamic markov compression;heuristic (computer science);lumix;model checking;state space;uppaal	Sebastian Kupferschmid;Klaus Dräger;Jörg Hoffmann;Bernd Finkbeiner;Henning Dierks;Andreas Podelski;Gerd Behrmann	2007		10.1007/978-3-540-71209-1_52	model checking;real-time computing;computer science;state space;theoretical computer science;programming language;abstraction model checking;algorithm	SE	-12.326632689941276	28.91822548113561	839
0aa249c46fc2c0262ec74655222f0fde9819159f	a decision support system for eco-efficient biorefinery process comparison using a semantic approach	uncertainty management;decision support system;biorefinery;ontology;bioprocess eco design;knowledge engineering	Enzymatic hydrolysis of the main components of lignocellulosic biomass is one of the promising methods to further upgrading it into biofuels. Biomass pre-treatment is an essential step in order to reduce cellu- lose crystallinity, increase surface and porosity and separate the major constituents of biomass. Scientific literature in this domain is increasing fast and could be a valuable source of data. As these abundant sci- entific data are mostly in textual format and heterogeneously structured, using them to compute biomass pre-treatment efficiency is not straightforward. This paper presents the implementation of a Decision Support System (DSS) based on an original pipeline coupling knowledge engineering (KE) based on semantic web technologies, soft computing techniques and environmental factor computation. The DSS allows using data found in the literature to assess environmental sustainability of biorefinery sys- tems. The pipeline permits to: (1) structure and integrate relevant experimental data, (2) assess data source reliability, (3) compute and visualize green indicators taking into account data imprecision and source reliability. This pipeline has been made possible thanks to innovative researches in the coupling of ontologies, uncertainty management and propagation. In this first version, data acquisition is done by experts and facilitated by a termino-ontological resource. Data source reliability assessment is based on domain knowledge and done by experts. The operational prototype has been used by field experts on a realistic use case (rice straw). The obtained results have validated the usefulness of the system. Further work will address the question of a higher automation level for data acquisition and data source reliabil- ity assessment.	decision support system	Charlotte Lousteau-Cazalet;Abdellatif Barakat;Jean Pierre Belaud;Patrice Buche;Guillaume Busset;Brigitte Charnomordic;Stéphane Dervaux;Sébastien Destercke;Juliette Dibie;Caroline Sablayrolles;Claire Vialle	2016	Computers and Electronics in Agriculture	10.1016/j.compag.2016.06.020	decision support system;computer science;systems engineering;engineering;artificial intelligence;knowledge engineering;ontology;data mining	Robotics	-34.27043157161706	-6.781080066788181	840
d74674682c8b6b332a17d15df6169caa16b61744	grasp a moving target from the air: system & control of an aerial manipulator		Grasping a moving target has been investigated extensively for fixed-base manipulator. However, such a task becomes much more challenging when the manipulator is free flying in the air with an UAV. Towards moving target grasping, this paper presents an aerial manipulator system composed of a hex-rotor and a 7-DoF (Degree of Freedom) manipulator. An independent control structure is used in the aerial manipulator control system, i.e., the hex-rotor and the manipulator are controlled separately. In the hex-rotor's controller, the system CoM (Center of Mass) offset motion is used to compensate disturbance of the robotic arm. In the manipulator's controller, the relative kinematics between the target and the aerial vehicle is taken into consideration to grasp the target. At last aerial grasping experiments are conducted to validate the feasibility of the proposed control scheme and the reliability of our aerial manipulator system.	aerial photography;control flow;control system;experiment;pony island;r.o.t.o.r.;robot;robotic arm;unmanned aerial vehicle	Guangyu Zhang;Yuqing He;Bo Dai;Feng Gu;Liying Yang;Jianda Han;Guangjun Liu;Juntong Qi	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461103	control theory;control theory;center of mass;vehicle dynamics;control engineering;engineering;offset (computer science);kinematics;robotic arm;control system;grasp	Robotics	63.23920524302773	-16.805481745475188	841
3ebb799f9e967ad6de4ff797fbc243affa86008d	average error rate evaluation of digital modulations in slow fading by prony approximation	maximum ratio combiner;rayleigh fading;bit error rate;data processing;moment generating function;curve fitting error rate evaluation fading channel error rate curve prony approximation universal numerical algorithm moment generating function bit error rate m ary phase shift keying ricean fading correlated multichannel rayleigh fading maximum ratio combining;diversity reception;phase shift keying;curve fitting rayleigh channels rician channels error statistics diversity reception phase shift keying awgn channels;error analysis digital modulation fading computer errors signal to noise ratio rayleigh channels detectors signal analysis computer simulation curve fitting;awgn channels;first order;rayleigh channels;numerical algorithm;fading channel;error rate;rician channels;error statistics;m ary phase shift keying;curve fitting;signal to noise ratio;computer simulation;digital modulation	A novel, remarkably simple semianalytical method for average error-rate evaluation over slowly fading channels is presented. Assume that an error-rate curve conditioned on the instantaneous signal-to-noise ratio at the detector input is known either analytically or can be estimated at a few points by a computer simulation. In the first step, a sum of the first-order exponentials is fitted to the conditional error-rate curve. The curve fitting by a sum of exponentials is well-known in many areas of data processing as Prony approximation. A universal numerical algorithm to find the parameters of Prony approximation is developed. In the second step, knowledge of the moment generating function of the signal-to-noise ratio is required to obtain an average error-rate. Hence, the proposed method can be shown to be an extension of the moment generating function method. The method is illustrated on an example of average bit-error rate evaluation for M-ary phase-shift keying over a generalized Ricean fading and over correlated multichannel Rayleigh fading with maximum ratio combining.	algorithm;approximation;bit error rate;computer simulation;curve fitting;first-order predicate;key (cryptography);modulation;numerical analysis;prony's method;rayleigh fading;signal-to-noise ratio	Pavel Loskot;Norman C. Beaulieu	2004	2004 IEEE International Conference on Communications (IEEE Cat. No.04CH37577)	10.1109/ICC.2004.1313166	computer simulation;bit error rate;data processing;telecommunications;word error rate;computer science;phase-shift keying;rayleigh fading;first-order logic;mathematics;signal-to-noise ratio;fading;moment-generating function;statistics;curve fitting	Robotics	40.64670453743308	75.35177281798728	842
6c97467b671b4df75f897363ea02d89ff853d5f0	combinatorial method in the coset enumeration of symmetrically generated groups	finite group;combinatorial algebra;20b05;enumetration;05a05;20b40;coset enumerations;i 1 2;involutory generators;symmetric generations;conjugacy class;finite simple group	We give an algorithm for enumerating cosets of a group defined as a finite homomorphic image of a semi-direct product of free products of cyclic groups by a group of monomial automorphisms. Lots of finite groups, including some of the sporadic simple groups can be defined in this mannar. Mathematics Subject Classification: 20Bxx	algorithm;coset enumeration;mathematics subject classification;monomial;semiconductor industry	Mohamed Sayed	2008	Int. J. Comput. Math.	10.1080/00207160701543376	combinatorics;discrete mathematics;conjugacy class;mathematics;algebra	Theory	40.63950266744562	34.529138426484046	843
56ea2c6ca5ae75684ec97b96f8a6410fc272df80	performance of multicode ds/cdma with noncoherent m-ary orthogonal modulation in multipath fading channels	analytical models;noncoherent m ary orthogonal modulation;equal gain combining;fading;rayleigh fading;video communications;bit error rate;cdma2000;ber;cellular radio;simulation;packet radio networks;error statistics code division multiple access spread spectrum communication rayleigh channels multipath channels modulation coding transceivers cellular radio data communication packet radio networks statistical analysis gaussian channels;interference terms;interference;egc;data communication;radio configuration 2;nakagami fading;radio configuration 1;spread spectrum communication;code division multiple access;statistical analysis;gaussian approximation;rayleigh channels;modulation coding;mobile communication;performance analysis;multiaccess communication fading bit error rate performance analysis analytical models rayleigh channels transceivers mobile communication interference gaussian approximation;noncoherent m ary orthogonal modulation performance analysis simulation multicode ds cdma system rayleigh fading nakagami fading transceiver reverse link is 95b cdma2000 radio configuration 1 radio configuration 2 data transfer video communications statistical characterization interference terms gaussian approximation bit error rate ber equal gain combining egc mobile user multipath channels;error statistics;statistical characterization;transceivers;multipath channels;is 95b;gaussian channels;transceiver;reverse link;data transfer;multicode ds cdma system;multiaccess communication;mobile user	This paper presents the performance analysis and simulation of a multicode DS/CDMA system with noncoherent M-ary modulation, operating over a Rayleigh or Nakagami fading environment. This type of transceiver is specified for the reverse link of the IS-95B and cdma2000 (Radio configurations 1 and 2) systems, and is intended to serve high-rate applications such as data transfer and video communications. After a statistical characterization of the interference terms, we make use of the Gaussian approximation (GA) in order to obtain the bit error rate (BER). However, unlike other analyses relying on the GA, in our derivation we take into account the fact that all the codes transmitted by a mobile user fade in unison. As demonstrated via computer simulations, this fact is crucial to obtain a reliable estimate of the BER, especially when equal-gain combining (EGC) is used at the receiver.	modulation;multipath propagation	Cyril-Daniel Iskander;P. Takis Mathiopoulos	2002		10.1109/VTC.2002.1002806	electronic engineering;bit error rate;telecommunications;computer science;computer network;transceiver	ML	39.81441705352269	77.82040852326924	844
e95805c3cc24007adbe17bac1bab1b4f2fa40232	reconfigurable self-embedding with high quality restoration under extensive tampering	high tampering rates high quality restoration content reconstruction problem erasure communication channel reconfigurable self embedding system reconstruction performance monte carlo simulations high reconstruction quality;monte carlo methods embedded systems image restoration image watermarking;image restoration;embedded systems;image reconstruction watermarking decoding psnr image restoration authentication streaming media;image watermarking;monte carlo methods;fragile watermarking content reconstruction self embedding image authentication	In this paper we analyze the content reconstruction problem with the use of a revised erasure communication channel. Based on this approach, we propose a reconfigurable self-embedding system which can be adapted to different requirements. Our approach eliminates two major problems with the design of efficient content reconstruction algorithms and allows for theoretical analysis of the reconstruction performance. The presented theoretical results are verified using Monte Carlo simulations. The proposed scheme is experimentally evaluated in a number of possible configurations, and allows to achieve high reconstruction quality even with high tampering rates.	algorithm;channel (communications);circuit restoration;display resolution;experiment;monte carlo method;reconstruction conjecture;requirement;simulation	Pawel Korus;Andrzej Dziech	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467329	image restoration;computer vision;computer science;theoretical computer science;mathematics;statistics;monte carlo method;computer graphics (images)	Robotics	39.269563008842084	-12.729798870162753	845
ee6ec6d7e8ea74dde80bcc00009e6843e6ce2acd	multi-agent negotiation of decentralized energy production in smart micro-grid		SmartGrid is an electricity network that can intelligently integrate the actions of all users connected to it in order to efficiently deliver sustainable, economic and secure electricity supplies. In this context the CoSSMic project aims at fostering a higher rate of self-consumption of decentralized renewable energy production, using innovative autonomic systems for management and control of power micro-grids on users behalf. To achieve this goal we have designed an ICT framework that integrates different appliances such as smartmeters, solar panels, batteries, etc., providing a common platform to support sharing of information and negotiation of energy exchanges between power producers and storages in accordance with policies defined by owners, weather forecasts, and habits and plans of participants.		Alba Amato;Beniamino Di Martino;Marco Scialdone;Salvatore Venticinque	2014		10.1007/978-3-319-10422-5_17	process management;grid;business;renewable energy;energy market;smart grid;negotiation;distributed generation;multi-agent system;information and communications technology	NLP	1.579343062596035	6.366799773139103	846
5490b757b3b6388c851f494e1be8f8cb4081762e	physical topology design for survivable routing of logical rings in wdm-based networks	metropolitan area networks;network topology;telecommunication links;telecommunication network routing;wavelength division multiplexing;wdm-based networks;dual hub structure;link failure;logical rings;physical topology design;survivable routing;two-connected logical topologies;wavelength-division multiplexing;network design;wdm;network survivability;routing;topology design;metropolitan area network	In a wavelength-division multiplexed (WDM)-based network, a single physical link failure may correspond to multiple logical link failures. As a result, two-connected logical topologies, such as rings routed on a WDM physical topology, may become disconnected after a single physical link failure. We consider the design of physical topologies that ensure logical rings can be embedded in a survivable manner. This is of particular interest in metropolitan area networks, where logical rings are in practice almost exclusively employed for providing protection against link failures. First, we develop necessary conditions for the physical topology to be able to embed all logical rings in a survivable manner. We then use these conditions to provide tight bounds on the number of physical links that an N-node physical topology must have in order to support all logical rings for different sizes K. We show that when K/spl ges/4 the physical topology must have at least 4N/3 links, and that when K/spl ges/6 the physical topology must have at least 3N/2 links. Subsequently, we generalize this bound for all K/spl ges/4. When K/spl ges/N-2, we show that the physical topology must have at least 2N-4 links. Finally, we design physical topologies that meet the above bounds for both K=4 and K=N-2. Specifically, our physical topology for embedding (N-2)-node rings has a dual hub structure and is able to embed all rings of size less than N-1 in a survivable manner. We also provide a simple extension to this topology that addresses rings of size K=N-1 and rings of size K=N for N odd. We observe that designing the physical topology for supporting all logical rings in a survivable manner does not use significantly more physical links than a design that only supports a small number of logical rings. Hence, our approach of designing physical topologies that can be used to embed all possible ring logical topologies does not lead to a significant overdesign of the physical topology.	embedded system;network topology;routing;usb hub;wavelength-division multiplexing	Aradhana Narula-Tam;Eytan Modiano;Andrew Brzezinski	2003	IEEE Journal on Selected Areas in Communications	10.1109/GLOCOM.2003.1258698	telecommunications;computer science;extension topology;comparison of topologies;wavelength-division multiplexing;computer network;logical topology	Metrics	-6.576395162419804	80.28862133129988	847
7acc6bf4d46aa112ad9abc4246249fa126c8240a	recursive properties of abstract complexity classes	complexity class;computational complexity;indexation;recursion theory	"""A complexity class Rt is the class of all recursive functions whose computation """"cost"""" is bounded by the function t. Rt is recursively presentable if there is a recursive set containing at least one index for each function in Rt and no index for functions not in Rt . It is proved that complexity classes of abstract measures of complexity need not be recursively presentable (r.p.). However, the complement of each class is shown to be r.p. The results are extended to complexity classes determined by partial functions, and the properties of these classes are investigated. Properties of effective presentations of complexity classes are also studied. For each measure, another measure with the same complexity classes is constructed such that almost every class admits an effective presentation of efficient devices. Finally, the family of complexity classes is shown not to be closed under intersection."""	axiomatic system;blum axioms;complexity class;computation;information theory and measure theory;parallel computing;recursion (computer science);recursively enumerable set	Lawrence H. Landweber;Edward L. Robertson	1972	J. ACM	10.1145/321694.321702	complete;time complexity;complexity class;circuit complexity;parameterized complexity;combinatorics;discrete mathematics;complexity;average-case complexity;ph;computability theory;decision tree model;quantum complexity theory;computer science;structural complexity theory;2-exptime;sparse language;computational resource;worst-case complexity;complexity index;mathematics;mutual recursion;computational complexity theory;asymptotic computational complexity;game complexity;algorithm;descriptive complexity theory	Theory	6.520364837361919	20.401421984906868	848
4f625f0293778d61f4d2b07931a8e0549911d449	a study of development of transmission systems for next-generation terrestrial 4 k uhd and hd convergence broadcasting	signal image and speech processing;information systems applications incl internet;communications engineering networks	The worldwide transition from analog to digital broadcasting has now been completed, and the need to study next-generation standards for ultra-high-definition TV (UHDTV) broadcasting, as well as broadcasting and communication convergence systems is rapidly growing. In particular, high-resolution mobile broadcasting services are needed to satisfy recent consumer demands. Therefore, the development of highly efficient convergence broadcasting systems that provide fixed/mobile broadcasting through a single channel is needed. In this paper, a service scenario and the requirements for providing 4 K UHD and high-definition (HD) convergence broadcasting services through a terrestrial single channel are analyzed by employing the latest transmission and video codec technologies. Optimized transmission parameters for 6- and 8-MHz terrestrial bandwidths are drawn, and receiving performances are measured under additive white Gaussian noise (AWGN) and time-varying typical urban (TU)-6 channel to find the threshold of visibility (TOV). From the results, reliable receiving of HD layer data can be achieved at a 6-MHz bandwidth when the maximum receiver velocity is 140 km/h and no higher due to the limit of bandwidth. When the bandwidth is extended to 8 MHz, reliable receiving of both 4 K UHD and HD layer data can be achieved under a very fast fading multipath channel.	terrestrial television	Jong Oh;Yong Won;Jin Lee;Yong-Hwan Kim;Jong Woo Paik;Joon Kim	2015	EURASIP J. Wireless Comm. and Networking	10.1186/s13638-015-0362-x	telecommunications;computer science;digital broadcasting;computer network	Mobile	21.237958462974845	85.20478948753677	849
9424ed240ed84be88d64cf6e653989e0e5ea3771	a self-selection technique for flooding and routing in wireless ad-hoc networks	signal strength;wireless networks;fault tolerant;point to point;wireless network;wireless ad hoc network;ad hoc flooding;leader election;routing protocol;ad hoc routing;wireless ad hoc networks	There is a fundamental difference between wireless and wired networks, since the latter employ point-to-point communication while the former use broadcast transmission as the communication primitive. In this paper, we describe an algorithm, called self-selection, which takes advantage of broadcast communication to efficiently implement the basic operation of selecting a node possessing some desired properties among all the neighbors of the requestor. Self-selection employs a prioritized transmission back-off delay scheme in which each node’s delay of transmitting a signal is dependent on the probability of the node’s ability to best perform a pertinent task, and in turn, enables the node to autonomously select itself for the task. We demonstrate the benefits of self-selection in two basic wireless ad hoc network communication algorithms: flooding and routing. By relating back-off delay to the signal strength of a received packet, we design an efficient variant of conventional flooding called Signal Strength Aware Flooding. By using distance-to-destination to derive back-off delay, we design a novel and fault-tolerant wireless ad hoc network routing protocol named Self-Selective Routing.	algorithm;fault tolerance;hoc (programming language);network packet;point-to-point protocol;point-to-point (telecommunications);relevance;routing;transmitter	Gilbert Chen;Joel W. Branch;Boleslaw K. Szymanski	2006	Journal of Network and Systems Management	10.1007/s10922-006-9036-7	vehicular ad hoc network;wireless mesh network;wireless routing protocol;wireless ad hoc network;exor;optimized link state routing protocol;routing;adaptive quality of service multi-hop routing;mobile ad hoc network;zone routing protocol;telecommunications;computer science;dynamic source routing;flooding;destination-sequenced distance vector routing;wireless network;ad hoc wireless distribution service;distributed computing;key distribution in wireless sensor networks;link-state routing protocol;hazy sighted link state routing protocol;geographic routing;computer network	Mobile	4.174756832980459	82.46156975975428	850
1283e5fd0a7a8c9407e505c20ffe06585e665df1	rectangular patch resonator sensors for characterization of biological materials	medical applications rectangular patch resonator sensors biological materials microwave characterization complex dielectric properties dielectric superstrate low loss materials inverse problem rpr prototypes biosensor noninvasive testing;inverse problems biosensors dielectric materials dielectric resonators;dielectric resonators;characterization of biological materials rectangular patch resonator rpr non invasive testing electromagnetic sensor;dielectric materials;biosensors;inverse problems;frequency measurement resonant frequency biosensors biological system modeling substrates	The concept of using rectangular patch resonator (RPR) sensors for microwave characterization of biological materials is the non-invasive nature of the technique. It is being used extensively for the complex dielectric properties measurements of materials in the microwave region. Applications in microwave systems, the dielectric substrate and superstrate are made with low loss materials for the best operation. When RPR is used as the sensor, the superstrate is made with the unknown material under test; the objective is then to extract from measurements the dielectric parameters of this material. An accurate modeling of structures, associated with measurement data, is required to obtain, right values of dielectric parameters of unknown material by solving the inverse problem. Our research was established on the implementation of RPR prototypes, in order to be used as a biosensor for non-invasive testing and medical applications to characterize the dielectric properties of various biological materials. Our measurements showed that the complex dielectric properties obtained by this technique are in good agreement with simulations using C.Gabriel & al comparative data.	data mining;microwave;resilient packet ring;sensor;simulation	Nabila Aouabdia;Nour Eddine Belhadj-Tahar;Georges Alquié	2014	2014 IEEE 11th International Multi-Conference on Systems, Signals & Devices (SSD14)	10.1109/SSD.2014.6808748	materials science;ceramic materials;electronic engineering;analytical chemistry	Embedded	92.89852762115902	-19.68465483916442	851
d45369ad192737da11cbf57a59cafbd362a74c44	cooperative sensing and compression in vehicular sensor networks for urban monitoring	compression approach;urban environment;interpolation;conference_paper;data compression;energy efficient;packet loss;monitoring wireless sensor networks sampling methods surveillance urban areas capacitive sensors power supplies computer networks vehicle dynamics network topology;computational capacity;sensor network;power supply;wireless sensor network;network topology;reconstruction quality cooperative sensing vehicular sensor networks urban monitoring vsn urban environment surveillance vehicle based sensors power supply computational capacity wireless sensor network wsn network topology packet losses distorted surveillance results zero inter sensor collaboration overhead sparse random projections reconstruction accuracy spatial correlation communication traffic load sampling algorithms urban environment data sets vehicular mobility models interpolation strategy;spatial correlation;data sensing;wireless sensor networks data compression interpolation road vehicles sampling methods signal reconstruction telecommunication network topology;communication cost;urban area;signal reconstruction;cooperative sensing;sampling methods;telecommunication network topology;mobility model;random projection;wireless sensor networks;road vehicles	A Vehicular Sensor Network (VSN) may be used for urban environment surveillance utilizing vehicle- based sensors to provide an affordable yet good coverage for the urban area. The sensors in VSN enjoy the vehicle's steady power supply and strong computational capacity not available in traditional Wireless Sensor Network (WSN). However, the mobility of the vehicles results in highly dynamic and unpredictable network topology, leading to packet losses and distorted surveillance results. To resolve these problems, we propose a cooperative data sensing and compression approach with zero inter-sensor collaboration overhead based on sparse random projections. The algorithm provides excellent reconstruction accuracy for the sensed field, and by taking advantage of the spatial correlation of the data, enjoys much smaller communication traffic load compared to traditional sampling algorithms in wireless sensor networks. Real urban environment data sets are used in the experiments to test the reconstruction accuracy and energy efficiency under different vehicular mobility models. The results show that our approach is superior to the conventional sampling and interpolation strategy which propagates data in an uncompressed form, with 4-5dB gain in reconstruction quality and 21-55% savings in communication cost for the same sampling times.	algorithm;data compression;experiment;interpolation;network packet;network topology;overhead (computing);power supply;random projection;real-time data;sampling (signal processing);sensor;simulation;sparse matrix	Xiaoxiao Yu;Huasha Zhao;Lin Zhang;Shining Wu;Basskar Krishnamachari;Victor O. K. Li	2010	2010 IEEE International Conference on Communications	10.1109/ICC.2010.5502562	wireless sensor network;telecommunications;computer science;computer security;statistics;computer network	Mobile	12.576284935508681	73.95776662415322	852
6070df043e821653d5b9671832365e82a9ba36df	stereo-based head pose tracking with motion compensation model	3d head model;human computer interaction;epipolar constraint;3d head model stereo vision head pose tracking feature tracking motion compensation model error compensation model epipolar constraint optical flow linear equation;degree of freedom;head tracking;motion compensation model stereo vision head pose tracking feature tracking;feature tracking;low complexity;motion estimation;motion compensated;error compensation model;tracking motion compensation head image reconstruction image motion analysis equations error compensation stereo image processing error analysis estimation error;stereo image processing image sequences motion estimation solid modelling;3d model;video conferencing;stereo image processing;stereo vision;error compensation;motion compensation model;optical flow;head pose tracking;facial expression;linear equations;linear equation;eye gaze;solid modelling;image sequences	We present a stereo based head tracking algorithm with motion error compensation model. We firstly reconstruct 3D head model by coupling feature tracking with the epipolar constraint from the stereo image pair. Based on the reconstructed 3D model, we can obtain the coarse 3D pose of the head at this point. We then analyze the errors in the estimation of the motion parameters and introduce motion compensation model to improve the accuracy in the head pose tracking. Coupled with the optical flow constraint equation, we can get a linear equation for the unknown parameters in motion compensation model, which can be solved with low complexity. Our method is able to track all the six degrees of freedom of the rigid part of head motions, even in the presence of partial occlusions, and/or dramatic facial expression changes, and promises useful applications in human-computer interaction and eye-gaze correction for video conferencing	3d modeling;algorithm;epipolar geometry;human–computer interaction;linear equation;motion capture;motion compensation;motion estimation;optical flow;polygonal modeling;pose (computer vision);six degrees of separation;stereopsis	Z. G. Liu;Y. F. Li;Paul Bao	2004	2004 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2004.1521866	computer vision;simulation;quarter-pixel motion;computer science;linear equation;computer graphics (images)	Robotics	52.688940829364334	-49.36895727452175	853
358d452f4bc43eea3448945186955a9380a101c2	service mining: concept and opportunity		This research devises a novel concept, service mining, which is derived from the concept of service science and traditional service management. Service mining is defined as a process to discover patterns from services. The goal of service mining is to detect something new from the service pool. Service mining proves the enfolded processes cover two major disciplines and extend to minor sub-disciplines such as service, business, and management. The concept of service mining not only investigates data from service sector but also focuses on the features of services. Hence, the enfolded process and framework of service mining aim to help researchers from multiple disciplines identify potential opportunities under the umbrella of service research.		Wei-Lun Chang;Yen-Hao Hsieh;Hui-Chi Chang	2012			computer science;knowledge management;management science;tertiary sector of the economy;service management	HPC	-71.17187152842438	9.095530805069467	854
90e542453b950231b740af77ca2dfa5394981d64	least squares ranking on graphs, hodge laplacians, time optimality, and iterative methods	conjugate gradient method;linear system;least squares problem;football;timing optimization;least square;krylov method;iteration method;hodge decomposition	Given a set of alternatives and some pairwise comparison values, ranking is a least squares computation on a graph. The graph vertices are the alternatives, with a weighted oriented edge between each pair for which there is a pairwise score. The orientations are arbitrary. The set of edges may be sparse or dense. The basic idea of the computation is very simple and old – come up with a vertex potential such that the potential difference matches the given edge data. Since an exact match will usually be impossible, one settles for matching the edge data in a least squares sense. This formulation was first described by Leake in 1976 for ranking football teams [21]. The residual can be further analyzed for discovering inconsistencies in the given pairwise comparison data, and this leads to a second least squares problem. This whole process was formulated recently by Jiang et al. as a Hodge decomposition of the edge values [19]. The second problem, besides being an important refinement of the basic least squares ranking, has other potential applications, such as in economics [19]. In a recent breakthrough paper, Koutis et al. [20] showed that symmetric diagonally dominant (SDD) linear systems can be solved in time approaching optimality (we’ll refer to their algorithm as the KMP solver). We first show as an easy consequence of their result that for an arbitrary graph, the first least squares problem of ranking can be solved in time approaching optimality using the KMP solver. We show that the second least squares problem involves the Hodge 2-Laplacian, which is different from the graph Laplacian. It has not been studied in the theoretical computer science literature. We show that if a graph is the 1-skeleton of a cell complex on a compact surface the second least squares system matrix is also SDD, which implies optimality via KMP. For all the above cases we also give bounds on the number of conjugate gradient iterations required to achieve a given error bound. For the surface graphs we do this first for boundaryless surface. We show that the system matrix is the same as the graph Laplacian for the dual graph. If the embedding surface does have boundary, then we use Cauchy’s interlacing theorem to give bounds on conjugate gradient iterations required to solve the second problem by patching the holes and using bounds for the boundaryless case. These special cases are important in computational topology, and we show that the least squares problems of ranking are a 2-norm version of the optimal homologous chain problem of computational topology [10]. For a general graph with cells filled in, we show that the second least squares system matrix is, in general, not diagonally dominant. Thus KMP does not apply directly and nothing is known about its spectrum in general. In this case the best approach is to use an iterative Krylov method and we show numerical results for several choices. Krylov methods are also useful for large graphs where the loss of sparsity in forming the system matrix might be a storage issue. The second problem’s system matrix will be in general singular and have a high dimensional kernel equal to the dimension of the second homology which is the number of independent spheres such as tetrahedra amongst the cliques. Krylov methods work in a space orthogonal to the kernel and no kernel modding is required. ∗Author for correspondence. Department of Computer Science, University of Illinois at Urbana-Champaign, hirani@cs.illinois.edu http://www.cs.illinois.edu/hirani †Department of Computer Science, University of Illinois at Urbana-Champaign, kalyana1@illinois.edu ‡Department of Mech. Sci. & Eng., University of Illinois at Urbana-Champaign, watts2@illinois.edu ar X iv :1 01 1. 17 16 v1 [ cs .N A ] 8 N ov 2 01 0	algorithm;computation;computational topology;conjugate gradient method;diagonally dominant matrix;dual graph;graph (discrete mathematics);homology (biology);interlacing (bitmaps);iteration;iterative method;kernel (operating system);klee's measure problem;krylov subspace;laplacian matrix;least squares;linear system;modding;numerical analysis;refinement (computing);solver;sparse matrix;theoretical computer science	Anil N. Hirani;Kaushik Kalyanaraman;Seth Watts	2010	CoRR		mathematical optimization;combinatorics;discrete mathematics;non-linear iterative partial least squares;mathematics;non-linear least squares;least squares;recursive least squares filter	ML	71.3172038182937	27.189604671223314	855
aa4a364a28f3351b0319a07589fa1a62a1629c95	blameworthiness in strategic games		There are multiple notions of coalitional responsibility. The focus of this paper is on the blameworthiness defined through the principle of alternative possibilities: a coalition is blamable for a statement if the statement is true, but the coalition had a strategy to prevent it. The main technical result is a sound and complete bimodal logical system that describes properties of blameworthiness in one-shot games.	epistemic modal logic;formal system;marc (archive);semantics (computer science)	Pavel Naumov;Jia Tao	2018	CoRR		machine learning;management science;artificial intelligence;computer science	AI	-16.752833946058097	4.4404998987604465	856
1543180ff5ed8f3a95d0774ff6f6fbe4281b7965	perceptual evaluation of multi-exposure image fusion algorithms	databases;image fusion correlation image quality computational modeling conferences multimedia communication databases;objective image quality assessment subjective image quality assessment multi exposure images image fusion;image fusion;subjective image quality assessment perceptual evaluation multiexposure image fusion algorithms quality enhancement technique electronic products fused images natural images source input images multiple exposure levels multistimulus scoring approach objective image quality models objective image quality assessment;image fusion consumer electronics electronic products;computational modeling;image quality;multimedia communication;correlation;conferences	Multi-exposure image fusion is considered an effective and efficient quality enhancement technique widely adopted in consumer electronics products. Nevertheless, little work has been dedicated to the quality assessment of fused images created from natural images captured at multiple exposure levels. In this work, we first build a database that contains source input images with multiple exposure levels (≥ 3) together with fused images generated by both classical and state-of-the-art image fusion algorithms. We then carry out a subjective user study using a multi-stimulus scoring approach to evaluate and compare the quality of the fused images. Considerable agreement between human subjects has been observed. Our results also show that existing objective image quality models developed for image fusion applications either poorly or only moderately correlate with subjective opinions.	algorithm;image fusion;image quality;usability testing	Kai Zeng;Kede Ma;Rania Hassen;Zhou Wang	2014	2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2014.6982278	image quality;subjective video quality;computer vision;image processing;computer science;digital image processing;multimedia;image fusion;computational model;automatic image annotation;correlation;information retrieval	Vision	62.470149466950495	-64.09274502262281	857
38af5d21cc8ffea1dc2f4a355eaec4455f86c87a	structured peer learning program - an innovative approach to computer science education		Structured Peer Learning (SPL) is a form of peer-based supplemental instruction that focuses on mentoring, guidance, and development of technical, communication, and social skills in both the students receiving assistance and the students in teaching roles. This paper explores the methodology, efficacy, and reasoning behind the practical realization of a SPL program designed to increase student knowledge and success in undergraduate Computer Science courses. Students expressed an increased level of comfort when asking for help from student teachers versus traditional educational resources, historically showed an increased average grade in lower-level courses, and felt that the program positively impacted their desire to continue in or switch to a Computer major. Additionally, results indicated that advances in programming, analytical thinking, and abstract analysis skills were evident in not only the students but also the student teachers, suggesting a strong bidirectional flow of knowledge.	computer science	Teresa Leyk;Robert McInvale;Ling Chen	2017	CoRR		teaching method;knowledge management;supplemental instruction;social skills;peer learning;flow (psychology);analytical skill;computer science	HCI	-77.74152844514155	-36.70142262831801	858
a8da6eebd2b10a6ec591eacd1e631393e05142b8	on the numerical simulation of unsteady solutions for the 2d boussinesq paradigm equation	second order;blow up;solitary wave;difference scheme;boundary condition;initial condition;truncation error;numerical simulation	For the solution of the 2D Boussinesq Paradigm Equation (BPE) an implicit, unconditionally stable difference scheme with second order truncation error in space and time is designed. Two different asymptotic boundary conditions are implemented: the trivial one, and a condition that matches the expected asymptotic behavior of the profile at infinity. The available in the literature solutions of BPE of type of stationary localized waves are used as initial conditions for different phase speeds and their evolution is investigated numerically. We find that, the solitary waves retain their identity for moderate times; for larger times they either transform into diverging propagating waves or blow-up.	computer simulation	Christo I. Christov;Natalia T. Kolkovska;Daniela Vasileva	2010		10.1007/978-3-642-18466-6_46	mathematical optimization;mathematical analysis;calculus;mathematics	Robotics	85.3405448313057	5.226849404899677	859
8ac37572867a6a81d29c8916d26f0d1ea7112633	conducting and evaluating critical interpretive research: examining criteria as a key component in building a research tradition		The collection, analysis, and interpretation of empirical materials are always conducted within some broader understanding  of what constitutes legitimate inquiry and valid knowledge. In the Information Systems field, there are wellknown and widely  accepted methodological principles consistent with the conventions of positivism. However, the same is not yet true of interpretive  research. The emergence of interpretivism in IS research was advocated by Walsham (1995) and corroborated by a series of special  issues in outstanding IS journals. An example of the effort to advance the legitimacy of studies grounded in an interpretive  position is the set of principles suggested by Klein and Myers (1999), which applies mostly to hermeneutics.However, because  not all interpretive studies are built on a hermeneutical philosophical base, they recommended that other researchers, representing  other forms of interpretivism, suggest additional principles. This paper follows in this vein, advocating the timely emergence  of a critical interpretive perspective in IS research and pressing the argument that an extended version of Golden-Biddle  and Locke’s (1993) criteria is not only appropriate but comprehensive as initial guidelines for conducting and evaluating  critical interpretive research. Critical interpretive research, research criteria, intensive research, qualitative research  		Marlei Pozzebon	2004		10.1007/1-4020-8095-6_16	systems engineering;political science;management science;social psychology	Crypto	-74.09131219987194	-16.2528336620163	860
e6942c72e749ce0aae92745ea169a3de6aad65ed	anisotropic and nonlinear diffusion applied to image enhancement and edge detection	image processing;nonlinear reaction diffusion equations;partial differential equations;pdes;anisotropic	In this paper, we suggest a new processing algorithm based on anisotropic diffusion and nonlinear process for removing noise and image enhancement. The principle of the proposed algorithm is to apply a Gaussian filter to the image gradient when computing the diffusion coefficient and to choose the gradient threshold parameter depending on the gradient of the image at each iteration in the model adopted by Morfu (2009). We compare this algorithm with that of Morfu. A number of experimental results are described to illustrate its performance and indicate that it is very efficient in removing noise, image enhancement and edge preserving.	algorithm;anisotropic diffusion;coefficient;edge detection;image editing;image gradient;image noise;iteration;nonlinear system	M. Ait Oussous;Noureddine Alaa;Youssef Ait Khouya	2014	IJCAT	10.1504/IJCAT.2014.060523	edge-preserving smoothing;mathematical optimization;mathematical analysis;discrete mathematics;scale space;image processing;computer science;mathematics;anisotropic diffusion;anisotropy;partial differential equation	Vision	55.270840525987424	-69.50517990111835	861
d3a40ad751c115829264507b87318e19bd312e25	concurrent scheduling for real-time staging in oversubscribed networks	real time		disk staging;overselling;real-time transcription;scheduling (computing)	Mohammed E. Eltayeb;Atakan Dogan;Füsun Özgüner	2003			real-time computing;scheduling (computing);computer science	Embedded	-11.53284779955045	62.55772802761964	862
2544b303db38fb9e00a5d702ec216a2219a2286d	joint source localization and separation in spherical harmonic domain using a sparsity based method		In this paper, we address the problem of source localization and separation using sparse methods over a spherical microphone array. A sparsity based method is developed from the observed data in spherical harmonic domain. A solution to the sparse model formulated herein is obtained by imposing orthonormal constraint on the sparsity matrix. Subsequently, a splitting method based on bregman iteration is used to jointly localize and separate the sources from the mixtures of sources. A joint estimate of location and the separated sources is finally obtained after fixed number of iterations. Experiments on source localization and separation are conducted at different SNRs on the grid database. Experimental results based on RMSE analysis and objective evaluation indicate a reasonable performance improvement when compared to other methods in literature.	bregman method;experiment;iteration;microphone;sparse matrix	Sachin N. Kalkur;C Reddy SandeepReddy;Rajesh Mahanand Hegde	2015			spherical harmonics;mathematical optimization;computer science	AI	84.39800050125807	-37.063887032649134	863
a7667ab6a0afbf06443ed3f993ccb2cd4dc0bddc	an owl ontology of set of experience knowledge structure	semantic network;decision maker;explicit knowledge;artificial intelligent;shared knowledge;knowledge structure;knowledge acquisition;information system;knowledge representation	Collecting, distributing and sharing knowledge in a knowledge-explicit way is a significant task for any company. However, collecting decisional knowledge in the form of formal decision events as the fingerprints of a company is an utmost advance. Such decisional fingerprint is called decisional DNA. Set of experience knowledge structure can assist on accomplishing this purpose. In addition, Ontology-based technology applied to set of experience knowledge structure would facilitate distributing and sharing companies’ decisional DNA. Such possibility would assist in the development of an e-decisional community, which will support decision-makers on their overwhelming job. The purpose of this paper is to explain the development of .an OWL decisional Ontology built upon set of experience, which would make decisional DNA, that is, explicit knowledge of formal decision events, a useful element in multiple systems and technologies, as well as in the construction of the e-decisional community.	dna barcoding;dna computing;fingerprint recognition;knowledge management;xml	Cesar Sanín;Edward Szczerbicki;Carlos Toro	2007	J. UCS		knowledge representation and reasoning;decision-making;computer science;knowledge management;artificial intelligence;explicit knowledge;body of knowledge;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;management science;procedural knowledge;knowledge extraction;semantic network;knowledge value chain;information system;domain knowledge	Web+IR	-32.26212520969372	-7.081232588425823	864
3155c2edf5b996d3b3ce2742894569db4a8930f2	hyperbolic fixed points are typical in the space of mixing operators for the infinite population genetic algorithm	population model;typical;generic;fixed point;hyperbolic fixed point;xed point;artificial intelligence;genetic algorithm;mixing;genetic algo rithm;population genetics	We study an infinite population model for the genetic algorithm, where the iteration of the algorithm corresponds to an iteration of a map G. The map G is a composition of a selection operator and a mixing operator, where the latter models effects of both mutation and crossover. We examine the hyperbolicity of fixed points of this model. We show that for a typical mixing operator all the fixed points are hyperbolic.	crossover (genetic algorithm);fixed point (mathematics);genetic algorithm;iteration;population model	Christina Hayes;Tomás Gedeon	2005		10.1145/1102256.1102336	fixed-point iteration;mathematical optimization;hyperbolic equilibrium point;combinatorics;discrete mathematics;genetic algorithm;population model;computer science;artificial intelligence;genetic operator;mathematics;fixed point;mixing;population genetics	ML	32.965743440162655	-0.39357306943047843	865
92c6a323711333dac3d13e7ffc4588bc4df71a91	configurable on-line global energy optimization in multi-core embedded systems using principles of analog computation	analogue circuits;configurable on-line global energy optimization;multicore embedded systems;dynamic voltage and frequency scaling;energy management;analog computation;circuit optimisation;system-on-chip;multiple processing elements;on-line energy optimization;energy management systems;embedded systems;energy management.;continuous optimization;system on chip;circuit design;energy dissipation;energy minimization;energy optimization;embedded system	This work presents the design of an on-line energy optimizer unit, which is capable of dynamically adjusting power supply voltages and operating frequencies of multiple processing elements (PE), tailored to the instantaneous workload information and is fully adaptive to variations in process and temperature. The circuit design borrows some of the basic principles of analog computation to continuously optimize the system-wide energy dissipation of multiple cores. The analogy between the energy minimization problem under timing constraints in a general task graph and the power minimization problem under Kirchhoffs current law (KCL) constraints in an equivalent resistive network is exploited	analog computer;computation;embedded system;multi-core processor	Zeynep Toprak Deniz;Yusuf Leblebici;Eric A. Vittoz	2006		10.1007/978-0-387-74909-9_13	embedded system;electronic engineering;real-time computing;computer science;engineering;continuous optimization;energy minimization	EDA	-1.5674629952076795	57.018256181727004	866
4002866577f284da1bad0603a6372d4018e97763	the accessibility of information systems for patients: use of touchscreen information systems by 345 patients with cancer in scotland	computer literacy;information systems	AIM To examine cancer patients' use, and satisfaction with touchscreen information systems. By examining the experience of subgroups, to address issues of equality of access.   PATIENTS 345 patients starting radiotherapy at the Beatson Oncology Centre (BOC), Glasgow.   METHODS Patients were invited to use a touchscreen computer at the start of treatment. They were sent a printout of what they saw on screen. Patients had open access to the system. Data were collected at recruitment, intervention, 3 weeks and 3 months. Predictor variables included: patients' demographics, information preferences, technology use, and psychological state. Outcome variables included: use and views of the computer and printout.   RESULTS Younger, broadsheet readers with previous computer use were more likely to find the system easy to use. Older, tabloid readers were more likely to find the content new and relevant.   DISCUSSION We need to make systems adapt to users' different needs. More effort should be made to provide affordable information for older, generally less literate and technologically less literate groups in suitable locations.	aim alliance;accessibility;auditory recruitment;demography;information system;medical oncology specialty;mental state;neoplasms;patients;tablet computer;tabloid;touchscreen	Janne Pearson;Ray B. Jones;Alison Cawsey;Sandra McGregor;Ann Barrett;W. Harper Gilmour;Jacqueline M. Atkinson;Jim McEwen	1999	Proceedings. AMIA Symposium		patient satisfaction;cancer;information system;computer literacy;nursing;demographics;touchscreen;medicine	HCI	-59.44522587731526	-64.84757772933901	867
24b3bb36758753db5cdd905fabcad73004ce7e1c	the use of linear programming in the construction of extremal solutions to linear inverse problems	non linear functional;unicidad solucion;linear inverse problem;extremal solutions;solution uniqueness;funcional no lineal;dirichlet problem;problema inverso;probleme dirichlet;fonctionnelle non lineaire;unicite solution;linear inverse problems;programacion lineal;inverse problem;problema dirichlet;linear programming;programmation lineaire;linear program;90c05;49k30;probleme inverse;86a22	While a finite collection of data does not specify a unique solution to a linear inverse problem, it can allow bounds to be placed on certain nonlinear solution functionals. Using the Dirichlet problem for the unit disc as an example, this note demonstrates the use of linear programming in constructing extremal solutions associated with a variety of such bounds.	linear programming	Stephen P. Huestis	1996	SIAM Review	10.1137/S003614459428895X	mathematical optimization;mathematical analysis;basic solution;inverse problem;linear programming;calculus;mathematics	Theory	72.17243278645181	19.92987508123388	868
de1753fa08a4b747f0e25464fc14a0d4840af92e	improving peak-picking using multiple time-step loss functions			loss function	Carl Southall;Ryan Stables;Jason Hockman	2018				ML	87.68261096315233	18.066635319275207	869
61f6bab30a12622ce292ae964f56ab81c1da9771	"""the """"most informative boolean function"""" conjecture holds for high noise"""		We prove the ”Most informative boolean function” conjecture of Courtade and Kumar for high noise ε ≥ 1/2− δ, for some absolute constant δ > 0. Namely, ifX is uniformly distributed in {0, 1} and Y is obtained by flipping each coordinate of X independently with probability ε, then, provided ε ≥ 1/2− δ, for any boolean function f holds I ( f(X);Y ) ≤ 1 − H(ε). This conjecture was previously known to hold only for balanced functions [5].	information;interactive financial exchange	Alex Samorodnitsky	2015	CoRR		combinatorics;discrete mathematics;mathematics	Theory	11.190600751653614	22.717215772308595	870
4cce0c9c76773ed750f2bad0a8a810bfd886cba0	lunares: lunar crater exploration with heterogeneous multi robot systems	cooperative robotic team;legged locomotion;lunar crater exploration;cooperative robotics;south pole;multi robot system;autonomous robots;robot arm;lessons learned;space robotics;wheeled locomotion;autonomous robot;robot team	The LUNARES (Lunar Crater Exploration Scenario) project emulates the retrieval of a scientific sample from within a permanently shadowed lunar crater by means of a heterogeneous robotic system. For the accomplished earth demonstration scenario, the Shakelton crater at the lunar south pole is taken as reference. In the areas of permanent darkness within this crater, samples of scientific interest are expected. For accomplishment of such kind of mission, an approach of a heterogeneous robotic team consisting of a wheeled rover, a legged scout as well as a robotic arm mounted on the landing unit was chosen. All robots act as a team to reach the mission goal. To prove the feasibility of the chosen approach, an artificial lunar crater environment has been established to test and demonstrate the capabilities of the robotic systems. Figure 1 depicts the systems in the artificial crater environment. For LUNARES, preexisting robots were used and modified were needed in order to integrate all subsystems into a common system control. A ground control station has been developed considering conditions of a real mission, requiring information of autonomous task execution and remote controlled operations to be displayed for human operators. The project successfully finished at the end of 2009. This paper reviews the achievements and lessons learned during the project. F. Cordes (B) · S. Bartsch · T. Birnschein · A. Dettmann · S. Haase · J. Hilljegerdes · S. Planthaber · T. M. Roehr · F. Kirchner DFKI Robotics Innovation Center, Robert-Hooke-Str. 5, 28359 Bremen, Germany e-mail: Florian.Cordes@dfki.de I. Ahrns · S. Estable EADS Astrium GmbH, Friedrichshafen, Germany D. Koebel · M. Scheper OHB System AG, Bremen, Germany	autonomous robot;email;emulator;german research centre for artificial intelligence;remote control;robotic arm;rover (the prisoner)	Florian Cordes;Ingo Ahrns;Sebastian Bartsch;Timo Birnschein;Alexander Dettmann;Stéphane Estable;Stefan Haase;Jens Hilljegerdes;David Koebel;Steffen Planthaber;Thomas M. Roehr;Marc Scheper;Frank Kirchner	2011	Intelligent Service Robotics	10.1007/s11370-010-0081-4	computer vision;simulation;robotic arm;computer science;artificial intelligence	Robotics	56.86001547997604	-28.75573194592761	871
8870372cd4688131d5f90a00554505e4a90cace0	hywrite: writing in hypermedia elearning environments	learning community;learning process;elearning;knowledge society;higher education;media art;spiral curriculum;learning communities;secondary education;social skill;social constructionist pedagogy scp;teacher training;point of view;media literacy;authoring tool;hypertext;cooperative work	"""The paper focuses on the practical reality of eLearning in Higher Education with special emphasis on media integration from the perspective of a spiral curriculum.[12] It presents an innovative model of media art education integrated in a creative writing program and concentrates upon the re-design of writing with the help of hypermedia in customized eLearning environments. HyWrite is the name of an authoring tool, an """"educational facilitator"""", currently being developed at the University of Passau.This tool is meant to be used both in higher education for teacher training purposes and in secondary education as a teacher and student's hypertext writing assistant. HyWrite is backed by an elaborate didactic design based on students' active participation and all-round involvement in the learning process. The didactic goals pursued are on the one hand the acquisition of a set of primary skills like for instance essay writing and on the other hand getting acquainted with a host of complementary skills not explicitly mentioned by the curriculum yet presupposed by the Information and Knowledge Society. These skills are dealt with in a systematical approach and reflect the actual state of affairs, namely what """"every schoolboy and girl should know"""".[3] An overview of the above mentioned competences is available in the core skill cluster below. The skill cluster under discussion covers competences pertaining to four ranges of action: writing techniques, media literacy, aesthetic features combined with ergonomic design from the point of view of accessibility and usability, and -last but not least- the social skills imposed by cooperative work."""	accessibility;computer cluster;human factors and ergonomics;hypermedia;hypertext;knowledge society;point of view (computer hardware company);spiral model;usability	Johanna Bucur	2006		10.1145/1149941.1149951	learning community;computer science;knowledge management;multimedia;world wide web	HCI	-66.99843218035835	-36.89607941808417	872
998ea6361a871f7f17c9c31469e9fdbabdcf115a	design space exploration for optimizing on-chip communication architectures	system level communication architectures;architectural design;protocols;communication system;communication architecture complexity;integrated circuit layout;network on chip;availability;efficient algorithm;space exploration;system level design design space exploration on chip communication architectures optimization system on chips on chip communication traffic system level communication architectures system performance network topologies system communications mapping on chip communication protocols cell forwarding unit atm switch communication architecture complexity bus architectures communication synthesis network on chip;indexing terms;design optimization;system performance;design space;system on a chip;on chip communication traffic;circuit complexity;chip;network topology;computer architecture;high level synthesis;telecommunication traffic;atm switch;network topologies;space exploration design optimization system on a chip computer architecture system performance protocols switches telecommunication traffic availability network topology;system on chip;bus architectures;system design;system level design;network topology integrated circuit layout circuit layout cad high level synthesis circuit optimisation system on chip circuit complexity;on chip communication protocols;communication protocol;circuit layout cad;cell forwarding unit;design space exploration;on chip communication;circuit optimisation;switches;system communications mapping;high performance;communication synthesis;on chip communication architectures optimization;system on chips	Rapid growth in the complexity of system-on-chips is being accompanied by increasing volume and diversity of on-chip communication traffic, which in turn, is driving the development of advanced system-level communication architectures. While these architectures have the potential to improve system performance, they pose significant new challenges to the system designer, owing to the complex design space defined by the availability of numerous network topologies, communication protocols, and mapping alternatives for system communications. In this paper, we address the problem of mapping a system's communication requirements to a given communication architecture template. We illustrate the nature of the communication architecture design space, and describe an exploration methodology that uses efficient algorithms to help automate the process of mapping the system communications to the selected template. In addition, we demonstrate the importance of simultaneously optimizing the on-chip communication protocols in order to maximize system performance. Experiments conducted on example systems, including a cell forwarding unit of an ATM switch, indicate that the proposed techniques aid in automatically constructing communication architectures that have high performance. For the systems we considered, the solutions generated using our methodology had 53% superior performance (on average), over those based on conventional architectures and mapping approaches. The algorithms used in the proposed methodology are computationally efficient, and scale well with increasing communication architecture complexity.	atm turbo;algorithm;algorithmic efficiency;cell (microprocessor);communications protocol;design space exploration;inter-process communication;network topology;requirement;systems design	Kanishka Lahiri;Anand Raghunathan;Sujit Dey	2004	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2004.828127	system on a chip;embedded system;communications protocol;electronic engineering;real-time computing;computer science;engineering;computer performance;network on a chip;network topology;computer network;computer engineering	EDA	3.0065732219123134	59.26650701510758	873
567a3a335cff09c66da07cc4e5b87154d7c4ae4c	enablers of supply chain integration: interpersonal and interorganizational relationship perspectives	supply chain integration;interorganizational relationship;interpersonal relationship	Purpose – Previous research on supply chain integration (SCI) enablers has primarily focussed on interorganizational relationships, the purpose of this paper is to broaden the discussion to include interpersonal relationships (IPRs). Design/methodology/approach – Based on a comprehensive literature review, a series of propositions are postulated and synthesized into a conceptual model of how IPRs maintain and enable SCI, which is decomposed into strategic alliance, information sharing, and process coordination. Findings – The authors find that IPRs including personal affection, communication, and credibility, have a positive influence on SCI, and these links are mediated by interorganizational relationships including trust, commitment, and power. Originality/value – The framework developed in this study provides new insights into the role of interpersonal networks in interorganizational relationships, which lead to SCI.		Bill Wang;Paul Childerhouse;Yuanfei Kang;Baofeng Huo;Sanjay Mathrani	2016	Industrial Management and Data Systems	10.1108/IMDS-09-2015-0403	interpersonal relationship;engineering;knowledge management;operations management;management	Robotics	-82.93719988817354	0.5792713813367293	874
e7dcbc1ef909265df537009cd0ab1af05c4f2bda	identification of activities of daily living in tremorous patients using inertial sensors		Tremor and voluntary movement of the upper limb are separated from IMUs signals.Seven daily living tasks are classified with an accuracy of 86%.Grain (precise or gross) of daily living tasks is identified with 79% accuracy.Direction (distal/ proximal) of daily living tasks is identified with 89% accuracy.Tremor kinematics can be coupled with the kind of task where it appears. BACKGROUNDMuch attention has been given to the use of inertial sensors for remote monitoring of individuals suffering from neurological pathologies. However, the focus has been mostly on the detection of symptoms like tremor or dyskinesia, not on identifying specific activities carried out by subjects. The objective of this study was to develop an automated segmentation and recognition methodology, from inertial sensor data, to identify tasks and motor patterns during activities of daily living. This will enable clinicians to contextualize the symptoms of these diseases and improve their treatment. METHODSWe designed and tested a methodology to automatically label continuous upper-limb activity from IMUs (Inertial Measurement Units). Three classification problems are considered: the task itself, the precision level required by the task with respect to movement (fine or gross) and the trajectory of the task (distal or proximal). These problems were identified by tremor experts as clinically important aspects to be monitored in tremor patients while performing daily activities. The proposed methodology reveals the relation between the functional context or activity and the patientu0027s on-going tremor to clinicians. RESULTSOverall task identification rate was 86%. Task precision and task trajectory are classified with 79% and 89% accuracy, respectively. Aligning the semantic nature of the activity with the tremor location and intensity can also provide novel and relevant information for clinical monitoring of tremor and help clinicians and researchers as a useful tool to develop new therapies or strategies and novel anti-tremor medications to combat context dependent tremor. CONCLUSIONSThe present study describes the development of a comprehensive methodology based on machine learning techniques to segment and detect activities of daily living in people with tremor using inertial sensors, which aims at facilitating detailed interpretation of tremor movements by neurologists.	sensor	José Ignacio Serrano;Stefan Lambrecht;M. Dolores del Castillo;Juan P. Romero;Julián Benito-León;Eduardo Rocon	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.04.032	machine learning;simulation;computer vision;inertial measurement unit;activities of daily living;computer science;artificial intelligence;essential tremor;dyskinesia	HCI	10.925574787733657	-87.00436618740399	875
b8635eb5e5242c8e6780a5aa4b654695d8d3a566	coordinating interfering transmissions in cooperative wireless lans	cooperative transmission;wireless networks;media access protocol;wireless channels;relay selection mechanism coordinating interfering transmissions cooperative wireless lan cooperative medium access control physical layer wireless networks packet transmissions;mac protocol;physical layer;wireless network;medium access control;packet radio networks;cooperative communications;interference;channel estimation;medium access control wireless networks analog network coding physical layer network coding interference cooperative communications;internet architecture;wireless communication;wireless lan access protocols cooperative communication packet radio networks;estimation;physical layer network coding;cooperative communication;analog network coding;data structures;access protocols;wireless lan;relays;information theory;relays channel estimation media access protocol estimation data structures	In this paper we present a cooperative medium access control (MAC) protocol that is designed for a physical layer that can decode interfering transmissions in distributed wireless networks. The proposed protocol pro-actively enforces two independent packet transmissions to interfere in a controlled and cooperative manner. The protocol ensures that when a node desires to transmit a unicast packet, regardless of the destination, it coordinates with minimal overhead with relay nodes in order to concurrently transmit over the wireless channel with a third node. The relay is responsible for allowing packets from the two selected nodes to interfere only when the desired packets can be decoded at the appropriate destinations and increase the sum-rate of the cooperative transmission. In case this is not feasible, classic cooperative or direct transmission is adopted. To enable distributed, uncoordinated, and adaptive operation of the protocol, a relay selection mechanism is introduced so that the optimal relay is selected dynamically and depending on the channel conditions. The most important advantage of the protocol is that interfering transmissions can originate from completely independent unicast transmissions from two senders. We present simulation results that validate the efficacy of our proposed scheme in terms of throughput and delay.	access control;network packet;overhead (computing);relay;simulation;throughput;unicast	Antonios Argyriou	2011	IEEE Transactions on Wireless Communications	10.1109/TWC.2011.091411.102084	telecommunications;information theory;computer science;wireless network;distributed computing;statistics;computer network	Mobile	10.57028882176084	91.39156840100834	876
4efaeb643b904ef0a89f2d9a41a54a151ed5e5a3	new constructions of quaternary low correlation zone sequences	gordon mills welch sequences lcz quaternary low correlation zone sequences composite integer binary sequences autocorrelation m sequences gmw;sequences extended sequences low correlation zone lcz sequences quaternary sequences;correlacion;sequences;sequence quaternaire;quaternary low correlation zone sequences;ma xiuwen kang baoyuan chen zhi li yuelong quasi synchronous code division multiple access qs new constructions of quaternary low correlation zone sequence sets;telecommunication sans fil;correlation theory;gordon mills welch sequences;gmw;extended sequences;composite integer;acces multiple repartition code;m sequences;code division multiaccess;codificacion;code division multiple access;code division multiple access correlation theory m sequences;low correlation zone lcz sequences;telecomunicacion sin hilo;binary sequences;coding;acceso multiple division codigo;delay effects multiaccess communication binary sequences autocorrelation local area networks wireless communication computer science galois fields;correlation;lcz;codage;autocorrelation;wireless telecommunication;quaternary sequences	In this paper, given a composite integer n, we propose a method of constructing quaternary low correlation zone (LCZ) sequences of period 2/sup n/-1 from binary sequences of the same length with ideal autocorrelation. These new sequences are optimal with respect to the bound by Tang, Fan, and Matsufuji. The correlation distributions of these new quaternary LCZ sequences constructed from m-sequences and Gordon-Mills-Welch (GMW) sequences are derived.	autocorrelation;welch's method	Sang-Hyo Kim;Ji-Woong Jang;Jong-Seon No;Habong Chung	2005	IEEE Transactions on Information Theory	10.1109/TIT.2005.844068	arithmetic;code division multiple access;combinatorics;autocorrelation;complementary sequences;calculus;sequence;mathematics;coding;correlation;statistics	Crypto	42.44726485197246	49.770811321592284	877
4291dd337d3415a06b90b69ce50ec11f74b5344b	backlog and delay reasoning in harq systems			hybrid automatic repeat request	Sami Akin;Markus Fidler	2015	CoRR			Logic	32.68467236365888	79.41166325826039	878
6ee780334a918bb27c819a4e18c30e6651e8b7ef	better fiber odfs from suboptimal data with autoencoder based regularization		We propose a novel way of estimating fiber orientation distribution functions (fODFs) from diffusion MRI. Our method combines convex optimization with unsupervised learning in a way that preserves the relative benefits of both. In particular, we regularize constrained spherical deconvolution (CSD) with a prior that is derived from an fODF autoencoder, effectively encouraging solutions that are similar to fODFs observed in high-quality training data. Our method improves results on independent test data, especially when only few measurements or relatively weak diffusion weighting (low b values) are available. Author-created pre-print of an article that has been accepted for Proc. Medical Image Computing and Computer Assisted Intervention 2018 The final publication will be available at Springer.	angularjs;autoencoder;cambridge structural database;constrained optimization;convex optimization;deconvolution;gradient;machine learning;mathematical optimization;matrix regularization;medical image computing;springer (tank);test data;unsupervised learning;voxel	Kanil Patel;Samuel Groeschel;Thomas Schultz	2018		10.1007/978-3-030-00931-1_7	computer science;artificial intelligence;pattern recognition;autoencoder;unsupervised learning;deconvolution;diffusion mri;convex optimization;test data;regularization (mathematics);weighting	ML	55.36740425605949	-75.20834490507315	879
841778ca6520f6a5231e91b653f96751c7944f8f	computer-assisted information retrieval (recherche d'information et ses applications) - riao 1997, 5th international conference, mcgill university, montreal, canada, june 25-27, 1997. proceedings	information retrieval		information retrieval		1997			library science;geography;media studies;operations research	Robotics	-57.0167312439623	-9.528774333139044	880
7b71f09ac8fecbfcfbb1bddb724c9a228d8dd941	insar patch categorization using sparse coding	minimization;image coding;dictionaries;image classification object detection sar patch categorization sparse coding synthetic aperture radar sar;encoding dictionaries synthetic aperture radar algorithm design and analysis image coding minimization data models;encoding;algorithm design and analysis;data models;synthetic aperture radar	"""This letter presents sparse coding for interferometric synthetic aperture radar (InSAR) patch categorization. Motivated by the fact that an optimal dual based <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula> analysis can achieve better recognition rates, this letter proposes sparse coding with optimal dual-based <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula> analysis, which is applied to the amplitude and phase of the InSAR patches. The minimization of cost functions for amplitude and phase was designed and solved differently. The cost function for the amplitude part of InSAR data was modeled using the optimal dual-based <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula> analysis, and the minimization of cost function was solved using the forward–backward splitting algorithm. The phase was coded sparsely using the <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula> minimization approach and it was solved using the gradient descent algorithm. The experimental results showed that the proposed method outperforms the complex-valued methods for SAR patch categorization and outperforms the bag of visual words method as well."""	approximation;bag-of-words model in computer vision;categorization;dbpedia;dictionary;forward–backward algorithm;gradient descent;loss function;machine learning;neural coding;newton;newton's method;optimization problem;sparse matrix;synthetic data	Peter Planin&#x0161;i&#x0109;;Du&#x0161;an Gleich	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2689506	data modeling;algorithm design;computer vision;synthetic aperture radar;computer science;machine learning;pattern recognition;encoding;remote sensing	Vision	29.107886208874874	-44.66568982628688	881
61efabe189de2cc22385f039d2cc0845f63942a7	exploring genomic context patterns for rhodobacter sphaeroides in the herbe knowledge discovery environment	knowledge management component;fuses data management solution;herbe knowledge discovery environment;rhodobacter sphaeroides;computational environment technology;powerful knowledge discovery resource;similarity box visualization software;similarity box;exploring genomic context patterns;knowledge method;large-scale data;gene neighbor pattern;data structure;entity relationship;data visualisation;genetics;interactive visualization;knowledge management;knowledge discovery;data management;data mining	Sophisticated information strategies are increasingly essential for biologists. We have built a powerful knowledge discovery resource using novel genomic context methods, interactive visualization strategies, and computational environment technologies. The Heuristic Entity Relationship Building Environment (HERBE) is a research platform for advanced database technologies that fuses data management solutions with knowledge management components to support the dynamic capture of concepts and observations as biologists explore large-scale data. The Similarity Box visualization software supports the ability of biologists to interact with large-scale computational results and evaluate relationships based on natural reasoning processes. We have applied these knowledge methods in the exploration of complex microbial genome relationships. We extracted a complete set of gene neighbor patterns for Rhodobacter sphaeroides using HERBE to map data structures for chromosomal contiguity against sequence similarity. We then organized these gene neighbor patterns by their phylogenetic profiles using Similarity Box to enable biologists to explore the results.	data structure;database;entity–relationship model;heuristic;homology (biology);interactive visualization;knowledge management;phylogenetics;visualization software	Heidi J. Sofia;Abigail L. Corrigan;Kyle R. Klicker;George Chin;Eric G. Stephan	2004	Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.	10.1109/CSB.2004.1332460	biology;interactive visualization;data structure;entity–relationship model;data management;computer science;bioinformatics;data science;machine learning;data mining	HPC	-7.298378698341086	-52.215658997221645	882
a02a6599d330274dc751fedaf18dafbce8b72005	a compact broadband antenna with an l-shaped notch	l shaped notch;l shape;forma de una l;tecnologia electronica telecomunicaciones;antenne microruban;microstrip antennas;diagrama radiacion;antenne large bande;monopole antenna;monopole antennas;ultra large bande;perte retour;antena monopolo;antenne monopole;radiation pattern;ultra wideband;banda ultraancha;bande frequence;frequency band;return loss;broadband antennas;tecnologias;grupo a;microstrip fed;banda frecuencia;forme en l;diagramme rayonnement;ultra wide band	A small microstrip-fed monopole antenna using an L-shaped notch is presented for ultra wideband applications. The proposed antenna, with compact size of 15.5 x 21 mm 2  including the ground plane, is designed to operate over the frequency band between 3.05 and 10.9 GHz for S  11  < -10 dB. Good return loss and radiation pattern characteristics are obtained in the frequency band of interest.		Jihak Jung;Wooyoung Choi;Jaehoon Choi	2006	IEICE Transactions	10.1093/ietcom/e89-b.6.1968	omnidirectional antenna;antenna factor;monopole antenna;antenna rotator;random wire antenna;periscope antenna;telecommunications;antenna measurement;antenna tuner;computer science;helical antenna;antenna noise temperature;patch antenna;antenna;ultra-wideband;j-pole antenna;radiation pattern;coaxial antenna;dipole antenna;antenna efficiency	Visualization	66.17353026050435	58.81716738661373	883
9d7bad1f58befaf13b58db0106b35aef8080bb6c	asia-pacific signal and information processing association annual summit and conference, apsipa 2016, jeju, south korea, december 13-16, 2016			information processing		2016				EDA	-55.73910470359653	-7.519210975133155	884
33d6532f7d2139ba5871e8737c0c80e91584e5e9	what device should be used for telementoring? randomized controlled trial	regulations;touchscreen;telementoring;usability;telestration	BACKGROUND AND OBJECTIVE The paper analyzes behavioral patterns of mentors while using different mentoring devices to demonstrate the feasibility of multi-platform mentoring. The fundamental differences of devices supporting telementoring create threats for the perception and interpretation of the transmitted video, highlighting the necessity of exploring hardware usability aspects in a safety critical surgical mentoring scenario.   MATERIALS AND METHODS Three types of devices, based on the screen size, formed the arms for the randomized controlled trial. Streaming video recordings of a laparoscopic procedure to the mentors imitated the mentoring scenario. User preferences and response times were recorded while participating in a session performed on all devices.   RESULTS Median response to a mentoring request times were similar for mobile platforms; expected durations were considerably longer for stationary computer. Ability to perceive and identify anatomical structures was insignificantly lower on small sized devices. Stationary and tablet platforms were nearly equally preferred by the most of participants as default telementoring hardware.   DISCUSSION As a side effect, incompatibility of daily duties of the surgeons in the hospital and telementoring responsibilities while implementing systems locally was identified. Scaling up the use of the service in combination with the organizational changes of clinical staff looks like a promising solution.   CONCLUSION The trial demonstrated the feasibility of using all three types of devices for the purpose of mentoring, allowing users to choose the preferred platform. The paper provided initial results on the quality assurance of telementoring systems imposed by the regulatory documents.		Andrius Budrionis;Gunnar Hartvigsen;Rolv-Ole Lindsetmo;Johan Gustav Bellika	2015	International journal of medical informatics	10.1016/j.ijmedinf.2015.05.004	regulation;simulation;medicine;usability;telecommunications;computer science;artificial intelligence;multimedia;law	HCI	-59.77668939850046	-60.441895488491944	885
572ecf22386cfe82a6b04b5dd4b47032199926b3	module: a modular programming environment in prolog	programming environment		integrated development environment;modular programming;prolog	Martin Hofmann;Anne Usha;Souri Das;Kazuhiko Kawamura;Atsushi Kara;Ravi Rastogi	1987			module pattern;first-generation programming language;constraint programming;computer architecture;higher-order programming;declarative programming;programming domain;reactive programming;functional reactive programming;computer science;extensible programming;functional logic programming;modular programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;system programming;concurrent object-oriented programming	PL	-24.828162770337844	22.83553596925708	886
27381a144bc3804292d9a5bc000376e9f41e8d81	three hierarchies of transducers	top down	Composition of top-down tree transducers yields a proper hierarchy of transductions and of output languages. The same is true for ETOL systems (viewed as transducers) and for two-way generalized sequential machines.	programming language;top-down and bottom-up design;transducer	Joost Engelfriet	1982	Mathematical systems theory	10.1007/BF01786975	combinatorics;discrete mathematics;computer science;top-down and bottom-up design;mathematics;algorithm	DB	-2.808854755016295	19.743407217089946	887
ef564c00f3120d299b8084b12e66869dcf84e091	decomposing complex reaction networks using random sampling, principal component analysis and basis rotation	escherichia coli;simulation and modeling;metabolic networks and pathways;metabolic network;monte carlo sampling;top down;systems biology;random sampling;gene regulatory networks;physiological cellular and medical topics;computational biology bioinformatics;metabolic regulation;principal component analysis;monte carlo method;biochemical network;algorithms;molecular mechanics;large classes;biological network;eigenvectors;steady state;bioinformatics	Metabolism and its regulation constitute a large fraction of the molecular activity within cells. The control of cellular metabolic state is mediated by numerous molecular mechanisms, which in effect position the metabolic network flux state at specific locations within a mathematically-definable steady-state flux space. Post-translational regulation constitutes a large class of these mechanisms, and decades of research indicate that achieving a network flux state through post-translational metabolic regulation is both a complex and complicated regulatory problem. No analysis method for the objective, top-down assessment of such regulation problems in large biochemical networks has been presented and demonstrated. We show that the use of Monte Carlo sampling of the steady-state flux space of a cell-scale metabolic system in conjunction with Principal Component Analysis and eigenvector rotation results in a low-dimensional and biochemically interpretable decomposition of the steady flux states of the system. This decomposition comes in the form of a low number of small reaction sets whose flux variability accounts for nearly all of the flux variability in the entire system. This result indicates an underlying simplicity and implies that the regulation of a relatively low number of reaction sets can essentially determine the flux state of the entire network in the given growth environment. We demonstrate how our top-down analysis of networks can be used to determine key regulatory requirements independent of specific parameters and mechanisms. Our approach complements the reductionist approach to elucidation of regulatory mechanisms and facilitates the development of our understanding of global regulatory strategies in biological networks.	biological network;complement system proteins;flux;genetic translation process;leucaena pulverulenta;metabolic process, cellular;monte carlo method;principal component analysis;reductionism;regenerative circuit;requirement;sampling (signal processing);sampling - surgical action;spatial variability;steady state;top-down and bottom-up design;translational regulation	Christian L. Barrett;Markus J. Herrgård;Bernhard O. Palsson	2008	BMC Systems Biology	10.1186/1752-0509-3-30	biology;bioinformatics;theoretical computer science;machine learning;systems biology;monte carlo method	ML	7.192921814854616	-62.5964869896133	888
2743ff159c1ee657ce82525759e4b8b486e5ad85	morphological analyzer as syntactic parser	syntactic parser;morphological analyzer;input word;real word;morphological analyzer return;morphological analyzer break;run-time parser;morphological analysis;main lexicon;lexicon look-up;finite lexicon;initial condition;formal grammar	"""We describe how a simple parser can be built on tile basis of nmrphology and a morphological analyzer. Our initial conditions have been tile tcclmiques and principles ol: Humor, a reversible, shing-bascd tmification tool (Prdszdky 1994). Parsing is perlorlngd by the Sillllc engine as morphological analysis. It is usefld when therc is not enough space to add a new engine to an existing morpl]of ogy-based application (e.g. a spell-checker), but you would like to handle sentence-level information, its well (e.g. a gramnlar checker). The morpimlogical analyzer breaks up words into several parts, all of which stored it] tile main lexicon, l:,ach part has a feature structure and the validily of tile input word is checked by unifying them. Thc morphological analyzer returns various information about a word including its categorization. In a sentence, the category of each word (or morphcme) is considered a rectaletter, and the sentencc itself can be transformed into a recta-word that essentially behaves like a real one. Thus tim set of sentences recognized by tile parser called Hum0rESl( can form a lexicon of recta-words that are processed much rite same way as lexicons of real words (morphology). This means that algorithmic parsing step are substituted by lexicon look-up, which, by definition, is pcrforn~cd following tile stlrJ'ace order of string elements. Both the finitizer that transfimns fornml grammars into finite lexicons and tim tun-tinm parser of the proposed model have running implementations.1 1 I N T R O I ) U C T I O N [,exical entries in a morphology-lmsed system are words. Because of tile similarity, syntactic constructions occurring as entries in a mctaqcxicon can be called recta-words. Mcta-letters, that is, letters o1"""" a recta-word arc morphosyntactic categories having an internal structure that describes syntaelic behavior of the entry in higher level con° structions. The system called Hum0rE,~K (Humor l';nhanced with Syntactic Knowledge, where Humor stands lbr I lighspeed Unification Morphology) to be shown here consists el: nu lnerous rectalexicons. Each o1: them has a name: lhe syntactic category it describes. Categories like S', S, NP, VI< etc. are described in separate lexicons. Meta-lexicons l'ornl a hierarchy, that is, letters in a lnetadexicon can refer to other (but only lower level) lexicons. Parsing on each level, therefore, can be realized as lexical h)ok-up. Neither backtracking, look-ahcad, tier other tilnc-consuming parsing steps arc needed in order to get the analysis of a sentence. The only on-line Ol)eratitm is a unit]ability check for each possible lexical entry that matches lhe sentence in question. I This work was partially supported by the Ihmgm tan National Scientific Fund (()TKA). Gramnmrs are compiled into a nmtti-lcvel pattern slrttchtrc. ()n a lower level, parsing a word results in a recta-letter, that is, part of a recta-word on a higher level. Such structures, lbr example, NI' and VP, are recta-letters coming from lower levels and form a recta-word that can be parsed as a sentence, because of the existence of a rule S -~ NP VP in the original gratltlllar. A COtlIpIcx setliellce gratllt/lar can be broken up into non-rectlrsivc, gralnttlars describing smaller grammatical units on different levels. These granlmars are, of course, nmch simpler than the original one. Recursive transition networks (P, TN) can also be made according to similar principles, but their recursivc nattu'e cannot be Ionnd in our method. In other words: the output symbol of any level does not occur in the actual or lowcr level dictionaries. Tile whole lexicon cascade can be generated front arbitrary grmnmars writ/en in any usual (for the time being, CI:, but in tile near furore any fcahne-based) tbrnmlism. We call this step grammar learninL< The sotl\wue tool we have developed for this reason lakes tile grallllllar ~ts inpttt, creates the largest regular subset of tile language it describes regarding the string-completion limit of Kornai (1985), then lbrms it finite pattern structure by depth limit and length limit fronl the above I'egtliar description. 2 P A R S I N G W I T H P A T T E R N S l'arsers are (conlputational) tools that read and analyze a sentence, and return a wide range of information ~tl)(,lttt it, that is, they recognize (1) if the input is a valid sentence (according to the yules of the object hmguage), (2) segment the input sentence as tnany ways its possible,"""	alloy analyzer;backtracking;categorization;compiler;dictionary;galaxy morphological classification;han unification;initial condition;lexicon;lookup table;mathematical morphology;microsoft word for mac;multitier architecture;np (complexity);naruto shippuden: clash of ninja revolution 3;os-tan;online and offline;packet analyzer;parser;recursion (computer science);reversible computing;spell checker;tun (product standard);tile processor;twisted nematic field effect	Gábor Prószéky	1996			natural language processing;parser combinator;speech recognition;morphological analysis;computer science;linguistics;formal grammar;top-down parsing;initial value problem	NLP	-26.766187253778046	-80.67736660562281	889
2f5436b0374b038c47f46945862d023b4c26b860	three-body systems with coulomb interaction. bound and quasi-bound ss-states	eigenvalues;resonances;matrix;quasi bound states;three body system;binding energy;eigenvectors	A simple Mathematica   (versions 7–9) code for computing SS-state energies and wave functions of three-particles systems is presented. The relevant systems include two-electron atoms, molecular electronic ions and mesomolecular exotic species. In addition to the bound SS-states the code enables one to compute the positions and widths of the lowest resonance, quasi-bound, states. The elegant technique derived from the classical papers of Pekeris is applied. The basis functions are composed of Laguerre functions. The method is based on the perimetric coordinates and specific properties of the Laguerre polynomials. A direct solution of the generalized eigenvalues and eigenvectors problem is used, distinct from Pekeris’ works. The complex scaling method is applied for calculating the resonance states.#R##N##R##N#The resultant wave functions have a simple analytical form, that enables calculation of expectation values of arbitrary physical operators without any difficulties. Only one mathematical parameter characterizing the basis size is required in the input. The other input parameters are of the physical nature.#R##N#Program summary#R##N#Program title: ThreeBSCbSSR#R##N##R##N#Catalogue identifier: AEPU_v1_0#R##N##R##N#Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPU_v1_0.html#R##N##R##N#Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland#R##N##R##N#Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html#R##N##R##N#No. of lines in distributed program, including test data, etc.: 240717#R##N##R##N#No. of bytes in distributed program, including test data, etc.: 14269618#R##N##R##N#Distribution format: tar.gz#R##N##R##N#Programming language: Mathematica 7–9.#R##N##R##N#Computer: Any PC.#R##N##R##N#Operating system: Any which supports Mathematica; tested under Microsoft Windows 7 and Linux.#R##N##R##N#RAM: <5 GB bytes#R##N##R##N#Classification: 2.1, 2.8, 2.9, 16.1, 16.10, 17.16.#R##N##R##N#Nature of problem:#R##N##R##N#The Schrodinger equation for a three-body system has not been solved analytically. Approximate methods must be applied in order to obtain the wave functions or other physical attributes from quantum mechanical calculations.#R##N##R##N#Solution method:#R##N##R##N#The SS-wave function is expanded into a triple set of basis functions which are composed of the exponentials combined with the Laguerre polynomials in the perimetric coordinates. Using specific properties of the Laguerre polynomials, a solution of the three-particles Schrodinger equation for SS-states reduces to solving the generalized eigenvalues and eigenvector problem for the proper Hamiltonian.#R##N##R##N#A complex scaling method is used for calculating the quasi-bound states.#R##N##R##N#Restrictions:#R##N##R##N#The basis size is the limiting factor of the method. In practice, up to 104104 basis states can be used without any problems.#R##N##R##N#Running time:#R##N##R##N#<35 min for 5456 basis states (depends on basis size and computer properties.)	biological system	Evgeny Z. Liverts;Nir Barnea	2013	Computer Physics Communications	10.1016/j.cpc.2013.06.013	eigenvalues and eigenvectors;computer science;theoretical computer science;pure mathematics;mathematics;physics;quantum mechanics	Theory	96.37158892685746	-1.2281873024024232	890
c27b0f68c230e1d186d006d4024a32e61988d706	a sharp lower bound of the randic index of cacti with r pendants	optimisation;combinatorics;optimizacion;combinatoria;randic index;combinatoire;index;connected graph;graphe simple;indice;informatique theorique;cactus;indexation;68r10;borne inferieure;optimization;graphe connexe;lower bound;chemical graph;cota inferior;computer theory;grafo conexo;informatica teorica	"""The Randic index of a simple connected graph G is defined as @?""""u""""v""""@?""""E""""(""""G"""")(d(u)d(v))^-^1^/^2. In this paper, we present a sharp lower bound on the Randic index of cacti with r pendants."""	randić's molecular connectivity index	Anhua Lin;Rong Luo;Xiaoya Zha	2008	Discrete Applied Mathematics	10.1016/j.dam.2007.08.031	database index;combinatorics;connectivity;calculus;mathematics;upper and lower bounds;algorithm	Theory	24.628681424918803	31.639990857707556	891
065f6a632be94b8f23f954c672fc8d02e0c67f6f	induced graph ramsey theory	ramsey number;upper bound;ramsey theory;edge coloring;complete graph	We say that a graph F strongly arrows (G, H) and write F (G, H) if for every edge-coloring of F with colors red and blue a red G or a blue H occurs as an induced subgraph of F . Induced Ramsey numbers are defined by r(G, H) = min{|V (F )| : F (G, H)}. The value of r(G, H) is finite for all graphs, and good upper bounds on induced Ramsey numbers in general, and for particular families of graphs are known. Most of these results, however, use the probabilistic method, and therefore do no yield explicit constructions. This paper provides several constructions for upper bounds on r(G, H) including r(Cn) = r (Cn, Cn) ≤ c (log n) , r(T, Kn) ≤ |T |n |T | log |T , r(B, Cn) ≤ |B| dlog , where T is a tree, B is bipartite, Kn is the complete graph on n vertices and Cn a cycle on n vertices. We also have some new upper bounds for small graphs: r(K3 + e) ≤ 21, and r(K4 − e) ≤ 46.	carrier-to-noise ratio;color;edge coloring;graph coloring;induced subgraph;ordered graph;pokémon red;ramsey's theorem;vertex (geometry)	Marcus Schaefer;Pradyut Shah	2003	Ars Comb.		mathematics;discrete mathematics;graph power;ramsey's theorem;combinatorics;voltage graph;ramsey theory;wagner graph;clebsch graph;moser spindle;triangle-free graph	Theory	30.681128061693634	28.438907723051027	892
875343201ae50cf18737b3e432256cfbf2187b07	growth and propagation of disturbances in a communication network model	complex dynamics;long range correlations communication network model disturbance propagation information networks communications traffic dynamical characteristics network capacity network dynamics distribution network models internet self similarities;telecommunication network planning;internet telecommunication network planning telecommunication traffic packet switching;system dynamics;distributed networks;network capacity;communication system traffic;packet switching;long range correlation;information network;communication networks telecommunication traffic traffic control spine hazards communication system traffic hardware local area networks displays ip networks;telecommunication traffic;internet;community networks;traffic analysis;network dynamics;dynamic characteristic;local area network	In a society in which information is one of the highestvalued commodities, information networks are the economic backbone. Therefore, network vulnerability is a major hazard. Analysis of communications system traffic suggests that there are a variety of similar dynamical characteristics in widely varying systems. There are some universal features of communication networks that may determine the main dynamics of communications. These are: 1) hardware limitations on network capacity, 2) forcing from the increasing number of users towards the network capacity, and 3) a high level of complexity in those systems. These common features may drive the system dynamics and determine its main intrinsic vulnerabilities. The nontrivial character of network dynamics has already been found in traffic analysis of local area networks that display selfsimilarities and long-range correlations. This suggests a complex dynamics in the traffic of packets over the network. In this work simple distribution network models with different levels of complexity are examined and compared to each other and to characteristics of real Internet data.	complex dynamics;complexity;critical point (network science);dynamical system;high-level programming language;ibm notes;internet backbone;network congestion;network model;network traffic control;operating point;portable document format;self-organization;smoothing;software propagation;system dynamics;telecommunications network;traffic analysis;vulnerability (computing)	David E. Newman;Nathaniel D. Sizemore;Vickie E. Lynch;Benjamin A. Carreras	2002		10.1109/HICSS.2002.993977	traffic generation model;local area network;interaction network;network traffic control;the internet;simulation;complex dynamics;telecommunications;network formation;computer science;dynamic network analysis;network dynamics;network simulation;system dynamics;law;packet switching;computer network	ML	-3.5359795740956192	7.339303184051615	893
c83f17f4b0d524f58c3f0dc83599a93c06e4a501	composite correlation filters for detection of geometrically distorted objects using noisy training images	correlation filters;composite filters;pattern recognition	Correlation filters for object detection use information about the appearance and shape of the object of interest. Therefore, detection performance degrades when the appearance of the object in the scene differs from the appearance used in the filter design process. This problem has been approached by utilizing composite filters designed from a training set containing known views of the object of interest. However, common composite filter design is usually carried out under the assumption that the ideal appearance and shape of the target are known. In this work we propose an algorithm for composite filter design using noisy training images. The algorithm is a modification of the class synthetic discriminant function technique that uses arbitrary filter impulse responses. Furthermore, filters can be adapted to achieve a prescribed discrimination capability for a class of backgrounds if a representative sample is known. Computer simulation results obtained with the proposed algorithm are presented and compared with those of common composite correlation filters.		Pablo M. Aguilar-González;Vitaly Kober	2012		10.1007/978-3-642-31149-9_9	adaptive filter;computer vision;computer science;machine learning;pattern recognition;mathematics;filter design;m-derived filter	Vision	54.605972828826886	-63.38935584092939	894
44e168a9aec0226498fd3aa1225b94c31f44a5f0	huffman base-4 text entry glove (h4 teg)	keyboards;text input;text analysis;presses;huffman codes;thumb;error analysis;indexes;visualization;input glove;text analysis data gloves error statistics huffman codes;design input glove text input experimentation;thumb keyboards error analysis indexes visualization presses;error statistics;data gloves;design;entry speed huffman base 4 text entry glove h4 teg pinches user right hand characters command base 4 huffman code error rate visual feedback;experimentation	We designed and evaluated a Huffman base-4 Text Entry Glove (H4-TEG). H4-TEG uses pinches between the thumb and fingers on the user's right hand. Characters and commands use base-4 Huffman codes for efficient input. In a longitudinal study, participants reached 14.0 wpm with error rates <;1%. In an added session without visual feedback, entry speed dropped only by 0.4 wpm. Yet another session was added that required entry of text with punctuation and other symbols. Entry speed dropped by 1.5 wpm.	code;huffman coding;power glove;text entry interface;words per minute;yet another	Bartosz Bajer;I. Scott MacKenzie;Melanie Baljko	2012	2012 16th International Symposium on Wearable Computers	10.1109/ISWC.2012.28	embedded system;design;text mining;simulation;speech recognition;visualization;computer science	HCI	-47.117015032535086	-45.37286935563028	895
43e2749a8fdb006eccdedf0385e264d1200f5a99	minimally covering nsec records and dnssec on-line signing		"""Status of This Memo This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the """"Internet Official Protocol Standards"""" (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Abstract This document describes how to construct DNSSEC NSEC resource records that cover a smaller range of names than called for by RFC 4034. By generating and signing these records on demand, authoritative name servers can effectively stop the disclosure of zone contents otherwise made possible by walking the chain of NSEC records in a signed zone."""	code signing;domain name system security extensions;internet;std bus	Samuel Weiler;Johan Ihren	2006	RFC	10.17487/RFC4470	engineering;communication;world wide web;computer security	Security	-25.986041106384153	88.33679768003648	896
125e2cb0a62548af5bfea7cb25a5b031025bb7b3	invited talk: symmetry and computability in anonymous networks			computability	Evangelos Kranakis	1996			discrete mathematics;computability;mathematics	Logic	47.121821284638315	28.10766858334763	897
9c1f370246661e1ab2e7852722c0bbe2ff0ca13f	associating colors to emotional concepts extracted from unstructured texts		Software creativity is a pervasive branch inside Artificial Intelligence where multiple approaches have place. In the case of developing a software artist the main difficulty resides in retrieving and expressing the feelings of the context properly. This paper addresses that issue introducing a framework able to produce color palettes and abstract paintings based on processing unstructured text. To achieve it, the tool extracts the concepts related to human mood from the text once it is analyzed. Then, they are checked to identify the color that represents their psychological meaning using related literature. The resulting picture is built according to the dimension of the canvas and the amount of colors from the palette obtained. A case study illustrates the applicability of the proposal using two texts selected from Wikipedia. They describe disparate concepts as love or death. A related work is considered to situate the approach in the context and establish comparisons.	artificial intelligence;color;palette (computing);pervasive informatics;situated cognition;wikipedia	Alberto Fernández-Isabel;Antonio F. G. Sevilla;Alberto Espuny Díaz	2016				AI	-56.43703742911009	-33.57896347691244	898
366e3ca6e0b0596ec8f02c4dc59b3d8fd3c3a1f0	real-time service provisioning for mobile and wireless networks	internet protocol;available bandwidth;distributed system;red sin hilo;occupation time;largeur bande;systeme reparti;online game;informatique mobile;mobile device;protocolo internet;temps service;reseau sans fil;pervasive computing;real time;wireless network;service provisioning;protocole internet;voice over ip;004 informatik;tiempo servicio;service time;qualite service;informatica difusa;mobile environment;sistema repartido;temps occupation;senal voceada;informatique diffuse;temps reel;comportement utilisateur;anchura banda;tiempo ocupacion;signal voise;tiempo real;bandwidth;real time services;voiced signal;jeu ordinateur;user behavior;network structure;quality of service;computer games;mobile computing;security;real time application;service provision;service quality;comportamiento usuario;mobile network;calidad servicio	As mobile devices and wireless networks are becoming ubiquitous, the interest of users to deploy real-time applications, e.g., online gaming or Voice-over-IP in such environments is also increasing. Due to the difference between traditional and wireless networks, in particular in terms of available bandwidth and network structure, the concepts used for supporting real-time applications in both networks are different. This paper gives an overview of the key technical challenges that are fundamental and need to be solved in order to easily support real-time applications in wireless and mobile environments. In a first step, issues related to service provisioning in mobile networks are discussed. This is followed by a look at the Quality of Service supported by wireless networks and possible techniques for improving it. Finally, concepts for securing the communication between the users of real-time applications in wireless and mobile networks are presented. For each of these issues, we provide a detailed analysis and an overview of the state-of-the-art. Moreover, we illustrate the main points using distributed online games as an example.	ip camera;mobile device;provisioning;quality of service;real-time clock;real-time computing;real-time locating system;real-time transcription	Károly Farkas;Oliver Wellnitz;Matthias Dick;X. Gu;Marcel Busse;Wolfgang Effelsberg;Yacine Rebahi;Dorgham Sisalem;Dan Grigoras;Kyriakos Stefanidis;Dimitrios N. Serpanos	2006	Computer Communications	10.1016/j.comcom.2005.06.005	internet protocol;multi-frequency network;cellular network;mobile search;simulation;wireless wan;quality of service;wireless site survey;telecommunications;computer science;radio resource management;operating system;wireless network;voice over ip;mobile device;key distribution in wireless sensor networks;municipal wireless network;mobile computing;computer security;service quality;bandwidth;computer network	Mobile	-14.0114435807136	96.12476288940849	899
5d86551b429ab9b24eb3fc7a4eca79db3a63881e	a 1.0–4.0-gb/s all-digital cdr with 1.0-ps period resolution dco and adaptive proportional gain control	ring oscillators;supply controlled ring oscillator;digitally controlled oscillator;decoding;all digital clock and data recovery circuit;mosfets;clocks;bit error rate;ring oscillator;bit rate 1 gbit s to 4 gbit s period resolution dco adaptive proportional gain control all digital clock and data recovery circuit adcdr digitally controlled oscillator supply controlled ring oscillator digitally controlled resistor dcr wide frequency range multiphase clock apgc low jitter clock digital frequency acquisition loop cmos process power consumption size 0 13 mum voltage 1 2 v;adaptive control;adaptive proportional gain control;apgc;jitter clocks mosfets resistors decoding ring oscillators;cmos process;dco;clock and data recovery;digital controller;adcdr;cmos digital integrated circuits;voltage 1 2 v;clock and data recovery circuit;high speed interface all digital clock and data recovery clock and data recovery digitally controlled oscillator digitally controlled resistor;low power electronics;digital frequency acquisition loop;uhf oscillators;resistors;all digital clock and data recovery;clock and data recovery circuits;low jitter clock;digital control;dcr;digitally controlled resistor;proportional control;power consumption;jitter;wide frequency range multiphase clock;bit rate 1 gbit s to 4 gbit s;high speed;period resolution;gain control;mmic oscillators;size 0 13 mum;uhf oscillators adaptive control clock and data recovery circuits clocks cmos digital integrated circuits digital control gain control jitter low power electronics mmic oscillators proportional control resistors;high speed interface	This paper describes the design and implementation of an all-digital clock and data recovery circuit (ADCDR) for multigigabit/s operation. The proposed digitally-controlled oscillator (DCO) incorporating a supply-controlled ring oscillator with a digitally-controlled resistor (DCR) generates wide-frequency-range multiphase clocks with fine resolution. With an adaptive proportional gain controller (APGC) which continuously adjusts a proportional gain, the proposed ADCDR recovers data with a low-jitter clock and tracks large input jitter rapidly, resulting in enhanced jitter performance. A digital frequency-acquisition loop with a proportional control greatly reduces acquisition time. Fabricated in a 0.13-μm CMOS process with a 1.2-V supply, the ADCDR occupies 0.074 mm2 and operates from 1.0 Gb/s to 4.0 Gb/s with a bit error rate of less than 10-14. At a 3.0-Gb/s 231 - 1 PRBS, the measured jitter in the recovered clock is 3.59 psrms and 29.4 pspp, and the power consumption is 11.4 mW.	bang file;bang–bang control;bit error rate;cmos;clock recovery;comparison of raster-to-vector conversion software;data recovery;device configuration overlay;digitally controlled oscillator;gigabyte;image resolution;image scaling;normalized frequency (unit);prototype;pseudorandom binary sequence;quantum clock;ring oscillator;transistor;tree accumulation	Heesoo Song;Deok-Soo Kim;Do-Hwan Oh;Suhwan Kim;Deog-Kyoon Jeong	2011	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2010.2082272	embedded system;electronic engineering;real-time computing;jitter;digital control;adaptive control;computer science;engineering;control theory;digitally controlled oscillator	Mobile	59.96927570452681	49.63547000447597	900
1a99ae94121cb218a8f7234868a47a42eda0ceca	a wide-band high-resolution spectrum analyzer	frequency dependence;high resolution;40 mhz digital spectrum analyser synchronisation nasa configuration control discrete fourier transform time domain input signal frequency domain sky survey extraterrestrial intelligence deep space network;fourier transform;spectrum analysis;real time;signal analysis;signal detection;broadband;spectrum;project seti;fast fourier transform;analog to digital converters;fourier transformation;fast fourier transformations;spectral analysis fast fourier transforms spectral analysers;signal processing;wideband spectral analysis discrete fourier transforms frequency domain analysis frequency synchronization signal analysis fourier transforms time domain analysis prototypes intelligent networks;discrete fourier transform;detection algorithm;sky surveys astronomy;analog to digital converter;fast fourier transforms;algorithms;time domain;deep space network;spectral analysis;frequency domain;measuring instruments;amplification;real time operation;jet propulsion laboratory;spectral analysers	This paper describes a two-million-channel 40-MHz-bandwidth, digital spectrum analyzer under development at the Jet Propulsion Laboratory. The analyzer system will serve as a prototype processor for the sky survey portion of NASA's Search for Extraterrestrial Intelligence program and for other applications in the Deep Space Network. The analyzer digitizes an analog input, performs a 2(21)-point, Discrete Fourier Transform, accumulates the output power, normalizes the output to remove frequency-dependent gain, and automates simple signal detection algorithms. Due to its built-in frequency-domain processing functions and configuration flexibility, the analyzer is a very powerful tool for real-time signal analysis and detection.	algorithm;analog;analyzer, device;detection theory;discrete fourier transform;image resolution;prototype;real-time clock;search for extraterrestrial intelligence;signal detection (psychology);signal processing;spectrum analyzer;united states national aeronautics and space administration;unix signal	Maureen P. Quirk;Michael F. Garyantes;Helmut C. Wilck;Michael J. Grimm	1988	IEEE transactions on acoustics, speech, and signal processing	10.1109/29.9030	fourier transform;computer vision;fast fourier transform;electronic engineering;spectrum analyzer;telecommunications;computer science;signal processing;mathematics;signal analyzer;logic analyzer	Embedded	56.17347643677729	63.54816186308304	901
debcaebb52094a29b7d3dee2cc10475615b28242	a knowledge based genetic algorithm for path planning in unstructured mobile robot environments	genetic operator;knowledge based genetic algorithm;mobile robot;path planning;evaluation method;mobile robots;genetic algorithms path planning mobile robots packaging genetic engineering knowledge engineering shape costs object detection neural networks;robot path planning knowledge based genetic algorithm unstructured mobile robot environments 2d robot environments collision detection;2 dimensional;2d robot environments;domain knowledge;collision detection;robot path planning;simulation study;genetic algorithm;genetic algorithms;search problems;unstructured mobile robot environments;local search;knowledge based systems;search problems genetic algorithms knowledge based systems mobile robots path planning;knowledge base	This paper proposes a knowledge based genetic algorithm (GA) for path planning of a mobile robot in unstructured environments. The algorithm uses a unique problem representation method to represent 2-dimensional robot environments with complex obstacle layouts of arbitrary obstacle shapes. An effective evaluation method is specially developed for the proposed genetic algorithm. The evaluation method is able to accurately detect collisions between a robot path and an arbitrarily shaped obstacle, and assigns costs that are very effective for the proposed genetic algorithm. The proposed GA uses problem-specific genetic algorithms for robot path planning instead of the standard GAs. The proposed knowledge based genetic algorithm incorporates the domain knowledge of robot path planning into its specialized operators, where some also combine a local search technique. The effectiveness and efficiency of the proposed genetic algorithm is demonstrated by simulation studies. The irreplaceable role of the specialized genetic operators for solving robot path-planning problem is shown by a comparison study	computation;fitness function;genetic algorithm;local search (optimization);mobile robot;population;randomness;simulation;time complexity	Yanrong Hu;Simon X. Yang;Lizhong Xu;Max Q.-H. Meng	2004	2004 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2004.1521879	mobile robot;computer vision;knowledge base;simulation;genetic algorithm;computer science;artificial intelligence;machine learning;genetic representation;population-based incremental learning	Robotics	54.91840801367776	-24.266968912908677	902
d22d70bbfc613f62d6f6678ee75f2f9140f1280a	smart cities and the role of is research in improving urban life	peer reviewed;chapter	With two thirds of the world’s population soon residing in cities, the question of how information technology affects urban life and global urbanization trends becomes more relevant than ever. Due to the interplay of economic, social, organizational, and technological dimensions, the IS discipline is in an excellent position to contribute to solving this grand societal challenge. This panel will explore various issues in this context, such as e-government, emergency response, and sustainable energy and mobility, to derive implications for future IS research directions and for successful collaborations with industry and municipal partners.		Tobias Brandt;Jamie Cudden;Wolfgang Ketter;David Prendergast;Mihoko Sakurai;Richard T. Watson	2016			peer review;social science;computer science;socioeconomics	HCI	-76.39642897191514	-9.858971759442468	903
7124cdce3c7053bd036b2c634efaf589c4e64b4d	a bifurcation of a synchronous oscillations into a torus in a system of two mutually inhibitory avlsi neurons: experimental observation	oscillations;cross correlation function;phase shift;spectrum;correlation dimension;half center oscillator;quasiperiodic oscillations;central pattern generator;silicon neuron	We studied a system of two ‘identical’ oscillatory aVLSI neurons with mutually inhibitory connections. The system demonstrates different oscillatory behaviors depending on the strength of the inhibitory connections: anti-phasic, synchronous, phase-shifted, and quasiperiodic oscillations. We experimentally observed a bifurcation of synchronous oscillations into quasiperiodic oscillations with two independent frequencies. This bifurcation was confirmed by the analysis of the phase between neuronal outputs, the cross-correlation function, the amplitude spectrum, and the correlation dimension. The observation of this bifurcation in a physical system suggests that this scenario might also occur in living half-center oscillators, such as those found in central pattern generators.	bifurcation theory	Vladimir E. Bondarenko;Gennady S. Cymbalyuk;Girish N. Patel;Stephen P. DeWeerth;Ronald L. Calabrese	2003	Neurocomputing	10.1016/S0925-2312(02)00739-7	spectrum;central pattern generator;correlation dimension;cross-correlation;control theory;mathematics;phase;oscillation	Robotics	17.38162297159834	-70.71026818054679	904
085524dfe24b50a2c2ce1641222cceeb352af078	system identification of microwave filters from multiplexers by rational interpolation	interpolation;computacion informatica;grupo de excelencia;continuous time filters;system identification;ciencias basicas y experimentales;system transfer functions;scattering parameters	Microwave multiplexers are multi-port structures composed of several two-port filters connected to a common junction. This paper addresses the de-embedding problem, in which the goal is to determine the filtering components given the measured scattering parameters of the overall multiplexer at several frequencies. Due to structural properties, the transmission zeros of the filters play a crucial role in this problem, and, consequently, in our approach. We propose a system identification algorithm for deriving a rational model of the filters' scattering matrix. The approach is based on rational interpolation with derivative constraints, with the interpolation conditions being located precisely at the filters' transmission zeros.	interpolation;microwave;multiplexer;system identification	Sanda Lefteriu;Martine Olivi;Fabien Seyfert;Matteo Oldoni	2017	Automatica	10.1016/j.automatica.2016.09.034	mathematical optimization;electronic engineering;system identification;interpolation;control theory;mathematics;scattering parameters	Logic	72.06954598701824	48.58023470297196	905
4405aea1bc911f5a95f434d7e42f1c59d23377b3	complexity and stability in biological systems	biological regulatory network;complexity;stability;evolutionary entropy;robustness	The hypothesis that a positive correlation exists between the complexity of a biological system, as described by its connectance, and its stability, as measured by its ability to recover from disturbance, derives from the investigations of the physiologists, Bernard and Cannon, and the ecologist Elton. Studies based on the ergodic theory of dynamical systems and the theory of large deviations have furnished an analytic support for this hypothesis. Complexity in this context is described by the mathematical object evolutionary entropy, stability is characterized by the rate at which the system returns to its stable conditions (steady state or periodic attractor) after a random perturbation of its robustness. This article reviews the analytical basis of the entropy — robustness theorem — and invokes studies of genetic regulatory networks to provide empirical support for the correlation between complexity and stability. Earlier investigations based on numerical studies of random matrix models and the notion of local stability have led to the claim that complex ecosystems tend to be more dynamically fragile. This article elucidates the basis for this claim which is largely inconsistent with the empirical observations of Bernard, Cannon and Elton. Our analysis thus resolves a long standing controversy regarding the relation between complex biological systems and their capacity to recover from perturbations. The entropy-robustness principle is a mathematical proposition with implications for understanding the basis for the large variations in stability observed in biological systems having evolved under different environmental conditions.	biological system;complexity	Jacques Demongeot;Lloyd A. Demetrius	2015	I. J. Bifurcation and Chaos	10.1142/S0218127415400131	complexity;stability;control theory;mathematics;mathematical economics;thermodynamics;statistics;robustness	Theory	0.2627655680936596	-10.820170128470414	906
07e5c2cc7a3c800093f8f8cb8f59492704911343	utilizing microblogs for web page relevant term acquisition		To allow advanced processing of information available on the Web, the web content necessitates semantic descriptions (metadata) processable by machines. Manual creation of metadata even in a lightweight form such as (web page) relevant terms is for us humans demanding and almost an impossible task, especially when considering open information space such as the Web. New approaches are devised continuously to automate the process. In the age of the Social Web an important new source of data to mine emerges – social annotations of web content. In this paper we utilize microblogs in particular. We present a method for relevant domain terms extraction for web resources based on processing of the biggest microblogging service to date – Twitter. The method leverages social characteristics of the Twitter network to consider different relevancies of Twitter posts assigned to the web resources. We evaluated the method in a user experiment while observing its performance for different types	experiment;web content;web page;web resource;world wide web	Tomás Uhercík;Marián Simko;Mária Bieliková	2013		10.1007/978-3-642-35843-2_39	database;world wide web;information retrieval	Web+IR	-27.044588496640703	-50.88856209812082	907
f5060e446b04a752d6dae930873e1d693e3643c2	comparative analysis of classification results between compact and fully polarimetric sar images in random forest classifier		This paper displays a case study which accomplishes crop classification of simulated compact polarimetric (CP) SAR and fully polarimetric (FP) SAR images with Random Forest. Since the potential of CP SAR in classification has been illustrated by various researches, we intend to find out which of the polarimetric features are more superior in crop classification, through the importance rank of Random Forest classifier. Experiments are carried out based on an L-band AIRSAR FP SAR image and an ALOS-2/PALSAR-2 SM-2 FP image. Comparison analysis of feature importance between CP and FP demonstrates the intrinsic connotations of selected polarimetric characteristics, which provide guiding information for future investigation of CP SAR classification.	l band;polarimetry;random forest	Lu Xu;Hong Zhang;Chao Wang	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127859	artificial intelligence;computer vision;computer science;random forest;polarimetry	Embedded	78.33046982326772	-58.21891680980684	908
93c34fa6eab9adfa4e81e4130c042f250bbd9270	failure analysis and modeling in large multi-site infrastructures		Every large multi-site infrastructure such as Grids and Clouds must implement fault-tolerance mechanisms and smart schedulers to enable continuous operation even when resource failures occur. Evaluating the efficiency of such mechanisms and schedulers requires representative failure models that are able to capture realistic properties of real world failure data. This paper shows that failures in multi-site infrastructures are far from being randomly distributed. We propose a failure model that captures features observed in real failure traces.	continuous operation;failure analysis;fault tolerance;lex (software);marginal model;provisioning;randomness;regret (decision theory);scheduling (computing);test harness;tracing (software);unavailability	Tran Ngoc Minh;Guillaume Pierre	2013		10.1007/978-3-642-38541-4_10	real-time computing;simulation;predictive failure analysis;distributed computing	OS	-25.166542158063518	61.805382097371826	909
1a231e2a7cc89c71e78f65e33661b9fc46ed99e1	a model for rendering high intensity lights	illumination;computer graphics;sintesis imagen;image synthesis;synthese image;rendu;eclairement;grafico computadora;infographie;alumbrado;rendering	-ln photo-realistic image rendering, light is the most important factor, which is determining if the generated image of a rendered scene will be perceived by the observer as the image of a real scene. Actual global lighting models are based on reflection and diffusion of light on surfaces of the modeled scene. This approach cannot render high intensity effects and glare effects, which are important not only for the artistic impression of the observer, but they are crucial in drive simulators. This paper proposes a new model for rendering high intensity lights, blooming, and glare effects. This model is based on the human eye structure and its physiology. An efficient algorithm for rendering those phenomena is also given.	algorithm;bloom (shader effect);reflection (computer programming);rendering (computer graphics);simulation	Przemyslaw Rokita	1993	Computers & Graphics	10.1016/0097-8493(93)90032-5	computer vision;image-based modeling and rendering;3d rendering;rendering;computer science;real-time rendering;computer graphics;image-based lighting;computer graphics (images)	Graphics	62.67601800398513	-52.50224370738789	910
233f4935709af55a6da5f1314a096c4d7f2637a8	speaker characterization using spectral subband energy ratio based on harmonic plus noise model	front end;modulated noise accounting;speaker characterization;harmonic speech analysis speaker characterization harmonic plus noise model feature extraction harmonics accounting signal periodicity modulated noise accounting glottal airflow turbulence speech signal decomposition harmonic plus noise model approach spectral subband energy ratio estimation sser estimation vocal tract speaker verification gmm ubm system conventional mfcc feature conventional mel frequency cepstral coefficients features;speech signal decomposition;signal analysis;signal periodicity;speech analysis;speaker recognition feature extraction harmonic analysis modulation;vocal tract;speech;harmonics accounting;harmonic plus noise model approach;speaker verification;speaker recognition;mel frequency cepstral coefficient;physical characteristic;spectral subband energy ratio estimation;harmonic plus noise model speaker verification spectral subband energy ratio;harmonic plus noise model;feature extraction;harmonic speech analysis;glottal airflow turbulence;gmm ubm system;bandwidth;conventional mel frequency cepstral coefficients features;conventional mfcc feature;atmospheric modeling;spectral analysis;experience base;sser estimation;spectral subband energy ratio;fundamental frequency;noise speech harmonic analysis feature extraction mel frequency cepstral coefficient atmospheric modeling bandwidth;noise;modulation;harmonic analysis	This paper proposes a feature extraction for speaker characterization by exploring the relationship between the two distinct components of the speech signal, one is harmonics accounting for the periodicity of the signal and the other is modulated noise accounting for the turbulences of the glottal airflow. The harmonic and noise parts of the speech signal are decomposed based on the Harmonic plus Noise Model approach. We estimate the spectral subband energy ratios (SSERs) as the speaker characteristic features, which are expected to reflect the interaction property of the vocal tract and glottal airflow of individual speakers for speaker verification. The speaker verification experiments based on a GMM-UBM system have shown the efficiency of the SSER features, reducing the error equal rate by 27.2% by combining with the conventional MFCC features.	experiment;feature extraction;modulation;quasiperiodicity;speaker recognition;tract (literature)	Yanhua Long;Zhi-Jie Yan;Frank K. Soong;Li-Rong Dai;Wu Guo	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947359	vocal tract;speaker recognition;atmospheric model;speech recognition;feature extraction;computer science;noise;speech;front and back ends;signal processing;harmonic analysis;pattern recognition;fundamental frequency;bandwidth;modulation	Robotics	-11.604849951064908	-91.11924334805117	911
051ad8de6c565a5478fe85927b21ea7ad75cbf33	unifying synchronous tree adjoining grammars and tree transducers via bimorphisms	command and control;tree adjoining grammar;formal language;context free language;functional programming	We place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms, continuing the unification of synchronous grammars and tree transducers initiated by Shieber (2004). Along the way, we present a new definition of the tree-adjoining grammar derivation relation based on a novel direct inter-reduction of TAG and monadic macro tree transducers. Tree transformation systems such as tree transducers and synchronous grammars have seen renewed interest, based on a perceived relevance to new applications, such as importing syntactic structure into statistical machine translation models or founding a formalism for speech command and control. The exact relationship among a variety of formalisms has been unclear, with a large number of seemingly unrelated formalisms being independently proposed or characterized. An initial step toward unifying the formalisms was taken (Shieber, 2004) in making use of the formallanguage-theoretic device of bimorphisms, previously used to characterize the tree relations definable by tree transducers. In particular, the tree relations definable by synchronous tree-substitution grammars (STSG) were shown to be just those definable by linear complete bimorphisms, thereby providing for the first time a clear relationship between synchronous grammars and tree transducers. St ua rt M .S hi eb er .2 00 6. U ni fy in g Sy nc hr on ou s Tr ee -A dj oi ni ng G ra m m ar s an d Tr ee Tr an sd uc er s vi a B im or ph is m s. In P ro ce ed in gs of th e 11 th C on fe re nc e of th e E ur op ea n C ha pt er of th e A ss oc ia tio n fo r C om pu ta tio na lL in gu is tic s (E AC L06 ), Tr en to ,I ta ly ,3 –7 A pr il. In this work, we show how the bimorphism framework can be used to capture a more powerful formalism, synchronous tree-adjoining grammars, providing a further uniting of the various and disparate formalisms. After some preliminaries (Section 1), we begin by recalling the definition of tree-adjoining grammars and synchronous tree-adjoining grammars (Section 2). We turn then to a set of known results relating context-free languages, tree homomorphisms, tree automata, and tree transducers to extend them for the tree-adjoining languages (Section 3), presenting these in terms of restricted kinds of functional programs over trees, using a simple grammatical notation for describing the programs. This allows us to easily express generalizations of the notions: monadic macro tree homomorphisms, automata, and transducers, which bear (at least some of) the same interrelationships that their traditional simpler counterparts do (Section 4). Finally, we use this characterization to place the synchronous TAG formalism in the bimorphism framework (Section 5), further unifying tree transducers and other synchronous grammar formalisms. We also, in passing, provide a new characterization of the relation between TAG derivation and derived trees, and a new simpler and more direct proof of the equivalence of TALs and the output languages of monadic macro tree transducers.	automata theory;context-free language;eb-eye;field electron emission;formal grammar;formal system;key derivation function;numerical aperture;ph (complexity);programming language;relevance;roland gs;secure digital;semantics (computer science);statistical machine translation;transducer;tree (data structure);tree automaton;tree-adjoining grammar;turing completeness;unification (computer science)	Stuart M. Shieber	2006				NLP	-3.728710465148745	20.78636473424984	912
72299fa20708e2c202c67ee2b61f743728c5c8be	towards autonomous decision making in multi-agent environments using fuzzy logic	debugging;puesta a punto programa;metodo analitico;multiagent system;metodo paso a paso;step by step method;systeme aide decision;soccer;decision borrosa;logique floue;logica difusa;decision floue;sistema ayuda decision;prise decision;development process;potential field;debogage;fuzzy logic;decision support system;football;analytical method;methode pas a pas;methode analytique;sistema multiagente;toma decision;development time;fuzzy system;fuzzy decision;systeme multiagent;futbol	In this paper, a step-by-step procedure for designing a fuzzy decision making system for RoboCup agents is introduced and implemented. By using this mechanism agents can determine, which direction to look at. The development process and debugging of this approach are much less time consuming and very simple to follow, in comparison to other analytical hand coded implementations with complex conditions and many parameters in the code. The results show that, the methodology can be employed in many decision-making problems like the soccer agent's case and other potential fields, to decrease the development time and improve the efficiency of a decision making system. Up until now, fuzzy systems have been used rarely by any of the participating teams in the annual RoboCup competitions. This paper could serve as the first inception for the design of a RoboCup agent, which uses internal fuzzy systems for many of its decision-making tasks.	autonomous robot;fuzzy logic	Hamid Haidarian Shahri	2003		10.1007/3-540-45023-8_24	fuzzy logic;simulation;decision support system;computer science;artificial intelligence;debugging;operations research;software development process	AI	0.5739756570204129	-30.672917170177925	913
47d0dd878529d314c98c84d597211059b0a0966a	social network analysis application in tacit knowledge management	user feedback information learning;immune algorithm;memorization;user semantic targets;self adaptation;user requirement;image feature representation immune algorithm content based image retrieval image lib system image similarity retrieval retrieval effectiveness memorization self adaptation user feedback information learning user semantic targets user requirement relevance feedback;feature extraction;retrieval effectiveness;image lib system;relevance feedback artificial immune systems content based retrieval feature extraction image retrieval learning artificial intelligence;image similarity retrieval;image retrieval feedback information retrieval content based retrieval clustering algorithms euclidean distance information technology target recognition testing decision trees;learning artificial intelligence;content based image retrieval;relevance feedback;content based retrieval;artificial immune systems;image feature representation;image retrieval;image similarity	Since its special spreading charactaristic and individual speciality, tacit knowledge become the difficult part in knowledge management. In this article, the author explains how to manage tacit knowledge effectively through social relation network analysis, and further analyze quantatively such key indexes as network's whole structural, network's mean density, average distance, node centrality in a network, network's degree centrality, betweenness centrality of network in an organization by social relation network. In the end, this article believes that social relation network analysis can be used to analyze quantitatively and manage tacit knowledge effectively.	betweenness centrality;gist;knowledge management;social network analysis	Weiping Zhu;Liangshan Shao;Zhuqing Huang	2007	Workshop on Intelligent Information Technology Application (IITA 2007)	10.1109/IITA.2007.46	computer vision;visual word;feature extraction;image retrieval;computer science;artificial intelligence;user requirements document;machine learning;pattern recognition;memorization;information retrieval	AI	-14.52765814685167	-62.50834434065324	914
d5b09138201d2e9f1ff004bb78b17ed0793d3cb5	multiple graph unsupervised feature selection	unsupervised learning;l 2 p;l 2;p norm;feature selection;multiple graph	Feature selection improves the quality of the model by filtering out the noisy or redundant part. In the unsupervised scenarios, the selection is challenging due to the unavailability of the labels. To overcome that, the graphs which can unfold the geometry structure on the manifold are usually used to regularize the selection process. These graphs can be constructed either in the local view or the global view. As the local graph is more discriminative, previous methods tended to use the local graph rather than the global graph. But the global graph also has useful information. In light of this, in this paper, we propose a multiple graph unsupervised feature selection method to leverage the information from both local and global graphs. Besides that, we enforce the l 2 , p norm to achieve more flexible sparse learning. The experiments which inspect the effects of multiple graph and l 2 , p norm are conducted respectively on various datasets, and the comparisons to other mainstream methods are also presented in this paper. The results support that the multiple graph could be better than the single graph in the unsupervised feature selection, and the overall performance of the proposed method is higher than the other comparisons. HighlightsA novel unsupervised feature selection algorithm is proposed which combines multiple graphs to uncover the manifold.The l 2 , p norm has more flexibility in controlling the sparse learning, thereby resulting in better performance.Combining multiple graph and l 2 , p norm results in better performance.	feature selection	Xingzhong Du;Yan Yan;Pingbo Pan;Guodong Long;Lei Zhao	2016	Signal Processing	10.1016/j.sigpro.2014.12.027	unsupervised learning;norm;mathematical optimization;null model;computer science;machine learning;pattern recognition;mathematics;feature selection	NLP	23.742052239870464	-42.81049663884036	915
7aede04ffbb009903e84188bce47afdd26322a5d	an efficient strategy for non-horn deductive data bases	base relacional dato;query processing;interrogation base donnee;interrogacion base datos;estrategia;dedeuctive database;logical programming;relational database;strategy;tratamiento cuestion;programmation logique;base dato deductiva;traitement question;non horm;base donnee relationnelle;base donnee deductive;programacion logica;strategie;database query;deductive databases	Many efficient strategies have been presented in the literature to derive answers to queries in the context of Deductive Databases. Most of them deal with definite Horn clauses. More recently, strategies have been defined to deal with non-Horn clauses but they adopt the Generalized Closed World Assumption to reduce incompleteness. In this paper we present an efficient strategy to deal with non-Horn clauses in pure logic, i.e. without any kind of assumption. This strategy is an extension of the so-called ALEXANDRE strategy to this particular context. The set of rules is transformed into another set of rules, in a compilation phase, in such a way that their execution in forward chaining simulates a variant of SL resolution. Particular attention is payed to the constants in the queries to reduce the set of derived clauses.	database	Robert Demolombe	1989		10.1016/0304-3975(51)90010-2	horn-satisfiability;strategy;relational database;computer science;data mining;database;algorithm	DB	-26.861195055112734	10.237086245815489	916
5c8607358b77e8e1ab068f1b97be9dfbd1faefbf	overlap-based icp tuning for robust localization of a humanoid robot		State estimation techniques for humanoid robots are typically based on proprioceptive sensing and accumulate drift over time. This drift can be corrected using exteroceptive sensors such as laser scanners via a scene registration procedure. For this procedure the common assumption of high point cloud overlap is violated when the scenario and the robot's point-of-view are not static and the sensor's field-of-view (FOV) is limited. In this paper we focus on the localization of a robot with limited FOV in a semi-structured environment. We analyze the effect of overlap variations on registration performance and demonstrate that where overlap varies, outlier filtering needs to be tuned accordingly. We define a novel parameter which gives a measure of this overlap. In this context, we propose a strategy for robust non-incremental registration. The pre-filtering module selects planar macro-features from the input clouds, discarding clutter. Outlier filtering is automatically tuned at run-time to allow registration to a common reference in conditions of non-uniform overlap. An extensive experimental demonstration is presented which characterizes the performance of the algorithm using two humanoids: the NASA Valkyrie, in a laboratory environment, and the Boston Dynamics Atlas, during the DARPA Robotics Challenge Finals.		Simona Nobili;Raluca Scona;Marco Caravagna;Maurice Fallon	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989547	point cloud;filter (signal processing);humanoid robot;control theory;clutter;robot;engineering;outlier;computer vision;artificial intelligence;robotics;planar	Robotics	53.96526235029445	-38.579212359704556	917
c0ecde0f379e2f486e48735cb3e7f68f50dbf1a6	weakly-supervised learning of metric aggregations for deformable image registration		Deformable registration has been one of the pillars of biomedical image computing. Conventional approaches refer to the definition of a similarity criterion that, once endowed with a deformation model and a smoothness constraint, determines the optimal transformation to align two given images. The definition of this metric function is among the most critical aspects of the registration process. We argue that incorporating semantic information (in the form of anatomical segmentation maps) into the registration process will further improve the accuracy of the results. In this paper, we propose a novel weakly supervised approach to learn domain specific aggregations of conventional metrics using anatomical segmentations. This combination is learned using latent structured support vector machines (LSSVM). The learned matching criterion is integrated within a metric free optimization framework based on graphical models, resulting in a multi-metric algorithm endowed with a spatially varying similarity metric function conditioned on the anatomical structures. We provide extensive evaluation on three different datasets of CT and MRI images, showing that learned multi-metric registration outperforms single-metric approaches based on conventional similarity measures.	algorithm;align (company);anatomic structures;computation (action);graphical model;image registration;map;mathematical optimization;numerous;supervised learning;support vector machine;biologic segmentation;registration - actclass	Enzo Ferrante;Puneet K. Dokania;Rafael Marini;Nikos Paragios	2018	IEEE journal of biomedical and health informatics	10.1109/JBHI.2018.2869700	computer vision;support vector machine;supervised learning;metric (mathematics);smoothness;artificial intelligence;anatomical segmentation;pattern recognition;computer science;image registration;graphical model	Vision	43.392289625289926	-77.69760586517066	918
5c46ddda82d42ec9456a615a2d438eb4665b4676	channel assignment algorithms for osa-enabled wlans exploiting prioritization and spectrum heterogeneity	wlan	Allowing WLANs to exploit opportunistic spectrum access (OSA) is a promising approach to alleviate spectrum congestion problems in overcrowded unlicensed ISM bands, especially in highly dense WLAN deployments. In this context, novel channel assignment mechanisms jointly considering available channels in both unlicensed ISM and OSA-enabled licensed bands are needed. Unlike classical schemes proposed for legacy WLANs, channel assignment mechanisms for OSAenabled WLAN should face two distinguishing issues: channel prioritization and spectrum heterogeneity. The first refers to the fact that additional prioritization criteria other than interference conditions should be considered when choosing between ISM or licensed band channels. The second refers to the fact that channel availability might not be the same for all WLAN Access Points because of primary users’ activity in the OSAenabled bands. This paper firstly formulates the channel assignment problem for OSA-enabled WLANs as a Binary Linear Programming (BLP) problem. The resulting BLP problem is optimally solved by means of branch and bound algorithms and used as a benchmark to develop more computationally efficient heuristics. Upon such a basis, a novel channel assignment algorithm based on weighted graph coloring heuristics and able to exploit both channel prioritization and spectrum heterogeneity is proposed. The algorithm is evaluated under different conditions of AP density and primary band availability. key words: binary linear programming, channel assignment, channel prioritization, OSA, spectrum heterogeneity, WLAN	algorithm;algorithmic efficiency;assignment problem;benchmark (computing);branch and bound;graph coloring;heuristic (computer science);implicit shape model;interference (communication);linear programming;network congestion	Francisco Novillo;Ramon Ferrús	2012	IEICE Transactions		wi-fi;mathematical optimization;telecommunications;computer science;operating system;computer network	Mobile	27.681271222683637	92.2446213926086	919
3606db137cfea3ae9ba887da03f46b801bd2df9c	eigenvectors of the discrete fourier transform based on the bilinear transform	signal image and speech processing;quantum information technology spintronics;discrete fourier transform;eigenvectors	Determining orthonormal eigenvectors of the DFT matrix, which is closer to the samples of Hermite-Gaussian functions, is crucial in the definition of the discrete fractional Fourier transform. In this work, we disclose eigenvectors of the DFT matrix inspired by the ideas behind bilinear transform. The bilinear transform maps the analog space to the discrete sample space. As jω in the analog s-domain is mapped to the unit circle one-to-one without aliasing in the discrete z-domain, it is appropriate to use it in the discretization of the eigenfunctions of the Fourier transform. We obtain Hermite-Gaussian-like eigenvectors of the DFT matrix. For this purpose we propose three different methods and analyze their stability conditions. These methods include better conditioned commuting matrices and higher order methods. We confirm the results with extensive simulations.	aliasing;bilinear transform;coefficient;dft matrix;discrete fourier transform;discretization;fractional fourier transform;map;one-to-one (data model);simulation	Ahmet Serbes;Lutfiye Durak-Ata	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/191085	fourier transform;discrete hartley transform;constant q transform;mathematical optimization;discrete-time fourier transform;mathematical analysis;discrete mathematics;hartley transform;s transform;harmonic wavelet transform;pseudo-spectral method;short-time fourier transform;eigenvalues and eigenvectors;dft matrix;fractional fourier transform;discrete sine transform;discrete fourier transform;discrete cosine transform;mathematics;discrete fourier transform;fourier analysis;non-uniform discrete fourier transform;fourier transform on finite groups;bilinear transform;cyclotomic fast fourier transform	ML	56.18065437318384	20.151842145809706	920
cbb4898de5545a509442128f118fc067e7982283	controller-dynamic-linearization-based model free adaptive control for discrete-time nonlinear systems	closed loop systems;nonlinear control systems;three tank liquid control experimental system controller dynamic linearization based model free adaptive control discrete time nonlinear systems mfac method mfac scheme designs compact form dynamic linearization based controller partial form dynamic linearization based controller discrete time siso nonlinear systems data driven control method controlled plant model controller parameter tuning controlled plant i o data closed loop system mfac prototype cfdlc mfac stability pfdlc mfac stability;adaptive control;discrete time systems;model free adaptive control mfac data driven control ddc discrete time nonlinear system dynamic linearization approach;stability;optimal control;data models nonlinear dynamical systems control systems mathematical model discrete time systems;control system synthesis;linearisation techniques;controllers;stability adaptive control closed loop systems control system synthesis controllers discrete time systems linearisation techniques nonlinear control systems optimal control	A new type of model free adaptive control (MFAC) method, including MFAC scheme designs with the compact-form-dynamic-linearization-based controller (CFDLc) and partial-form-dynamic-linearization-based controller (PFDLc), is presented for a class of discrete-time SISO nonlinear systems. The proposed method is a pure data-driven control method since the controller is independent of the model of the controlled plant, and controller parameter tuning is merely based on the measured I/O data of the controlled plant in closed loop. Differing from the MFAC prototype, the proposed method uses the dynamic linearization approach not only on ideal controller but also on the plant. The stability of the CFDLc-MFAC and PFDLc-MFAC is guaranteed by rigorous theoretical analysis, and the effectiveness is evaluated on simulation examples and a three-tank liquid control experimental system.	experimental system;input/output;nonlinear system;prototype;pure data;simulation interoperability standards organization	Zhongsheng Hou;Yuanming Zhu	2013	IEEE Transactions on Industrial Informatics	10.1109/TII.2013.2257806	control engineering;electronic engineering;controller;optimal control;stability;adaptive control;control theory;mathematics;statistics	Embedded	65.58651496252787	-4.462680571012768	921
1b0f99b9b63e57fd97a5c886c25132366ebb97c5	a 12-bit 80 ms/s 110 mw floating analog-to-digital converter	0 6 micron;cmos integrated circuits;resolution;double poly treble metal cmos technology;high resolution;cmos integrated circuits analogue digital conversion low power electronics adaptive signal processing;decoding;12 bit;clocks;signal design;mid a d conversion;coarse a d conversion;0 6 micron floating analog to digital converter size speed resolution low power dissipation adaptive time sampling interval adjustment coarse a d conversion mid a d conversion fine a d conversion power dissipation area double poly treble metal cmos technology 12 bit 110 mw;fine a d conversion;chip;speed;low power;adaptive signal processing;low power dissipation;signal processing;analogue digital conversion;floating analog to digital converter;power dissipation;voltage;low power electronics;analog to digital converter;signal resolution;area;sampling methods decoding signal processing voltage signal resolution signal design clocks;adaptive time sampling interval adjustment;sampling methods;size;110 mw;high speed	An 12-bit 80Msh 1 1 OmW CMOS floating analog-to-digital converter (ADC) has been integrated into an area of 0.5 mm2. This ADC has excellent characteristics, such as simple architecture, small size, high speed, high resolution, low power dissipation and adaptive time sampling interval adjustment. In this design, three stages, coarse, mid and fine N D conversion, are accomplished in floating architecture, which saves area and power dissipation. Consequently, a large number of comparators have been removed. The chip used 0.6pm double poly, treble metal CMOS technology.	12-bit;analog-to-digital converter;cmos;cpu power dissipation;comparator;image resolution;sampling (signal processing)	Hongwei Wang;Cheong-fat Chan;Oliver Chiu-sing Choy	2002		10.1109/ISCAS.2002.1010179	chip;adaptive filter;embedded system;sampling;computer vision;electronic engineering;voltage;resolution;image resolution;computer science;engineering;electrical engineering;dissipation;signal processing;area;speed;size;cmos;low-power electronics;statistics	Arch	58.69315357619656	50.09367766538113	922
8f0e510d0144abd3139d0191029cca974a8aedad	image compression with xvc		This paper describes xvc – a format for efficient compression of visual data – originally developed for compression of video sequences, but in the context of this paper applied to still images. The xvc codec is a block based lossy codec using a traditional approach for prediction, residual representation and entropy coding. There are no elements of Machine Learning or Artificial Neural Networks in the xvc encoder or decoder. The xvc codec offers support for tuning towards PSNR or perceptual quality. The images submitted for the CLIC challenge and the descriptions included in this paper are based on the perceptually tuned setting.	artificial neural network;codec;encoder;entropy encoding;image compression;lossy compression;machine learning;peak signal-to-noise ratio	Jonatan Samuelsson;Per Hermansson	2018			computer vision;image compression;artificial intelligence;computer science	AI	44.297857298460954	-18.982377576323966	923
97c36d5d4dbe4540c86f8e3bd9e1800bff0a16e1	midway video equalization	midway equalization;dithering method;histogram matching	This article presents an implementation of the ‘Midway Equalization’ method for videos. This technique allows us to modify the image histograms so that they present similar luminances. We propose two algorithms: the first one based on histogram inversion and the second one on the sorting of images intensities. The former computes the histograms and then finds the contrast change functions by convolving the inverse histograms with a Gaussian function. The latter starts by sorting the pixels of each image by intensity; the temporal signals, composed of all gray levels of the same rank, are then convolved with a Gaussian function. In this sorting method, the resulting histograms are more similar and homogeneous. Nevertheless, the histogram strategy is faster and provides good results in general. The algorithms include a ‘dithering’ option for reducing quantization artifacts. The whole implementation depends on a single parameter, the standard deviation, that is used for Gaussian convolutions. The experiments show several examples, including the quantization artifacts that appear in some situations and the benefits of dithering. We observe that these artifacts are usually more important in the histogram method. Source Code The reviewed source code and documentation for this algorithm are available from the web page of this article. Compilation and usage instruction are included in the README.txt file of the archive.	algorithm;archive;battle of midway;convolution;dither;documentation;experiment;grayscale;image histogram;pixel;quantization (signal processing);readme;ringing artifacts;sorting;web page	Javier Sánchez	2017	IPOL Journal	10.5201/ipol.2017.181	histogram matching;adaptive histogram equalization;histogram equalization	ML	61.7063340922614	-66.81524623905833	924
12c1ba7d37abbf4a4fbc73b267c025a571f5e23f	dozuki brings technical manuals into the 21st century: a breakthrough in the world of documentation	technical documentation dozuki ifixit electronics industry technical manual;manuals;document handling;chief executive officer;information science;low resolution;electronics industry;document image processing;consumer products;information science electronics industry;consumer products documentation professional communication manuals document image processing document handling;documentation;professional communication	iFixit is at the forefront of the electronics industry. The company works with service manuals from every major consumer electronics company. Poor source documentation has been a constant impediment to make electronics repair accessible to non-engineers. For years, the company wondered why technical documentation hasn't improved at the pace of the rest of the industry. The software company, Dozuki, makes it easv to create technical documentation that users love. Dozuki is a platform for living, breathing technical documentation. Technical documentation should be both easv to use and simple to keep up to date.	microsoft forefront;technical documentation	Kyle Wiens	2012	IEEE Consumer Electronics Magazine	10.1109/MCE.2011.2172078	technical writing;common source data base;human–computer interaction;technical communication;information science;documentation;computer science;marketing;operating system;technical documentation;multimedia;user analysis	Embedded	-63.89458963329296	-24.61867865484319	925
84fa91bdcefec00fff58183d98b15d69775cfc72	acoustic gait-based person identification using hidden markov models	gait recognition;hidden markov models;audio analysis	We present a system for identifying humans by their walking sounds. This problem is also known as acoustic gait recognition. The goal of the system is to analyse sounds emitted by walking persons (mostly the step sounds) and identify those persons. These sounds are characterised by the gait pattern and are influenced by the movements of the arms and legs, but also depend on the type of shoe. We extract cepstral features from the recorded audio signals and use hidden Markov models for dynamic classification. A cyclic model topology is employed to represent individual gait cycles. This topology allows to model and detect individual steps, leading to very promising identification rates. For experimental validation, we use the publicly available TUM GAID database, which is a large gait recognition database containing 3 050 recordings of 305 subjects in three variations. In the best setup, an identification rate of 65.5% is achieved out of 155 subjects. This is a relative improvement of almost 30% compared to our previous work, which used various audio features and support vector machines.	acoustic cryptanalysis;cepstrum;coat of arms;gait analysis;hidden markov model;markov chain;pattern language;support vector machine	Jürgen T. Geiger;Maximilian Kneißl;Björn W. Schuller;Gerhard Rigoll	2014		10.1145/2668024.2668027	computer vision;speech recognition;computer science;audio analyzer;hidden markov model	Vision	0.8836341395315009	-89.23434823801111	926
7f6e4020d1cba3c8ca5f42dd04a43c5cb06761f0	the threshold based queueing system with hysteresis for performance analysis of clouds	hysteresis;virtual machining;stationary probability distribution threshold based queueing system cloud performance analysis hysteresis service centers virtual machines queue occupation dynamic behavior queueing model reverse threshold sequence vm deactivation states stochastic comparison method;servers;computational modeling;stochastic processes;mathematical model;virtual machines cloud computing hysteresis queueing theory statistical distributions stochastic processes;numerical models;mathematical model hysteresis computational modeling servers stochastic processes virtual machining numerical models	In this paper, we propose to model a node cloud by a threshold based queueing system with hysteresis. The client requests (or jobs) arrive into the buffer, and are executed by service centers or Virtual Machines (VMs). We suppose that virtual machines are activated and deactivated according to the occupation of the queue, in order to model the dynamic behavior of the system. The queueing model is represented by a forward threshold sequence which has different values than the reverse threshold sequence. The forward thresholds represent the numbers of the customers in the queue from which we increase by one the number of VMs. Similarly, the backward thresholds are the values from which we decrease by one the number of VMs. When the forward thresholds are different then the backward thresholds then hysteresis is present. The relevance of hysteresis for the cloud model is to reduce the frequent transitions between activation and deactivation states of the VMs. We will use stochastic comparison method in order to prove and guarantee that the hysteresis model can be bounded by models with the same sequences of forward and backward threshold. The advantage of the bounding models is that the stationary probability distribution can be computed exactly and easily from a mathematical formula. We present some numerical results for the performance measures in order to show that the bounding values provide an accurate coverage for the exact values.	hysteresis;job stream;numerical analysis;profiling (computer programming);queueing theory;relevance;stationary process;virtual machine	Farah Aït-Salaht;Hind Castel-Taleb	2015	2015 International Conference on Computer, Information and Telecommunication Systems (CITS)	10.1109/CITS.2015.7297721	real-time computing;simulation;computer science;operations management;layered queueing network	Metrics	7.962966241741586	10.274681500496202	927
b90868ba4f692fca7d733917f94a3d73435dc03f	compliance based trustworthiness calculation mechanism in cloud environment	trustworthiness;sla;trust;cloud computing	Establishing trust is one of the most challenging issues in emerging cloud computing area. It is becoming increasingly complex for cloud users to make distinction (with respect to trustworthiness) among service providers offering similar kinds of services. There must be some mechanisms in the hands of users to determine trustworthiness of service providers so that they can select service providers with confidence and with some degree of assurance that service provider will not behave unpredictably or maliciously. Though various approaches exist to form trust between service providers and users, little work has been done in the area of forming trust based on compliance of QoS parameters which have been promised in SLA. In this paper an attempt has been made to design and simulate a mechanism to calculate trustworthiness of service providers based on their compliance to promised SLA parameters. The model has been simulated in MATLAB. The validation has been done using synthetic data set. Validation results show that approach is workable and can be used to evaluate trustworthiness of service providers in a cloud environment. © 2014 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Program Chairs of ITDS-2014.	cloud computing;matlab;quality of service;service-level agreement;simulation;synthetic data;trust (emotion)	Jagpreet Sidhu;Sarbjeet Singh	2014		10.1016/j.procs.2014.08.066	service provider;mobile qos;knowledge management;computer security	ECom	-48.95288504769136	56.91226479167595	928
77f70732666b3b571b8f96ab69dde3c5cb0e8a96	visualization of energy consumption: motivating for a sustainable behaviour through social media	prototypes energy consumption visualization art media real time systems collaboration;social media applications sustainability and energy aware collaboration and hci;sustainable development data visualisation energy consumption groupware mobile computing social networking online;social network household energy consumption visualization sustainable behaviour social media mobile application eco visualization advanced measurement instruments collaboration competition	This paper presents a design solution for a mobile application inspired by eco-visualization, suggesting a visualization of real time energy consumption of a household. The aspiration of this paper is to explore a solution that may influence individual users to a more sustainable behaviour by combining data collected from advanced measurement instruments, together with social media. By creating a low fidelity prototype incorporating the idea of social media, collaboration, competition and eco-visualization, the paper aims to assess the users reaction and sentiment towards an unconventional visualization of energy consumption, collaboration for creating art and competing amongst other participants in a social network to lower the users energy consumption. An evaluation of the prototype design shows two interesting results: the need for flexible and individual feedback as well as positive feedback for collaborative art approach.	mobile app;mobile operating system;offset binary;paper prototyping;positive feedback;prototype;social media;social network;usability testing	Caroline Sofie Olsen	2014	2014 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2014.6867642	simulation;human–computer interaction;multimedia	HCI	-59.65251985048151	-43.666106536058116	929
8a498b33164be906fd9ef926bb4e68d7718ec2e5	network-centric versus user-centric multihoming strategies in lte/wifi networks	wireless networks;ieee 802 11 standard resource management throughput couplings wireless networks context 3gpp;resource management;long term evolution lte wifi interworking multihoming network centric resource allocation user centric;3gpp;user centric multihoming lte wifi interworking resource allocation network centric;couplings;ieee 802 11 standard;context;throughput	In this paper, we consider the interworking between multiple wireless access networks and study the multihomed users’ performance for different fourth-generation (4G)/WiFi multihoming techniques. We specifically compare two points of view for multihoming: network centric—wherein the scheduler is controlled by the network—versus user centric—where each user chooses a splitting policy for its packets. In the former, we study two proportional fairness strategies: a global one applied to the whole network and a local one applied to each wireless access network apart. In the user-centric approach, we study user policies based on the information received from the network and consider two variants: a simple one in which the user has only information on the peak rates of the radio interfaces and an optimized selfish policy in which the user receives complete information on the interfaces peak rates and traffic intensities. We then prove that this optimal selfish strategy achieves a global optimum for the system. Our numerical results show that network-centric strategies are better than user-centric ones in terms of achievable throughput but are worse in terms of computational complexity.	access network;compaq lte;computational complexity theory;fairness measure;global optimization;multihoming;numerical analysis;proportionally fair;scheduling (computing);throughput	Ghina Dandachi;Salah-Eddine Elayoubi;Tijani Chahed;Nada Chendeb Taher	2017	IEEE Transactions on Vehicular Technology	10.1109/TVT.2016.2597442	throughput;telecommunications;computer science;resource management;wireless network;distributed computing;coupling;computer network	Mobile	28.77598279440417	92.32570037307477	930
f93395dfcc51fb3394f0976dfacbc80770bb389c	automatic extraction of named entity translingual equivalence based on multi-feature cost minimization	cost minimization;named entity	7UDQVOLQJXDO HTXLYDOHQFH UHIHUV WR WKH UHODWLRQVKLS EHWZHHQ H[SUHVVLRQV RI WKH VDPH PHDQLQJ IURP GLIIHUHQW ODQJXDJHV ,GHQWLI\LQJ WUDQVOLQJXDO HTXLYDOHQFH RI QDPHG HQWLWLHV 1( FDQ VLJQLILFDQWO\ FRQWULEXWH WR PXOWLOLQJXDO QDWXUDO ODQJXDJH SURFHVVLQJ VXFK DV FURVVOLQJXDO LQIRUPDWLRQ UHWULHYDO FURVVOLQJXDO LQIRUPDWLRQ H[WUDFWLRQ DQG VWDWLVWLFDO PDFKLQH WUDQVODWLRQ ,Q WKLV SDSHU ZH SUHVHQW DQ LQWHJUDWHG DSSURDFK WR H[WUDFW 1( WUDQVOLQJXDO HTXLYDOHQFH IURP D SDUDOOHO &KLQHVH (QJOLVK FRUSXV 6WDUWLQJ IURP D ELOLQJXDO FRUSXV ZKHUH 1(V DUH DXWRPDWLFDOO\ WDJJHG IRU HDFK ODQJXDJH 1( SDLUV DUH DOLJQHG LQ RUGHU WR PLQLPL]H WKH RYHUDOO PXOWL IHDWXUH DOLJQPHQW FRVW $Q 1( WUDQVOLWHUDWLRQ PRGHO LV SUHVHQWHG DQG LWHUDWLYHO\ WUDLQHG XVLQJ QDPHG HQWLW\ SDLUV H[WUDFWHG IURP D ELOLQJXDO GLFWLRQDU\ 7KH WUDQVOLWHUDWLRQ FRVW FRPELQHG ZLWK WKH QDPHG HQWLW\ WDJJLQJ FRVW DQG ZRUG EDVHG WUDQVODWLRQ FRVW FRQVWLWXWH WKH PXOWL IHDWXUH DOLJQPHQW FRVW 7KHVH IHDWXUHV DUH GHULYHG IURP VHYHUDO LQIRUPDWLRQ VRXUFHV XVLQJ XQVXSHUYLVHG DQG SDUWO\ VXSHUYLVHG PHWKRGV $ JUHHG\ VHDUFK DOJRULWKP LV DSSOLHG WR PLQLPL]H WKH DOLJQPHQW FRVW ([SHULPHQWV VKRZ WKDW WKH SURSRVHG DSSURDFK H[WUDFWV 1( WUDQVOLQJXDO HTXLYDOHQFH ZLWK ) VFRUH DQG LPSURYHV WKH WUDQVODWLRQ VFRUH IURP WR	data quality;letter-quality printer;logical volume management;named entity;rs-232;turing completeness	Fei Huang;Stephan Vogel;Alexander H. Waibel	2003		10.3115/1119384.1119386	natural language processing;speech recognition;computer science;pattern recognition	DB	-30.1999326921656	-77.60461420099939	931
1895fdaae35ec6c226459cc51da82041a5da4736	high efficiency detection of bi-stranded abasic clusters in -irradiated dna by putrescine	dna;escherichia coli;dna apurinic or apyrimidinic site lyase;nucleic acid heteroduplexes;bacteriophage t7;spectrum;double strand break;dna viral;oligonucleotides;carbon oxygen lyases;gamma rays;escherichia coli proteins;apurinic acid;putrescine;ionizing radiation;high efficiency;dna damage;deoxyribonuclease iv phage t4 induced;gamma irradiation	Bi-stranded abasic clusters, an abasic (AP) site on one DNA strand and another nearby AP site or strand break on the other, have been quantified using Nfo protein from Escherichia coli to produce a double-strand break at cluster sites. Since recent data suggest that Nfo protein cleaves inefficiently at some clusters, we tested whether polyamines, which also cut at AP sites, would cleave abasic clusters at higher efficiency. The data show that Nfo protein cleaves poorly at clusters containing immediately opposed AP sites and those separated by 1 or 3 bp. Putrescine (PUTR) cleaved more efficiently than spermidine or spermine, and did not cleave undamaged DNA. It cleaved abasic clusters in oligonucleotide duplexes more effectively than Nfo protein, including immediately opposed or closely spaced clusters. PUTR cleaved more efficiently than Nfo protein by a factor of ∼1.7 or ∼2 for DNA that had been γ-irradiated in moderate or non-radioquenching conditions, respectively. This suggests that the DNA environment during irradiation affects the spectrum of cluster configurations. Further comparison of PUTR and Nfo protein cleavage may provide useful information on abasic cluster levels and configurations induced by ionizing radiation.	.nfo;computer cluster;strand (programming language)	Alexandros G. Georgakilas;Paula V. Bennett;Betsy M. Sutherland	2002	Nucleic Acids Research	10.1093/nar/gkf393	gamma ray;biology;biochemistry;molecular biology;genetics	Comp.	5.08859805766914	-64.00270883832266	932
d03437ca65bb73efc15fd092de867abee1924c17	an algorithm for improving throughput guarantee of topology-transparent mac scheduling strategy	cover free set;mac scheduling;qos;combinatorial design;superimposed code	Topology-transparent MAC scheduling strategies nowadays are all based on combinatorial design. To get throughput guarantee, a cover-free set is output as scheduling strategy of network. In this paper, we aim to modify the cover-free set so that better throughput can be guaranteed. At the first step, the redundant slot of the cover-free set is proposed and found to have negative influence on the minimal guaranteed throughput. Second, we prove that any subset of a cover-free set is still a cover-free set after its redundant slots were squashed out. Our algorithm chooses the subset which has the maximal number of redundant slots, squashes all of its redundant slots, and then designates it as the network scheduling strategy. Therefore, better through- put can be guaranteed if the squashed subset is adopted as network scheduling strategy. For any topology- transparent node scheduling strategy, both the increased minimal throughput and decreased maximal transmission delay can be gotten by just using our algorithm as an extra accessory.	algorithm;scheduling (computing);throughput	Chaonong Xu	2010	Wireless Sensor Network	10.4236/wsn.2010.210096	fair-share scheduling;combinatorial design;real-time computing;quality of service;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;round-robin scheduling;computer network	Mobile	1.8279229045233518	93.62881912446012	933
c17ed04e166705de59028a35e2cc7ed9d2237294	higher-order asymptotics of mutual information for nonlinear channels with non-gaussian noise	gaussian noise;higher order;random function;mutual information;signal to noise ratio;density functional	Abstract—Nonlinear channels with non-Gaussian noise where the transmitted signal is a random function of the input signal are considered. Under some assumptions on smoothness and the behavior of tails of the noise density function, higher-order asymptotics of the mutual information between the input and output signals in such channels is obtained, as the mean power of the input signal (or, equivalently, the signal-to-noise ratio) goes to zero.	input/output;mutual information;nonlinear programming;nonlinear system;signal-to-noise ratio;tails	Vyacheslav V. Prelov;Edward C. van der Meulen	2003	Probl. Inf. Transm.	10.1023/B:PRIT.0000011271.44336.b2	gradient noise;gaussian noise;effective input noise temperature;noise spectral density;mathematical optimization;higher-order logic;signal transfer function;value noise;noise temperature;random function;multiplicative noise;noise;mathematics;white noise;noise figure;mutual information;noise floor;signal-to-noise ratio;stochastic resonance;carrier-to-receiver noise density;statistics	Theory	52.11218970378037	14.42947132114211	934
69da47701a87e983da5796bd278b87f6c60bc6bb	laying the foundation to use raspberry pi 3 v2 camera module imagery for scientific and engineering purposes	calibration;cameras	A comprehensive radiometric characterization of raw-data format imagery acquired with the Raspberry Pi 3 and V2.1 camera module is presented. The Raspberry Pi is a high-performance single-board computer designed to educate and solve real-world problems. This small computer supports a camera module that uses a Sony IMX219 8 megapixel CMOS sensor. This paper shows that scientific and engineering-grade imagery can be produced with the Raspberry Pi 3 and its V2.1 camera module. Raw imagery is shown to be linear with exposure and gain (ISO), which is essential for scientific and engineering applications. Dark frame, noise, and exposure stability assessments along with flat fielding results, spectral response measurements, and absolute radiometric calibration results are described. This low-cost imaging sensor, when calibrated to produce scientific quality data, can be used in computer vision, biophotonics, remote sensing, astronomy, high dynamic range imaging, and security applications, to name a few. © The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. [DOI: 10.1117/1.JEI.26.1.013014]	cmos;camera module;coefficient;computer vision;dark-frame subtraction;display resolution;dynamic range;high-dynamic-range imaging;image sensor;metric;range imaging;raspberry pi 3 model b (latest version);raw image format;single-board computer	Mary Pagnutti;Robert E. Ryan;George Cazenavette;Maxwell Gold;Ryan Harlan;Edward Leggett;James Pagnutti	2017	J. Electronic Imaging	10.1117/1.JEI.26.1.013014	computer vision;calibration;computer science;computer graphics (images)	Visualization	62.954660985811444	-57.87157476067823	935
b8963b084b23a80ac7d746caee9795d8d7705f78	an xml multi-agent system for e-learning and skill management	multiagent system;learning algorithm;teleenseignement;multi agent system;systeme apprentissage;ingenierie connaissances;xml language;specification;educational software program;knowledge management;service web;intelligence artificielle;algorithme apprentissage;didacticiel;web service;learning systems;gestion ressources humaines;saber hacer;feedback;internet;especificacion;know how;human resource;savoir faire;artificial intelligence;personalized learning;teleensenanza;programa didactico;inteligencia artificial;boucle reaction;remote teaching;sistema multiagente;retroalimentacion;algoritmo aprendizaje;human resource management;langage xml;lenguaje xml;systeme multiagent;knowledge engineering	E-learning is nowadays recognized as one of the key components of Enterprise Knowledge Management platforms. Given a project specification, the platform should be able to suggest a project team, to measure human resources competence gaps and to contribute to reduce them by creating personalized learning paths. In this paper we propose an XML based Multi-Agent System to perform the following tasks: (i) supporting Chief Learning Officers in defining roles, associated competencies and knowledge level required; (ii) managing the skill map of the organization; (iii) measuring human resources competence gaps; (iv) supporting employees in filling their competence gaps as related to their roles; (v) enriching a given courseware or creating personalized learning paths according to feedbacks user provides in order to optimize the acquisition of needed competencies; (vi) assisting Chief Learning Officers in choosing the most appropriate employee for a given role.	debugging;electronic trading;emoticon;jade;java;knowledge level;knowledge management;multi-agent system;ontology (information science);personalization;prototype;software deployment;software framework;warez;xml	Alfredo Garro;Luigi Palopoli	2002		10.1007/3-540-36559-1_21	web service;personalized learning;the internet;xml;simulation;computer science;knowledge management;artificial intelligence;human resource management;knowledge engineering;feedback;database;specification	AI	-80.37468459065632	-48.35629691928008	936
f6423260d18a4f12889886143c415ac7b543f524	experimental setup of motion sickness and situation awareness in automated vehicle riding experience		Automated vehicle users are likely to engage in non-driving tasks while traveling. Most of these activities require their visual attention and prevent them from getting information from outside vehicle. This leads to motion sickness symptom because of conflicts to what they see and what they feel during traveling. In this paper, an experiment is designed to study the effect of enhancing situation awareness by providing haptic feedback to mitigate the motion sickness inside a vehicle while doing a non-driving task. A preliminary study (N=10) shows that the experimental setup has promising results that can provide insightful information between situation awareness and motion sickness for future car designers.	haptic technology	Nidzamuddin Md. Yusof;Juffrizal Karjanto;Shivam Kapoor;Jacques M. B. Terken;Frank Delbressine;Matthias Rauterberg	2017		10.1145/3131726.3131761	simulation;human–computer interaction;engineering;haptic technology;situation awareness;motion sickness	HCI	-46.79892174026735	-51.91722726070534	937
cc07ea19d6e827a1d48113a76f41d6204b58c176	learning in navigation goal finding in graphs	random graph;algoritmo busqueda;learning;algorithme recherche;grafo aleatorio;search algorithm;search strategy;robotics;graphe aleatoire;constraint satisfaction;aprendizaje;random graphs;satisfaction contrainte;navigation;apprentissage;fonction densite;density function;funcion densidad;strategie recherche;robotique;goal finding;satisfaccion restriccion;empirical versus theoretical delaunay arc length distribution;estrategia investigacion	"""A robotic agent operating in an unknown and complex environment may employ a search strategy of some kind to perform a navigational task such as reaching a given goal. In the process of performing the task, the agent can attempt to discover characteristics of its environment that enable it to choose a more efficient search strategy for that environment. If the agent is able to do this, we can say that it has """"learned to navigate"""" — i.e., to improve its navigational performance. This paper describes how an agent can learn to improve its goal-finding performance in a class of discrete spaces, represented by graphs embedded in the plane. We compare several basic search strategies on two different classes of """"random"""" graphs and show how information collected during the traversal of a graph can be used to classify the graph, thus allowing the agent to choose the search strategy best suited for that graph."""		Peter Cucka;Nathan S. Netanyahu;Azriel Rosenfeld	1996	IJPRAI	10.1142/S0218001496000281	random graph;simulation;computer science;artificial intelligence;machine learning;mathematics;robotics	Theory	44.44549275201989	-28.51810773110006	938
9a326068582247cb886bb97bc18e869259048b4b	soft sensing for propylene purity using partial least squares and support vector machine		Many important process variables in propylene distillation actual system is difficultly detected directly or not easy online survey, especially propylene purity which will vary with other process parameter. This paper presents an online soft sensing method by combining partial least squares and support vector machine. First multivariate data analysis was performed using partial least squares, then soft sens- ing regression model was constructed. Simulation shows that the proposed measur- ing scheme guarantees parameter estimation and predicting accuracy.	partial least squares regression;purity (quantum mechanics);support vector machine	Zhiru Xu;Desheng Liu;Jingguo Zhou;Qingjun Shi	2009		10.1007/978-3-642-01216-7_29	econometrics;non-linear iterative partial least squares;machine learning;statistics	ML	35.553176685789715	-26.68300746797762	939
6cdfa27b8a0a4d1f3ea415ac0821d83f531d28bc	a novel statistical detector for contourlet domain image watermarking using 2d-garch model		In this paper, we propose a novel watermark detector in contourlet domain using likelihood ratio test (LRT). Since the accuracy of an LRT based watermark detector is dependent on the efficiency of the applied statistical model, first, we study the statistical properties of the contourlet coefficients. Using different tests, we demonstrate that the marginal distribution of contourlet coefficients is heavy-tailed and heteroscedasticity exists in these coefficients. All of the previously proposed models for contourlet coefficients assume that these coefficients are identically distributed, so they can not capture the characteristics of the contourlet coefficients. To overcome this problem, we propose using two dimensional generalized autoregressive conditional heteroscedasticity (2D-GARCH) model for contourlet coefficients that provides an efficient structure for the dependencies of these coefficients. Based on using 2D-GARCH model, a novel LRT based heteroscedastic watermark detector is designed in contourlet domain. Experimental results confirm the efficiency of the proposed watermark detector under different types of attacks and its outperformance compared with alternative watermarking methods.	contourlet;digital watermarking	Maryam Amirmazlaghani	2017		10.1007/978-3-319-68548-9_50	computer vision;watermark;artificial intelligence;contourlet;computer science;autoregressive model;heteroscedasticity;pattern recognition;statistical model;independent and identically distributed random variables;likelihood-ratio test;autoregressive conditional heteroskedasticity	Vision	61.04457009118695	-69.7109089610814	940
0be7af9c339c215fffe5b97e8c795d9fd92c7773	crowdsourcing: a tool for organizational knowledge creation	ba;theoretical model;seci model;innovation;organizational knowledge creation;co creation;crowdsourcing	Crowdsourcing is a new, online-based, way of outsourcing that relies on large and undefined networks of people. This process has been used by several successful organizations to solve their internal Innovation challenges with very good results. Assuming that, at a fundamental level, Innovation results from the creation and application of organizational knowledge, this work aims to better understand, from the firm’s perspective, how can Crowdsourcing be used to enhance the creation of organizational knowledge. Therefore, we analysed Crowdsourcing through the lenses of the theory of organizational knowledge creation. Based on the main elements of the theory, our analysis raised theoretical assumptions that serve to question some practical aspects of the Crowdsourcing initiatives and may serve as basis for future research and practical experiments. Our analysis shows that Crowdsourcing for Innovation, has all the characteristics of a tool for organizational knowledge creation through the endogenisation of knowledge, ideas and expertise of external individuals. Nevertheless, this theoretical perspective of Crowdsourcing highlights a rather complex process that requires organizational effort and resources throughout all the process. Although a critical part of the work, in a Crowdsourcing initiative is developed outside of the company, the seeker should not neglect its efforts, doing so may result in unsuccessful Crowdsourcing experiences.	crowdsourcing;experience;experiment;outsourcing;semantic network;undefined behavior	Fábio Oliveira;Isabel Ramos	2014			public relations;innovation;organizational learning;economics;computer science;knowledge management;data science;management;crowdsourcing	Web+IR	-77.79896040421566	1.828555191674967	941
b6ea3a4aa827cde11ac14722f80e28748c3ca87d	on prefixal factorizations of words		We consider the class P1 of all infinite words x ∈ A over a finite alphabet A admitting a prefixal factorization, i.e., a factorization x = U0U1U2 · · ·where each Ui is a non-empty prefix of x.With each x ∈ P1 one naturally associates a ‘‘derived’’ infiniteword δ(x) which may or may not admit a prefixal factorization. We are interested in the class P∞ of all words x of P1 such that δn(x) ∈ P1 for all n ≥ 1. Our primary motivation for studying the class P∞ stems from its connection to a coloring problem on infinite words independently posed by T. Brown and by the second author. More precisely, let P be the class of all words x ∈ A such that for every finite coloring φ : A → C there exist c ∈ C and a factorization x = V0V1V2 · · · with φ(Vi) = c for each i ≥ 0. In a recent paper (de Luca et al., 2014), we conjectured that a word x ∈ P if and only if x is purely periodic. In this paper we prove that P ( P∞, so in other words, potential candidates to a counter-example to our conjecture are amongst the non-periodic elements of P∞. We establish several results on the class P∞. In particular, we prove that a Sturmian word x belongs to P∞ if and only if x is nonsingular, i.e., no proper suffix of x is a standard Sturmian word. © 2015 Elsevier Ltd. All rights reserved. E-mail addresses: aldo.deluca@unina.it (A. de Luca), lupastis@gmail.com (L.Q. Zamboni). http://dx.doi.org/10.1016/j.ejc.2015.08.007 0195-6698/© 2015 Elsevier Ltd. All rights reserved. 60 A. de Luca, L.Q. Zamboni / European Journal of Combinatorics 52 (2016) 59–73	existential quantification;graph coloring;qr decomposition;substring	Aldo de Luca;Luca Q. Zamboni	2016	Eur. J. Comb.	10.1016/j.ejc.2015.08.007	arithmetic;combinatorics;mathematics;geometry;algebra	Logic	38.75661197125284	34.67595010083937	942
ac57a8f855a8d4ea298607d7e972c293f576bf6e	research of granular support vector machine	granular computing;gsvm learning model;granular support vector machine gsvm;gsvm applications;support vector machine	Granular support vector machine (GSVM) is a new learning model based on Granular Computing and Statistical Learning Theory. Compared with the traditional SVM, GSVM improves the generalization ability and learning efficiency to a large extent. This paper mainly reviews the research progress of GSVM. Firstly, it analyzes the basic theory and the algorithm thought of GSVM, then tracking describes the research progress of GSVM including the learning model and specific applications in recent years, finally points out the research and development prospects.	algorithm;granular computing;statistical learning theory;support vector machine	Shifei Ding;Bingjuan Qi	2011	Artificial Intelligence Review	10.1007/s10462-011-9235-9	support vector machine;granular computing;computer science;artificial intelligence;machine learning;data mining	AI	6.617753260603158	-24.11935934236854	943
bddab2104815b6222ff0f7d058cf18f439a75da2	keynote address: virtual machines: past, present, and future	virtual machine			Mendel Rosenblum	2004			virtual machine;distributed computing;computer science	OS	-28.417441467127098	56.416484492944235	944
359708f4c57e4524ae19bc0dbd832d37ad370e46	visibility problems for orthogonal objects in two-or three-dimensions	concepcion asistida;visibilite;computer aided design;visibilidad;three dimensions;geometry;geometrie;projective plane;algorithme;algorithm;algorritmo;visibility;hidden line removal;conception assistee;geometria;elimination ligne cachee	LetP be a set ofl points in 3-space, and letF be a set ofm opaque rectangular faces in 3-space with sides parallel tox- ory-axis. We present anO(n logn) time andO(n) space algorithm for determining all points inP which are visible from a viewpoint at (0,0,∞), wheren=l+m. We also present anO(n logn+k) time andO(n) space algorithm for the hidden-line elimination problem for the orthogonal polyhedra together with a viewpoint at (0,0,∞), wheren is the number of vertices of the polyhedra andk is the number of edge intersections in the projection plane.	algorithm;optic axis of a crystal;polyhedron;projection plane	Jeong-In Doh;Kyung-Yong Chwa	1988	The Visual Computer	10.1007/BF01905560	projective plane;three-dimensional space;combinatorics;topology;hidden line removal;visibility;computer aided design;mathematics;geometry;algorithm	Theory	66.75633863906178	-40.51980116579192	945
9b31901b686e8040517bc3eeda6dd749f7dda1b4	efficient topology estimation for large scale optical mapping	mapeo optico;navegacion visual;estimacion de topologia;global alignment;visual navigation;mosaico de imagenes;tesis doctoral;robotica submarina;underwater robotics;alineament global;alineamiento global;efficient topology estimation for large scale optical mapping;info eu repo semantics doctoralthesis;estimacio de topologia;topology estimation;mapeig optic;navegacio visual;optical mapping;mosaic d imatges;image mosaicing;info eu repo semantics publishedversion	Large scale image mosaicing methods are in great demand among scientists who study different aspects of the seabed, and have been fostered by impressive advances in the capabilities of underwater robots in gathering optical data from the seafloor. Cost and weight constraints mean that low-cost Remotely operated vehicles (ROVs) usually have a very limited number of sensors. When a low-cost robot carries out a seafloor survey using a downlooking camera, it usually follows a predefined trajectory that provides several non time-consecutive overlapping image pairs. Finding these pairs (a process known as topology estimation) is indispensable to obtaining globally consistent mosaics and accurate trajectory estimates, which are necessary for a global view of the surveyed area, especially when optical sensors are the only data source. This thesis presents a set of consistent methods aimed at creating large area image mosaics from optical data obtained during surveys with low-cost underwater vehicles. First, a global alignment method developed within a Feature-based image mosaicing (FIM) framework, where nonlinear minimisation is substituted by two linear steps, is discussed. Then, a simple four-point mosaic rectifying method is proposed to reduce distortions that might occur due to lens distortions, error accumulation and the difficulties of optical imaging in an underwater medium. The topology estimation problem is addressed by means of an augmented state and extended Kalman filter combined framework, aimed at minimising the total number of matching attempts and simultaneously obtaining the best possible trajectory. Potential image pairs are predicted by taking into		Armagan Elibol;Nuno Gracias;Rafael García	2013		10.1007/978-3-642-30313-5	computer vision;simulation;geography;cartography	Robotics	55.299931976577994	-40.51227508669999	946
6f02e94ce2eb8f6f7b6333790edb20ccf26db057	wage distributions by bargaining regime	wage setting;wage equation;wage dispersion;labor market;wage bargaining;lohnstruktur;collective wage bargaining;minimum wage;collective bargaining;kernel density estimate;german structure of earnings survey;industrial relations;gehalts und lohnstrukturerhebung;wage structure;tariflohnverhandlungen;wage distribution;variance decomposition	Using data from the German Structure of Earnings Survey 2001, this paper provides a comprehensive picture of the wage structure in three wage-setting regimes prevalent in the German system of industrial relations. We analyze wage distributions for various labor market subgroups by means of kernel density estimation, variance decompositions, and wage regressions. Unions’ impact through collective and firmlevel bargaining mainly works towards a higher wage level and reduced overall and residual wage dispersion. Yet observed effects are considerably heterogeneous across different labor market groups. There is no clear evidence for wage floors formed by collectively bargained low wage brackets which would operate as minimum wages for different groups of workers.	kernel density estimation	Karsten Kohn;Alexander C. Lembcke	2007	AStA Wirtschafts- und Sozialstatistisches Archiv	10.1007/s11943-007-0023-6	kernel density estimation;economics;wage share;mathematics;microeconomics;industrial relations;market economy;efficiency wage;variance decomposition of forecast errors;statistics;labour economics	Web+IR	-2.950165538746199	-9.943871795161726	947
0e00d02951e5fbaa4272214a91bf46a285655731	software-based copy protection for temporal media during dissemination and playback	presentacion documento;intervalo tiempo;duracion;occupation time;interfase usuario;replacement;protection copie;remplacement;user interface;document layout;copy protection;securite informatique;dissemination;analisis intervalo;attaque informatique;time interval;duration;presentation document;computer security;temps occupation;criptografia;cryptography;traitement document;seguridad informatica;tiempo ocupacion;computer attack;ataque informatica;cryptographie;interface utilisateur;reemplazo;aritmetica intervalo;document processing;interval arithmetic;arithmetique intervalle;analyse intervalle;diseminacion;duree;tratamiento documento;interval analysis;intervalle temps	We present a software-based protection mechanism to prevent unauthorized copying of media documents during their presentation on a clients host. Our solution enforces the execution and continuous replacement of security mechanisms on the clients host. Each protection mechanism is only used for a short time interval before it is replaced. The short duration of the time interval prevents a successful analysis and attack of the mechanism. In this way we solve a shortcoming of current solutions. As those employ fixed protection mechanisms they will eventually be circumvented because attackers have virtually unlimited time to analyze them.		Gisle Grimen;Christian Mönch;Roger Midtstraum	2005		10.1007/11734727_29	telecommunications;computer science;interval arithmetic;computer security	DB	-44.22191693198337	78.12509852678096	948
4b2659de235a12e197781c8907e7c075b8d712b9	methods for characterizing and comparing populations of shock wave curves	onionskin;functional data analysis;grupo de excelencia;ciencias basicas y experimentales;hierarchical modeling;matematicas;nonparametric regression;b splines;gaussian process	At Los Alamos National Laboratory, engineers conduct experiments to evaluate how well detonators and high explosives work. The experimental unit, often called an “onionskin,” is a hemisphere consisting of a detonator and a booster pellet surrounded by high explosive material. When the detonator explodes, a streak camera mounted above the pole of the hemisphere records when the shock wave arrives at the surface. The output from the camera is a two-dimensional image that is transformed into a curve that shows the arrival time as a function of polar angle. The statistical challenge is to characterize the population of arrival time curves and to compare the baseline population of onionskins to a new population. The engineering goal is to manufacture a new population of onionskins that generate arrival time curves with the same shape as the baseline. We present two statistical approaches that test for differences in mean curves and provide simultaneous confidence bands for the difference: (i) a B-Spline basis ...	population	Curtis B. Storlie;Michael Fugate;David M. Higdon;Aparna V. Huzurbazar;Elizabeth G. Francois;Douglas C. McHugh	2013	Technometrics	10.1080/00401706.2013.805662	b-spline;econometrics;functional data analysis;gaussian process;mathematics;onionskin;nonparametric regression;statistics	Vision	79.03282655913132	-49.89829874605566	949
aebae4408dacb854cdd138deccea95484006e040	secure volunteer computing for distributed cryptanalysis			cryptanalysis;volunteer computing	Nils Kopal	2018			computer security;volunteer;cryptanalysis;computer science	Crypto	-44.708111228795744	76.03361989517792	950
53f38196fc976a32a533f8e685ec6ef81b7d21fb	a geolinguistic web application based on linked open data	digital geolinguistics;linked open data;ontology;rdf	Digital Geolinguistic systems encourage collaboration between linguists, historians, archaeologists, ethnographers, as they explore the relationship between language and cultural adaptation and change. In this demo, we propose a Linked Open Data approach for increasing the level of interoperability of geolinguistic applications and the reuse of the data. We present a case study of a geolinguistic project named Atlante Sintattico d'Italia, Syntactic Atlas of Italy (ASIt).	interoperability;linked data;web application	Emanuele Di Buccio;Giorgio Maria Di Nunzio;Gianmaria Silvello	2013		10.1145/2484028.2484219	natural language processing;computer science;rdf;ontology;linked data;data mining;database;world wide web;information retrieval	NLP	-42.57002306750277	2.1677351756461234	951
af249f7d3e582891a6e8e2c3bec6d516338d3fe3	hybrid models in decision making under uncertainty: the case of training provider evaluation	promethee;decision making under uncertainty;fuzzy mcdm;topsis;ahp;decision analysis;hybrid model;service outsourcing	This paper provides a novel design for two hybrid models in modeling decision making under uncertainty: AHP-Fuzzy PROMETHEE and AHP-Fuzzy TOPSIS. The analytic hierarchy process’ (AHP) excellent ability in problem structuring allows weights of criteria to be easily gathered from experts in the decision problem. Nonetheless, the pairwise comparisons required are immense, thus inducing decision making fatigue as the number of evaluation objects and criteria increase. We show that the number of pairwise comparisons can be reduced by integrating PROMETHEE or TOPSIS into AHP. The former two techniques are distance based methods. PROMETHEE allows the evaluators to choose a set of preference function and calculates the distance between the evaluator’s judgment and his limits. TOPSIS, on the other hand, computes the distance of a judgment from the best and worst cases. Fuzzy linguistics are incorporated into PROMETHEE and TOPSIS, thus effectively modeling decision making subjectivity – aside from eliminating the need for evaluators to specify their preference limits in PROMETHEE. These techniques are applied in a strategic outsourcing decision of a company that seeks to evaluate their training providers. The final results indicate that both AHP-Fuzzy TOPSIS and AHP-Fuzzy PROMETHEE achieved consistent results and arrived at the same ranking order.	analytical hierarchy;best, worst and average case;decision problem;decision theory;interpreter (computing);k-nearest neighbors algorithm;outsourcing;preference ranking organization method for enrichment evaluation;two-hybrid screening	Joshua Ignatius;Seyyed Mahdi Hosseini Motlagh;Mohammad Mehdi Sepehri;Majid Behzadian;Adli Mustafa	2010	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-2010-0443	topsis;analytic hierarchy process;decision analysis;management science;vikor method	AI	-6.271251857100069	-15.946428332498622	952
6c1f23a44fe059271e5188a82cb42a3363c2ddc4	on the impact of ice emissivity on sea ice temperature retrieval using passive microwave radiance data	seawater;temperature advanced microwave scanning radiometer eos amsr e sea ice snow;advanced microwave scanning radiometer eos amsr e;sea ice;frequency 6 925 ghz;passive microwave;frequency 6 925 ghz ice emissivity effects sea ice temperature retrieval passive microwave radiance data advanced microwave scanning radiometer eos amsr e ice temperature algorithm snow ice interface temperature vertically polarized ice emissivity ad 2007 07 snow melt thermodynamic model;first year;snow;ocean temperature;information retrieval;polarization;ice emissivity effects;snow melt;microwave radiometry;radiometry;computer aided software engineering;geoscience;snow ice interface temperature;springs;advanced microwave scanning radiometer;penetration depth;sea ice temperature retrieval;thermodynamics emissivity microwave measurement ocean temperature oceanographic techniques radiometry sea ice seawater;microwave measurement;thermodynamic model;ice temperature algorithm;emissivity;thermodynamics;ad 2007 07;passive microwave radiance data;temperature;vertically polarized ice emissivity;oceanographic techniques;transition period;sea ice ocean temperature information retrieval snow microwave radiometry springs polarization thermodynamics computer aided software engineering geoscience;physical properties;eos amsr e	This letter examines the performance of two Advanced Microwave Scanning Radiometer-EOS (AMSR-E) ice temperature algorithms over first-year sea ice during the spring transition period where ice concentrations are close to 100%. The results showed, before snow melt, that the old AMSR-E algorithm overestimated the ice temperature by up to 18 K, which is relative to in situ and thermodynamically calculated snow/ice interface temperatures (T si). An adjustment of vertically polarized ice emissivity of 6.9 GHz [epsivI(6V)] to 0.98, which was identical to the constant value used in the latest version of the AMSR-E ice temperature algorithm (posted July 2007), demonstrated a significant improvement in ice temperature retrieval. However, after snow melt, the ice temperature retrieval with any constant epsivI(6V) failed to correctly estimate the ice temperatures due to large variability in the physical properties of snow and, in turn, penetration depth and epsivI(6V). The results suggest that a local adjustment of epsivI(6V), which is by incorporating a simple thermodynamic model into the AMSR-E ice temperature algorithm, would be useful in improving the performance of the algorithm.	advanced spaceborne thermal emission and reflection radiometer;algorithm;eos;heart rate variability;microwave	Byong Jun Hwang;David G. Barber	2008	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2008.917266	meteorology;sea ice thickness;sea surface temperature;snow;seawater;sea ice growth processes;radiometry;penetration depth;atmospheric sciences;temperature;polarization;snowmelt;sea ice concentration;emissivity;optics;computer-aided software engineering;sea ice;physical property;physics;remote sensing	Metrics	83.6952292820661	-62.37766128014773	953
ad33ba9236711687ce5724811a7c9672c6c53132	robot contouring performance study for nonconstrained process applications	performance measure;robot kinematics aerodynamics error correction orbital robotics motion control service robots error analysis testing application software machine tools;robots error compensation;6 d o f cartesian type gantry robot nonconstrained process applications dynamic contouring errors multiaxis robot axis delay times bounded frequency bandwidth;robots;error compensation;controller design;delay time	The dynamic contouring errors attributed to a multiaxis robot performing a nonconstrained contouring process are discussed, and a way of interpreting this contouring error based on a relaxed performance measure is introduced. With this concept, multiaxis contouring errors can be reduced by matching axis delay times over a bounded frequency bandwidth rather than increasing loop gains in the axis controllers. A dynamic contouring error model is developed and used to establish axis-controller design criteria. Robust axis controllers are designed and implemented in a 6 d.o.f. Cartesian-type gantry robot. Test data are obtained to verify the performance of the controller design and to validate the error model. >		Ming-Yih Lee;Timothy Holt;Albert J. Sturn;Fredric N. Bailey	1990		10.1109/ROBOT.1990.126298	robot;control engineering;simulation;computer science;artificial intelligence;control theory	HCI	67.13321573295768	-17.62440031958562	954
fd3f952cb77c8a77bd2f618c996ed6f530befe48	soft decision making for patients suspected influenza	t technology;computer model;influenza artificial dataset;medical decision making;set theory;artificial intelligent;point of view;soft set theory;qa76 computer software	Computational models of the artificial intelligence such as soft set theory have several applications. Parameterization reduction under soft set theory can be considered as a technique for medical decision making. One possible application is the decision making of patients suspected influenza. In this paper, we present the applicability of soft set theory for decision making of patients suspected influenza. The proposed technique is based on maximal supported objects by parameters. At this stage of the research, results are presented and discussed from a qualitative point of view against recent soft decision making techniques through an artificial dataset.	artificial intelligence;computation;computational model;information system;maximal set;medical decision making;set theory	Tutut Herawan;Mustafa Mat Deris	2010		10.1007/978-3-642-12179-1_34	computer simulation;influence diagram;computer science;artificial intelligence;machine learning;data mining;set theory	AI	1.2597582562849483	-27.28521570923224	955
761d5b5fcc0ab7302a8f7494d0bc790f70c91b4f	prediction-compensated polyphase multiple description image coding with adaptive redundancy control	optimal redundancy allocation;redundancy decoding encoding image coding discrete cosine transforms image reconstruction resource management;rate distortion;computational complexity prediction compensated polyphase multiple description image coding adaptive redundancy control multiple description coding mdc system polyphase mdc scheme prediction compensated polyphase mdc error resilience coding efficiency quincunx downsampled description primary subdescription dual subdescription h 264 avc intra coding mode guided directional prediction algorithm;multiple description;video coding adaptive control computational complexity;multiple description coding mdc;polyphase downsampling;image coding;decoding;adaptive control;resource manager;resource management;polyphase downsampling granular noise h264 avc image coding multiple description coding mdc optimal redundancy allocation;estimation algorithm;journal article;discrete cosine transform;video coding;redundancy;drntu engineering computer science and engineering data coding and information theory;computational complexity;discrete cosine transforms;image reconstruction;multiple description coding;error resilience;bit allocation;granular noise;encoding;predictive coding;optimality condition;h264 avc	In this paper, a novel multiple description coding (MDC) system is proposed, consisting of two thrust contributions: 1) a new polyphase MDC scheme, called the prediction-compensated polyphase MDC (PCP-MDC); and 2) an adaptive redundancy control (ARC) scheme for yielding optimal tradeoff between coding efficiency and error resilience. The PCP-MDC partitions each quincunx-downsampled description into two subdescriptions, called the primary subdescription (PS) and dual subdescription (DS). The PS is encoded by the H.264/AVC intra coding, while the DS is subject to prediction coding based on the reconstructed PS. For prediction, the mode-guided directional prediction algorithm is developed to conduct a fast and accurate prediction for the DS. For the second thrust contribution, two fundamental issues regarding the inserted redundancy are addressed in the proposed ARC scheme: quality and quantity. For the quality issue, the residuals of cross prediction are inserted into each description. For the quantity issue, the amount of redundancy bits allocated to each description is determined according to the network condition; for that, the probability of channel failure is incorporated into mathematical formulation on the derivation of optimal redundancy allocation condition. To implement the derived optimal condition, a fast estimation algorithm of the rate-distortion function is then developed, followed by exploiting successive approaching algorithm to identify the optimal partition of the target bitrate between the primary part of the description and its complementary part (i.e., the redundancy part). We also develop a post-processing filter, called the switching Gaussian filter, to remove the granular artifacts that tend to occur in any polyphase MDC approach when two descriptions are received and encoded at low bitrates. Extensive simulation results have consistently shown that the proposed MDC system significantly outperforms other state-of-the-art MDC methods on coding performance with much lower computational complexity.	algorithm;algorithmic efficiency;beta encoder;computational complexity theory;decimation (signal processing);digital raster graphic;distortion;h.264/mpeg-4 avc;intra-frame coding;multiple description coding;oversampling;pcp theorem;polyphase matrix;polyphase quadrature filter;rate–distortion theory;ruby document format;simulation;smart game format;thrust;video post-processing	Zhe Wei;Kai-Kuang Ma;Canhui Cai	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2168131	iterative reconstruction;real-time computing;computer science;resource management;theoretical computer science;multiple description coding;discrete cosine transform;redundancy;computational complexity theory;algorithm;encoding	AI	46.76579576398395	-16.896451873993076	956
6aee466fb2ea1e0962e28656d0aad54656c80564	map-tbs: map process enactment traces and analysis	pattern clustering;data mining trace based management system process mining map process model;data mining;process mining;map process model;clustering map tbs map process enactment traces map process enactment analysis variation points decision points data mining domain trace management system recommendation based guidance;recommender systems data mining pattern clustering;trace based management system;data mining clustering algorithms algorithm design and analysis context modeling adaptation models complexity theory;recommender systems	Map formalism allows specifying processes with a high level of variability. However, this means many variation points, and therefore we need guidance to enact maps by customizing them. Traditional guidance consists in raising decision points to navigate in a map. The limit is that many decision points are raised at the same time, and the user (who enacts the map) does not know which decision to make first. Another kind of guidance, yet to be explored, consists in providing recommendations to the user. Such recommendations can be drawn from collections of profiles collected from map enactment traces using techniques from the data mining domain. This paper proposes a trace management system adapted to maps that was designed to support recommendation-based guidance. The paper shows how data mining algorithms can be used to find profile clusters in a collection of map enactment traces, used then to provide recommendations to the users.	algorithm;data mining;high-level programming language;map;semantics (computer science);spatial variability;tracing (software)	Charlotte Hug;Rébecca Deneckère;Camille Salinesi	2012	2012 Sixth International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2012.6240435	computer science;data science;data mining;database;process mining;world wide web;recommender system	Robotics	-52.94870577205744	17.90813856853186	957
b25a1bd96328ec6bd0963d2e3fb2097ad357222b	information systems foundations - karl popper's third world	world wide web;www;information system	The various information professions have matured separately over the years, developing different bodies of theory and practice to meet their evolving purposes and needs. A problem arises however, when different information professions address the same knowledge domain and there is no explicit correspondence between the conceptual structures embedded independently in each. In this situation, a knowledge worker involved in the domain is faced with a range of possibly incompatible structures presented in different forms by a range of information professions. This is a common problem that is being exacerbated by the explosion in information production and the widening access to information distribution technology, notably the World Wide Web. Information Systems now need to combine the best of what the information professions in a domain have to offer the domain’s knowledge workers. This paper examines the problem by exploring one of the foundations of the information disciplines Karl Popper’s 3 Worlds theory, applying it to a case study and suggesting that the Information Systems discipline alone has a sufficiently broad agenda to integrate the various Informatics themes needed to support today’s knowledge workers. POPPER’S THREE WORLDS The idea that the universe of human experience can be analysed in terms of three interacting worlds is an old one. Versions of the idea date back at least to Plato’s cave analogy and to Aristotle, but the elaboration by Karl Popper is an interesting contemporary formulation. In Popper’s theory, reality is divided into three parts: ... first, the world of physical objects or of physical states; secondly the world of states of consciousness, or of mental states, or perhaps of behavioural dispositions to act; and thirdly, the world of objective contents of thought. (Popper, 1972, p.106). Popper distinguished thought, in the sense of the content of statements, and thought, in the sense of thought processes, as belonging to two entirely different worlds: If we call the world of ‘things’ – of physical objects – the first world, and the world of subjective experiences (such as thought processes) the second world, we may call the world of statements in themselves the third world. (Popper, 1976, pp.180-1). The Three Worlds theory is diagrammatically represented in Figure 1. objects & events mental processes statements 3: Information World 2: Cognitive World 1: Physical World Figure 1: The Three Worlds Turning first to the relationship between Worlds 1 and 2, consider a human, conscious mind (so in World 2) that perceives objects in the physical world (World 1). What it perceives is constrained first by the nature of the sense organs and lower brain functions, which exclude much potential stimulus, and, secondly, by the nervous system and brain which organise the stimulus that has been detected. Perception is the change of state in the mind as a result of paying attention to objects in World 1. Humans do not only perceive World 1, they also conceive it. That is, their sensing of the world is tempered and interpreted in terms of what they have learned and what they understand to be the significance of what they perceive. So when the mind in our scenario sees a full moon it is perceiving the light as a disc and conceiving of what the full moon might mean; perhaps romance, or danger. 8 All translations of classical works follow conventions for identifying paragraphs. The relevant references are Plato's Republic 514-516 and Aristotle's On Interpretation 16a4. AJIS Special Issue December 2002 60 There is, however, a two-way interaction between Worlds 1 and 2. The one described above is perception, the other is action. When the person makes a deliberate act, there is an effect on World 1 as a result of the World 2 state. Making a statement about the world is a special kind of human act. In its physical form, it is a part of World 1, speech sending out sound vibrations, for example. However, as a statement it is part of World 3. Statements are symbolic representations of concepts held by the speaker. These representations take the form of words and linguistic structures. Examples of objective knowledge are theories published in journals and books and stored in libraries; discussions of such theories; difficulties and problems pointed out in connection with such theories; and so on; “... we can call ... the world of the logical contents of books, libraries, computer memories, and suchlike ‘World 3’” (Popper 1972 pp.73-4). World 2 acts on World 3 by writing, and inversely, by reading. Clearly words have physical properties and as such are in World 1. They are perceived through the senses, but what sets them worlds apart is that the concepts they evoke in the mind of the perceiver are not those of black marks on a page, or letters of the alphabet or words of the language, but what those words denote. A link in the mind is being made between the word, or symbol, and the World 1 reality to which it refers (shown by the dotted arrow). Figure 2 shows the interaction between the Three Worlds. Perception 3: Information World 2: Cognitive World 1: Physical World Action Reading Writing		Craig McDonald	2002	Australasian J. of Inf. Systems		information needs;social science;information;epistemology;computer science;engineering;knowledge management;artificial intelligence;software engineering;management information systems;management science;sociology;information quality;informatics;management;world wide web;information system	AI	-55.40236067763176	-23.213911874746035	958
1f647c0efbcc53a020dd23233947a64c10ea2f36	computing optimal policies for partially observable decision processes using compact representations	dynamic programming;bayesian network;partially observed markov decision process;generic model;approximation method;system dynamics;dynamic programming algorithm;course of action;optimal policy;compact representation;markov process;decision theoretic planning;artificial intelligence;algorithms;decision process;planning;partial observation;influence diagram;decision tree analysis;mathematics computers information science management law miscellaneous	Partially-observable Markov decision processes provide a very general model for decision-theoretic planning problems, allowing the trade-offs between various courses of actions to be determined under conditions of uncertainty, and incorporating partial observations made by an agent. Dynamic programming algorithms based on the information or belief state of an agent can be used to construct optimal policies without explicit consideration of past history, but at high computational cost. In this paper, we discuss how structured representations of the system dynamics can be incorporated in classic POMDP solution algorithms. We use Bayesian networks with structured conditional probability matrices to represent POMDPs, and use this representation to structure the belief space for POMDP algorithms. This allows irrelevant distinctions to be ignored. Apart from speeding up optimal policy construction, we suggest that such representations can be exploited to great extent in the development of useful approximation methods. We also briefly discuss the difference in perspective adopted by influence diagram solution methods vis à vis POMDP techniques.	algorithm;algorithmic efficiency;approximation;automated planning and scheduling;bayesian network;computation;dynamic programming;grammar-based code;influence diagram;markov chain;partially observable markov decision process;partially observable system;relevance;system dynamics;theory	Craig Boutilier;David Poole	1996			mathematical optimization;partially observable markov decision process;computer science;artificial intelligence;machine learning;dynamic programming	AI	21.264980446702385	-15.346812225994283	959
a2b0636934125013ef536cc01e79d805cd23500a	high-order bound-preserving discontinuous galerkin methods for compressible miscible displacements in porous media on triangular meshes				Nattaporn Chuenjarern;Ziyao Xu;Yang Yang	2019	J. Comput. Physics	10.1016/j.jcp.2018.11.003		Theory	86.55004696040372	10.728074597556175	960
3966bb7f7fb23ef84e3c1987b80b79197a5bda29	problem oriented project work in a distance education program in health informatics		At Aalborg University, an important part of the distance education program within Health Informatics is problem oriented project work. Traditionally, distance education has been characterized by one-way communication and self study whereas the problem oriented project study form is based on cooperation and dialogue. In this paper, we describe the way in which we have implemented the problem oriented study form in a program within Health Informatics which is based on distance learning. First, we describe the program with regard to student, structure, aim, and activities. Second, we introduce the problem oriented project study form and present the basic principles behind this approach. Third, we explain important concepts and distinctions within the area of distance education. Finally, we describe the way in which we try to put the ideals of the problem oriented project work into practice. The use of a computer conferencing system is essential but in our experience, it is not in itself sufficient to provide the necessary support for the student project work.	education, distance;education, pharmacy, graduate;informatics (discipline);learning disorders;occupational health services;one-way function;women's health services	Ann Bygholm;Ole K. Hejlesen;Christian Nøhr	1998	Studies in health technology and informatics	10.3233/978-1-60750-896-0-740	knowledge management;health informatics;informatics;distance education;computer science	SE	-75.82442442363278	-33.51390884993385	961
60ecf268cf823a518002e36531857e05eeb40850	towards fuzzy conceptual graph programs	inference rule;conceptual graph;modus ponens	Fuzzy conceptual graphs (CGS) are formulated by means of fuzzy truth values in order to allow them to express uncertain and imprecise information in a more appropriate and flexible manner than is possible by a real number based approach. Corresponding projection and other related operators are defined and their properties investigated. These FCGs and operators provide the basis on which FCG programs with declarative semantics and a sound modus ponens inference rule are developed.	conceptual graph;fuzzy logic	Vilas Wuwongse;Cao Hoang Tru	1996		10.1007/3-540-61534-2_17	conceptual graph;backward chaining;modus ponens;deduction theorem;computer science;machine learning;pattern recognition;algorithm;rule of inference	AI	-0.620544133445119	-25.719305780829988	962
9c13232b1cd112bfefac89759e622ae609d2dbf6	situated web service: context-aware approach to high-speed web service communication	protocols;context aware;situated web service;web service;context aware high speed web service communication;web services context aware services context xml simple object access protocol encoding costs communications technology data encapsulation informatics;web services;speech recognition;ubiquitous computing;web services protocols ubiquitous computing;operation usage situated web service context aware high speed web service communication protocols;operation usage;real time application;high speed	A framework is proposed to improve Web service performance based on context-aware communication. Two key ideas are introduced to represent a client context: (1) available protocols that the client can handle, and (2) operation usage that shows how the client uses Web service operations. We call our context aware approach a situated Web service (SiWS). We implemented and evaluated the SiWS and found that the overall performance was improved if more than three Web services were executed between context changes	context awareness;redundancy (engineering);situated;transmitter;web service;xml	Ikuo Matsumura;Toru Ishida;Yohei Murakami;Yoshiyuki Fujishiro	2006	2006 IEEE International Conference on Web Services (ICWS'06)	10.1109/ICWS.2006.122	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;knowledge management;ws-policy;web navigation;web page;ws-addressing;database;web intelligence;web 2.0;law;world wide web;ubiquitous computing;mashup	Visualization	-38.377563236402615	49.79458412446107	963
e065ec73a5c9b75dcb00c4a040a7d17cac352fd7	differential co-expression network centrality and machine learning feature selection for identifying susceptibility hubs in networks with scale-free structure	biological patents;biomedical journals;text mining;europe pubmed central;citation search;data mining and knowledge discovery;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;computer appl in life sciences;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Biological insights into group differences, such as disease status, have been achieved through differential co-expression analysis of microarray data. Additional understanding of group differences may be achieved by integrating the connectivity structure of the differential co-expression network and per-gene differential expression between phenotypic groups. Such a global differential co-expression network strategy may increase sensitivity to detect gene-gene interactions (or expression epistasis) that may act as candidates for rewiring susceptibility co-expression networks. We test two methods for inferring Genetic Association Interaction Networks (GAIN) incorporating both differential co-expression effects and differential expression effects: a generalized linear model (GLM) regression method with interaction effects (reGAIN) and a Fisher test method for correlation differences (dcGAIN). We rank the importance of each gene with complete interaction network centrality (CINC), which integrates each gene’s differential co-expression effects in the GAIN model along with each gene’s individual differential expression measure. We compare these methods with statistical learning methods Relief-F, Random Forests and Lasso. We also develop a mixture model and permutation approach for determining significant importance score thresholds for network centralities, Relief-F and Random Forest. We introduce a novel simulation strategy that generates microarray case–control data with embedded differential co-expression networks and underlying correlation structure based on scale-free or Erdos-Renyi (ER) random networks. Using the network simulation strategy, we find that Relief-F and reGAIN provide the best balance between detecting interactions and main effects, plus reGAIN has the ability to adjust for covariates and model quantitative traits. The dcGAIN approach performs best at finding differential co-expression effects by design but worst for main effects, and it does not adjust for covariates and is limited to dichotomous outcomes. When the underlying network is scale free instead of ER, all interaction network methods have greater power to find differential co-expression effects. We apply these methods to a public microarray study of the differential immune response to influenza vaccine, and we identify effects that suggest a role in influenza vaccine immune response for genes from the PI3K family, which includes genes with known immunodeficiency function, and KLRG1, which is a known marker of senescence.	centrality;embedded system;embedding;erdős–rényi model;feature selection;flow network;gene co-expression network;generalized linear model;immunologic deficiency syndromes;influenza virus vaccine;interaction network;klrg1 gene;lasso;machine learning;microarray;mixture model;random forest;regression analysis;sensor;simulation	Caleb A. Lareau;Bill C. White;Ann L. Oberg;Brett A. McKinney	2015		10.1186/s13040-015-0040-x	text mining;medical research;computer science;bioinformatics;data science;data mining	Comp.	6.000308988281893	-55.69683915851484	964
5620e28b55fb5bd0d26db53a98468f3c3c685140	learning robocup-keepaway with kernels	text;high dimensionality;reinforcement learning;least squares policy iteration;function approximation;policy evaluation;state space;policy iteration;article	We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the highdimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained with tilecoding by Stone et al. (2005).	approximation algorithm;basis function;discretization;iteration;kernel (operating system);least squares;markov decision process;real-time locating system;recursion;reinforcement learning;simulation;state space	Tobias Jung;Daniel Polani	2007			mathematical optimization;function approximation;computer science;state space;artificial intelligence;theoretical computer science;machine learning;mathematics;reinforcement learning;statistics	ML	25.180642136980833	-31.397306744086325	965
cc2fb3653a4a760a74a3aeacf9332f76ad688b81	new materials and advances in making electronic skin for interactive robots	robotics;tactile sensing;novel materials;electronic skin	Flexible electronics has huge potential to bring revolution in robotics and prosthetics as well as to bring about the next big evolution in electronics industry. In robotics and related applications, it is expected to revolutionise the way with which machines interact with humans, real-world objects and the environment. For example, the conformable electronic or tactile skin on robot’s body, enabled by advances in flexible electronics, will allow safe robotic interaction during physical contact of robot with various objects. Developing a conformable, bendable and stretchable electronic system requires distributing electronics over large non-planar surfaces and movable components. The current research focus in this direction is marked by the use of novel materials or by the smart engineering of the traditional materials to develop new sensors, electronics on substrates that can be wrapped around curved surfaces. Attempts are being made to achieve flexibility/stretchability in e-skin while retaining a relia...	electronic skin;robot	Nivasan Yogeswaran;Wenting Dang;William Taube Navaraj;D. Shakthivel;S. Khan;Emre O Polat;Shoubhik Gupta;Hadi Heidari;Mohsen Kaboli;Leandro Lorenzelli;Gordon Cheng;Ravinder Dahiya	2015	Advanced Robotics	10.1080/01691864.2015.1095653	computer science;engineering;artificial intelligence;biological engineering;robotics;electronic skin	Robotics	77.4849960809464	-23.12387270417343	966
a8e539c6e585d3afd2e463e273193b356c37879a	mapping two complete binary trees into the star graph with quick fault recovery	embedding;complete binary tree;star graph;fault tolerant;edge label;generating function;fault recovery;binary tree	This paper proposes an approach for embedding two complete binary trees (CBT) into ann-dimensional star graph (S n), and provides a fault-tolerant scheme for the trees. First, aCBT with height Σ m=2 n ⌊logm⌋ is embedded into theS n with dilation 3. The height of theCBT is very close to ⌊Σ m=2 n logm⌋, the height of the largest possibleCBT which can be embedded into theS n. Shifting the firstCBT by generating function productg 2 g 3 g 4 g 3, anotherCBT with height Σ m=2 n ⌊logm⌋ can also be embedded into theS n without conflicting with the first one. Moreover, if three-eights of nodes in the firstCBT and all nodes in the secondCBT are faulty, all of them can be recovered. Under the condition that the firstCBT with smaller height (⌊Σ m=2 n logm⌋ − 1) is embedded, all the replacement nodes will be free. As a consequence, even in the case that all nodes in the two trees are faulty, they can be recovered in the smallest number of recovery steps and only with dilation 5.	binary tree;dilation (morphology);embedded system;fault tolerance	Chiun-Chieh Hsu	1997	New Generation Computing	10.1007/BF03037299	binary tree;computer science;distributed computing;programming language	Theory	23.61845024845931	34.41134479740771	967
2b9d999bb1b82f04eb9023c0ba5d065b57be5fe5	comparative analysis of the dc performance of dg mosfets on highly-doped and near-intrinsic silicon layers	silicon;corriente dren;courant continu;evaluation performance;double gate;comparative analysis;oxyde grille;leakage current;performance evaluation;drain current;soi;polycristal;silicon layers;electrical characteristic;corriente escape;evaluacion prestacion;doped materials;materiau dope;polycrystal;power circuit;low power circuit;miniaturisation;transistor de compuerta doble;low power;fully depleted;circuit puissance;courant fuite;modelo 2 dimensiones;courant drain;grille transistor;caracteristique electrique;low power electronics;transistor mosfet;dual gate transistor;policristal;modele 2 dimensions;circuito potencia;transistor grille double;rejilla transistor;mosfet;miniaturization;silicon on insulator technology;technologie silicium sur isolant;si;miniaturizacion;silicium;direct current;electronique faible puissance;caracteristica electrica;silicio;transistor gate;two dimensional model;gate oxide;oxido rejilla;corriente continua;device simulation;tecnologia silicio sobre aislante	A comparison of dc characteristics of fully depleted double-gate (DG) MOSFETs with respect to low-power circuit applications and device scaling has been performed by two-dimensional device simulation. Three different DG MOSFET structures including a conventional Nþ polysilicon gate device with highly doped Si layer, an asymmetrical Pþ/Nþ polysilicon gate device with low doped Si layer and a midgap metal gate device with low doped Si layer have been analysed. It was found that DG MOSFET with mid-gap metal gates yields the best dc parameters for given off-state drain leakage current and highest immunity to the variation of technology parameters (gate length, gate oxide thickness and Si layer thickness). It is also found that an asymmetrical Pþ/Nþ polysilicon gate DG MOSFET design offers comparable dc characteristics, but better parameter immunity to technology tolerances than a conventional DG MOSFET. q 2004 Elsevier Ltd. All rights reserved.	discontinuous galerkin method;doping (semiconductor);gate oxide;image scaling;low-power broadcasting;metal gate;simulation;spectral leakage;thickness (graph theory)	Nebojsa D. Jankovic;G. Alastair Armstrong	2004	Microelectronics Journal	10.1016/j.mejo.2004.04.007	metal gate;qualitative comparative analysis;electronic engineering;telecommunications;engineering;polysilicon depletion effect;electrical engineering;silicon on insulator;gate oxide;miniaturization;leakage;silicon;physics;quantum mechanics;low-power electronics	EDA	91.94814885268684	-10.432078328091503	968
75587ef42df4e827876660cd576286ce947ab06c	a sub-threshold fpga with low-swing dual-vdd interconnect in 90nm cmos	field programmable gate array;integrated circuit interconnections field programmable gate arrays;low swing dual vdd interconnect;chip;size 90 nm;size 90 nm sub threshold fpga low swing dual vdd interconnect field programmable gate array;synchronization;integrated circuit interconnections;fabrics;field programmable gate arrays;table lookup;sub threshold fpga;field programmable gate arrays delay integrated circuit interconnections table lookup fabrics benchmark testing synchronization;benchmark testing	This paper presents a sub-threshold Field Programmable Gate Array (FPGA) that uses a low-swing dual-VDD global interconnect fabric to reduce energy and improve delay. A 90nm chip implements the FPGA with 1134 LUTs, which is 2.7X smaller, 14X faster, and 4.7X less energy than a sub-threshold FPGA using conventional interconnect and 22X less energy than an equivalent FPGA at full VDD.	cmos;field-programmable gate array;switched fabric;value-driven design	Joseph F. Ryan;Benton H. Calhoun	2010	IEEE Custom Integrated Circuits Conference 2010	10.1109/CICC.2010.5617466	embedded system;electronic engineering;parallel computing;telecommunications;reconfigurable computing;programmable logic array;computer science;engineering;field-programmable gate array	EDA	16.248120939281694	56.90933014599818	969
dff538f7c6b64792233aad006c77742b65653acd	etude de la contribution des systèmes immunitaires artificiels au pilotage de systèmes de production en environnement perturbé. (study of the contribution of artificial immune systems to the monitoring and control production systems in the presence of production flow disruptions)			artificial immune system;linear algebra	Saber Darmoul	2010				OS	-103.22437776567926	17.83280772866839	970
788fe8c945aa3e738247b907e32ba6b6b1e023df	a new cnn ic for stereo visual system	real time systems stereo image processing neural chips cellular neural nets;real time;cellular neural network;real time processing;real time processing stereo visual system environment sensing three dimensional information extraction cellular neural network algorithm analogue ic;cellular neural nets;three dimensional;chip;neural chips;cellular neural networks visual system stereo vision data mining real time systems computational efficiency robot sensing systems image analysis image reconstruction hardware;stereo image processing;stereo vision;visual system;settore ing ind 31 elettrotecnica;real time systems	In environment sensing, one of the most relevant knots is certainly represented by the implementation of an effective system for the real time extraction of the three-dimensional information. Several effective approaches on this topic have been proposed in literature. Among the others, the Stereo Vision approach seems to be really attractive. Moreover, such an algorithm can be effectively mapped on a Cellular Neural Network. Consequently, it can be analogue implemented overcoming to the difficulties leading from the relative high computational cost and the required real time processing. In this paper, a new CNN chip well suited for a Stereo Visual System will be presented.		M. Salerno;F. Sargeni;V. Bonaiuto	2001		10.1109/ISCAS.2001.921319	chip;embedded system;three-dimensional space;stereo cameras;computer vision;cellular neural network;visual system;computer science;stereopsis;computer graphics (images)	Vision	45.95783115607877	-34.9001764990738	971
dee082154f07fda10491e6c82776c88b68caaf75	characterization of some binary words with few squares	combinatorial problems;repetitions;avoidability	Thue proved that the factors occurring infinitely many times in square-free words over {0,1,2} avoiding the factors in {010,212} are the factors of the fixed point of the morphism 0 7→ 012, 1 7→ 02, 2 7→ 1. He similarly characterized square-free words avoiding {010,020} and {121,212} as the factors of two morphic words. In this paper, we exhibit smaller morphisms to define these two square-free morphic words and we give such characterizations for six types of binary words containing few distinct squares.	fixed point (mathematics);morphic (software);morphic word;square-free word	Golnaz Badkobeh;Pascal Ochem	2015	Theor. Comput. Sci.	10.1016/j.tcs.2015.03.044	arithmetic;combinatorics;discrete mathematics;mathematics	Logic	36.59073240099889	36.79125022575176	972
862d2b39142383d6d20eb86464ae4cf75fece7e1	fragmented digital infrastructures - the case of social (news) media		Digitization and contemporary use of social media platforms is changing how we perceive and use IT, both in organizations and as citizens or consumers. These developments have also transformed and changed many industries, bringing both opportunities and challenges. One industry that was affected by digitization relatively early is the media industry, leading to studies on online journalism, gatekeeping, reader interaction and other changing practices enabled by IT. This study attempts to highlight the IT infrastructure behind the changing practice to understand the relationship between the IT and the use. Through an online ethnography, this paper investigates the use of social media platforms in the newspaper industry. The findings indicate that newspapers either use an integration strategy, where they relinquish control over the feature to the social media platform, or an appropriation strategy where they take the integration a step further, to have the social media platform work for them.	americas conference on information systems;digital journalism;multimedia framework;rico;social media;usb hub	Esbjörn Ebbesson	2015			public relations;media relations;engineering;multimedia;advertising	HCI	-82.08124199629844	-13.224978626803061	973
b4122ca848a9933f8017954b8b36805f6463c3de	introduction to numerically intensive computing for capacity planners and performance specialists			numerical integration	Clifford J. Goosmann	1993			management science;computer science	HPC	-49.87082961672827	-17.202596861311495	974
10d498b2a8c98473526e211a5446f5d111d2baed	performing design analysis: game design creativity and the theatre of the impressed	creativity;computer gaming and animation;design education;centre for creative arts research;game design;innovation;critical design;queensland college of art;design thinking;design creativity;190202 computer gaming and animation;190202	We report and reflect upon the early stages of a research project that endeavours to establish a culture of critical design thinking in a tertiary game design course. We first discuss the current state of the Australian game industry and consider some perceived issues in game design courses and graduate outcomes. The second section presents our response to these issues: a project in progress which uses techniques originally exploited by Augusto Boal in his work, Theatre of the Oppressed. We appropriate Boal's method to promote critical design thinking in a games design class. Finally, we reflect on the project and the ontology of design thinking from the perspective of Bruce Archer's call to reframe design as a 'third academic art'.	the australian;video game design	Truna Aka J. Turner;David Browning;Gordon Moyes	2012		10.1145/2336727.2336730	humanities;game design;design;simulation;level design;design research;experience design;communication design;engineering;game art design;game developer;multimedia;design education;game design document	HCI	-73.08863745764343	-33.062407462952805	975
c0b38c0fa9643e808f82d5dcffd91bd1ec5446db	applicability of the homotopy method to the determination of fixed points in chemical kinetics models	chemical kinetics;homotopie;systeme equation;equation differentielle;quasi steady state assumption;metodo reduccion;numerical method;homotopia;chemical kinetics system;differential equation;cinetique chimique;nonlinear dynamical system;punto fijo;reduction schemes;turbulent combustion;homotopy;power method;homotopy method;fixed point;ecuacion diferencial;dynamical system;stability;systeme dynamique;sistema ecuacion;metodo numerico;point fixe;cinetica quimica;equation system;systeme non lineaire;stability analysis;methode reduction;newton raphson;sistema dinamico;stabilite;sistema no lineal;kinetics;reduction method;fix point;non linear system;methode numerique;nonlinear dynamic system;estabilidad;quasi steady state;chemical reaction kinetics	Reducing the complex chemical kinetics is an important challenge to implement chemical schemes in a turbulent combustion code. An important step in the reduction of chemical kinetics is to extract the fixed points of the set of differential equations associated with the chemical scheme since they are useful to determine the low manifolds which may be used to reduce kinetic mechanisms. This paper aims at testing a potentially powerful method, namely the Homotopy Method, for extracting fixed points from nonlinear dynamical systems. The method is tested on a 3D and a 7D model of the well-known Belousov-Zhabotinskii reaction. This study shows that the Homotopy Method has a significantly better efficiency than available Newton-Raphson algorithms but that the stiffness of the chemical kinetics equations still resists this method when the number of species increases.	fixed point (mathematics);kinetics internet protocol	H. Labro;B. Maheu;A. Garo	2000	Applied Mathematics and Computation	10.1016/S0096-3003(98)10116-9	homotopy perturbation method;stability;chemical kinetics;dynamical system;homotopy;calculus;mathematics;geometry;fixed point;differential equation;algorithm;kinetics;quantum mechanics;statistics;algebra	Logic	81.980238333942	12.202858209711449	976
37b10fa019007f9affb91e37acea520338065bf1	kinematic design of a novel 4-dof parallel manipulator		A new four degrees-of-freedom (DOF) parallel manipulator that can produce 3-DOF translations and 1-DOF rotation (3T1R), has been proposed in this paper. It has two identical limbs connected to the moving platform through passive revolute joints, and each limb has two identical branches driven by a pair of base mounted collinear prismatic joints. Due to such a unique “4-2-1” kinematic structure, the 4-DOF parallel manipulator has the advantages of simple kinematics, large workspace, high speed, and high positioning accuracy. These advantages make it an appropriate candidate for high-speed and high-precision pick-and-place operations. To validate the proposed parallel manipulator design, mobility analysis is conducted based on the screw theory. Other critical design analysis issues, such as displacement, singularity, and workspace analyses, have been addressed in details.	computer case screws;displacement mapping;parallel manipulator;smt placement equipment;singularity project;workspace	Cuncun Wu;Guilin Yang;Chin-Yin Chen;Shulin Liu;Tianjiang Zheng	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989724	screw theory;workspace;control engineering;control theory;mobile manipulator;engineering;revolute joint;kinematics;parallel manipulator;singularity;serial manipulator	Robotics	70.69112145158209	-21.447908987526365	977
0440f110e8b50e33ce0d130e69115b60e6cb8a1e	hardware-software codesign for high-speed signature-based virus scanning	libraries;high speed signature based virus scanning;hardware software codesign;c 0 general;simulation of multiple processor systems;hardware throughput engines viruses medical libraries filters packaging degradation application software inspection;network security;high speed networks;c 2 0 a architecture;viruses medical;hardware engine;digital signatures;deep packet inspection string matching hardware software codesign;c 4 a design studies;deep packet inspection;data passing process;high speed network content security application hardware software codesign high speed signature based virus scanning hardware engine data passing process offload signature matching;computer viruses;content analysis;filtering algorithms;c 1 processor architectures;c 2 1 network architecture and design;c 4 g measurement;high speed network content security application;driver circuits;matched filters;evaluation;string matching;modeling;offload signature matching;article;high speed;security of data;security of data computer viruses digital signatures hardware software codesign;throughput;hardware;computer systems organization	High-speed network content security applications often offload signature matching to hardware. In such systems, the throughput of the overall system, rather than the hardware engine alone, is significant. The authors offload virus scanning in the ClamAV antivirus package to the BFAST* hardware engine. They find that the data-passing processes significantly degrade system throughput.	antivirus software;clam antivirus;content security policy;throughput	Ying-Dar Lin;Po-Ching Lin;Yuan-Cheng Lai;Tai-Ying Liu	2009	IEEE Micro	10.1109/MM.2009.81	embedded system;deep packet inspection;digital signature;throughput;parallel computing;real-time computing;systems modeling;content analysis;computer science;evaluation;network security;operating system;matched filter;computer virus;string searching algorithm	Arch	-60.838663021330376	68.73565104293561	978
284c47b9a58438184a6fb97aa90518774991d9d0	a discussion of simultaneous localization and mapping	tecnologia industrial tecnologia mecanica;mobile robot;uncertainty;slam;grupo de excelencia;navigation;estimation;simultaneous localization and mapping;mobile robot navigation;formal specication;information matrix;tecnologias	This paper aims at a discussion of the structure of the SLAM problem. The analysis is not strictly formal but based both on informal studies and mathematical derivation. The first part highlights the structure of uncertainty of an estimated map with the key result being “Certainty of Relations despite Uncertainty of Positions”. A formal proof for approximate sparsity of so-called information matrices occurring in SLAM is sketched. It supports the above mentioned characterization and provides a foundation for algorithms based on sparse information matrices. Further, issues of nonlinearity and the duality between information and covariance matrices are discussed and related to common methods for solving SLAM. Finally, three requirements concerning map quality, storage space and computation time an ideal SLAM solution should have are proposed. The current state of the art is discussed with respect to these requirements including a formal specification of the term “map quality”.	approximation algorithm;computation;correspondence problem;dimension 3;distortion;formal proof;formal specification;integer factorization;jacobian matrix and determinant;mathematical model;nonlinear system;relevance;requirement;robot;simultaneous localization and mapping;sparse matrix;time complexity;tree accumulation;uncertain data	Udo Frese	2006	Auton. Robots	10.1007/s10514-006-5735-x	mobile robot;computer vision;estimation;navigation;simulation;uncertainty;computer science;artificial intelligence;fisher information;mobile robot navigation;simultaneous localization and mapping	Robotics	56.32542763227367	-40.60800631618546	979
9502b8e8969dd22c298b144f887e4bcf6badb75b	energy efficient magnetic tunnel junction based hybrid lsi using multi-threshold utbb-fd-soi device	magnetic tunnel junction;ultra low power;forward back bias;near sub threshold computing;nonvolatile lsi;28nm utbb fd soi	The energy scalability of ultra-low power nonvolatile (NV) large-scale integration (LSI) is explored in this paper. Multi-threshold computing (super/near/sub-$V_t$) in hybrid CMOS/ magnetic tunnel junction (MTJ) circuits are investigated based on SPICE-compatible MTJ model and fully depleted silicon on insulator (FD-SOI) devices. Ultra-low supply voltage operation bottlenecks associated with performance loss, parametric variations and function failure are studied in differential pair-based sensing circuit, MTJ writing/control circuit and other building blocks. A case study is performed with three typical NV-flip-flops (NV-FF), which are implemented with 28nm FD-SOI low $V_t$ (LVT) device and forward back-bias. Results show that MTJ writing/control circuit must operate at nominal supply (super-$V_t$) region to guarantee MTJ switching; sensing circuit is configured with near-$V_t$ operation (0.6V) with robustness consideration, whereas other parts could be implemented with near/sub-$V_t$ computing to achieve ultra-low power consumption and energy efficient operations.	bottleneck (software);differential signaling;flops;integrated circuit;nv network;spice;scalability;silicon on insulator;topological insulator;very-large-scale integration	Hao Cai;Yilin Wang;Lirida A. B. Naviner	2017		10.1145/3060403.3060413	electronic engineering;tunnel magnetoresistance;telecommunications;engineering;electrical engineering	EDA	16.247546603369354	60.57733913567893	980
a18b5838000f02e7239beedd3c95b6714c96963e	building a framework for the influence of digital content on student course engagement		Grounded in the Motivated Learning Strategies Model, this paper proposes a testable framework to explore how access to digital information impacts learning through examining the mediating factor of engagement. While the implications of patterns of behavior on learning and learning outcomes are significant, we first must understand what drives students to engage with their course content in the first place. In this paper, we propose to explore whether students perceive electronic content delivery (engaging e-textbooks, YouTube videos, Ted Talks, professor-presented video lectures, etc.) will improve their likelihood of engagement with the material. The purpose of this paper is to present the framework for a testable model examining the impact of digital, open educational resources (OER) on student engagement and involvement in learning.	digital data;digital distribution;ted	Rhonda Syler;Elizabeth Baker	2016			simulation;engineering;knowledge management;student engagement;multimedia	HCI	-74.730855819347	-40.612444463166185	981
086d07c0a703e8dd0d6b3fe9ff3db32bd95c00b0	the parameterized hardness of the k-center problem in transportation networks		In this paper we study the hardness of the k-Center problem on inputs that model transportation networks. For the problem, an edge-weighted graph G = (V,E) and an integer k are given and a center set C ⊆ V needs to be chosen such that |C| ≤ k. The aim is to minimize the maximum distance of any vertex in the graph to the closest center. This problem arises in many applications of logistics, and thus it is natural to consider inputs that model transportation networks. Such inputs are often assumed to be planar graphs, low doubling metrics, or bounded highway dimension graphs. For each of these models, parameterized approximation algorithms have been shown to exist. We complement these results by proving that the k-Center problem is W[1]-hard on planar graphs of constant doubling dimension, where the parameter is the combination of the number of centers k, the highway dimension h, and even the treewidth t. Moreover, under the Exponential Time Hypothesis there is no f(k, t, h) · n √ k+h) time algorithm for any computable function f . Thus it is unlikely that the optimum solution to k-Center can be found efficiently, even when assuming that the input graph abides to all of the above models for transportation networks at once! Additionally we give a simple parameterized (1+ε)-approximation algorithm for inputs of doubling dimension d with runtime (k/ε) · n. This generalizes a previous result, which considered inputs in D-dimensional Lq metrics.		Andreas Emil Feldmann;Dániel Marx	2018		10.4230/LIPIcs.SWAT.2018.19	combinatorics;discrete mathematics;exponential time hypothesis;approximation algorithm;mathematics;bounded function;planar graph;computable function;parameterized complexity;integer;treewidth	Theory	23.516393735729924	19.627651218422223	982
aaab940fcd4841e20de7f54a08bb2d860629551c	a logic cell architecture exploiting the shannon expansion for the reduction of configuration memory	scalable logic modules logic cell architecture shannon expansion field programmable gate arrays look up table k input lut configuration memory cells soft error rate;table lookup field programmable gate arrays;microprocessors;memory management;table lookup microprocessors field programmable gate arrays benchmark testing delays memory management;field programmable gate arrays;table lookup;benchmark testing;delays	Most modern field-programmable gate arrays (FPGAs) employ a look-up table (LUT) as their basic logic cell. Although a k-input LUT can implement any k-input logic, its functionality relies on a large amount of configuration memory. As FPGA scales improve, the increased quantity of configuration memory cells required for FPGAs will require a larger area and consume more power. Moreover, the soft-error rate per device will also increase as more configuration memory cells are embedded. We propose scalable logic modules (SLMs), logic cells requiring less configuration memory, reducing configuration memory by making use of partial functions of Shannon expansion for frequently appearing logics. Experimental results show that SLM-based FPGAs use much less configuration memory and have smaller area than conventional LUT-based FPGAs.	cell (microprocessor);embedded system;field-programmability;field-programmable gate array;itil;lookup table;scalability;shannon (unit);soft error	Qian Zhao;Kyosei Yanagida;Motoki Amagasaki;Masahiro Iida;Morihiro Kuga;Toshinori Sueyoshi	2014	2014 24th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2014.6927460	embedded system;benchmark;interleaved memory;parallel computing;sense amplifier;logic gate;programmable logic array;computer science;theoretical computer science;operating system;computer memory;complex programmable logic device;registered memory;field-programmable gate array;computing with memory;memory management	EDA	6.051212346833039	46.1457868785788	983
0ab2b226b9870eb86e828378c9374642e43fe732	when is a concept algebra boolean?	distributive lattice;boolean algebra;concept lattice	"""INTRODUCTION: Boolean algebra is formal a way to express digital logic equations, and to represent a logical design in an alphanumeric way. The present Boolean algebra format, and many of the logic manipulation rules and techniques were formalized around 1850 by George Boole, an Irish mathematician. It was used as a systematic approach to solving problems in logic and reasoning. With the advent of modern electronics, and digital systems in particular, Boolean algebra found a natural home. In addition to being used as a tool for deductive reasoning, it is now an almost indispensable tool for designing digital logic circuits and machines. A line over the top of a single digital variable or group of variables ORed together, ANDed together, or in any combination of AND and OR operations, changes the result of the term or expression from a """"1"""" to a """"0"""" or from a """"0"""" to a """"1"""". A line covering a single letter, term, or group of terms means that the logical result of the letter or expression under the line is inverted, or in the opposite binary state. The expression A+B reads A or B. If either A or B is at a """"1"""" state, then their combined result, or in this case their ORed result, is a """"1"""". If a line is drawn over the top of the expression A+B, the line inverts the result, changing it from a """"1"""" to a """"0"""". In the process of using Boolean algebra and DeMorgan's Theorem, more than one line often appears over terms. If there are an even number of lines over a term, the lines cancel. If there are an odd number of lines over a term, they can be reduced to a single line. An AND or an OR expression can be taken in any order without changing the result of the expression. Imagine a basic AND gate or an OR gate. Either input can be used to bring in the data without affecting the results. The terms or variables of a logic expression can be presented and acted upon in any order without changing the meaning of the expression. Variables can be factored out of Boolean algebra terms and expressions in much the same way as in classical algebra. There are 12 basic rules of Boolean algebra. They are logical in nature and follow the rules of gates covered in previous modules. The word …"""	and gate;boolean algebra;de morgan's laws;digital electronics;formal concept analysis;logic gate;or gate	Léonard Kwuida	2004		10.1007/978-3-540-24651-0_14	distributive property;distributive lattice;filtered algebra;boolean algebra;combinatorics;discrete mathematics;boolean algebra;boolean domain;ideal;boolean expression;congruence lattice problem;stone's representation theorem for boolean algebras;boolean algebras canonically defined;relation algebra;mathematics;complemented lattice;map of lattices;complete boolean algebra;concept class;allen's interval algebra;two-element boolean algebra;free boolean algebra;parity function;algebra	Logic	19.784219402518996	43.90767545945936	984
2867067d182774fbb547cd2f943d83287eccfcdc	methods for reducing events in sequential circuit fault simulation	sequential circuits circuit faults circuit simulation discrete event simulation circuit testing computational modeling fault diagnosis electrical fault detection fault detection logic circuits;fault simulation;sequential circuits;sequential circuits circuit analysis computing fault location logic testing;logic testing;circuit analysis computing;proofs synchronous sequential circuit fault simulator sequential circuit fault simulation reducing events fault free circuit;fault location	Methods for Reducing Events in Sequential Circuit Fault Simulation Elizabeth M. Rudnick Thomas M. Niermann Janak H. Patel Center for Reliable and Sunrise Test Systems Inc. Center for Reliable and High-Performance Computing Sunnyvale, CA 94086 High-Performance Computing University of Illinois University of Illinois Urbana, IL 61801 Urbana, IL 61801 Abstract Methods are investigated for reducing events in sequential circuit fault simulation by reducing the number of faults simulated for each test vector. Inactive faults, which are guaranteed to have no e ect on the output or the next state, are identi ed using local information from the fault-free circuit in one technique. In a second technique, the StarAlgorithm is extended to handle sequential circuits and provide global information about inactive faults, based on the fault-free circuit state. Both techniques are integrated into the PROOFS synchronous sequential circuit fault simulator. An average 28% reduction in faulty circuit gate evaluations is obtained for the 19 ISCAS-89 benchmark circuits studied using the rst technique, and 33% reduction for the two techniques combined. Execution times decrease by an average of 17% when the rst technique is used. For the largest circuits, further improvements in execution time are made when the Star-Algorithm is included.	algorithm;benchmark (computing);fault simulator;run time (program lifecycle phase);sequential logic;simulation;test vector	Elizabeth M. Rudnick;Thomas M. Niermann;Janak H. Patel	1991		10.1109/ICCAD.1991.185328	embedded system;electronic engineering;real-time computing;fault coverage;asynchronous circuit;fault indicator;computer science;stuck-at fault;automatic test pattern generation;fault model;sequential logic;circuit extraction;synchronous circuit	EDA	22.658630409969	50.77342539509472	985
ade9d0f83110c62b9517a6549d8fb7e0805b1479	analyzing memory access on cpu-gpgpu shared llc architecture	shared memory systems cache storage graphics processing units parallel processing;benchmark testing graphics processing units bandwidth memory management ports computers parallel processing;gpgpu l2 cache memory access analysis cpu gpgpu co simulator gem5 gpgpu sim memory contention last level caches shared llc structure llc buffer occupation llc cache bank llc parallelism;memory shared llc gpgpu cpu;gpgpu;shared llc;cpu;memory	The data exchange between GPGPUs and CPUs are becoming more and more important nowadays. One trend in industry to alleviate the long latency is to integrate CPUs and GPGPUs on a single chip. In this paper, we analyze the reference interactions between CPU and GPGPU applications with a CPU-GPGPU co-simulator that integrates the gem5 and gpgpu-sim together. Since the memory controllers are shared among all cores, we observe severe memory contention between them. The CPU applications suffer a 1.26x slowdown and 64.79% blocked time in main memory when they run parallels with GPGPU applications. To alleviate the contention and provide more memory band-width, shared last level caches (LLCs) are commonly employed in such systems. We test a banked shared LLC structure that implanted into the co-simulator. We show that a simple shared LLC contributes mostly to the GPGPU (2.13x to running alone and 1.7x to running in parallel), rather than CPU. With the help of LLC, the memory requests issued to main memory is reduced to 30.74%, the blocked time is reduced to 49.64%, which provides more memory bandwidth. The latency-sensitive CPU applications are suffered as the LLC buffer occupation is very high when they run with GPGPU in parallel. Besides, as the number of LLC cache bank grows, we reveal that CPU achieves higher speedup than GPGPUs by increasing LLC parallelism. Finally, we also discuss the impact of GPGPU L2 cache. And we find that fewer GPGPU L2 cache banks will lower the performance as they limits the parallelism of GPGPU. The observations and inferences in this paper may serve as a reference guide to future CPU-GPGPU shared LLC design.	cpu cache;central processing unit;computer data storage;elegant degradation;general-purpose computing on graphics processing units;interaction;lunar lander challenge;memory bandwidth;parallel computing;parallels desktop for mac;simulation;speedup	Jianliang Ma;Licheng Yu;Tianzhou Chen;Minghui Wu	2015	2015 14th International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2015.18	uniform memory access;distributed shared memory;shared memory;parallel computing;real-time computing;computer hardware;computer science;operating system;central processing unit;memory;cache pollution;general-purpose computing on graphics processing units	Arch	-9.20644152669928	52.660269413480414	986
8e62a1723b014224710241c798364e28b5e2e371	an empirical test of type-indeterminacy in the prisoner's dilemma	quantum probability;prisoner s dilemma;type indeterminate;cheap talk	In this paper, we test the type indeterminacy hypothesis by analyzing an experiment that examines the stability of preferences in a Prisoner Dilemma with respect to decisions made in a context that is both payoff and informationally unrelated to that Prisoner Dilemma. More precisely we carried out an experiment in which participants were permitted to make promises to cooperate to agents they saw, followed by playing a Prisoner’s Dilemma game with another, independent agent. It was found that, after making a promise to the first agent, participants exhibited higher rates of cooperation with other agents. We show that a classical model does not account for this effect, while a type indeterminacy model which uses elements of the formalism of quantum mechanics is able to capture the observed effects reasonably well.	formal system;indeterminacy in concurrent computation;nondeterministic algorithm;prisoner's dilemma;quantum mechanics	Peter D. Kvam;Jerome R. Busemeyer;Ariane Lambert-Mogiliansky	2013		10.1007/978-3-642-54943-4_19	psychology;superrationality;traveler's dilemma;artificial intelligence;social psychology;welfare economics	AI	-15.012042585306055	-13.806927257731667	987
3454e7bc68713f03000e6eeaab67d6401ba24531	transferring attributes for person re-identification	visual databases cameras computer vision identification image classification learning artificial intelligence;measurement;training;semantics;accuracy measurement correlation semantics training training data cameras;training data;accuracy;whitening transformation person reidentification computer vision task handcrafted image features camera angle reidentification methods attribute classifiers metric learning approach attribute labels target dataset;correlation;cameras	Person re-identification is an important computer vision task with many applications in areas such as surveillance or multimedia. Approaches relying on handcrafted image features struggle with many factors (e.g. lighting, camera angle) which lead to a large variety in visual appearance for the same individual. Features based on semantic attributes of a person's appearance can help with some of these challenges. In this work we describe an approach that integrates such attributes with existing re-identification methods based on low-level features. We start by training a set of attribute classifiers and present a metric learning approach that uses these attributes for person re-identification. The method is then applied to a second dataset without attributes labels by transferring the attributes classifiers. Performance on the target dataset can be increased by applying a whitening transformation prior to transfer. We present experiments on publicly available datasets and demonstrate the performance improvement gained by this added re-identification cue.	attribute grammar;computer vision;decorrelation;experiment;high- and low-level;whitening transformation	Arne Schumann;Rainer Stiefelhagen	2015	2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2015.7301803	computer vision;training set;computer science;machine learning;pattern recognition;semantics;accuracy and precision;correlation;measurement;statistics	Vision	30.452081443917688	-50.235552629050645	988
85e2c176dd730ab6b4082a70f64b5c075b64c49b	new general secret sharing scheme based on unauthorized subsets: improvement of information rates for specified participants	k n threshold scheme;general access structure;secret sharing scheme		authorization;secret sharing	Kouya Tochikubo	2016	JIP	10.2197/ipsjjip.24.772	theoretical computer science;shamir's secret sharing;homomorphic secret sharing;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;computer security;verifiable secret sharing	Crypto	-40.72082378610586	72.57460868507302	989
cf5ab9c21b8a7f3bcd8203c7c3b96f7415998a3c	a soft sensor model based on rough set theory and its application in estimation of oxygen concentration	modelizacion;sensor model;rough set theory;modelisation;captador medida;measurement sensor;capteur mesure;theorie ensemble approximatif;modeling	Modern societies have reached a point where everyday life relies heavily on the reliable operation and intelligent management of critical infrastructures, such as electric power systems, telecommunication networks, water distribution networks, transportation systems, etc. Designing, monitoring and controlling such systems is becoming increasingly more challenging as their size, complexity and interactions are steadily growing. Moreover, these critical infrastructures are susceptible to natural disasters, frequent failures, as well as malicious attacks. There is an urgent need to develop a common system-theoretic framework for modeling the behavior of critical infrastructure systems and for designing algorithms for intelligent monitoring, control and security of such systems. The goal of this presentation is to motivate the need for fault diagnosis and security of critical infrastructure systems and to provide a methodology for detecting, isolating and accommodating both abrupt and incipient faults in a class of complex nonlinear dynamic systems. A detection and approximation estimator based on computational intelligence techniques is used for online health monitoring. Once a fault is detected, a bank of isolation estimators is activated for the purpose of fault isolation. A key design issue is the adaptive residual threshold associated with each isolation estimator. Various adaptive approximation techniques and learning algorithms will be presented and illustrated, and directions for future research will be discussed. Bibliography Marios M. Polycarpou is a Professor of Electrical and Computer Engineering and Director of the KIOS Research Center for Intelligent Systems and Networks at the University of Cyprus. He received the B.A. degree in Computer Science and the B.Sc. degree in Electrical Engineering both from Rice University, Houston, TX, USA in 1987, and the M.S. and Ph.D. degrees in Electrical Engineering from the University of Southern California, Los Angeles, CA, in 1989 and 1992 respectively. In 1992, he joined the University of Cincinnati, Ohio, USA, where he reached the rank of Professor of Electrical and Computer Engineering and Computer Science. In 2001, he was the first faculty and the founding department Chair of the newly established of Electrical and Computer Engineering Dept at the University of Cyprus. His teaching and research interests are in intelligent systems and control, adaptive and cooperative control systems, computational	algorithm;approximation;artificial intelligence;complexity;computation;computational intelligence;computer engineering;computer science;consensus dynamics;control system;dynamical system;electrical engineering;fault detection and isolation;ibm power systems;interaction;machine learning;nonlinear system;rough set;sensor;set theory	Xingsheng Gu;Dazhong Sun	2005		10.1007/11539506_160	systems modeling;rough set;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics	AI	13.483293035496906	-15.503428072559867	990
5da3e3d440bd2b1bf9c383400eb1b2150e1fee2a	quality of service and routing in wireless mesh networks. (qualité de service et routage dans les réseaux maillés sans fil)		"""Wireless mesh networks are a promising technology for providing last-mile broadband wireless Internet to a large number of users spread across large geographical regions. Due to their peculiar limitations and the increasing demand of users for high performance (high throughput, low delays etc), mesh networks have attracted the attention of researchers world-wide. Towards this goal, this dissertation contributes in several areas of Routing and Quality-ofService provisioning in IEEE 802.11-based wireless mesh networks. Chapter 1 introduces wireless mesh networks and their architectural and functional components. The difference between mesh networks and traditional wireless multi-hop networks is emphasized. A comprehensive background study of routing and QoS solutions for multi-hop wireless networks is presented with an emphasis on mesh-specific solutions. Chapter 2 presents our first contribution : route selection in wireless mesh networks. A primary research problem in mesh networks is to find the """"best"""" available route between a pair of mesh routers. Recent research shows that selecting the shortest path is a poor decision as it does not take into account other factors pertaining to link quality. We propose an efficient routing metric Expected Link Performance metric (ELP) which considers a number of factors including link loss ratio, link interference, and link capacity to find the ”best” route between a pair of mesh routers. Performance evaluation of ELP is carried out against contemporary routing metrics. A part of metric is also evaluated on a mesh testbed. An extension of the metric is proposed for the special case of mesh traffic directed at gateways. A gateway discovery protocol is also proposed which integrates the extended metric and performance evaluation is carried out against traditional gateway and gateway-route selection schemes. Chapter 3 presents our second contribution : route maintenance in mesh networks. After route selection, the next research problem that we consider is the maintenance of that route. The route maintenance mechanism of on-demand routing protocols in 802.11-based mesh networks is inaccurate and results in frequent route breakages which cause route instability and performance degradation. The chapter discusses the problem of route stability for ondemand protocols in detail. The Efficient Route Maintenance (ERM) scheme is proposed which improves route maintenance for on-demand routing protocols in wireless mesh networks by using cross-layering to get information from lower layers. The ERM scheme is then extended for multi-radio multi-channel mesh scenarios. ERM is evaluated against classical route maintenance mechanism of the on-demand routing protocols. Chapter 4 presents the final contribution of the thesis : QoS framework for bandwidth guarantees in multi-radio multi-channel mesh networks. Providing QoS guarantees is particularly important for users who use the mesh network to access the Internet. The framework provides flow-specific reservation-based bandwidth guarantees in mesh networks. Link diversity (the availability of multiple redundant links between neighbors) is exploited for proposing a novel QoS provisioning solution which can provide better load-balancing in the network and provide a higher flow admittance ratio."""	catastrophic interference;elegant degradation;instability;interference (communication);internet;last mile;load balancing (computing);mesh networking;metrics (networking);performance evaluation;provisioning;quality of service;routing;sans institute;shortest path problem;testbed;throughput;wireless mesh network	Usman Ashraf	2010				Mobile	-2.748867455150029	85.41117825254365	991
2bfa571615b0858c6b1006ac0f9519bc57f542b8	reducing device yield fallout at wafer level test with electrohydrodynamic (ehd) cleaning	contact resistance;surface contamination;performance evaluation;integrated circuit yield;electrohydrodynamic cleaning;device yield;integrated circuit production;tungsten carbide;testing;surface cleaning;testing probes cleaning abrasives contact resistance surface contamination surface resistance electrohydrodynamics performance evaluation tungsten;probes;electrohydrodynamics integrated circuit yield integrated circuit testing contact resistance surface cleaning surface contamination;abrasives;surface resistance;integrated circuit production device yield wafer level testing electrohydrodynamic cleaning contact resistance surface contamination probe card abrasive cleaning;wafer level testing;electrohydrodynamics;integrated circuit testing;probe card;tungsten;abrasive cleaning;cleaning	AJSTRACT Unstable contact resistance (C.& during wafer test can signipcantly affect device yield, the need for reprobe, and equipment uptime. Abrasive cleaning during off-line probe card repair and maintenance is effective for reducing C R ~ s and removing surface contaminants from probe tips. This type of cleaning, however, shortens probe card life and can compromise probe card planarity and alignment. Off-line electrohydrodynamic (EHD) cleaning uses charged molecular microclusters to reduce Cms without damaging probe card materials or misaligning the probe tips. An engineering evaluation was performed using two groups of probe cards that were regularly cleaned off-line with a tungsten carbide abrasive plate and the EHD cleaning tool, respectively. For the evaluation, a very high volume device sensitive to C ~ S stability and variation was identiped and probe card performance was monitored for several weeks across multiple lots. Probe card performance after abrasive cleaning did not deviate significantly from the historical median. However, after EHD cleaning an increase of device yield was consistently observed. Based on the results, the EHD cleaning technique was found to be an effective addition to off-line abrasive cleaning that might 8 have additional beneJits to production.	control theory;fallout;online and offline;photographic plate;planar graph;plasma cleaning;probe card;sputter cleaning;uptime;wafer (electronics)	Jerry J. Broz;James C. Andersen;Reynaldo M. Rincon	2000		10.1109/TEST.2000.894240	sheet resistance;electronic engineering;electrohydrodynamics;contact resistance;probe card;software testing;tungsten;forensic engineering	HCI	87.90420442084465	-13.82873162195559	992
ce2a9c7c421552abeb2ea7774c29648a91e76c75	functionalized electromagnetic actuation method for aggregated nanoparticles steering		Despite the promising results in magnetic nanoparticles (MNPs) based targeted drug delivery (TDD), the aggregation of the magnetic nanoparticles deteriorates targeting performance. This paper aims to introduce a magnetic actuation function for aggregated nanoparticles steering in vascular network. To improve the drug delivery performance, first the governing dynamics has been introduced, next the modified field function (MFF) concept has been proposed and finally a computational platform for a Y-shape channel has been used to simulate the particles steering performance. The results showed an acceptable agreement with the experimental results. The proposed actuation method enables us to more accurately steer aggregated nanoparticles and improves targeting performance.	actuation dosing unit;blood vessel tissue;computation;drug delivery systems;experiment;nash equilibrium;numerous;simulation;subscriber identity module;test-driven development;video-in video-out;cellular targeting	Ali Kafash Hoshiar;T. A. Le;Faiz Ul Amin;Myeong Ok Kim;Jungwon Yoon	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8036966	electronic engineering;targeted drug delivery;computer science;magnetic nanoparticles;drug delivery;electromagnetic phenomena;nanoparticle;nanotechnology;magnetite nanoparticles	Robotics	97.34660792889024	-21.640504923976454	993
7d97ceadbd382b72bd8bbbff4599f7a011b79dde	measuring robustness of community structure in complex networks	cs si;期刊论文;physics soc ph	The theory of community structure is a powerful tool for real networks, which can simplify their topological and functional analysis considerably. However, since community detection methods have random factors and real social networks obtained from complex systems always contain error edges, evaluating the robustness of community structure is an urgent and important task. In this letter, we employ the critical threshold of resolution parameter in Hamiltonian function, γC , to measure the robustness of a network. According to spectral theory, a rigorous proof shows that the index we proposed is inversely proportional to robustness of community structure. Furthermore, by utilizing the co-evolution model, we provides a new efficient method for computing the value of γC . The research can be applied to broad clustering problems in network analysis and data mining due to its solid mathematical basis and experimental effects.	cluster analysis;complex network;complex systems;computation;data mining;random graph;social network	Hui-Jia Li;Hao Wang;Luonan Chen	2015	CoRR	10.1209/0295-5075/108/68009	network analysis;machine learning;quantum mechanics;complex network;robustness (computer science);social network;physics;cluster analysis;functional analysis;artificial intelligence;community structure;spectral theory	ML	-15.05371328830974	-40.50376150649546	994
a926fae08db5c28c5f20644bf4bc2f1eb4022c4a	a multimodal approach to relevance and pertinence of documents	document classification;bag of words	Automated document classification process extracts information with a systematical analysis of the content of documents. This is an active research field of growing importance due to the large amount of electronic documents produced in the world wide web and made readily available thanks to diffused technologies including mobile ones. Several application areas benefit from automated document classification, including document archiving, invoice processing in business environments, press releases and search engines. Current tools classify or “tag” either text or images separately. In this paper we show how, by linking image and text-based contents together, a technology improves fundamental document management tasks like retrieving information from a database or automatically routing documents. We present a formal definition of pertinence and relevance concepts, that apply to those documents types we name “multimodal”. These are based on a model of conceptual spaces we believe compulsory for document investigation while using joint information sources coming from text and images forming complex documents.	multimodal interaction;relevance	Matteo Cristani;Claudio Tomazzoli	2016		10.1007/978-3-319-42007-3_14	natural language processing;speech recognition;computer science;bag-of-words model;information retrieval	NLP	-31.05378549034215	-57.59691746226177	995
96605ec29e32ac0f9e4d15dc3503509ad7984fca	ordinary innovations of older adults: an interview study to understand interest development in aging		Building and maintaining personal interests are important for older adults to promote an active and healthy lifestyle. In this paper, we discuss older adults' innovative ways of developing their interests and the related influence on their health and well-being. We conducted an interview study with 20 healthy older adults to investigate the practices around interest development and the impact of those experiences on their quality of life. We found out that older adults pursued unexpectedly creative ways to develop their personal interests, which not only contributed to emotional well-being, but also significantly influenced the liveliness and the sustainability of their local community which in turn sustained active aging.	experience	Xiying Wang;Tiffany Knearem;John M. Carroll	2018		10.1145/3272973.3274068	knowledge management;gerontology;sustainability;local community;computer science;quality of life	HCI	-59.821191848599334	-53.31792968257307	996
04b9d99e6b18b69eced71c0a7572af7a980f7d50	agent-based knowledge logistics for coalition operations: main technologies and a case study	research prototype agent based knowledge logistics multiagent system coalition operation internet business internet government internet health care openstandard based information global information environment ksnet approach knowledge sharing binni scenario knowledge exchange;agent based;logistics computer aided software engineering ontologies crisis management intelligent agent technology management informatics automation prototypes hospitals;real time;knowledge management;agent communication;medical computing;multi agent systems;medical computing multi agent systems knowledge management health care;knowledge sharing;knowledge exchange;open standard;health care	"""E-husiness, e-govemment and e-health care applications require cooperation and open standard-based infomatiodknowledge exchange between all the participants of the global information environment in realtime. As a result a new scientific direction of knowledge logistics has emerged. The paper describes a developed KSNet-approach addressing the knowledge logistics problem. An ontology-driven methodology for knowledge sharing and reuse suggested by the approach is described first. The paper also describes a multi agent community implementation designed as a part of research prototype of the knowledge logistics system """"KSNet"""". Results of the developed multi agent community application to coalitionbased operations support are presented via a portable hospital configuration case study based on the Binni scenario."""	logistics;prototype;web ontology language	Alexander V. Smirnov;Mikhail Pashkin;Nikolay Shilov;Tatiana Levashova	2003		10.1109/IRI.2003.1251473	knowledge base;open standard;computer science;knowledge management;artificial intelligence;operating system;multi-agent system;open knowledge base connectivity;management science;personal knowledge management;domain knowledge;health care	AI	-50.694292059743276	10.612516204557066	997
ca255b0b778c3c514201031a89358ba0d6bd792f	a high dynamic range cmos variable gain filter for adsl	noise figure;transconductors;circuit noise;harmonic distortion;transconductor;transconductance gain;continuous variable;gain;filters;transconductance;variable gain amplifier;variable gain filter;input referred noise;circuit topology;digital subscriber lines;frequency response;low noise;1 1 mhz;cmos analogue integrated circuits;butterworth filters;dynamic range gain filters transconductors circuit noise voltage resistors transconductance noise figure circuit topology;thd;voltage;dynamic range;continuous variable gain current to current converter;resistors;circuit tuning;butterworth filters circuit tuning frequency response harmonic distortion cmos analogue integrated circuits active filters digital subscriber lines low pass filters;low pass filters;0 to 31 db;high dynamic range;lowpass butterworth gm c filter;active filters;adsl;1 to 3 v;cmos;0 25 micron dynamic range cmos variable gain filter adsl lowpass butterworth gm c filter variable gain amplifier transconductor transconductance gain continuous variable gain current to current converter thd input referred noise 1 1 mhz 1 to 3 v 0 to 31 db;0 25 micron	A 3.6 order, 1.1 MHq lowpass butterworth Gm-c filter has been combined with a low noise Variable Gain Amplifier to form the Variable Gain Filter (VGF). I n this VGF an improved transconductor is used and B new method is used IO adjust the transeonduelance gain for tuning application. A continuous variable gain current-to-current converter is used to tune the transeonduetor value. The THD of the VGF i s -85 dB for I Vpp. input signal and -50 dB for 3 V,,a input signal. VGF gain can be varied from 0 to 31 dB with 1 dB wristion steps. Input referred noise is 20 nV/\IHz for maximum gain. All the circuits arc designed based on a 0.25 pm CMOS process technology.	asymmetric digital subscriber line;butterworth filter;cmos;decibel;high dynamic range;low-pass filter;total harmonic distortion;variable-gain amplifier	Saeid Mehrmanesh;Seyed Mojtaba Atarodi	2002		10.1109/ISCAS.2002.1010439	control engineering;computer vision;electronic engineering;telecommunications;computer science;engineering;electrical engineering;control theory;total harmonic distortion	EDA	62.69704775460124	50.21561428359208	998
2c10d2eab970f1b9bd3594635d730abf7af764e9	distance encoding in vibro-tactile guidance cues	haptic interfaces;human computer interaction;distance encoding;distance information;vibro-tactile guidance cues;walking accuracy;walking speed;distance encoding;space awareness;tactograms;vibro-tactile guidance cues	To navigate in unfamiliar places is, for obvious reasons, particularly difficult for blind people. In this research we used vibro-tactile guidance cues with the goal of allowing users to reach their destination with the most efficiency. Our hypothesis was that adding distance information should improve walking speed and accuracy, however, similar results were obtained with and without distance information.		Markus Straub;Andreas Riener;Alois Ferscha	2009	2009 6th Annual International Mobile and Ubiquitous Systems: Networking & Services, MobiQuitous		preferred walking speed;computer vision;navigation;simulation;computer science;cognitive neuroscience of visual object recognition;vibration;encoding	HCI	-44.49294381689042	-45.836424432134045	999
dc25a0d88282e1e65d0ba33d7f517880b5b3e261	a new approach on communications architectures for intelligent transportation systems		Abstract A Vehicular Adhoc Network (VANET) is a generic communications conceptualization that can be applied to Intelligent Transportation Systems (ITS) and its main goal is to allow exchange of information between moving vehicles, fixed infrastructures, pedestrians with personal devices, and all other electronic devices able to connect to a VANET environment. Information exchange between different stakeholders brings a relevant potential to the development of applications to help users in different areas such as traffic safety and efficiency, infotainment and personal comfort. However, due to the expected heterogeneity (different processing power and storage capabilities, communications technologies and mobility patterns) and large scale on the number of devices involved, application interoperability in VANET contexts can be a challenging problem. Non-agnostic standard communications architectures for ITS systems have some deploying limitations and lack important specific implementation details. This paper presents an agnostic VANET architecture (it permits the use of several communication technologies in an open and modular framework), which is an adaption of present standards approach, to be deployed on ITS systems as a mean to overcome their main limitations.		Susana Sousa;Alexandre Santos;António Costa;Bruno Dias;Bruno Ribeiro;Fábio Gonçalves;Joaquim Macedo;Maria João Nicolau;Óscar Gama	2017		10.1016/j.procs.2017.06.101	electronics;vehicular ad hoc network;information exchange;interoperability;vehicular communication systems;computer network;modular design;computer science;intelligent transportation system;wireless ad hoc network	Robotics	-7.726300937592216	74.96630073337836	1000
